[
  "A vector is a mathematical object that has both magnitude (length) and direction.",
  "The magnitude of a vector is its length, denoted by |v|.",
  "The direction of a vector is its orientation in space, denoted by v\u0302.",
  "The dot product (or scalar product) of two vectors v1 and v2 is a scalar value that represents the amount of \"similarity\" between the two vectors: v1 \u00b7 v2 = |v1||v2|cos(\u03b8), where \u03b8 is the angle between the two vectors.",
  "The cross product of two vectors v1 and v2 is a vector that is perpendicular to both v1 and v2: v1 \u00d7 v2 = ||v1|| ||v2|| sin(\u03b8) n\u0302, where n\u0302 is a unit vector perpendicular to both v1 and v2.",
  "The magnitude of the cross product of two vectors v1 and v2 is the area of the parallelogram spanned by the two vectors.",
  "The dot product of a vector v and the zero vector 0 is 0, and the cross product of v and 0 is the zero vector.",
  "A matrix is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns.",
  "The order of a matrix is its number of rows and columns, denoted by m x n.",
  "The transpose of a matrix A, denoted by A^T, is obtained by interchanging its rows and columns.",
  "The determinant of a matrix A is a scalar value that can be used to determine the solvability of a system of linear equations.",
  "The inverse of a matrix A is a matrix that, when multiplied by A, gives the identity matrix I.",
  "The eigenvalues and eigenvectors of a matrix A are scalar and vector values that satisfy the equation Ax = \u03bbx, where \u03bb is the eigenvalue and x is the eigenvector.",
  "A linear transformation is a function that maps one vector space to another while preserving the operations of vector addition and scalar multiplication.",
  "A linear transformation can be represented by a matrix, and its matrix representation is unique.",
  "The matrix representation of a linear transformation T can be obtained by applying T to the standard basis vectors of the domain vector space.",
  "A basis of a vector space is a set of linearly independent vectors that span the entire space.",
  "The dimension of a vector space is the number of vectors in its basis.",
  "A basis of a vector space can be obtained by finding the linearly independent vectors that span the space.",
  "Two vectors v1 and v2 are orthogonal if their dot product is 0: v1 \u00b7 v2 = 0.",
  "A set of vectors {v1, v2, ..., vn} is orthogonal if the dot product of any two distinct vectors is 0.",
  "The Gram-Schmidt process is a method for obtaining an orthogonal basis of a vector space from a given basis.",
  "Eigenvalue decomposition is a method for representing a matrix as the product of a diagonal matrix and a matrix of eigenvectors.",
  "The eigenvalues of a matrix A are the values \u03bb that satisfy the equation Ax = \u03bbx.",
  "The eigenvectors of a matrix A are the vectors x that satisfy the equation Ax = \u03bbx.",
  "Singular value decomposition (SVD) is a method for representing a matrix as the product of three matrices: U, \u03a3, and V^T.",
  "The SVD of a matrix A is a factorization of the form A = U \u03a3 V^T, where U and V are orthogonal matrices, and \u03a3 is a diagonal matrix of singular values.",
  "A set of vectors {v1, v2, ..., vn} is linearly independent if the only way to express the zero vector as a linear combination of the vectors is with all coefficients equal to 0.",
  "The span of a set of vectors {v1, v2, ..., vn} is the set of all linear combinations of the vectors.",
  "A linear system is a system of linear equations, such as Ax = b, where A is a matrix, x is a vector, and b is a vector.",
  "The solution to a linear system Ax = b can be obtained by multiplying both sides by the inverse of A, if it exists.",
  "A change of basis is a linear transformation that maps one basis of a vector space to another.",
  "The matrix representation of a change of basis can be obtained by applying the transformation to the standard basis vectors of the domain vector space.",
  "A set of vectors {v1, v2, ..., vn} is linearly dependent if there exists a non-trivial linear combination of the vectors that equals the zero vector.",
  "A change of variables is a linear transformation that maps one coordinate system to another.",
  "The matrix representation of a change of variables can be obtained by applying the transformation to the standard basis vectors of the domain coordinate system.",
  "A set of vectors {",
  "Quadratic forms and inner product spaces",
  "Orthogonality and orthonormality",
  "Orthogonal projections and projections onto subspaces",
  "Null spaces and left null spaces of matrices",
  "Row and column spaces of matrices",
  "Linear independence and dependence of vectors and matrices",
  "Rank and nullity of matrices",
  "Matrix multiplication and its properties",
  "Vector spaces with different bases (e.g., finite-dimensional and infinite-dimensional)",
  "Vector spaces with different inner product structures (e.g., Euclidean and Minkowski spaces)",
  "Vector spaces with different topological properties (e.g., compact and separable spaces)",
  "Linear systems of equations with non-square matrices",
  "Systems of linear equations with non-rectangular coefficient matrices",
  "Matrix factorizations (e.g., LU, Cholesky, QR)",
  "Eigenvector and eigenvalue theorems for non-square matrices",
  "Singular value decomposition for non-square matrices",
  "Minimal polynomial of matrices",
  "Cayley-Hamilton theorem",
  "Linear systems with non-rectangular coefficient matrices and non-square matrices"
]