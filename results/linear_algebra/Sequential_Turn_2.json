[
  "A vector is a mathematical object with both magnitude (length) and direction.",
  "The magnitude (or length) of a vector is denoted by |v| or ||v||.",
  "The dot product of two vectors a and b is denoted by a \u00b7 b and is calculated as the sum of the products of their corresponding components.",
  "The cross product of two vectors a and b is denoted by a \u00d7 b and is calculated as the area of the parallelogram formed by the two vectors.",
  "The magnitude of the cross product of two vectors a and b is denoted by |a \u00d7 b| and is calculated as the area of the parallelogram formed by the two vectors.",
  "The unit vector in the direction of a vector v is denoted by v/|v| and has a magnitude of 1.",
  "A linear combination of vectors v1, v2, ..., vn is a vector that can be expressed as a linear combination of the individual vectors: c1v1 + c2v2 + ... + cnvn.",
  "The span of a set of vectors v1, v2, ..., vn is the set of all linear combinations of the individual vectors.",
  "A set of vectors v1, v2, ..., vn is said to be linearly independent if the only linear combination that equals the zero vector is the trivial solution (i.e., all coefficients are zero).",
  "A set of vectors v1, v2, ..., vn is said to be linearly dependent if there exists a non-trivial linear combination that equals the zero vector.",
  "A matrix is a rectangular array of numbers arranged in rows and columns.",
  "The elements of a matrix are denoted by aij, where i is the row index and j is the column index.",
  "The transpose of a matrix A is denoted by A^T and is obtained by interchanging the rows and columns of A.",
  "The determinant of a matrix A is denoted by det(A) and is calculated using various methods such as expansion by minors or cofactor expansion.",
  "The inverse of a matrix A is denoted by A^-1 and is calculated using various methods such as Gauss-Jordan elimination or LU decomposition.",
  "A vector space is a set of vectors that is closed under addition and scalar multiplication.",
  "A linear transformation T: V \u2192 W is a function that maps vectors from a vector space V to a vector space W in a linear fashion.",
  "A basis for a vector space V is a set of linearly independent vectors that span V.",
  "The dimension of a vector space V is the number of vectors in a basis for V.",
  "A linear transformation T: V \u2192 W has a null space (or kernel) if there exists a vector v in V such that T(v) = 0.",
  "A linear transformation T: V \u2192 W has an image (or range) if there exists a vector w in W such that w = T(v) for some vector v in V.",
  "An eigenvector of a linear transformation T: V \u2192 W is a non-zero vector v such that T(v) = \u03bbv for some scalar \u03bb (the eigenvalue).",
  "The eigenvalues of a linear transformation T: V \u2192 W are the scalars \u03bb such that T(v) = \u03bbv for some non-zero vector v.",
  "The eigenspace of a linear transformation T: V \u2192 W corresponding to an eigenvalue \u03bb is the set of all eigenvectors of T that have the same eigenvalue \u03bb.",
  "The characteristic polynomial of a linear transformation T: V \u2192 W is a polynomial that encodes information about the eigenvalues of T.",
  "Two vectors a and b are said to be orthogonal if their dot product is zero: a \u00b7 b = 0.",
  "The orthogonal projection of a vector a onto a vector b is the vector that is in the direction of b and has the same magnitude as a \u00b7 b / |b|.",
  "The orthogonal projection of a vector a onto a set of vectors S is the vector that is in the span of S and has the same magnitude as a \u00b7 v / |v| for all v in S.",
  "The SVD of a matrix A is a factorization of the form A = U \u03a3 V^T, where U and V are orthogonal matrices and \u03a3 is a diagonal matrix.",
  "The singular values of a matrix A are the square roots of the eigenvalues of A^T A.",
  "The SVD of a matrix A can be used to diagonalize A and to compute the inverse of A.",
  "The determinant of a matrix A is a scalar that encodes information about the invertibility of A.",
  "A matrix A is invertible if and only if its determinant is non-zero.",
  "The inverse of a matrix A is a matrix that, when multiplied by A, gives the identity matrix.",
  "The determinant of the inverse of a matrix A is the reciprocal of the determinant of A.",
  "A set of vectors v1, v2, ..., vn is said to be linearly independent if the only linear combination that equals the zero vector is the trivial solution.",
  "A linear transformation T: V \u2192 W can be represented by a matrix A if T(v) = Av for all v in V.",
  "The matrix representation of a linear transformation T: V \u2192 W is a matrix that encodes information about T.",
  "The determinant of a matrix A is a scalar that",
  "The sum of two vectors a and b is denoted by a + b and is calculated as the vector with components a1 + b1, a2 + b2, ..., an + bn.",
  "The difference of two vectors a and b is denoted by a - b and is calculated as the vector with components a1 - b1, a2 - b2, ..., an - bn.",
  "The scalar multiple of a vector a by a scalar c is denoted by ca and is calculated as the vector with components ca1, ca2, ..., cahn.",
  "The dot product of a vector a and a vector b is denoted by a \u00b7 b and is calculated as the sum of the products of their corresponding components.",
  "The cross product of two vectors a and b is denoted by a \u00d7 b and is calculated as the vector with components (a2b3 - a3b2, a3b1 - a1b3, a1b2 - a2b1).",
  "The null space of a linear transformation T: V \u2192 W is the set of all vectors v in V such that T(v) = 0.",
  "The inverse of a linear transformation T: V \u2192 W is a linear transformation that, when composed with T, gives the identity transformation.",
  "The kernel of a linear transformation T: V \u2192 W is the set of all vectors v in V such that T(v) = 0.",
  "The Gram-Schmidt process is a method for orthonormalizing a set of vectors.",
  "The adjugate (or classical adjugate) of a matrix A is a matrix that encodes information about the determinant of A.",
  "Linear algebra has numerous applications in physics, engineering, computer science, and other fields.",
  "Linear algebra is used to solve systems of linear equations, to find eigenvalues and eigenvectors, to compute determinants and inverses, and to perform other tasks.",
  "The Fundamental Theorem of Linear Algebra states that every linear transformation has an eigenvalue and an eigenvector.",
  "The Spectral Theorem for matrices states that a matrix can be diagonalized using an orthogonal matrix.",
  "The Linear Independence Theorem states that a set of vectors is linearly independent if and only if the only linear combination that equals the zero vector is the trivial solution.",
  "The Span Theorem states that the span of a set of vectors is the set of all linear combinations of the individual vectors.",
  "The notation for the dot product of two vectors a and b is a \u00b7 b.",
  "The notation for the magnitude of a vector a is |a|.",
  "The notation for the unit vector in the direction of a vector a is a/|a|.",
  "The notation for the orthogonal projection of a vector a onto a vector b is proj_b(a).",
  "The notation for the Gram-Schmidt process is GSP."
]