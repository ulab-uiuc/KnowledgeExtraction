[
  {
    "representative_text": "A vector is a mathematical object with both magnitude (length) and direction.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A vector is a mathematical object with both magnitude (length) and direction.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vectors can be represented graphically using arrows or in component form using coordinates.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vectors can be represented graphically using arrows or in component form using coordinates.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The magnitude of a vector is its length, and the direction is its orientation in space.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The magnitude of a vector is its length, and the direction is its orientation in space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The dot product (or scalar product) of two vectors is a measure of their similarity in direction and magnitude.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The dot product (or scalar product) of two vectors is a measure of their similarity in direction and magnitude.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The cross product of two vectors results in a vector that is perpendicular to both original vectors.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The cross product of two vectors results in a vector that is perpendicular to both original vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector addition: The sum of two or more vectors.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector addition: The sum of two or more vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector subtraction: The difference between two or more vectors.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector subtraction: The difference between two or more vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scalar multiplication: A vector multiplied by a scalar (a number).",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar multiplication: A vector multiplied by a scalar (a number).",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector magnitude: The length of a vector.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector magnitude: The length of a vector.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector unit vector: A vector with a magnitude of 1.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Vector unit vector: A vector with a magnitude of 1.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Unit vector: A unit vector is a vector with magnitude 1.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9309317736766658
      }
    ]
  },
  {
    "representative_text": "Linear independence: A set of vectors is linearly independent if none of the vectors can be expressed as a linear combination of the others.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear independence: A set of vectors is linearly independent if none of the vectors can be expressed as a linear combination of the others.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear independence: A set of vectors is linearly independent if none of the vectors can be expressed as a linear combination of the others.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0000000000000002
      }
    ]
  },
  {
    "representative_text": "Linear dependence: A set of vectors is linearly dependent if at least one vector can be expressed as a linear combination of the others.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear dependence: A set of vectors is linearly dependent if at least one vector can be expressed as a linear combination of the others.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span: The set of all linear combinations of a set of vectors.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Span: The set of all linear combinations of a set of vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Span: The span of a set of vectors is the set of all linear combinations of the vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9827643331872294
      }
    ]
  },
  {
    "representative_text": "Basis: A set of linearly independent vectors that span the same space as another set of vectors.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis: A set of linearly independent vectors that span the same space as another set of vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear transformation: A function that maps vectors from one space to another while preserving certain properties (e.g., preserving dot products).",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear transformation: A function that maps vectors from one space to another while preserving certain properties (e.g., preserving dot products).",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix representation: A matrix that represents a linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix representation: A matrix that represents a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Invertible linear transformation: A linear transformation that has an inverse.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Invertible linear transformation: A linear transformation that has an inverse.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant: A scalar value that can be used to determine the invertibility of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant: A scalar value that can be used to determine the invertibility of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inverse matrix: A matrix that, when multiplied by the original matrix, results in the identity matrix.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Inverse matrix: A matrix that, when multiplied by the original matrix, results in the identity matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Inverse Matrix: The concept of an inverse matrix, which is a matrix that, when multiplied by the original matrix, results in the identity matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9341473054606548
      },
      {
        "text": "Inverse matrix: The inverse of a matrix is a matrix that, when multiplied by the original matrix, results in the identity matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9772899357023186
      }
    ]
  },
  {
    "representative_text": "Eigenvalue: A scalar value that represents how much a linear transformation stretches or shrinks a vector.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvalue: A scalar value that represents how much a linear transformation stretches or shrinks a vector.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue: An eigenvalue is a scalar that represents how much a linear transformation stretches or shrinks a vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9878681707710605
      }
    ]
  },
  {
    "representative_text": "Eigenvector: A non-zero vector that, when multiplied by the linear transformation, results in the same vector scaled by the eigenvalue.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvector: A non-zero vector that, when multiplied by the linear transformation, results in the same vector scaled by the eigenvalue.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Eigenvector: An eigenvector is a non-zero vector that, when transformed by a linear transformation, results in a scaled version of itself.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9201053123344096
      }
    ]
  },
  {
    "representative_text": "Orthogonal vectors: Vectors that are perpendicular to each other.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal vectors: Vectors that are perpendicular to each other.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal projection: A method for finding the projection of one vector onto another.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal projection: A method for finding the projection of one vector onto another.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Symmetric matrix: A matrix that is equal to its transpose.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Symmetric matrix: A matrix that is equal to its transpose.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Symmetric Matrix: The concept of a symmetric matrix, which is a matrix that is equal to its transpose.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9447454229047079
      },
      {
        "text": "Symmetric matrix: A symmetric matrix is a square matrix that is equal to its own transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9363002912040207
      }
    ]
  },
  {
    "representative_text": "Skew-symmetric matrix: A matrix that is equal to the negative of its transpose.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Skew-symmetric matrix: A matrix that is equal to the negative of its transpose.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Skew-Symmetric Matrix: The concept of a skew-symmetric matrix, which is a matrix that is equal to the negative of its transpose.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9510528030872278
      },
      {
        "text": "Skew-symmetric matrix: A skew-symmetric matrix is a square matrix that is equal to the negative of its own transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9407765763128226
      }
    ]
  },
  {
    "representative_text": "Diagonalization: A process for transforming a matrix into a diagonal matrix using orthogonal transformations.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Diagonalization: A process for transforming a matrix into a diagonal matrix using orthogonal transformations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Change of basis: A process for transforming a vector from one basis to another.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Change of basis: A process for transforming a vector from one basis to another.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear system: A system of linear equations that can be solved using linear algebra techniques.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear system: A system of linear equations that can be solved using linear algebra techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Augmented matrix: A matrix that represents a linear system.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Augmented matrix: A matrix that represents a linear system.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian elimination: A method for solving linear systems using row operations.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian elimination: A method for solving linear systems using row operations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "LU decomposition: A factorization of a matrix into a product of lower and upper triangular matrices.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "LU decomposition: A factorization of a matrix into a product of lower and upper triangular matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "QR decomposition: A factorization of a matrix into a product of an orthogonal matrix and an upper triangular matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "QR decomposition: A factorization of a matrix into a product of an orthogonal matrix and an upper triangular matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue decomposition: A factorization of a matrix into a product of a diagonal matrix and an orthogonal matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue decomposition: A factorization of a matrix into a product of a diagonal matrix and an orthogonal matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "SVD decomposition: A factorization of a matrix into a product of three matrices: U, Σ, and V.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "SVD decomposition: A factorization of a matrix into a product of three matrices: U, Σ, and V.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product: A generalization of the dot product to higher dimensions, which can be used to define the length of a vector and the angle between two vectors.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner Product: A generalization of the dot product to higher dimensions, which can be used to define the length of a vector and the angle between two vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt Process: An algorithm for orthogonalizing a set of vectors, which can be used to find the orthogonal basis for a given set of vectors.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt Process: An algorithm for orthogonalizing a set of vectors, which can be used to find the orthogonal basis for a given set of vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Diagonalization: A process for diagonalizing a symmetric matrix using orthogonal transformations.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Diagonalization: A process for diagonalizing a symmetric matrix using orthogonal transformations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD): A factorization of a matrix into a product of three matrices: U, Σ, and V, which can be used to find the eigenvalues and eigenvectors of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD): A factorization of a matrix into a product of three matrices: U, Σ, and V, which can be used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Decomposition (SVD) and its Applications: A factorization of a matrix into a product of three matrices: U, Σ, and V, which can be used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9325770309575119
      },
      {
        "text": "The Singular Value Decomposition (SVD) of a Matrix: A factorization of a matrix into a product of three matrices: U, Σ, and V, which can be used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9768629373099802
      }
    ]
  },
  {
    "representative_text": "Inverse of a Matrix: A matrix that, when multiplied by the original matrix, results in the identity matrix. This can be found using various methods, including Gaussian elimination and LU decomposition.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inverse of a Matrix: A matrix that, when multiplied by the original matrix, results in the identity matrix. This can be found using various methods, including Gaussian elimination and LU decomposition.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Null Space: The set of all vectors that, when multiplied by a given matrix, result in the zero vector. This is a fundamental concept in linear algebra.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Null Space: The set of all vectors that, when multiplied by a given matrix, result in the zero vector. This is a fundamental concept in linear algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Row and Column Space: The row space and column space of a matrix are the sets of all linear combinations of the rows and columns of the matrix, respectively.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Row and Column Space: The row space and column space of a matrix are the sets of all linear combinations of the rows and columns of the matrix, respectively.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cofactor Matrix: A matrix that is formed by taking the determinant of each minor of a matrix and multiplying it by (-1)^(i+j), where i and j are the row and column indices.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cofactor Matrix: A matrix that is formed by taking the determinant of each minor of a matrix and multiplying it by (-1)^(i+j), where i and j are the row and column indices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Characteristic Polynomial: A polynomial that is formed by taking the determinant of (A - λI), where A is a matrix, λ is a scalar, and I is the identity matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Characteristic Polynomial: A polynomial that is formed by taking the determinant of (A - λI), where A is a matrix, λ is a scalar, and I is the identity matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Minimal Polynomial: A polynomial that annihilates a matrix, i.e., (M - λI)^n = 0, where M is a matrix, λ is a scalar, and I is the identity matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minimal Polynomial: A polynomial that annihilates a matrix, i.e., (M - λI)^n = 0, where M is a matrix, λ is a scalar, and I is the identity matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lie Algebra: A mathematical structure that consists of a set of linear transformations, a bilinear form, and a Lie bracket operation, which satisfies certain properties.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lie Algebra: A mathematical structure that consists of a set of linear transformations, a bilinear form, and a Lie bracket operation, which satisfies certain properties.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Product: A way of combining two linear transformations to form a new linear transformation, which can be used to find the tensor product of two matrices.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tensor Product: A way of combining two linear transformations to form a new linear transformation, which can be used to find the tensor product of two matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kronecker Product: A way of combining two matrices to form a new matrix, which can be used to find the Kronecker product of two matrices.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Kronecker Product: A way of combining two matrices to form a new matrix, which can be used to find the Kronecker product of two matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Kronecker Product of Matrices: A way of combining two matrices to form a new matrix, which can be used to find the Kronecker product of two matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9907317421506335
      }
    ]
  },
  {
    "representative_text": "Polar Decomposition: A factorization of a matrix into a product of a positive semidefinite matrix and an orthogonal matrix, which can be used to find the polar decomposition of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Polar Decomposition: A factorization of a matrix into a product of a positive semidefinite matrix and an orthogonal matrix, which can be used to find the polar decomposition of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Polar Decomposition of a Matrix: A factorization of a matrix into a product of a positive semidefinite matrix and an orthogonal matrix, which can be used to find the polar decomposition of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9930075273312872
      }
    ]
  },
  {
    "representative_text": "Commutator: A way of combining two linear transformations to form a new linear transformation, which can be used to find the commutator of two matrices.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Commutator: A way of combining two linear transformations to form a new linear transformation, which can be used to find the commutator of two matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Jordan Canonical Form: A way of diagonalizing a matrix by transforming it into a block diagonal matrix with Jordan blocks, which can be used to find the eigenvalues and eigenvectors of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jordan Canonical Form: A way of diagonalizing a matrix by transforming it into a block diagonal matrix with Jordan blocks, which can be used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Product of Matrices: A way of combining two matrices to form a new matrix, which can be used to find the tensor product of two matrices.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tensor Product of Matrices: A way of combining two matrices to form a new matrix, which can be used to find the tensor product of two matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Distribution: The distribution of eigenvalues of a matrix, which can be used to analyze the behavior of a linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Distribution: The distribution of eigenvalues of a matrix, which can be used to analyze the behavior of a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Intrinsic Geometry of Linear Algebra: The study of the geometric properties of linear transformations, such as the distance between vectors and the angle between two vectors.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Intrinsic Geometry of Linear Algebra: The study of the geometric properties of linear transformations, such as the distance between vectors and the angle between two vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bilinear Form and its Properties: A way of combining two linear transformations to form a new linear transformation, which can be used to find the bilinear form of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bilinear Form and its Properties: A way of combining two linear transformations to form a new linear transformation, which can be used to find the bilinear form of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lie Group and its Representations: A mathematical structure that consists of a set of linear transformations, a bilinear form, and a Lie bracket operation, which satisfies certain properties.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lie Group and its Representations: A mathematical structure that consists of a set of linear transformations, a bilinear form, and a Lie bracket operation, which satisfies certain properties.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Groups: A mathematical structure that consists of a set of linear transformations, a bilinear form, and a Lie bracket operation, which satisfies certain properties.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Groups: A mathematical structure that consists of a set of linear transformations, a bilinear form, and a Lie bracket operation, which satisfies certain properties.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Classical Linear Algebra and its Applications: The study of linear algebra in the context of classical mechanics and physics, such as the motion of objects and the behavior of systems.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Classical Linear Algebra and its Applications: The study of linear algebra in the context of classical mechanics and physics, such as the motion of objects and the behavior of systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra in Computer Science: The study of linear algebra in the context of computer science, such as computer graphics, machine learning, and data analysis.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra in Computer Science: The study of linear algebra in the context of computer science, such as computer graphics, machine learning, and data analysis.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Linear Algebra: A theorem that states that every linear transformation can be represented by a matrix, and that every matrix can be diagonalized.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Linear Algebra: A theorem that states that every linear transformation can be represented by a matrix, and that every matrix can be diagonalized.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Spectral Theorem for Symmetric Matrices: A theorem that states that every symmetric matrix can be diagonalized by an orthogonal matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Spectral Theorem for Symmetric Matrices: A theorem that states that every symmetric matrix can be diagonalized by an orthogonal matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Affine Transformation: A linear transformation that also includes a translation component.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Affine Transformation: A linear transformation that also includes a translation component.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonalization of a Matrix: An algorithm for finding an orthogonal matrix that represents a given matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonalization of a Matrix: An algorithm for finding an orthogonal matrix that represents a given matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Canonical Form of a Matrix: A matrix that is similar to a given matrix, but with all eigenvalues and eigenvectors simplified.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Canonical Form of a Matrix: A matrix that is similar to a given matrix, but with all eigenvalues and eigenvectors simplified.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cayley-Hamilton Theorem: A theorem that states that every square matrix satisfies its own characteristic equation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Cayley-Hamilton Theorem: A theorem that states that every square matrix satisfies its own characteristic equation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Cayley-Hamilton Theorem for Matrices: The concept of the Cayley-Hamilton theorem for matrices, which states that every square matrix satisfies its own characteristic equation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9451970435559549
      }
    ]
  },
  {
    "representative_text": "Minimal Polynomial of a Matrix: A polynomial that annihilates a matrix, but is not necessarily the same as the characteristic polynomial.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minimal Polynomial of a Matrix: A polynomial that annihilates a matrix, but is not necessarily the same as the characteristic polynomial.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quadratic Form: A way of combining two linear transformations to form a new linear transformation, which can be used to find the quadratic form of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quadratic Form: A way of combining two linear transformations to form a new linear transformation, which can be used to find the quadratic form of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination for Large Matrices: A method for solving large linear systems using row operations, which can be used to find the solution to a system of linear equations.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination for Large Matrices: A method for solving large linear systems using row operations, which can be used to find the solution to a system of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "LU Decomposition for Large Matrices: A factorization of a matrix into a product of lower and upper triangular matrices, which can be used to solve large linear systems.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "LU Decomposition for Large Matrices: A factorization of a matrix into a product of lower and upper triangular matrices, which can be used to solve large linear systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Iterative Methods for Linear Systems: Methods for solving linear systems, such as the Jacobi method, Gauss-Seidel method, and conjugate gradient method.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Iterative Methods for Linear Systems: Methods for solving linear systems, such as the Jacobi method, Gauss-Seidel method, and conjugate gradient method.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Clustering: A method for grouping eigenvalues of a matrix into clusters, which can be used to analyze the behavior of a linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Clustering: A method for grouping eigenvalues of a matrix into clusters, which can be used to analyze the behavior of a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Clustering: A method for grouping singular values of a matrix into clusters, which can be used to analyze the behavior of a linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Clustering: A method for grouping singular values of a matrix into clusters, which can be used to analyze the behavior of a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Principal Component Analysis (PCA): A method for reducing the dimensionality of a matrix by finding the principal components of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Principal Component Analysis (PCA): A method for reducing the dimensionality of a matrix by finding the principal components of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Karhunen-Loève Expansion: A method for representing a matrix as a sum of orthogonal components, which can be used to find the principal components of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Karhunen-Loève Expansion: A method for representing a matrix as a sum of orthogonal components, which can be used to find the principal components of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fourier Transform: A way of representing a matrix as a sum of orthogonal components, which can be used to find the frequency domain representation of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Fourier Transform: A way of representing a matrix as a sum of orthogonal components, which can be used to find the frequency domain representation of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Toeplitz Matrix: A matrix with constant entries on each row, which can be used to model certain types of linear systems.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Toeplitz Matrix: A matrix with constant entries on each row, which can be used to model certain types of linear systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Triangular Matrix: A matrix with all entries below or above the main diagonal equal to zero, which can be used to model certain types of linear systems.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Triangular Matrix: A matrix with all entries below or above the main diagonal equal to zero, which can be used to model certain types of linear systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Symmetric Positive Definite Matrix: A matrix that is symmetric and positive definite, which can be used to model certain types of linear systems.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Symmetric Positive Definite Matrix: A matrix that is symmetric and positive definite, which can be used to model certain types of linear systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Positive Semi-Definite Matrix: A matrix that is symmetric and positive semi-definite, which can be used to model certain types of linear systems.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Positive Semi-Definite Matrix: A matrix that is symmetric and positive semi-definite, which can be used to model certain types of linear systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hermitian Matrix: A matrix that is equal to its conjugate transpose, which can be used to model certain types of linear systems.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hermitian Matrix: A matrix that is equal to its conjugate transpose, which can be used to model certain types of linear systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Unitary Matrix: A matrix that is equal to its conjugate transpose and has a determinant of 1, which can be used to model certain types of linear systems.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Unitary Matrix: A matrix that is equal to its conjugate transpose and has a determinant of 1, which can be used to model certain types of linear systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence of Span: The concept of linear independence of the span of a set of vectors, which is related to the dimension of the span.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Independence of Span: The concept of linear independence of the span of a set of vectors, which is related to the dimension of the span.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of Span: This concept is related to the dimension of the span of a set of vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9735476066147252
      }
    ]
  },
  {
    "representative_text": "Basis Vectors: The concept of basis vectors, which are the vectors that form a basis for a vector space.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis Vectors: The concept of basis vectors, which are the vectors that form a basis for a vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence of the Set of All Vectors: The concept of linear independence of the set of all vectors in a vector space.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence of the Set of All Vectors: The concept of linear independence of the set of all vectors in a vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dual Space: The concept of the dual space of a vector space, which is the space of all linear functionals on the original vector space.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dual Space: The concept of the dual space of a vector space, which is the space of all linear functionals on the original vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dual Basis: The concept of a dual basis, which is a basis for the dual space of a vector space.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Dual Basis: The concept of a dual basis, which is a basis for the dual space of a vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Dual Basis: This concept is related to the basis for the dual space of a vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9303523568039959
      }
    ]
  },
  {
    "representative_text": "Linear Functionals: The concept of linear functionals, which are functions that satisfy the properties of linearity.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Functionals: The concept of linear functionals, which are functions that satisfy the properties of linearity.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product Spaces: The concept of inner product spaces, which are vector spaces equipped with an inner product.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner Product Spaces: The concept of inner product spaces, which are vector spaces equipped with an inner product.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement: The concept of the orthogonal complement of a subspace, which is the set of all vectors that are orthogonal to every vector in the subspace.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement: The concept of the orthogonal complement of a subspace, which is the set of all vectors that are orthogonal to every vector in the subspace.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Complement: This concept is related to the set of all vectors that are orthogonal to every vector in a subspace.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9291391587269834
      },
      {
        "text": "Orthogonal complement: The orthogonal complement of a subspace is the set of vectors that are orthogonal to every vector in the subspace.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9278711016759513
      }
    ]
  },
  {
    "representative_text": "Orthogonal Decomposition: The concept of orthogonal decomposition, which is the process of decomposing a vector space into a direct sum of subspaces.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal Decomposition: The concept of orthogonal decomposition, which is the process of decomposing a vector space into a direct sum of subspaces.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Decomposition: This concept is related to the process of decomposing a vector space into a direct sum of subspaces.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9499807579027402
      }
    ]
  },
  {
    "representative_text": "Singular Value Singular Value Decomposition (SVD) of a Matrix: The concept of SVD of a matrix, which is a factorization of a matrix into a product of three matrices.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Singular Value Decomposition (SVD) of a Matrix: The concept of SVD of a matrix, which is a factorization of a matrix into a product of three matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Product of Linear Transformations: The concept of the tensor product of linear transformations, which is a way of combining two linear transformations to form a new linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Tensor Product of Linear Transformations: The concept of the tensor product of linear transformations, which is a way of combining two linear transformations to form a new linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Tensor Product of Linear Transformations: This concept is related to combining two linear transformations to form a new linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9386422123565185
      }
    ]
  },
  {
    "representative_text": "Kronecker Product of Matrices: The concept of the Kronecker product of matrices, which is a way of combining two matrices to form a new matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Kronecker Product of Matrices: The concept of the Kronecker product of matrices, which is a way of combining two matrices to form a new matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Kronecker Product of Matrices: This concept is related to combining two matrices to form a new matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9252009661338632
      }
    ]
  },
  {
    "representative_text": "Minimal Polynomial of a Matrix: The concept of the minimal polynomial of a matrix, which is a polynomial that annihilates a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minimal Polynomial of a Matrix: The concept of the minimal polynomial of a matrix, which is a polynomial that annihilates a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Pairs: The concept of eigenvalue and eigenvector pairs, which are pairs of scalar values and vectors that satisfy a linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Pairs: The concept of eigenvalue and eigenvector pairs, which are pairs of scalar values and vectors that satisfy a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Jordan Block: The concept of a Jordan block, which is a block diagonal matrix with a repeated eigenvalue on the diagonal.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jordan Block: The concept of a Jordan block, which is a block diagonal matrix with a repeated eigenvalue on the diagonal.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block Diagonal Matrix: The concept of a block diagonal matrix, which is a matrix whose blocks are diagonal matrices.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block Diagonal Matrix: The concept of a block diagonal matrix, which is a matrix whose blocks are diagonal matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Upper Triangular Matrix: The concept of an upper triangular matrix, which is a matrix with all entries below the main diagonal equal to zero.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Upper Triangular Matrix: The concept of an upper triangular matrix, which is a matrix with all entries below the main diagonal equal to zero.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lower Triangular Matrix: The concept of a lower triangular matrix, which is a matrix with all entries above the main diagonal equal to zero.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lower Triangular Matrix: The concept of a lower triangular matrix, which is a matrix with all entries above the main diagonal equal to zero.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hermitian Matrix: The concept of a Hermitian matrix, which is a matrix that is equal to its conjugate transpose.",
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Hermitian Matrix: The concept of a Hermitian matrix, which is a matrix that is equal to its conjugate transpose.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Hermitian matrix: A Hermitian matrix is a square matrix that is equal to its own conjugate transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.928465096143294
      }
    ]
  },
  {
    "representative_text": "Unitary Matrix: The concept of a unitary matrix, which is a matrix that is equal to its conjugate transpose and has a determinant of 1.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Unitary Matrix: The concept of a unitary matrix, which is a matrix that is equal to its conjugate transpose and has a determinant of 1.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Positive Semi-Definite Matrix: The concept of a positive semi-definite matrix, which is a matrix that is symmetric and positive semi-definite.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Positive Semi-Definite Matrix: The concept of a positive semi-definite matrix, which is a matrix that is symmetric and positive semi-definite.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Positive Definite Matrix: The concept of a positive definite matrix, which is a matrix that is symmetric and positive definite.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Positive Definite Matrix: The concept of a positive definite matrix, which is a matrix that is symmetric and positive definite.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Toeplitz Matrix: The concept of a Toeplitz matrix, which is a matrix with constant entries on each row.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Toeplitz Matrix: The concept of a Toeplitz matrix, which is a matrix with constant entries on each row.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Toeplitz Matrix with Constant Diagonal: The concept of a Toeplitz matrix with constant diagonal entries.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Toeplitz Matrix with Constant Diagonal: The concept of a Toeplitz matrix with constant diagonal entries.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Toeplitz Matrix with Constant Entries on the Diagonal: The concept of a Toeplitz matrix with constant entries on the diagonal.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9807555695545565
      }
    ]
  },
  {
    "representative_text": "Toeplitz Matrix with Constant Entries on the Superdiagonal: The concept of a Toeplitz matrix with constant entries on the superdiagonal.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Toeplitz Matrix with Constant Entries on the Superdiagonal: The concept of a Toeplitz matrix with constant entries on the superdiagonal.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Toeplitz Matrix with Constant Entries on the Subdiagonal: The concept of a Toeplitz matrix with constant entries on the subdiagonal.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Toeplitz Matrix with Constant Entries on the Subdiagonal: The concept of a Toeplitz matrix with constant entries on the subdiagonal.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Inversion: The concept of matrix inversion, which is the process of finding a matrix that, when multiplied by the original matrix, results in the identity matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Inversion: The concept of matrix inversion, which is the process of finding a matrix that, when multiplied by the original matrix, results in the identity matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix: The concept of the determinant of a matrix, which is a scalar value that can be used to determine the invertibility of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix: The concept of the determinant of a matrix, which is a scalar value that can be used to determine the invertibility of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using Cofactors: The concept of the determinant of a matrix using cofactors, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using Cofactors: The concept of the determinant of a matrix using cofactors, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using Expansion by Minors: The concept of the determinant of a matrix using expansion by minors, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using Expansion by Minors: The concept of the determinant of a matrix using expansion by minors, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using LU Decomposition: The concept of the determinant of a matrix using LU decomposition, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using LU Decomposition: The concept of the determinant of a matrix using LU decomposition, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using Gaussian Elimination: The concept of the determinant of a matrix using Gaussian elimination, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using Gaussian Elimination: The concept of the determinant of a matrix using Gaussian elimination, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using SVD Decomposition: The concept of the determinant of a matrix using SVD decomposition, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using SVD Decomposition: The concept of the determinant of a matrix using SVD decomposition, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using Eigenvalues: The concept of the determinant of a matrix using eigenvalues, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using Eigenvalues: The concept of the determinant of a matrix using eigenvalues, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using Eigenvectors: The concept of the determinant of a matrix using eigenvectors, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using Eigenvectors: The concept of the determinant of a matrix using eigenvectors, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using the Trace: The concept of the determinant of a matrix using the trace, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using the Trace: The concept of the determinant of a matrix using the trace, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using the Sum of Diagonal Entries: The concept of the determinant of a matrix using the sum of diagonal entries, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 16,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using the Sum of Diagonal Entries: The concept of the determinant of a matrix using the sum of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Squares of Diagonal Entries: The concept of the determinant of a matrix using the sum of squares of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9278862963511421
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9376162164745887
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Squares of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of squares of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9675004454333832
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9679479842798518
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Squares of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of squares of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9808286132620151
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9761450230958842
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Squares of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of squares of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9854874218329872
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9805771982384657
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Squares of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of squares of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9873382880695543
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.982607122291696
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Squares of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of squares of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9880385622503397
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Products of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of products of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9839601591907206
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Products of Squares of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of products of squares of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9875572006273696
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Products of Products of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of products of products of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9843649113447689
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Products of Products of Squares of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of products of products",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9574238964333336
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using the Sum of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 22,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using the Sum of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Squares of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of squares of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9367335382574064
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Cubes of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of cubes of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9234650056505087
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9351891838913767
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Squares of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of squares of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9654060502980828
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Cubes of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of cubes of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.948862845447765
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.963265366630082
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Squares of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of squares of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9755084693909029
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Cubes of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of cubes of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9543616179212485
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9713817140017008
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Squares of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of squares of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9793338617742237
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Cubes of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of cubes of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9603719415669829
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9757621749724966
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Squares of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of squares of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9805722488178663
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Cubes of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of cubes of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9636749528363868
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9784947195798865
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Squares of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of squares of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9805228295086889
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Cubes of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of cubes of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9651843608867184
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Products of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of products of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9791877950223576
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Products of Squares of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of products of squares of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9794781970976985
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Products of Cubes of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of products of cubes of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.965529303186814
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Products of Products of Off-Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of products of products of off-diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9799357438466078
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Using the Sum of Cubes of Diagonal Entries: The concept of the determinant of a matrix using the sum of cubes of diagonal entries, which is a method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Using the Sum of Cubes of Diagonal Entries: The concept of the determinant of a matrix using the sum of cubes of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Cubes of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of cubes of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9704705412858593
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Cubes of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of cubes of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.986183663675906
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Cubes of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of cubes of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9897817092438961
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Cubes of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of cubes of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9916878077134076
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Cubes of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of cubes of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9924070315843941
      },
      {
        "text": "Determinant of a Matrix Using the Sum of Products of Products of Products of Products of Products of Products of Cubes of Diagonal Entries: The concept of the determinant of a matrix using the sum of products of products of products of products of products of products of cubes of diagonal entries, which is a method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9927919021204473
      }
    ]
  },
  {
    "representative_text": "Differential Geometry of Linear Algebra: This includes the study of the geometric properties of linear transformations, such as the distance between vectors and the angle between two vectors.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Differential Geometry of Linear Algebra: This includes the study of the geometric properties of linear transformations, such as the distance between vectors and the angle between two vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lie Algebraic Completions: This is a way of extending a linear algebra to a larger algebraic structure, such as a Lie algebra.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lie Algebraic Completions: This is a way of extending a linear algebra to a larger algebraic structure, such as a Lie algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Algebra: This is a way of combining multiple linear transformations to form a new linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tensor Algebra: This is a way of combining multiple linear transformations to form a new linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Exterior Algebra: This is a way of combining multiple linear transformations to form a new linear transformation, using the exterior product.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Exterior Algebra: This is a way of combining multiple linear transformations to form a new linear transformation, using the exterior product.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Grassmann Algebra: This is a way of combining multiple linear transformations to form a new linear transformation, using the Grassmann product.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Grassmann Algebra: This is a way of combining multiple linear transformations to form a new linear transformation, using the Grassmann product.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Multilinear Algebra: This is a way of combining multiple linear transformations to form a new linear transformation, using multiple linear forms.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Multilinear Algebra: This is a way of combining multiple linear transformations to form a new linear transformation, using multiple linear forms.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Operator Theory: This is a way of studying the properties of linear transformations using algebraic techniques.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Operator Theory: This is a way of studying the properties of linear transformations using algebraic techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Functional Analysis: This is a way of studying the properties of linear transformations using functional analysis techniques.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Functional Analysis: This is a way of studying the properties of linear transformations using functional analysis techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Geometry: This is a way of studying the geometric properties of linear transformations using algebraic techniques.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Geometry: This is a way of studying the geometric properties of linear transformations using algebraic techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Geometric Algebra: This is a way of combining multiple linear transformations to form a new linear transformation, using geometric algebra techniques.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Geometric Algebra: This is a way of combining multiple linear transformations to form a new linear transformation, using geometric algebra techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "K-theory of Linear Algebra: This is a way of studying the properties of linear transformations using K-theory.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "K-theory of Linear Algebra: This is a way of studying the properties of linear transformations using K-theory.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Algebraic K-Theory of Linear Algebra: This is a way of studying the properties of linear transformations using algebraic K-theory.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9552199646746447
      }
    ]
  },
  {
    "representative_text": "Topological Linear Algebra: This is a way of studying the properties of linear transformations using topological techniques.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Topological Linear Algebra: This is a way of studying the properties of linear transformations using topological techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-commutative Linear Algebra: This is a way of studying the properties of linear transformations using non-commutative algebra.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-commutative Linear Algebra: This is a way of studying the properties of linear transformations using non-commutative algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-associative Linear Algebra: This is a way of studying the properties of linear transformations using non-associative algebra.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-associative Linear Algebra: This is a way of studying the properties of linear transformations using non-associative algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Functionals: This concept is related to functions that satisfy the properties of linearity.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Functionals: This concept is related to functions that satisfy the properties of linearity.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product Spaces: This concept is related to vector spaces equipped with an inner product.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner Product Spaces: This concept is related to vector spaces equipped with an inner product.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Polar Decomposition of a Matrix: This concept is related to factoring a matrix into a product of a positive semidefinite matrix and an orthogonal matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Polar Decomposition of a Matrix: This concept is related to factoring a matrix into a product of a positive semidefinite matrix and an orthogonal matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Operator Theory: This is not only has already mentioned in relation to study of Linear Algebraic Information: This is not only in Linear Algebraic properties of Linear Algebraickeghenforcement of Linear Algebra: This is a way of Linear Algebra: This is not only contains more information.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Operator Theory: This is not only has already mentioned in relation to study of Linear Algebraic Information: This is not only in Linear Algebraic properties of Linear Algebraickeghenforcement of Linear Algebra: This is a way of Linear Algebra: This is not only contains more information.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Based on the provided list, here are some additional missing knowledge points that are some additional missing knowledge points, here are some additional missing knowledge points related to identify and additional missing knowledge points that are some of Linear Algebra:",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Based on the provided list, here are some additional missing knowledge points that are some additional missing knowledge points, here are some additional missing knowledge points related to identify and additional missing knowledge points that are some of Linear Algebra:",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are some additional missing knowledge points, some additional missing knowledge points that is already exists more information about the study of Linear Algebra:",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are some additional missing knowledge points, some additional missing knowledge points that is already exists more information about the study of Linear Algebra:",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are missing knowledge points, there are not yet to be found  and its Applications of Linear Algebra:",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are missing knowledge points, there are not yet to be found  and its Applications of Linear Algebra:",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are some of Linear Algebra:",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are some of Linear Algebra:",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The list has been removed theore flexible way of the above the following are some of a way of Linear Algebra:",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The list has been removed theore flexible way of the above the following are some of a way of Linear Algebra:",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are some of Linear Algebra: This is the above is not yet, and its Applications of a related to have been already listed above mentioned above.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are some of Linear Algebra: This is the above is not yet, and its Applications of a related to have been already listed above mentioned above.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector addition: Two or more vectors can be added by adding their corresponding components.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector addition: Two or more vectors can be added by adding their corresponding components.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector subtraction: A vector can be subtracted from another vector by subtracting their corresponding components.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector subtraction: A vector can be subtracted from another vector by subtracting their corresponding components.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scalar multiplication: A vector can be multiplied by a scalar by multiplying each component by the scalar.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar multiplication: A vector can be multiplied by a scalar by multiplying each component by the scalar.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scalar addition: A scalar can be added to a vector by adding the scalar to each component of the vector.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar addition: A scalar can be added to a vector by adding the scalar to each component of the vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector magnitude (length or norm): The magnitude of a vector is the square root of the sum of the squares of its components.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector magnitude (length or norm): The magnitude of a vector is the square root of the sum of the squares of its components.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector magnitude formula: ||v|| = √(v1^2 + v2^2 + ... + vn^2)",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector magnitude formula: ||v|| = √(v1^2 + v2^2 + ... + vn^2)",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dot product: The dot product of two vectors is the sum of the products of their corresponding components.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dot product: The dot product of two vectors is the sum of the products of their corresponding components.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis: A basis for a vector space is a set of linearly independent vectors that span the space.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis: A basis for a vector space is a set of linearly independent vectors that span the space.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear transformation: A linear transformation is a function from a vector space to itself that preserves the operations of vector addition and scalar multiplication.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear transformation: A linear transformation is a function from a vector space to itself that preserves the operations of vector addition and scalar multiplication.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix representation: A linear transformation can be represented by a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix representation: A linear transformation can be represented by a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Characteristic equation: The characteristic equation of a matrix is a polynomial equation that represents the eigenvalues of the matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Characteristic equation: The characteristic equation of a matrix is a polynomial equation that represents the eigenvalues of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Diagonalization: A matrix can be diagonalized if it has a basis of eigenvectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Diagonalization: A matrix can be diagonalized if it has a basis of eigenvectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant: The determinant of a matrix is a scalar that represents the volume scaling factor of the linear transformation represented by the matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant: The determinant of a matrix is a scalar that represents the volume scaling factor of the linear transformation represented by the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Minor: The minor of an element in a matrix is the determinant of the submatrix formed by removing the row and column containing the element.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minor: The minor of an element in a matrix is the determinant of the submatrix formed by removing the row and column containing the element.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cofactor: The cofactor of an element in a matrix is the minor of the element multiplied by (-1)^(i+j), where i and j are the row and column indices of the element.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cofactor: The cofactor of an element in a matrix is the minor of the element multiplied by (-1)^(i+j), where i and j are the row and column indices of the element.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal vectors: Two vectors are orthogonal if their dot product is zero.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal vectors: Two vectors are orthogonal if their dot product is zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthonormal vectors: An orthonormal set of vectors is a set of vectors that are orthogonal to each other and have unit magnitude.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthonormal vectors: An orthonormal set of vectors is a set of vectors that are orthogonal to each other and have unit magnitude.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt process: The Gram-Schmidt process is a method for orthonormalizing a set of vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt process: The Gram-Schmidt process is a method for orthonormalizing a set of vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bilinear form: A bilinear form is a function from a vector space to the real numbers that is linear in each argument.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bilinear form: A bilinear form is a function from a vector space to the real numbers that is linear in each argument.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner product: An inner product is a bilinear form that is symmetric and positive definite.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner product: An inner product is a bilinear form that is symmetric and positive definite.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalues of symmetric and skew-symmetric matrices: The eigenvalues of a symmetric matrix are real, while the eigenvalues of a skew-symmetric matrix are purely imaginary.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalues of symmetric and skew-symmetric matrices: The eigenvalues of a symmetric matrix are real, while the eigenvalues of a skew-symmetric matrix are purely imaginary.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal projection: An orthogonal projection is a linear transformation that maps a vector to its closest orthogonal vector in a subspace.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal projection: An orthogonal projection is a linear transformation that maps a vector to its closest orthogonal vector in a subspace.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Projection matrix: A projection matrix is a matrix that represents an orthogonal projection.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Projection matrix: A projection matrix is a matrix that represents an orthogonal projection.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "SVD: The SVD of a matrix is a factorization of the matrix into three matrices: U, Σ, and V.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "SVD: The SVD of a matrix is a factorization of the matrix into three matrices: U, Σ, and V.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular values: The singular values of a matrix are the square roots of the eigenvalues of the matrix squared.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular values: The singular values of a matrix are the square roots of the eigenvalues of the matrix squared.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rank: The rank of a matrix is the number of non-zero singular values.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Rank: The rank of a matrix is the number of non-zero singular values.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Trigonometric functions: Trigonometric functions can be applied to vectors in the same way they are applied to scalars, resulting in new vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Trigonometric functions: Trigonometric functions can be applied to vectors in the same way they are applied to scalars, resulting in new vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dot product with a scalar: The dot product of a vector with a scalar is a new vector with components equal to the dot product of the original vector with the scalar.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dot product with a scalar: The dot product of a vector with a scalar is a new vector with components equal to the dot product of the original vector with the scalar.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cross product: The cross product of two vectors results in a new vector that is orthogonal to both original vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cross product: The cross product of two vectors results in a new vector that is orthogonal to both original vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector triple product: The vector triple product is a new vector resulting from the cross product of two vectors multiplied by a third vector.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector triple product: The vector triple product is a new vector resulting from the cross product of two vectors multiplied by a third vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gradient: The gradient of a scalar function at a point is a vector that points in the direction of the maximum increase of the function.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gradient: The gradient of a scalar function at a point is a vector that points in the direction of the maximum increase of the function.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Curl: The curl of a vector field at a point is a vector that points in the direction of the rotation of the field around the point.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Curl: The curl of a vector field at a point is a vector that points in the direction of the rotation of the field around the point.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector norm and length: The vector norm and length can be calculated using different methods, such as the Euclidean norm, the Manhattan norm, or the infinity norm.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector norm and length: The vector norm and length can be calculated using different methods, such as the Euclidean norm, the Manhattan norm, or the infinity norm.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector unit length: A vector with unit length can be normalized by dividing it by its length.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector unit length: A vector with unit length can be normalized by dividing it by its length.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span and basis: A basis for a vector space is a set of linearly independent vectors that span the space, and the span of a set of vectors is the set of all linear combinations of the vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span and basis: A basis for a vector space is a set of linearly independent vectors that span the space, and the span of a set of vectors is the set of all linear combinations of the vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear combination: A linear combination of vectors is a new vector resulting from adding scalar multiples of the original vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear combination: A linear combination of vectors is a new vector resulting from adding scalar multiples of the original vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spanning set: A spanning set for a vector space is a set of vectors that span the space, and the dimension of a vector space is equal to the number of vectors in a basis.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spanning set: A spanning set for a vector space is a set of vectors that span the space, and the dimension of a vector space is equal to the number of vectors in a basis.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Homomorphism: A homomorphism is a linear transformation that preserves the operations of vector addition and scalar multiplication.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Homomorphism: A homomorphism is a linear transformation that preserves the operations of vector addition and scalar multiplication.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear isomorphism: A linear isomorphism is a linear transformation that is both one-to-one and onto.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear isomorphism: A linear isomorphism is a linear transformation that is both one-to-one and onto.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inverse function: The inverse of a linear transformation is a linear transformation that, when composed with the original transformation, results in the identity transformation.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inverse function: The inverse of a linear transformation is a linear transformation that, when composed with the original transformation, results in the identity transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Diagonalization: A matrix can be diagonalized if it has a basis of eigenvectors, resulting in a diagonal matrix that represents the linear transformation.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Diagonalization: A matrix can be diagonalized if it has a basis of eigenvectors, resulting in a diagonal matrix that represents the linear transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue decomposition: The eigenvalue decomposition of a matrix is a factorization of the matrix into a product of eigenvectors and eigenvalues.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue decomposition: The eigenvalue decomposition of a matrix is a factorization of the matrix into a product of eigenvectors and eigenvalues.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Jordan canonical form: The Jordan canonical form of a matrix is a diagonal matrix that represents the linear transformation, with Jordan blocks representing the eigenvectors and eigenvalues.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jordan canonical form: The Jordan canonical form of a matrix is a diagonal matrix that represents the linear transformation, with Jordan blocks representing the eigenvectors and eigenvalues.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cofactor expansion: The cofactor expansion of a determinant is a method for calculating the determinant of a matrix by expanding along a row or column.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cofactor expansion: The cofactor expansion of a determinant is a method for calculating the determinant of a matrix by expanding along a row or column.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Sarrus' rule: Sarrus' rule is a method for calculating the determinant of a 3x3 matrix by expanding along the first row.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sarrus' rule: Sarrus' rule is a method for calculating the determinant of a 3x3 matrix by expanding along the first row.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Minor expansion: The minor expansion of a determinant is a method for calculating the determinant of a matrix by expanding along a row or column.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minor expansion: The minor expansion of a determinant is a method for calculating the determinant of a matrix by expanding along a row or column.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonalization: Orthogonalization is the process of making a set of vectors orthogonal to each other.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonalization: Orthogonalization is the process of making a set of vectors orthogonal to each other.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthonormalization: Orthonormalization is the process of making a set of vectors orthogonal to each other and having unit length.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthonormalization: Orthonormalization is the process of making a set of vectors orthogonal to each other and having unit length.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Givens rotation: Givens rotation is a method for orthonormalizing a set of vectors using rotations around the origin.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Givens rotation: Givens rotation is a method for orthonormalizing a set of vectors using rotations around the origin.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Skew-Hermitian matrix: A skew-Hermitian matrix is a square matrix that is equal to the negative of its own conjugate transpose.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Skew-Hermitian matrix: A skew-Hermitian matrix is a square matrix that is equal to the negative of its own conjugate transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Positive definite matrix: A positive definite matrix is a square matrix that is always positive when multiplied by a vector.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Positive definite matrix: A positive definite matrix is a square matrix that is always positive when multiplied by a vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular value decomposition: The singular value decomposition of a matrix is a factorization of the matrix into three matrices: U, Σ, and V.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular value decomposition: The singular value decomposition of a matrix is a factorization of the matrix into three matrices: U, Σ, and V.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal projection matrix: An orthogonal projection matrix is a projection matrix that preserves the length of the original vector.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal projection matrix: An orthogonal projection matrix is a projection matrix that preserves the length of the original vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computer graphics: Linear algebra is used in computer graphics to perform transformations, projections, and lighting calculations.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computer graphics: Linear algebra is used in computer graphics to perform transformations, projections, and lighting calculations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Machine learning: Linear algebra is used in machine learning to perform dimensionality reduction, clustering, and classification.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Machine learning: Linear algebra is used in machine learning to perform dimensionality reduction, clustering, and classification.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Data analysis: Linear algebra is used in data analysis to perform regression, correlation, and principal component analysis.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Data analysis: Linear algebra is used in data analysis to perform regression, correlation, and principal component analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Exponential function: The exponential function can be applied to vectors in the same way it is applied to scalars, resulting in a new vector.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Exponential function: The exponential function can be applied to vectors in the same way it is applied to scalars, resulting in a new vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Trigonometric identities: Trigonometric identities can be applied to vectors in the same way they are applied to scalars, resulting in new vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Trigonometric identities: Trigonometric identities can be applied to vectors in the same way they are applied to scalars, resulting in new vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector derivative: The derivative of a vector-valued function can be calculated using the same rules as the derivative of a scalar-valued function.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector derivative: The derivative of a vector-valued function can be calculated using the same rules as the derivative of a scalar-valued function.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector integral: The integral of a vector-valued function can be calculated using the same rules as the integral of a scalar-valued function.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector integral: The integral of a vector-valued function can be calculated using the same rules as the integral of a scalar-valued function.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector derivative and integral: The derivative and integral of a vector-valued function can be calculated using the same rules as the derivative and integral of a scalar-valued function.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector derivative and integral: The derivative and integral of a vector-valued function can be calculated using the same rules as the derivative and integral of a scalar-valued function.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector magnitude and direction: The magnitude and direction of a vector can be calculated using different methods, such as the polar coordinates or the spherical coordinates.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector magnitude and direction: The magnitude and direction of a vector can be calculated using different methods, such as the polar coordinates or the spherical coordinates.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear independence and dependence: A set of vectors is linearly independent if none of the vectors can be expressed as a linear combination of the others, and the set is linearly dependent if at least one vector can be expressed as a linear combination of the others.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear independence and dependence: A set of vectors is linearly independent if none of the vectors can be expressed as a linear combination of the others, and the set is linearly dependent if at least one vector can be expressed as a linear combination of the others.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear transformation and matrix: A linear transformation can be represented by a matrix, and the matrix can be used to calculate the transformation.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear transformation and matrix: A linear transformation can be represented by a matrix, and the matrix can be used to calculate the transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and eigenvector: An eigenvalue is a scalar that represents how much a linear transformation stretches or shrinks a vector, and an eigenvector is a non-zero vector that, when transformed by a linear transformation, results in a scaled version of itself.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue and eigenvector: An eigenvalue is a scalar that represents how much a linear transformation stretches or shrinks a vector, and an eigenvector is a non-zero vector that, when transformed by a linear transformation, results in a scaled version of itself.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant and matrix: The determinant of a matrix can be calculated using different methods, such as the cofactor expansion or the Sarrus' rule.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant and matrix: The determinant of a matrix can be calculated using different methods, such as the cofactor expansion or the Sarrus' rule.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Householder transformation: Householder transformation is a method for orthonormalizing a set of vectors using reflections around the origin.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Householder transformation: Householder transformation is a method for orthonormalizing a set of vectors using reflections around the origin.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Symmetric and skew-symmetric matrices: A symmetric matrix is a square matrix that is equal to its own transpose, and a skew-symmetric matrix is a square matrix that is equal to the negative of its own transpose.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Symmetric and skew-symmetric matrices: A symmetric matrix is a square matrix that is equal to its own transpose, and a skew-symmetric matrix is a square matrix that is equal to the negative of its own transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "SVD and matrix: The SVD of a matrix can be used to calculate the matrix, and the matrix can be used to calculate the SVD.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "SVD and matrix: The SVD of a matrix can be used to calculate the matrix, and the matrix can be used to calculate the SVD.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Physics: Linear algebra is used in physics to describe the motion of objects, the behavior of forces, and the properties of materials.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Physics: Linear algebra is used in physics to describe the motion of objects, the behavior of forces, and the properties of materials.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Engineering: Linear algebra is used in engineering to design and analyze systems, optimize processes, and model complex phenomena.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Engineering: Linear algebra is used in engineering to design and analyze systems, optimize processes, and model complex phenomena.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector convolution: The vector convolution of two vectors is a new vector resulting from the dot product of the two vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector convolution: The vector convolution of two vectors is a new vector resulting from the dot product of the two vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector cross product with a scalar: The cross product of a vector with a scalar is a new vector resulting from the cross product of the vector with the scalar.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector cross product with a scalar: The cross product of a vector with a scalar is a new vector resulting from the cross product of the vector with the scalar.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector triple product with a scalar: The vector triple product with a scalar is a new vector resulting from the cross product of the vector with the scalar.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector triple product with a scalar: The vector triple product with a scalar is a new vector resulting from the cross product of the vector with the scalar.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Exponential map: The exponential map of a vector-valued function is a new vector resulting from the exponential function of the function.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Exponential map: The exponential map of a vector-valued function is a new vector resulting from the exponential function of the function.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fundamental theorem of linear algebra: The fundamental theorem of linear algebra states that every linear transformation has an inverse.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Fundamental theorem of linear algebra: The fundamental theorem of linear algebra states that every linear transformation has an inverse.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rank-nullity theorem: The rank-nullity theorem states that the rank of a matrix plus the nullity of a matrix is equal to the number of columns of the matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Rank-nullity theorem: The rank-nullity theorem states that the rank of a matrix plus the nullity of a matrix is equal to the number of columns of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear independence and span: The linear independence and span theorem states that a set of vectors is linearly independent if and only if it spans the space.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear independence and span: The linear independence and span theorem states that a set of vectors is linearly independent if and only if it spans the space.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and eigenvector theorem: The eigenvalue and eigenvector theorem states that a matrix has an eigenvalue if and only if it has an eigenvector.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue and eigenvector theorem: The eigenvalue and eigenvector theorem states that a matrix has an eigenvalue if and only if it has an eigenvector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian elimination: The Gaussian elimination algorithm is a method for solving systems of linear equations.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian elimination: The Gaussian elimination algorithm is a method for solving systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "LU decomposition: The LU decomposition algorithm is a method for decomposing a matrix into the product of two matrices.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "LU decomposition: The LU decomposition algorithm is a method for decomposing a matrix into the product of two matrices.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cholesky decomposition: The Cholesky decomposition algorithm is a method for decomposing a symmetric positive definite matrix into the product of two matrices.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cholesky decomposition: The Cholesky decomposition algorithm is a method for decomposing a symmetric positive definite matrix into the product of two matrices.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "QR decomposition: The QR decomposition algorithm is a method for decomposing a matrix into the product of an orthogonal matrix and an upper triangular matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "QR decomposition: The QR decomposition algorithm is a method for decomposing a matrix into the product of an orthogonal matrix and an upper triangular matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "MATLAB: MATLAB is a software package that provides a wide range of linear algebra functions and tools.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "MATLAB: MATLAB is a software package that provides a wide range of linear algebra functions and tools.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "NumPy: NumPy is a software package that provides a wide range of linear algebra functions and tools.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "NumPy: NumPy is a software package that provides a wide range of linear algebra functions and tools.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "SciPy: SciPy is a software package that provides a wide range of linear algebra functions and tools.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "SciPy: SciPy is a software package that provides a wide range of linear algebra functions and tools.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra Library: The Linear Algebra Library is a software package that provides a wide range of linear algebra functions and tools.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra Library: The Linear Algebra Library is a software package that provides a wide range of linear algebra functions and tools.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector product: The vector product of two vectors is a new vector that is orthogonal to both original vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector product: The vector product of two vectors is a new vector that is orthogonal to both original vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scalar triple product: The scalar triple product of three vectors is a new scalar that represents the volume of the parallelepiped formed by the vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar triple product: The scalar triple product of three vectors is a new scalar that represents the volume of the parallelepiped formed by the vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector magnitude and length: The vector magnitude and length can be calculated using different methods, such as the Euclidean norm, the Manhattan norm, or the infinity norm.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector magnitude and length: The vector magnitude and length can be calculated using different methods, such as the Euclidean norm, the Manhattan norm, or the infinity norm.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector triple product: The vector triple product of three vectors is a new vector that is orthogonal to both original vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector triple product: The vector triple product of three vectors is a new vector that is orthogonal to both original vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span and basis: A basis for a vector space is a set of linearlyinearlyearlier independence of a set of a set of a set of a basis theorem: A basis theorem: A set of a set of a set of a set is a linear independence: A basis: A set of a set of a set of a linear independence: A Additional points that can be a set of a set of a linear independence: A set of a set theory of a linear independence theorem: The fundamental theorem: Theorem: Theorem",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span and basis: A basis for a vector space is a set of linearlyinearlyearlier independence of a set of a set of a set of a basis theorem: A basis theorem: A set of a set of a set of a set is a linear independence: A basis: A set of a set of a set of a linear independence: A Additional points that can be a set of a set of a linear independence: A set of a set theory of a linear independence theorem: The fundamental theorem: Theorem: Theorem",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are linearlyearlier linear is a set of a theorem: Theorem: Theorem: Theorem: Theorem of a set of a set of a set of a theorem theorem: Theorem: Theorem: The fundamental theorem: The fundamental theorem: Theorem: The final theorem: Theorem: The fundamental theorem: Theorem: The final theorem: Theore how is a set of a set of a set of a set of a set of a theorem: The final theorem: Theorem: Theorevectors: The final theorem: The finalization theorem: Theorem: The fundamental theorem**: The final answer",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are linearlyearlier linear is a set of a theorem: Theorem: Theorem: Theorem: Theorem of a set of a set of a set of a theorem theorem: Theorem: Theorem: The fundamental theorem: The fundamental theorem: Theorem: The final theorem: Theorem: The fundamental theorem: Theorem: The final theorem: Theore how is a set of a set of a set of a set of a set of a theorem: The final theorem: Theorem: Theorevectors: The final theorem: The finalization theorem: Theorem: The fundamental theorem**: The final answer",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are linear algebra: The final theorem: The fundamental theorem: The final theorem: The linear algebra: Theorem: The final **More",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are linear algebra: The final theorem: The fundamental theorem: The final theorem: The linear algebra: Theorem: The final **More",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "assistant:",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "assistant:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Assistant: The final theorem**: The final answer",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Assistant: The final theorem**: The final answer",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are most of a set of a set of a set of a vector**: Theorem of a linear is not to theore to be that a basis of a vector.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are most of a set of a set of a set of a vector**: Theorem of a linear is not to theore to be that a basis of a vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are theoretheth that theorem: The final theorem: Theorethé Eigenvalue decomposition: The final theorem: The finalizing theorem: The final theorem: The final answer to be a set of a set of a basis of a set of a matrix: Theorem: The fundamental theorem: The final theorem theorem: Theoremathematic theorem: Theorem: Theorem: The final theorem: Theorem: The final theorem: The fundamental theorem: The linear independence of a set of a set of a set of linear: The final theorem: The final theorem: The final theorem theorem theorem: The final theorem: The final theorem: The final theorem: The final theorem: Theorem: The linear algebra: The final theorem theorem: The final theorem: The final theorem: The final theorem: The final vector: The finalization of a set of a set of a set of a vector",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are theoretheth that theorem: The final theorem: Theorethé Eigenvalue decomposition: The final theorem: The finalizing theorem: The final theorem: The final answer to be a set of a set of a basis of a set of a matrix: Theorem: The fundamental theorem: The final theorem theorem: Theoremathematic theorem: Theorem: Theorem: The final theorem: Theorem: The final theorem: The fundamental theorem: The linear independence of a set of a set of a set of linear: The final theorem: The final theorem: The final theorem theorem theorem: The final theorem: The final theorem: The final theorem: The final theorem: Theorem: The linear algebra: The final theorem theorem: The final theorem: The final theorem: The final theorem: The final vector: The finalization of a set of a set of a set of a vector",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  }
]