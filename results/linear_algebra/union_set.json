[
  {
    "representative_text": "Vector Space: A set of vectors V over a field F, denoted as V, together with two operations:",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Vector Space: A set of vectors V over a field F, denoted as V, together with two operations:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Definition 1: A vector space, also known as a linear space, is a set V equipped with two binary operations, addition (+) and scalar multiplication (c·), satisfying the following properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8397685331344064
      },
      {
        "text": "Vector Space: A set of vectors that satisfies certain properties, such as closure under addition and scalar multiplication, and contains the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8485368901915432
      },
      {
        "text": "Vector Space: A set of vectors that is closed under vector addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9276450835395464
      },
      {
        "text": "Vector Space: A vector space is a set of vectors that is closed under addition and scalar multiplication, and satisfies certain axioms such as commutativity, associativity, distributivity, and the existence of additive identity and inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9296296370950587
      },
      {
        "text": "Vector Space: A set of vectors that is closed under addition and scalar multiplication, with the operations of addition and scalar multiplication satisfying certain properties (e.g., commutativity, associativity, distributivity).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.969128045686463
      },
      {
        "text": "Vector Space: A vector space is a set of vectors that satisfies certain properties, including closure under addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9506049996008319
      },
      {
        "text": "Vector Spaces: A set of vectors that can be added together and scaled by numbers, subject to certain properties (closure, commutativity, associativity, distributivity, existence of additive identity and inverse).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.934475971248376
      },
      {
        "text": "Vector spaces: A set of vectors that satisfy certain properties, such as closure under addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9128347637448228
      },
      {
        "text": "Definition of a Vector Space:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8602362755657333
      },
      {
        "text": "The concept of a vector space, including the axioms and properties that define it.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8520991336078361
      },
      {
        "text": "Definition of a vector space: A set of vectors that is closed under vector addition and scalar multiplication.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9399714763941729
      }
    ]
  },
  {
    "representative_text": "Span (or Linear Combination): The set of all linear combinations of a set of vectors {v1, v2, ..., vn} ∈ V, denoted as span({v1, v2, ..., vn}) = {a1v1 + a2v2 + ... + anvn | a1, a2, ..., an ∈ F}.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 51,
    "detailed_sources": [
      {
        "text": "Span (or Linear Combination): The set of all linear combinations of a set of vectors {v1, v2, ..., vn} ∈ V, denoted as span({v1, v2, ..., vn}) = {a1v1 + a2v2 + ... + anvn | a1, a2, ..., an ∈ F}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Combination: An expression of the form a1v1 + a2v2 + ... + anvn, where a1, a2, ..., an ∈ F and v1, v2, ..., vn ∈ V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8179498544093537
      },
      {
        "text": "Span Theorem: The span of a set of vectors {v1, v2, ..., vn} is the set of all linear combinations of these vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8573441699775483
      },
      {
        "text": "Span Theorem: span({v1, v2, ..., vn}) = {a1v1 + a2v2 + ... + anvn | a1, a2, ..., an ∈ F}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8850577890561042
      },
      {
        "text": "Spanning Set Theorem: A set of vectors {v1, v2, ..., vn} spans V if and only if every vector w ∈ V can be expressed as a linear combination of these vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8748902507373055
      },
      {
        "text": "Span: The set of all linear combinations of the basis vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8660615043587658
      },
      {
        "text": "Definition 5: A linear combination of vectors v1, v2, ..., vn in a vector space V is a vector of the form a1v1 + a2v2 + ... + anvn, where a1, a2, ..., an are scalars.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8249805208847458
      },
      {
        "text": "Span: The span of a set of vectors v1, v2, ..., vn is the set of all linear combinations of these vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9361428231028264
      },
      {
        "text": "Theorem 1: A set of vectors v1, v2, ..., vn spans a vector space V if and only if every vector in V can be expressed as a linear combination of these vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8287032595896968
      },
      {
        "text": "Span: The set of all linear combinations of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9280565085453745
      },
      {
        "text": "Linear Combination: A sum of scalar multiples of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8212460830207767
      },
      {
        "text": "Spanning Set: A set of vectors is said to span a vector space if every vector in the space can be expressed as a linear combination of the vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8807183587877814
      },
      {
        "text": "Span of a Set of Vectors is a Subspace: The span of a set of vectors is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8199185703007601
      },
      {
        "text": "The span of a set of vectors is the set of all linear combinations of the vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9356099044694072
      },
      {
        "text": "Linear Combination: A linear combination of vectors is an expression of the form $a1v1 + a2v2 + \\cdots + anvn$, where $vi$ are vectors in the set and $ai$ are scalars.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8486217085109488
      },
      {
        "text": "Span Theorem: If a set of vectors spans a vector space, then the set is a spanning set for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8338809340950534
      },
      {
        "text": "Linear Combination Property: The span of a set of vectors is closed under linear combinations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8553906572655116
      },
      {
        "text": "Span: The span of a set of vectors $\\{v1, v2, ..., vn\\}$, denoted by $span\\{v1, v2, ..., vn\\}$, is the set of all linear combinations of these vectors. In other words, it is the set of all vectors that can be expressed as $a1v1 + a2v2 + ... + anvn$, where $a1, a2, ..., a_n$ are scalars.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9255414704383156
      },
      {
        "text": "Span of Span: The span of the span of a set of vectors $\\{v1, v2, ..., v_n\\}$ is equal to the span of the original set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.819345110851484
      },
      {
        "text": "Spanning Set: A set of vectors that spans a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8543981137837082
      },
      {
        "text": "Linear Combination: A linear combination of a set of vectors is a vector that can be expressed as a linear combination of those vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8288306768221666
      },
      {
        "text": "Span: The span of a set of vectors is the set of all linear combinations of those vectors. It represents the set of all possible vectors that can be obtained by combining the vectors in the given set with scalar multiples.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9311166748739685
      },
      {
        "text": "Span of a vector space: The span of a vector space V is the set of all linear combinations of vectors in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.92991715525759
      },
      {
        "text": "Span of a subspace: The span of a subspace W is the set of all linear combinations of vectors in W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8522495815229472
      },
      {
        "text": "Linear combination: A linear combination of vectors u1, u2, ..., un is a vector v = a1u1 + a2u2 + ... + an*un, where a1, a2, ..., an are scalars.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8266484460515071
      },
      {
        "text": "Spanning set: A set of vectors that spans a vector space is called a spanning set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8284240400435138
      },
      {
        "text": "Span Theorem: The span of a set of vectors is equal to the smallest subspace that contains the set. This means that the span of a set of vectors is the smallest subspace that contains the set and is contained in the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8179627109075329
      },
      {
        "text": "Characterization of span: A set of vectors spans a vector space if and only if every vector in the space can be expressed as a linear combination of the vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8693720917558261
      },
      {
        "text": "Span: The set of all linear combinations of the vectors in the given set. It is denoted as $Span(\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n)$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9501766117637677
      },
      {
        "text": "Span of a Set: The span of a set of vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n\\}$ is the set of all linear combinations of these vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9384187195712393
      },
      {
        "text": "Linear Combination Theorem: Every vector in the span of a set of vectors can be expressed as a linear combination of the vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9068846406692652
      },
      {
        "text": "Span Notation: The span of a set of vectors is denoted as $Span(\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n)$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8221400078807567
      },
      {
        "text": "span(V) (span of a set of vectors V)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8161744424122866
      },
      {
        "text": "Span: The span of a set of vectors is the set of all linear combinations of those vectors. It represents the range or image of the linear transformation defined by the set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8969101021140595
      },
      {
        "text": "Span Theorem: A set of vectors spans a vector space if and only if every vector in the space can be expressed as a linear combination of the vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9048877188969208
      },
      {
        "text": "Spanning Set: A spanning set of a vector space is a set of vectors that spans the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8540724208606978
      },
      {
        "text": "Span of a Linearly Independent Set: The span of a linearly independent set is the set of all linear combinations of those vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8715198425192034
      },
      {
        "text": "Span of a Vector: The span of a vector is the set of all scalar multiples of that vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8321905203159514
      },
      {
        "text": "Spanning Set Theorem: A set of vectors is a spanning set of a vector space if and only if it spans the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8628500214177286
      },
      {
        "text": "Spanning Set Theorem: If a set of vectors spans a vector space, then every vector in the space can be expressed as a linear combination of the spanning set vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8909737058722718
      },
      {
        "text": "Definition: The span of a vector space is the set of all linear combinations of the vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9449767689835986
      },
      {
        "text": "Properties: The span of a vector space is equal to the closure of the original vector space under linear combinations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8425777143947064
      },
      {
        "text": "L: A linear combination of vectors in a set S.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8007608347276933
      },
      {
        "text": "Span: The span of a set of vectors is the set of all linear combinations of those vectors. It is also the set of all vectors that can be expressed as a linear combination of the vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9266296218951018
      },
      {
        "text": "Span: The set of all linear combinations of a set of vectors. The span of a set of vectors is the smallest subspace that contains all the vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9188492229233294
      },
      {
        "text": "Trivial Span Theorem: The span of a set of vectors $\\{v1, v2, ..., v_n\\}$ is equal to the set of all linear combinations of these vectors, which is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8745699597829729
      },
      {
        "text": "Span of a Linear Combination of Vectors: The span of a linear combination of vectors $\\{v1, v2, \\ldots, vn\\}$ is equal to the span of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8825632173332795
      },
      {
        "text": "Span of a Set of Linearly Independent Vectors: The set of all linear combinations of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9143777597704426
      },
      {
        "text": "Span of a Linear Combination of Vectors for Infinite-Dimensional Vector Spaces: The span of a linear combination of vectors is equal to the span of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8557223823862022
      },
      {
        "text": "Definition of span: The set of all linear combinations of a set of vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9331269965577528
      },
      {
        "text": "The span of a set of vectors is the set of all linear combinations of those vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9463616988797516
      }
    ]
  },
  {
    "representative_text": "Basis: A set of linearly independent vectors {v1, v2, ..., vn} ∈ V that span V, denoted as basis(V).",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 43,
    "detailed_sources": [
      {
        "text": "Basis: A set of linearly independent vectors {v1, v2, ..., vn} ∈ V that span V, denoted as basis(V).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis: A set of linearly independent vectors that span the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9081546126749505
      },
      {
        "text": "Span Basis: A basis for a vector space is a set of linearly independent vectors that span the entire space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9354094322039785
      },
      {
        "text": "Definition 4: A basis for a vector space V is a set of linearly independent vectors that span V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9248752069948193
      },
      {
        "text": "Basis: A set of linearly independent vectors that span a vector space V is called a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.973901039159603
      },
      {
        "text": "Basis Vectors are Linearly Independent: If a basis for a vector space contains a vector, then the basis is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8237837103488659
      },
      {
        "text": "A basis for a vector space is a set of vectors that spans the space and is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9151856188820028
      },
      {
        "text": "Span of a Basis: The span of a basis for a vector space is equal to the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8089875018751767
      },
      {
        "text": "Basis Property: A set of vectors is a basis for a vector space if and only if it is linearly independent and spans the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9268863149345521
      },
      {
        "text": "Spanning basis: A spanning set that is also a basis for the vector space is called a spanning basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8217203489999707
      },
      {
        "text": "Basis: A basis of a vector space is a set of vectors that spans the space and is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.959917656117933
      },
      {
        "text": "Basis Vectors: Vectors that form a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8465785925221285
      },
      {
        "text": "Spanning Property: A basis spans a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8745520502027371
      },
      {
        "text": "Spanning Basis Property: A basis with one vector spans a vector space with one dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8195475925995678
      },
      {
        "text": "Basis Spanning a Vector Space: If a set of vectors spans a vector space, then the set is a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9016959474505826
      },
      {
        "text": "Standard Basis: A basis consisting of standard basis vectors (e.g., (1, 0, 0), (0, 1, 0), (0, 0, 1)).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.801206560966226
      },
      {
        "text": "General Basis: A basis consisting of any vectors that span a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8537382037321595
      },
      {
        "text": "Basis: A set of vectors is said to be a basis of a vector space if it is both linearly independent and spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9460479574508706
      },
      {
        "text": "Basis is a Spanning Set: A basis is always a spanning set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8849827192231271
      },
      {
        "text": "Basis is Both Linearly Independent and Spanning: A basis is both a linearly independent set and a spanning set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9079104518190882
      },
      {
        "text": "Basis Vectors: A basis is a set of vectors that is both linearly independent and spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9599213115959065
      },
      {
        "text": "Basis: A set of vectors that spans a vector space and is linearly independent. It is a fundamental concept in linear algebra, as it allows us to describe a vector space in a unique way.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8863112967885198
      },
      {
        "text": "Free Basis: A set of linearly independent vectors that span the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9013143176436116
      },
      {
        "text": "Free Basis Implies Spanning: A free basis is a set of linearly independent vectors that span the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8329818203869371
      },
      {
        "text": "Basis(S): A set of vectors that spans the entire vector space and is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9472615182580169
      },
      {
        "text": "Basis of a Vector Space: A basis of a vector space is a free basis that spans the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8958896828001062
      },
      {
        "text": "Definition: A basis for a vector space is a set of vectors that spans the space and is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9353069887920777
      },
      {
        "text": "Properties: The basis of a vector space is equal to the span of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8596665685641752
      },
      {
        "text": "Properties: The free basis of a vector space is equal to the span of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8082303467203751
      },
      {
        "text": "Basis: A basis of a vector space is a set of vectors that spans the space and is linearly independent. Every vector space has a basis, and it is unique up to scalar multiples.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.93138194515872
      },
      {
        "text": "Basis for a Subspace: A set of vectors that span a subspace and are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9114937486833696
      },
      {
        "text": "Span and Basis: The span of a set of vectors is the set of all linear combinations of those vectors, while a basis is a set of vectors that spans the space and is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8756614545250454
      },
      {
        "text": "The concept of a \"basis of a subspace\": A set of vectors that spans a subspace and is linearly independent with respect to the subspace is called a basis of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9004229374162085
      },
      {
        "text": "The concept of a \"basis for a subspace\": A basis for a subspace is a set of vectors that spans the subspace and is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.898185544391288
      },
      {
        "text": "Basis is a Spanning Set: A basis for a vector space is also a spanning set for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9200468236194856
      },
      {
        "text": "Basis for a Subspace and its Span: A set of vectors that span a subspace and are linearly independent, and how this relates to the dimension of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8927949999269746
      },
      {
        "text": "Relationship between Span and Basis: The span of a set of vectors is equal to the set of all linear combinations of those vectors, while a basis for a vector space is a set of linearly independent vectors that spans the entire space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8548502575931434
      },
      {
        "text": "Relationship between Spanning Set and Basis: A basis of a vector space is a spanning set that is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9052800147163154
      },
      {
        "text": "The concept of a basis for a vector space, including the properties of a basis and the span of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8709730031913538
      },
      {
        "text": "The concept of \"Basis of a subspace\": A set of vectors that spans a subspace and is linearly independent with respect to the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8825766590099828
      },
      {
        "text": "Basis and Spanning Set Relationship: A basis is a set of vectors that is both linearly independent and spans the vector space. A spanning set is a set of vectors that spans the vector space but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9128831867947799
      },
      {
        "text": "Definition of basis: A set of vectors that spans the vector space and is linearly independent.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9254536247819598
      },
      {
        "text": "Basis of a Vector Space: A set of vectors that spans the vector space and is linearly independent, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8638037675604171
      }
    ]
  },
  {
    "representative_text": "Linearly Independent: A set of vectors {v1, v2, ..., vn} ∈ V such that the only linear combination a1v1 + a2v2 + ... + anvn = 0 is when all coefficients a1, a2, ..., an = 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 47,
    "detailed_sources": [
      {
        "text": "Linearly Independent: A set of vectors {v1, v2, ..., vn} ∈ V such that the only linear combination a1v1 + a2v2 + ... + anvn = 0 is when all coefficients a1, a2, ..., an = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linearly Dependent: A set of vectors {v1, v2, ..., vn} ∈ V such that there exists a non-trivial linear combination a1v1 + a2v2 + ... + anvn = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9073397297303276
      },
      {
        "text": "Linear Independence Theorem: A set of vectors {v1, v2, ..., vn} is linearly independent if and only if the only linear combination a1v1 + a2v2 + ... + anvn = 0 is when all coefficients a1, a2, ..., an = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8661967947677462
      },
      {
        "text": "Linear Independence: A set of vectors is linearly independent if none of the vectors can be expressed as a linear combination of the others.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8899073820239235
      },
      {
        "text": "Linear Independence Theorem: A set of vectors is linearly independent if and only if it has the maximum possible number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8460314656505733
      },
      {
        "text": "Linear Independence Dimension: The number of linearly independent vectors in a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8165976047073453
      },
      {
        "text": "Linear Dependence: A set of vectors is linearly dependent if at least one vector can be expressed as a linear combination of the others.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8892053205838076
      },
      {
        "text": "Linear independence: A set of vectors v1, v2, ..., vn is said to be linearly independent if the only way to express the zero vector as a linear combination of these vectors is with all coefficients equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.957390244716654
      },
      {
        "text": "Theorem 1: A set of vectors v1, v2, ..., vn is linearly independent if and only if the only way to express the zero vector as a linear combination of these vectors is with all coefficients equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8593481164833283
      },
      {
        "text": "A set of vectors is said to be linearly independent if none of the vectors in the set can be expressed as a linear combination of the other vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8771684211888867
      },
      {
        "text": "A set of vectors is said to be linearly dependent if at least one vector in the set can be expressed as a linear combination of the other vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8356343840873639
      },
      {
        "text": "Trivial Solution Theorem: If a set of vectors is linearly independent, then the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8121623803644659
      },
      {
        "text": "Linear Independence Criterion: If the vectors are linearly independent, then the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8566031426731602
      },
      {
        "text": "Linear Independence: A set of vectors $\\{v1, v2, ..., vn\\}$ is said to be linearly independent if the equation $a1v1 + a2v2 + ... + anvn = 0$ implies that $a1 = a2 = ... = an = 0$. This means that the only way to express the zero vector as a linear combination of the vectors is with all coefficients being zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9365248642814128
      },
      {
        "text": "Linear Independence: A set of vectors is said to be linearly independent if none of the vectors in the set can be expressed as a linear combination of the others. In other words, the only way to express the zero vector as a linear combination of the vectors in the set is with all coefficients being zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9333602197035171
      },
      {
        "text": "Linearly Independent Set: A set of vectors is linearly independent if the equation $a1v1 + a2v2 + \\cdots + anvn = 0$, where $vi$ are the vectors in the set and $ai$ are scalars, has only the trivial solution ($a1 = a2 = \\cdots = a_n = 0$).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9259335552166874
      },
      {
        "text": "Trivial Solution Theorem: If a set of vectors is linearly independent, then the only solution to the equation $a1v1 + a2v2 + \\cdots + anvn = 0$ is $a1 = a2 = \\cdots = a_n = 0$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8389226145658308
      },
      {
        "text": "Two Vectors: Two vectors $v1$ and $v2$ are linearly independent if the equation $a1v1 + a2v2 = 0$ has only the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8634745956661221
      },
      {
        "text": "Three Vectors: Three vectors $v1$, $v2$, and $v3$ are linearly independent if the equation $a1v1 + a2v2 + a3v_3 = 0$ has only the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8433449521420476
      },
      {
        "text": "Four Vectors: Four vectors $v1$, $v2$, $v3$, and $v4$ are linearly independent if the equation $a1v1 + a2v2 + a3v3 + a4v4 = 0$ has only the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8086847080278179
      },
      {
        "text": "Characterization of linear independence: A set of vectors is linearly independent if and only if the only way to express the zero vector as a linear combination of the vectors is with all coefficients equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9317985386760872
      },
      {
        "text": "Linearly Independent Vectors: A set of vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}n\\}$ is said to be linearly independent if the equation $a1\\mathbf{v}1 + a2\\mathbf{v}2 + ... + an\\mathbf{v}n = \\mathbf{0}$ implies that $a1 = a2 = ... = an = 0$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9535783700617468
      },
      {
        "text": "Linearly Independent Set: A set of vectors where none of the vectors can be expressed as a linear combination of the others.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8774911291380535
      },
      {
        "text": "Linear Independence of a Set: A set of vectors is linearly independent if and only if the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has only the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9310470187686575
      },
      {
        "text": "Linear Independence of a Set: A set of vectors is linearly independent if and only if the only linear combination of the vectors that equals the zero vector is the trivial solution (i.e., all coefficients are zero).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9538238582685222
      },
      {
        "text": "Linear Independence Theorem (Fundamental Theorem of Linear Independence): A set of vectors is linearly independent if and only if the only linear combination of those vectors that equals the zero vector is the trivial solution (i.e., all coefficients are zero).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9185901017494613
      },
      {
        "text": "Proof of Linear Independence Theorem: Suppose a set of vectors is linearly independent. If the only linear combination of those vectors that equals the zero vector is the trivial solution, then any linear combination of those vectors must equal the zero vector. Conversely, if the only linear combination of those vectors that equals the zero vector is the trivial solution, then the set of vectors is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8633848777523702
      },
      {
        "text": "Linear Independence of a Set: A set of vectors $\\{v1, v2, \\ldots, vn\\}$ is linearly independent if the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ implies that all $ai$ are zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9419592685639031
      },
      {
        "text": "Linearly Independent Sets with Zero Vectors: A set of vectors {v1, v2, ..., vn} is linearly independent even if it contains zero vectors, but it is not a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8110997365777334
      },
      {
        "text": "Linear Independence of Sets of Infinite Vectors: A set of infinite vectors {v1, v2, ...} is linearly independent if and only if the only linear combination a1v1 + a2v2 + ... = 0 is when all coefficients a1, a2, ... = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.866990440300694
      },
      {
        "text": "Linear Independence of Vector Spaces: A vector space V is linearly independent if and only if the only linear combination a1v1 + a2v2 + ... = 0 is when all coefficients a1, a2, ... = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9142594971134065
      },
      {
        "text": "Linear Independence of Subspaces: A subspace W of a vector space V is linearly independent if and only if the only linear combination av + bw = 0 is when a = b = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8172198027951076
      },
      {
        "text": "Linear Independence of the Standard Basis: A set of vectors that is linearly independent, where each vector has a 1 in a different position and 0s elsewhere.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8027524744174144
      },
      {
        "text": "Linear Independence of the Null Space: A set of vectors is linearly independent if and only if the null space of the corresponding matrix has a trivial solution (i.e., only the zero vector).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9055960586219529
      },
      {
        "text": "The Fundamental Theorem of Linear Independence: This theorem states that a set of vectors is linearly independent if and only if the equation $a1v1 + a2v2 + \\cdots + anvn = 0$ has only the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8691068494307845
      },
      {
        "text": "Linear independence of a set of polynomials: A set of polynomials is linearly independent if and only if no polynomial in the set can be expressed as a linear combination of the other polynomials.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8643914284567538
      },
      {
        "text": "Linear Independence of a Linear Combination: A set of vectors $\\{v1, v2, \\ldots, vn\\}$ is linearly independent if and only if the linear combination of the vectors is equal to zero only if all coefficients are zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9471374414968369
      },
      {
        "text": "Linear Independence of the Standard Basis: A set of vectors that is linearly independent, where each vector has a 1 in a different position and 0s elsewhere, and how this relates to the dimension of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8402948501646728
      },
      {
        "text": "Linear Independence and Dependence of Linear Combinations: A set of vectors {v1, v2, ..., vn} is linearly independent if and only if the only linear combination a1v1 + a2v2 + ... + anvn = 0 is when all coefficients a1, a2, ..., an = 0. A set of vectors {v1, v2, ..., vn} is linearly dependent if and only if there exists a non-trivial linear combination a1v1 + a2v2 + ... + anvn = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9103212650747774
      },
      {
        "text": "The relationship between linear independence and the intersection of subspaces: A set of vectors is linearly independent if and only if the intersection of its span and a subspace is the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8584293542130427
      },
      {
        "text": "Theorem on the Existence of a Linearly Independent Set: This theorem states that a set of vectors is linearly independent if and only if the only linear combination of the vectors is the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8782275548374469
      },
      {
        "text": "Theorem on the Existence of a Non-Trivial Linear Combination: This theorem states that a set of vectors is linearly dependent if and only if there exists a non-trivial linear combination of the vectors that equals the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.830170883928814
      },
      {
        "text": "Linear Independence of a Set of Functions: A theorem that states that a set of functions is linearly independent if and only if the only linear combination that equals the zero function is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8496214713909636
      },
      {
        "text": "Linear Independence of Linear Combinations: This concept states that a linear combination of vectors is linearly independent if and only if the coefficients of the linear combination are non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8448384014353536
      },
      {
        "text": "Linear Independence and Boundedness: A set of vectors is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is not necessarily have been linear independence: A set of Linear Independence of a set of a set of this of theore of linear independence of Linear Independence of a set of Linear Independence of Linear Independence of Linear Independence of linear in topological spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8338073713757703
      },
      {
        "text": "Definition of linear independence: A set of vectors is linearly independent if none of the vectors in the set can be expressed as a linear combination of the other vectors in the set.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9023948582136545
      },
      {
        "text": "A set of vectors is linearly independent if none of the vectors can be expressed as a linear combination of the others.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8921222242608469
      }
    ]
  },
  {
    "representative_text": "Commutativity of Addition: Vector addition is commutative, i.e., u + v = v + u for all u, v ∈ V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Commutativity of Addition: Vector addition is commutative, i.e., u + v = v + u for all u, v ∈ V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Associativity of Addition: Vector addition is associative, i.e., (u + v) + w = u + (v + w) for all u, v, w ∈ V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9045433196070345
      },
      {
        "text": "Commutativity of Addition: The order in which vectors are added does not change the result.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9099559842389157
      },
      {
        "text": "Associativity of Addition: The order in which vectors are added, when more than two, does not change the result.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.923546716111233
      },
      {
        "text": "Addition of vectors: The sum of two vectors u, v in V is defined as u + v = v + u, and it results in a vector in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8483797232952608
      }
    ]
  },
  {
    "representative_text": "Existence of Additive Identity: There exists a vector 0 ∈ V such that u + 0 = u for all u ∈ V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Existence of Additive Identity: There exists a vector 0 ∈ V such that u + 0 = u for all u ∈ V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of Additive Identity (Zero Vector): There exists a vector, often denoted as 0, such that the sum of a vector and the zero vector is the original vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9134464858554303
      },
      {
        "text": "The zero vector is the additive identity for vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.868528533865644
      }
    ]
  },
  {
    "representative_text": "Existence of Additive Inverse: For each vector u ∈ V, there exists a vector -u ∈ V such that u + (-u) = 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Existence of Additive Inverse: For each vector u ∈ V, there exists a vector -u ∈ V such that u + (-u) = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of Additive Inverse: For every vector in the space, there exists another vector that, when added together, results in the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9357140272445443
      }
    ]
  },
  {
    "representative_text": "Distributivity of Scalar Multiplication over Vector Addition: c(u + v) = cu + cv for all c ∈ F and u, v ∈ V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Distributivity of Scalar Multiplication over Vector Addition: c(u + v) = cu + cv for all c ∈ F and u, v ∈ V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Distributive Property: Scalar multiplication distributes over vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8780379517551689
      },
      {
        "text": "Distributivity of Scalar Multiplication over Vector Addition: The scalar can be distributed over the sum of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9621049066211944
      },
      {
        "text": "Scalar Multiplication Distributive Law: Multiplying a scalar by a product of vectors is the same as multiplying the scalar by each vector separately.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.860171517406036
      },
      {
        "text": "Addition of Scalar Multiples: The sum of scalar multiples of vectors is another linear combination of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8220929073576382
      },
      {
        "text": "Scalar Multiplication and Linear Combinations: The scalar multiplication of a vector v with a scalar c can be extended to linear combinations of the form a1v + a2v + ... + anvn, where a1, a2, ..., an are scalars in F.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8156202993366182
      },
      {
        "text": "Properties of scalar multiplication:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8730648218038294
      }
    ]
  },
  {
    "representative_text": "Scalar Multiplication is Associative: (cd)u = c(du) for all c, d ∈ F and u ∈ V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar Multiplication is Associative: (cd)u = c(du) for all c, d ∈ F and u ∈ V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Existence of Basis Theorem: Every vector space V has a basis, i.e., there exists a set of linearly independent vectors {v1, v2, ..., vn} ∈ V that span V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "Existence of Basis Theorem: Every vector space V has a basis, i.e., there exists a set of linearly independent vectors {v1, v2, ..., vn} ∈ V that span V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem 2: A set of vectors v1, v2, ..., vn is a basis for a vector space V if and only if it is linearly independent and spans V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8283144794301571
      },
      {
        "text": "Basis Theorem: If a set of vectors is linearly independent and spans a vector space, then the set is a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9170923848471195
      },
      {
        "text": "Basis Theorem: Every vector space has a basis, and every basis is a linearly independent set of vectors that spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.953572817435425
      },
      {
        "text": "Existence of Basis: If a set of vectors is linearly independent and spans a vector space, then it is a basis for that space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.916740247709946
      },
      {
        "text": "Trivial Basis Theorem: Every vector space has a basis, which is a linearly independent set that spans the entire space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9024176325112405
      },
      {
        "text": "Existence: Every vector space has a basis, which is a linearly independent set that spans the entire space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8995052347574861
      },
      {
        "text": "Basis Theorem: Every vector space has a basis. The basis is unique up to scalar multiples.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9094308292364416
      },
      {
        "text": "Fundamental Theorem of Finite-Dimensional Vector Spaces: This theorem states that every finite-dimensional vector space has a basis, and that every basis has the same number of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8277047043670669
      },
      {
        "text": "The Fundamental Theorem of Finite-Dimensional Vector Spaces: Every finite-dimensional vector space has a basis, and this theorem provides a fundamental connection between linear independence and bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8866974630918257
      },
      {
        "text": "Existence of a Basis for a Vector Space: A basis for a vector space exists if and only if the set of vectors spans the entire space and is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8986268995007856
      },
      {
        "text": "Existence of a Basis for a Vector Space with respect to Linear Transformations: A basis for a vector space exists if and only if the set of vectors spans the entire space, is linearly independent, and is also a basis for the range of the linear transformation associated with the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8667778031900247
      },
      {
        "text": "Basis Theorem for Finite-Dimensional Vector Spaces: This theorem states that every finite-dimensional vector space has a basis, and every basis is a linearly independent set of vectors that spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9277683155443408
      },
      {
        "text": "The Fundamental Theorem of Finite-Dimensional Vector Spaces: A theorem that states that every finite-dimensional vector space has a basis in Non-",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8620471522699766
      }
    ]
  },
  {
    "representative_text": "Basis Extension Theorem: If {v1, v2, ..., vn} is a basis for V and w ∈ V, then there exist scalars a1, a2, ..., an such that w = a1v1 + a2v2 + ... + anvn.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Basis Extension Theorem: If {v1, v2, ..., vn} is a basis for V and w ∈ V, then there exist scalars a1, a2, ..., an such that w = a1v1 + a2v2 + ... + anvn.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Basis Extension Theorem for Vector Spaces: This theorem states that if {v1, v2, ..., vn} is a basis for V and w ∈ V, then there exist scalars a1, a2, ..., an such that w = a1v1 + a2v2 + ... + anvn.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9560867685423625
      }
    ]
  },
  {
    "representative_text": "Dimension Theorem: The dimension of a vector space V is equal to the number of vectors in a basis for V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 39,
    "detailed_sources": [
      {
        "text": "Dimension Theorem: The dimension of a vector space V is equal to the number of vectors in a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span Basis Theorem: Every vector space has a basis, and every basis has the same number of vectors as the dimension of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8753261740200422
      },
      {
        "text": "Dimension: The number of vectors in a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8235439674189664
      },
      {
        "text": "Span Dimension: The number of linearly independent vectors in the span of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8091473214953993
      },
      {
        "text": "Definition 3: The dimension of a vector space V is the number of vectors in a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8836650740722979
      },
      {
        "text": "Dimension: The dimension of a vector space V is the number of vectors in a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9364132310669804
      },
      {
        "text": "Theorem 1: The dimension of a vector space V is equal to the number of vectors in a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8728911161437829
      },
      {
        "text": "Dimensionality: The dimensionality of a vector space V is the number of dimensions in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8579112681989773
      },
      {
        "text": "$\\dim(V)$: The dimension of the vector space $V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8009983934281705
      },
      {
        "text": "Dimension Theorem: The dimension of a vector space is equal to the number of vectors in a spanning basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9306504576593969
      },
      {
        "text": "Dimension Theorem: The dimension of a vector space is equal to the maximum number of linearly independent vectors in the span of the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8908129678250027
      },
      {
        "text": "Trivial Basis Theorem: If a vector space has a basis, then the basis has the same number of vectors as the dimension of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8595035125224485
      },
      {
        "text": "Dimension of a Vector Space: The dimension of a vector space is equal to the number of vectors in a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9284210401503086
      },
      {
        "text": "Dimension: The number of vectors in a basis for a vector space. It is a fundamental concept in linear algebra, as it allows us to describe the structure of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8358585856672808
      },
      {
        "text": "Properties: The dimension of a vector space is equal to the number of vectors in a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9437918921923387
      },
      {
        "text": "Dimension Theorem: The dimension of a vector space $V$ is equal to the number of vectors in a basis for $V$, i.e., $\\dim(V) = \\text{dim}(\\text{span}(\\{v1, v2, \\ldots, v_n\\}))$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9353338593676335
      },
      {
        "text": "Dimension: The number of linearly independent vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8717765394659573
      },
      {
        "text": "Dimension of a Subspace: The dimension of a subspace W of a vector space V is equal to the number of vectors in a basis for W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8510741896600937
      },
      {
        "text": "Dimension of a Vector Space: The dimension of a vector space is equal to the number of vectors in a basis for the space. This concept is closely related to linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9323810576594134
      },
      {
        "text": "The relationship between the dimension of a vector space and the number of vectors in a basis: The dimension of a vector space is equal to the number of vectors in a basis, but not necessarily the maximum number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8803564346773565
      },
      {
        "text": "The concept of a \"dimension of a subspace\": The dimension of a subspace is the number of vectors in a basis of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8822984426416942
      },
      {
        "text": "The Dimension of a Vector Space: The dimension of a vector space is the number of vectors in a basis for the space, which is equivalent to the number of linearly independent vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9378418439491893
      },
      {
        "text": "Dimension Theorem: The dimension of a vector space is equal to the maximum number of linearly independent vectors in the span of the set. This theorem is closely related to the concept of basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9040726176585887
      },
      {
        "text": "Dimension of a Vector Space using Linear Independence: The dimension of a vector space can be found using the concept of linear independence, where the dimension is equal to the number of linearly independent vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8679637515930292
      },
      {
        "text": "Dimension of a Vector Space: The dimension of a vector space $V$ is equal to the number of vectors in a basis for $V$, and the dimension of the null space of a linear transformation $T: V \\to W$ is equal to the nullity of $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8905737401047324
      },
      {
        "text": "Dimension and Basis: The dimension of a vector space is the number of vectors in a basis for the vector space. A basis for a vector space is a set of linearly independent vectors that span the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9317631035418015
      },
      {
        "text": "Linear Independence and the Dimension of a Vector Space: The relationship between linear independence and the dimension of a vector space. Specifically, the dimension of a vector space is equal to the maximum number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.853169041565236
      },
      {
        "text": "Dimension of a Vector Space: The dimension of a vector space is equal to the number of vectors in a basis for the space, which is also equal to the maximum number of linearly independent vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9326174907501985
      },
      {
        "text": "Existence of a Basis in a Vector Space: The existence of a basis in a vector space can be used to determine the dimension of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8443161804356226
      },
      {
        "text": "Dimension of a Vector Space with a Non-Unique Basis: The dimension of a vector space can be used to determine the number of linearly independent vectors required to span the space, even if the basis is not unique.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8698764927937419
      },
      {
        "text": "Dimension of a Vector Space with a Non-Standard Linear Independence: The dimension of a vector space can be used to determine the number of linearly independent vectors required to span the space, even if the linear independence is not standard.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8423625904132733
      },
      {
        "text": "Dimension of a Vector Space using Basis: The dimension of a vector space can be found using the concept of basis, where the dimension is equal to the number of vectors in the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9174702276285929
      },
      {
        "text": "The Dimension of the Span of a Vector Space: The dimension of the span of a vector space is equal to the number of linearly independent vectors in a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9161447904875832
      },
      {
        "text": "Dimension of a Vector Space with respect to Linear Transformations: The dimension of a vector space is equal to the number of vectors in a basis for the space, which is also equal to the maximum number of linearly independent vectors in the space and is also equal to the dimension of the range of the linear transformation associated with the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8867370079413126
      },
      {
        "text": "Dimension of a Vector Space: The dimension of a vector space is the maximum number of linearly independent vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9025031582687293
      },
      {
        "text": "Dimension of the Span of a Set using the Basis Theorem: The dimension of the span of a set of vectors can be found using the basis theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8335856858041337
      },
      {
        "text": "Theorem on the Dimension of a Vector Space: This theorem states that the dimension of a vector space is equal to the number of vectors in a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8924508721336651
      },
      {
        "text": "Dimension of the Vector Space using the Fundamental Theorem of Linear Algebra: The dimension of a vector space can be found using the Fundamental Theorem of Linear Algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8121173494872971
      },
      {
        "text": "Dimension of a Subspace: The dimension of a subspace is the number of vectors in a basis of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8766277023734854
      }
    ]
  },
  {
    "representative_text": "Null Space: The set of vectors x ∈ V such that Ax = 0, where A is a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 21,
    "detailed_sources": [
      {
        "text": "Null Space: The set of vectors x ∈ V such that Ax = 0, where A is a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Null Space: The null space of a linear transformation is the set of vectors that are mapped to the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8817020782008016
      },
      {
        "text": "Null Space: The set of vectors v such that Av = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9168738165842589
      },
      {
        "text": "Null Space: The set of all vectors that satisfy the equation Ax = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9470771210281576
      },
      {
        "text": "Null Space: The null space of a matrix is the set of all vectors that are mapped to the zero vector by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9342002207185166
      },
      {
        "text": "Null Space: The null space of a linear transformation $T: V \\to W$ is the set of all vectors $v \\in V$ such that $T(v) = 0$, i.e., $\\text{Null}(T) = \\{v \\in V \\mid T(v) = 0\\}$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8690325989210284
      },
      {
        "text": "The Nullspace of a Matrix: The set of all vectors that are mapped to the zero vector by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9449707475182673
      },
      {
        "text": "Null Space and Column Space: The null space of a matrix A is the set of vectors x such that Ax = 0, and the column space of A is the span of the columns of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8367710505301572
      },
      {
        "text": "Null Space and Column Space: The null space and column space of a matrix are the sets of vectors that, when multiplied by the matrix, result in the zero vector and the set of vectors that are linear combinations of the columns of the matrix, respectively.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8722604503412084
      },
      {
        "text": "The Null Space of a Matrix: The null space of a matrix is the set of all vectors that are mapped to the zero vector. The nullity of a matrix is equal to the dimension of its null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8942118490608959
      },
      {
        "text": "Null Space: The null space of a matrix A is the set of all vectors v such that Av = 0. Eigenvalues and eigenvectors can be used to analyze the null space of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8404560644007129
      },
      {
        "text": "The Null Space of a Matrix: The null space of a matrix is the set of all vectors that are mapped to the zero vector by the matrix, which is equivalent to the set of all linear combinations of the null space vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9228898579956615
      },
      {
        "text": "The null space of a matrix: The null space of a matrix is the set of vectors that are mapped to the zero vector by the matrix. This concept is related to the concept of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8702026064223041
      },
      {
        "text": "Null Space of a Linear Transformation: The null space of a linear transformation $T: V \\to W$ is a subspace of $V$ that is spanned by the eigenvectors of $T$ corresponding to zero eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8233577809322674
      },
      {
        "text": "Null Space and Determinant: The null space of a matrix A is the set of all vectors x such that Ax = 0. The dimension of the null space of A is equal to the nullity of A, which can be determined using the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8064478699821762
      },
      {
        "text": "The concept of the null space being the set of vectors that are mapped to the zero vector",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8720939253142703
      },
      {
        "text": "The Null Space Theorem: The null space theorem states that the null space of a matrix is a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8176889408470522
      },
      {
        "text": "The concept of a null space and its relation to the orthogonal complement: The null space of a linear transformation is the set of vectors that the transformation maps to the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8847861774326804
      },
      {
        "text": "The Null Space of a Matrix with Respect to a Given Basis: The null space of a matrix with respect to a given basis is the set of all vectors that can be expressed as a linear combination of the basis vectors and map to the zero vector when multiplied by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8827367586783638
      },
      {
        "text": "The Null Space and Column Space Theorem for Vector Spaces: This theorem states that the null space of a matrix A is the set of vectors x such that Ax = 0, and the column space of A is the span of the columns of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.854812497100928
      },
      {
        "text": "Null space and column space: The set of all vectors that, when multiplied by a matrix, result in the zero vector, and the set of all vectors that can be expressed as linear combinations of the columns of a matrix, respectively.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8781202889338304
      }
    ]
  },
  {
    "representative_text": "Column Space: The span of the columns of a matrix A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Column Space: The span of the columns of a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Row Space: The span of the rows of a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9159092473421506
      },
      {
        "text": "Column Space: The set of all linear combinations of the columns of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8960625832856114
      },
      {
        "text": "Row Space: The set of all linear combinations of the rows of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9147464504315876
      },
      {
        "text": "Column Space: The column space of a matrix is the set of all linear combinations of the columns of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8887573811451774
      },
      {
        "text": "Column Space: The column space of a matrix $A$ is the set of all linear combinations of the columns of $A$, i.e., $\\text{col}(A) = \\{a1A \\mid a1 \\in \\mathbb{R}\\}$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8832255129446129
      },
      {
        "text": "The Column Space of a Matrix: The set of all linear combinations of the columns of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9450175915413526
      },
      {
        "text": "Row and column spaces: The row space of a matrix is equal to the span of its rows, and the column space of a matrix is equal to the span of its columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.839086047326941
      },
      {
        "text": "Column Space of a Matrix: The column space of a matrix $A$ is equal to the span of the columns of $A$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8908960888610105
      },
      {
        "text": "The Column Space and Its Applications: The column space of a matrix is the set of vectors that are linear combinations of the columns of the matrix. This concept has many applications in linear algebra, including the fact that the column space of a matrix is equal to the image of the linear transformation represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8250218032325067
      },
      {
        "text": "The Column Space of a Matrix: The column space of a matrix is the set of vectors that are linear combinations of the columns of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9358852739130225
      },
      {
        "text": "The concept of the column space being the set of vectors that are linear combinations of the columns of a matrix",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8828970693647891
      }
    ]
  },
  {
    "representative_text": "Cofactor Expansion: A method for computing the determinant of a matrix using the cofactors of each element.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 36,
    "detailed_sources": [
      {
        "text": "Cofactor Expansion: A method for computing the determinant of a matrix using the cofactors of each element.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Expansion by Minors: A method for computing the determinant of a square matrix by expanding along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8905969691031667
      },
      {
        "text": "Cofactor Expansion: The determinant of a matrix can be calculated using cofactor expansion along any row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.876366722892646
      },
      {
        "text": "Laplace Expansion: The determinant of a matrix can be calculated using Laplace expansion, which involves expanding along a row or column and using cofactors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8866345942967504
      },
      {
        "text": "Cofactor Expansion: The determinant of a matrix can be expressed as a sum of the products of its elements and their cofactors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9205462906048216
      },
      {
        "text": "Determinant of a Square Matrix: The determinant of a square matrix A can be calculated using the Laplace expansion formula, which involves expanding the determinant along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.844446789449917
      },
      {
        "text": "Cofactor Expansion along a Row: The cofactor expansion along a row of a matrix A is calculated as det(A) = Σ(aij * Cij), where Cij is the cofactor of the element aij.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8507954703069985
      },
      {
        "text": "Cofactor Expansion along a Column: The cofactor expansion along a column of a matrix A is calculated as det(A) = Σ(aij * Cij), where Cij is the cofactor of the element aij.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8603509275726144
      },
      {
        "text": "Laplace Expansion: The Laplace expansion formula states that the determinant of a square matrix A can be calculated by expanding along a row or column using the formula det(A) = Σ(aij * Cij), where Cij is the cofactor of the element aij.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8839959423033856
      },
      {
        "text": "Laplace Expansion: A method for calculating the determinant of a square matrix using cofactor expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9210570110723946
      },
      {
        "text": "Expansion by Minors: The determinant of an nxn matrix can be calculated by expanding along any row or column, using the formula:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8678823142811716
      },
      {
        "text": "Laplace Expansion: A method for computing the determinant of a matrix by expanding along a row or column using Laplace's formula.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.902561874921415
      },
      {
        "text": "Statement: The determinant of a square matrix can be computed using the Laplace expansion along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.891149507605774
      },
      {
        "text": "Laplace Expansion Formula: The formula for computing the determinant of a square matrix using the Laplace expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9001379441625924
      },
      {
        "text": "Statement: The formula for computing the determinant of a square matrix using the Laplace expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.865387578579685
      },
      {
        "text": "Expansion by Minors: det(A) = ∑[a{ij}M{ij}], where M{ij} is the minor of a{ij}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8830874096295234
      },
      {
        "text": "Cofactor Expansion: det(A) = ∑[±a{ij}C{ij}], where C{ij} is the cofactor of a{ij}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9112242270951941
      },
      {
        "text": "Laplace Expansion: det(A) = ∑[a{ij}(-1)^{i+j}M{ij}].",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.917506558730782
      },
      {
        "text": "The Laplace Expansion: A method for finding the determinant of a matrix by expanding it along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.914276001826813
      },
      {
        "text": "The Leibniz Formula: A method for finding the determinant of a matrix by using a sum of products of its elements and their cofactors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8588974635681236
      },
      {
        "text": "Minor Determinant Theorem: This theorem states that the determinant of a matrix is equal to the sum of the products of the elements of any row (or column) and their respective minors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8146293303184131
      },
      {
        "text": "Laplace's Theorem: This theorem states that the determinant of a matrix is equal to the sum of the products of the elements of any row (or column) and their respective cofactors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8689872872790455
      },
      {
        "text": "Cauchy-Binet Formula: This formula is used to calculate the determinant of a matrix using the Cauchy-Binet expansion, which involves expanding the determinant along a row or column using the cofactors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.862420590481452
      },
      {
        "text": "Laplace Expansion: This method is used to calculate the determinant of a matrix by expanding along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9303379488778085
      },
      {
        "text": "Laplace's Theorem: This is a theorem that states that the determinant of a matrix can be calculated using a formula that involves the determinants of smaller submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8643971242232448
      },
      {
        "text": "Cofactor Expansion: The cofactor expansion of a matrix is a method for computing the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9177999841940719
      },
      {
        "text": "Determinant of a Matrix using the Laplace Expansion: This involves the study of the Laplace expansion, which is a method for calculating the determinant of a matrix by expanding along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8954337616235928
      },
      {
        "text": "Laplace Expansion by minors: This method is used to calculate the determinant of a matrix by expanding along a row or column using the minors of the elements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9197672149663263
      },
      {
        "text": "The properties of the determinant of a linear transformation, including the concept of the adjoint operator and the cofactor expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8129026522364144
      },
      {
        "text": "Determinant of a Matrix using the Cauchy-Binet Formula: This involves the study of the Cauchy-Binet formula, which is a method for calculating the determinant of a matrix by expanding along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.877148309279348
      },
      {
        "text": "Buchberger's Formula: A formula for computing the determinant of a matrix using its minors and cofactors, which is an extension of the Laplace expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8213040706807593
      },
      {
        "text": "Laplace Expansion by Minors: A method for computing the determinant of a matrix using minors and cofactors, which is an extension of the Laplace expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9091022058809437
      },
      {
        "text": "Laplace Expansion by Cofactors: A method for computing the determinant of a matrix using cofactors, which is an extension of the Laplace expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9132423147571068
      },
      {
        "text": "The Cofactor Expansion: This concept is related to the determinant of a linear transformation and states that the cofactor expansion of a matrix is a method for computing the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8745749947105531
      },
      {
        "text": "Laplace Expansion for Block Matrices: A generalization of the Laplace expansion for block matrices, which involves expanding along a row or column and using cofactors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8615861107261199
      },
      {
        "text": "The Cofactor Expansion for Linear Operators: This concept is related to the determinant of linear operators and involves understanding the properties of the cofactor expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8055446235809859
      }
    ]
  },
  {
    "representative_text": "Vector Space (V): A set of vectors that satisfies certain properties, including closure under addition and scalar multiplication, commutativity of addition, associativity of addition, distributive property, existence of additive identity (zero vector), and existence of additive inverse (negative vector).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector Space (V): A set of vectors that satisfies certain properties, including closure under addition and scalar multiplication, commutativity of addition, associativity of addition, distributive property, existence of additive identity (zero vector), and existence of additive inverse (negative vector).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector: An element of a vector space, often represented as a mathematical object that has both magnitude (length) and direction.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Vector: An element of a vector space, often represented as a mathematical object that has both magnitude (length) and direction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Definition: A vector is a mathematical object that has both magnitude (length) and direction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9028143233150769
      },
      {
        "text": "Vector: A mathematical object that has both magnitude (length) and direction. In linear algebra, vectors are often represented as arrows in a coordinate system.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.932355614198386
      },
      {
        "text": "Definition of a vector: A mathematical object with both magnitude (length) and direction.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9555248425765778
      },
      {
        "text": "A vector is a mathematical object with both magnitude (length) and direction.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9235990941300026
      }
    ]
  },
  {
    "representative_text": "Scalar: A real number that can be multiplied by a vector to produce another vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Scalar: A real number that can be multiplied by a vector to produce another vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Scalar: A single number used to multiply a matrix or vector. It is denoted by a lowercase letter (e.g., a, b, c).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8551956596226342
      },
      {
        "text": "Scalar: A single number, often used as a multiplier or divisor in matrix operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9242494777657093
      }
    ]
  },
  {
    "representative_text": "Addition: The operation of combining two or more vectors by connecting their corresponding components.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Addition: The operation of combining two or more vectors by connecting their corresponding components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Vector Addition: The operation of adding two or more vectors together to produce a new vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9399100200780938
      }
    ]
  },
  {
    "representative_text": "Scalar Multiplication: The operation of multiplying a vector by a scalar, which results in a new vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Scalar Multiplication: The operation of multiplying a vector by a scalar, which results in a new vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Scalar multiplication of vectors: The product of a scalar c and a vector u in V is defined as c·u, and it results in a vector in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8324429979334611
      },
      {
        "text": "Scalar Multiplication: Multiplying a vector by a scalar is another vector in the same space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.899851786467083
      },
      {
        "text": "Scalar Multiplication: The product of a scalar and a vector, denoted by cv, where c is the scalar and v is the vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8791544240432072
      },
      {
        "text": "Scalar Multiplication: The product of a vector and a scalar is a new vector with the same direction as the original vector, but with a magnitude equal to the product of the original magnitude and the scalar.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.888130042467594
      },
      {
        "text": "Scalar multiplication: the product of a vector and a scalar is the vector obtained by multiplying each component of the original vector by the scalar.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9111619766479666
      }
    ]
  },
  {
    "representative_text": "Closure under Addition: The result of adding two vectors is always another vector in the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Closure under Addition: The result of adding two vectors is always another vector in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "A set V of vectors that is closed under addition and scalar multiplication, and satisfies the following properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8168079624095502
      },
      {
        "text": "Closure under Addition: A subspace must be closed under the operation of vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8436453070385668
      }
    ]
  },
  {
    "representative_text": "Closure under Scalar Multiplication: The result of multiplying a vector by a scalar is always another vector in the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Closure under Scalar Multiplication: The result of multiplying a vector by a scalar is always another vector in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Closure under Scalar Multiplication: A subspace must be closed under the operation of scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8843619834041841
      },
      {
        "text": "Scalar Multiplication Property: The span of a set of vectors is closed under scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732458268202284
      },
      {
        "text": "Closure under scalar multiplication: The span of a set of vectors is closed under scalar multiplication, meaning that the span of a set of vectors is also a subspace of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8831303429247267
      },
      {
        "text": "Span is Closed Under Scalar Multiplication: The span of a set of vectors is closed under scalar multiplication, meaning that the scalar multiple of any linear combination of the vectors is also a linear combination of the vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9155317876257205
      }
    ]
  },
  {
    "representative_text": "Basis Extension Theorem: A set of linearly independent vectors can be extended to a basis for the entire vector space if and only if the set is linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 22,
    "detailed_sources": [
      {
        "text": "Basis Extension Theorem: A set of linearly independent vectors can be extended to a basis for the entire vector space if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span Extension: A set of vectors can be extended to a basis for the entire vector space by adding one or more linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8108830481467043
      },
      {
        "text": "Basis Extension Theorem: If a basis for a subspace is extended to a basis for the original vector space, then the extended basis spans the original space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.893111566614109
      },
      {
        "text": "Basis Extension Theorem: If a set of vectors is a basis for a vector space, then any set of vectors that spans the same space can be extended to a basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9589168974635394
      },
      {
        "text": "Basis Extension Theorem: Given a free basis for a vector space, there exists a basis for the entire space that extends the free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9052913663380429
      },
      {
        "text": "Extension of a Free Basis: Given a free basis for a vector space, there exists a basis for the entire space that extends the free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8967845460344355
      },
      {
        "text": "Definition: Given a free basis for a vector space, an extension of the free basis is a basis for the entire space that extends the free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8540528115522629
      },
      {
        "text": "Existence: There exists a basis for the entire space that extends a given free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8860054236620425
      },
      {
        "text": "Construction: To construct an extension of a free basis, we can add one new vector at a time to the free basis, ensuring that the resulting set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8034619664038452
      },
      {
        "text": "Properties: The extension of a free basis has the same dimension as the original free basis, and the span of the extension is equal to the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8510634322318966
      },
      {
        "text": "Basis Extension Theorem for Infinite Vectors: If {v1, v2, ...} is a linearly independent set of vectors, then there exists a basis for the span of {v1, v2, ...} that contains {v1, v2, ...}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8953235369961144
      },
      {
        "text": "Basis Extension Theorem: Given a basis of a vector space, any additional vectors that span the space can be added to the basis without affecting the linear independence of the new basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.920944478524431
      },
      {
        "text": "Basis Extension Theorem: Given a basis for a subspace, we can extend it to a basis for the entire vector space by adding one more vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9323647120520688
      },
      {
        "text": "Theorem of Linear Independence and Basis Extension: A basis for a vector space can be extended to a basis for a larger vector space if and only if the original basis is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9181420931887446
      },
      {
        "text": "Linear Independence Implies Basis Extension Theorem: If a set of linearly independent vectors is extended to a basis for the entire vector space, then the basis is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8916067703958064
      },
      {
        "text": "Basis Extension Theorem (continued): The basis extension theorem states that a set of linearly independent vectors can be extended to a basis for the entire vector space by adding a linearly independent vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9298340771722564
      },
      {
        "text": "Basis Extension Theorem and Dimension: The basis extension theorem states that a set of linearly independent vectors can be extended to a basis for the entire vector space by adding a linearly independent vector. This theorem implies that the dimension of a vector space is equal to the number of linearly independent vectors in the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8915711133920698
      },
      {
        "text": "Basis Extension Theorem (with non-standard basis): Given a basis for a subspace with non-standard basis, we can extend the basis to a larger basis by adding more vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9000713964324754
      },
      {
        "text": "Basis Extension: A basis extension of a basis of a subspace is a basis of the entire vector space that contains the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8570028483659147
      },
      {
        "text": "Vector Space Dimension and Basis Extension: The dimension of a vector space and its basis can be extended by adding new vectors to the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8600062066915388
      },
      {
        "text": "Basis Extension Theorem Variations: While this is a basis",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8435051656142222
      },
      {
        "text": "Basis Extension: A basis extension of a basis of a subspace is a basis of the entire vector space that contains theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8769582430718665
      }
    ]
  },
  {
    "representative_text": "Basis Contraction Theorem: A basis for a vector space can be reduced to a smaller basis by removing one or more vectors, as long as the resulting set is still linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Basis Contraction Theorem: A basis for a vector space can be reduced to a smaller basis by removing one or more vectors, as long as the resulting set is still linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span Reduction: A basis for a vector space can be reduced to a smaller basis by removing one or more vectors, as long as the resulting set is still linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8186175532947768
      },
      {
        "text": "Basis Reduction Theorem: If we have a basis for a vector space and we remove one of its vectors, the resulting set is still a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9147892811186056
      },
      {
        "text": "Basis Extension and Reduction: Given a basis for a subspace, we can extend it to a basis for the entire vector space by adding one more vector (Basis Extension Theorem). Conversely, we can reduce a basis to a smaller basis by removing vectors (Basis Reduction Theorem).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8439029175811175
      },
      {
        "text": "Basis Extension and Reduction: Methods for extending or reducing a basis of a vector space to a larger or smaller basis, respectively.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8185568255116369
      },
      {
        "text": "Basis Extension and Reduction with Dependent Vectors: Given a basis for a subspace with dependent vectors, we can extend it to a basis for the entire vector space by adding one more vector, or reduce it to a smaller basis by removing vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8905354160505285
      },
      {
        "text": "Basis Extension and Reduction with Dependent Vectors (with Non-Standard Bases): Given a basis for a subspace with dependent vectors and a non-standard basis, we can extend the basis to a larger basis by adding more vectors or reduce the basis to a smaller basis by removing vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8997192829133892
      },
      {
        "text": "Basis Reduction Theorem (with non-standard basis): If we have a basis for a vector space with non-standard basis and we remove one of its vectors, the resulting set is still a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9042478344451996
      },
      {
        "text": "Basis Extension and Reduction in Non-Standard Vector Spaces with Non-Standard Metric: Given a basis for a subspace with non-standard basis and non-standard metric, we can extend the basis to a larger basis by adding more vectors or reduce the basis to a smaller basis by removing vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.856904691391734
      }
    ]
  },
  {
    "representative_text": "Finite-Dimensional Vector Space: A vector space that has a finite basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Finite-Dimensional Vector Space: A vector space that has a finite basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of a Vector Space is Finite: The dimension of a vector space is finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8341217008846247
      }
    ]
  },
  {
    "representative_text": "Infinite-Dimensional Vector Space: A vector space that does not have a finite basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Infinite-Dimensional Vector Space: A vector space that does not have a finite basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product Space: A vector space that has an inner product, which is a function that assigns a scalar value to each pair of vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 22,
    "detailed_sources": [
      {
        "text": "Inner Product Space: A vector space that has an inner product, which is a function that assigns a scalar value to each pair of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Definition 1: An inner product space is a vector space V equipped with an inner product, which is a function that takes two vectors u and v in V and returns a scalar such that the following properties hold:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8508029171078417
      },
      {
        "text": "Theorem 1: An inner product space is a vector space V equipped with an inner product, which is a function that takes two vectors u and v in V and returns a scalar such that the following properties hold:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8882260900005797
      },
      {
        "text": "An inner product on a vector space V is a function that takes two vectors u and v in V and returns a scalar <u, v>.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8617479566110706
      },
      {
        "text": "An inner product space is a vector space equipped with an inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9131250928012389
      },
      {
        "text": "Inner Product: A way to associate a scalar value with each pair of vectors in a vector space, satisfying certain properties such as linearity and positive definiteness.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8883281461184629
      },
      {
        "text": "Inner Product Space: A vector space equipped with an inner product, denoted as (v, w) or <v, w>.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9154268794617513
      },
      {
        "text": "Definition of Inner Product:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8643571569343182
      },
      {
        "text": "Inner Product Space:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8583797229036578
      },
      {
        "text": "Inner Product Spaces: An inner product space is a vector space with an inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9342914972737373
      },
      {
        "text": "Inner Product Space: A vector space equipped with an inner product, which can be used to define the concept of orthogonal transformations and orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8415619093220408
      },
      {
        "text": "Inner Product Spaces and Orthogonality: An inner product space is a vector space with an inner product, and orthogonality is a fundamental concept in these spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8458214210361772
      },
      {
        "text": "The Inner Product Spaces: An inner product space is a vector space equipped with an inner product, which can be used to define the concept of orthogonal transformations and orthogonal projections. This concept is essential in understanding the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8116926583421462
      },
      {
        "text": "The concept of an inner product space, including the properties of the inner product and the norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8601714067490394
      },
      {
        "text": "Inner Product and Norm: The inner product of two vectors u and v in a vector space V is a function (u, v) that satisfies certain properties, such as linearity in the first argument and conjugate symmetry. The norm of a vector u is defined as ||u|| = sqrt((u, u)).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8207481628701683
      },
      {
        "text": "The Inner Product Spaces: An inner product space is a vector space equipped with an inner product, which can be used to define the concept of orthogonal transformations and orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9058850248432694
      },
      {
        "text": "The concept of an inner product space being a vector space equipped with an inner product",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9159850352846723
      },
      {
        "text": "Inner Product Space: A vector space equipped with an inner product, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8726946922724795
      },
      {
        "text": "Scalar Product and Inner Product: A scalar product is a function that assigns a scalar value to each pair of vectors in a vector space. Inner product spaces are vector spaces with a scalar product that satisfy certain properties, such as linearity and positive definiteness.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8043453419501178
      },
      {
        "text": "Inner Product Spaces: An inner product space is a vector space V equipped with an inner product (u, v), which satisfies certain properties, such as linearity in the first argument and conjugate symmetry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9517556571986724
      },
      {
        "text": "Inner Product Spaces: An inner product space is a vector space equipped with an inner product, which is a function that assigns a scalar value to each pair of vectors. The inner product is used to define norms and distances between vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9362704963203496
      },
      {
        "text": "Definition: An inner product space is a vector space equipped with an inner product, which is a function that assigns a scalar value to each pair of vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9352667138548079
      }
    ]
  },
  {
    "representative_text": "Orthogonal Space: A vector space that has an orthogonal basis, which means that the vectors are mutually orthogonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Space: A vector space that has an orthogonal basis, which means that the vectors are mutually orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant: A scalar value that can be used to determine the invertibility of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 20,
    "detailed_sources": [
      {
        "text": "Determinant: A scalar value that can be used to determine the invertibility of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant: A scalar value that can be computed from the elements of a square matrix, and has the property that det(AB) = det(A)det(B).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.911235753128371
      },
      {
        "text": "Determinant: A scalar value associated with a square matrix A, denoted by det(A) or |A|, which can be computed using various methods such as expansion by minors, cofactor expansion, or LU decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9113409595242659
      },
      {
        "text": "Determinant of a Matrix: The determinant of a matrix A, denoted by det(A), is a scalar value that can be calculated using various methods, such as the Laplace expansion or cofactor expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8361876017793255
      },
      {
        "text": "Determinant (det or |A|): A scalar value calculated from a square matrix A, denoted as det(A) or |A|.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9370052195178011
      },
      {
        "text": "Determinant: A scalar value that represents the volume scaling factor of a linear transformation. For a 2x2 matrix A, the determinant is calculated as det(A) = ad - bc.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9275772787551525
      },
      {
        "text": "Determinant of a Matrix: The determinant of a matrix A is a scalar value that can be used to determine the invertibility of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.893965090328378
      },
      {
        "text": "Determinant (det): A scalar value that can be used to find the volume scaling factor of a linear transformation represented by a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9369743122133533
      },
      {
        "text": "Determinant: A scalar value that can be computed from the elements of a square matrix, representing the volume scaling factor of the linear transformation described by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.949654236027766
      },
      {
        "text": "Definition of a Determinant:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.872753595429476
      },
      {
        "text": "The determinant of a square matrix A, denoted by det(A) or |A|, is a scalar value that can be computed from the elements of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9112840055547304
      },
      {
        "text": "Determinant: The determinant of a matrix A is a scalar value that represents the volume of the parallelepiped spanned by the columns of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9153092095401403
      },
      {
        "text": "Determinant: A scalar value that represents the volume or area of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9446780824686083
      },
      {
        "text": "The determinant of a matrix is a scalar value that can be used to determine the invertibility of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8797441038254166
      },
      {
        "text": "The Determinant of a Matrix: A scalar value that can be used to describe the properties of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9125074129609376
      },
      {
        "text": "Determinant: The determinant of a matrix is a scalar value that can be used to compute the volume scaling factor of the linear transformation represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9504713115536397
      },
      {
        "text": "Determinant of a Matrix: A scalar value that can be used to determine the linear independence of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9143561241896216
      },
      {
        "text": "Definition of determinant: A scalar that represents the volume scaling factor of a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9075481529002329
      },
      {
        "text": "Determinant of a matrix: A scalar that represents the volume scaling factor of a linear transformation, which can be used to determine whether a matrix is invertible.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9395916133990543
      },
      {
        "text": "Determinant of a Matrix with Applications: A scalar that represents the volume scaling factor of a linear transformation, which is used in linear algebra to determine whether a matrix is invertible.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9234995898962743
      }
    ]
  },
  {
    "representative_text": "Matrix Representation: A matrix that represents a linear transformation between vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Matrix Representation: A matrix that represents a linear transformation between vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation Matrix: A matrix that represents a linear transformation between two vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9543727828885709
      },
      {
        "text": "Matrix Representation: A way of representing a linear transformation T: V → W as a matrix A, such that T(v) = Av for all v in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9228520900617274
      },
      {
        "text": "Linear Operator: A linear transformation between two vector spaces that can be represented as a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8984934897550485
      },
      {
        "text": "Linear Operator: A linear transformation between finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.884066531284089
      },
      {
        "text": "Linear Transformation: A function that takes a vector as input and returns a vector as output. Linear transformations are represented by matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8934761024637625
      },
      {
        "text": "Matrix representations: A way to represent linear transformations using matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8508518685034363
      },
      {
        "text": "Correspondence between Linear Transformations and Matrices: A linear transformation can be represented by a matrix, and a matrix can represent a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8603404954799386
      },
      {
        "text": "Matrix Representation of Linear Transformations: This involves representing a linear transformation as a matrix, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8008166799046978
      },
      {
        "text": "Linear Transformations and Matrices: The connection between linear transformations and matrices, including how matrices can represent linear transformations and how matrix operations can be used to solve systems of equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8206906391249615
      }
    ]
  },
  {
    "representative_text": "Invertible Matrix: A matrix that has an inverse, which means that the matrix can be used to \"undo\" the linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 23,
    "detailed_sources": [
      {
        "text": "Invertible Matrix: A matrix that has an inverse, which means that the matrix can be used to \"undo\" the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inverse: A matrix A such that AA^(-1) = I, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8384285599792987
      },
      {
        "text": "Invertible Matrix: A square matrix that has an inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9354855023262396
      },
      {
        "text": "Invertible Matrix: A square matrix A such that there exists a matrix B (denoted by A^(-1)) satisfying the equation AA^(-1) = A^(-1)A = I, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9281042842434172
      },
      {
        "text": "Inverse of a Matrix: The inverse of a matrix A, denoted by A^-1, is a matrix that satisfies the equation A * A^-1 = I.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8734525714949513
      },
      {
        "text": "Inverse Matrix: A matrix that, when multiplied by the original matrix, results in the identity matrix (I). The inverse of a matrix A is denoted as A^(-1) or A^T, where T represents the transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8599191614508745
      },
      {
        "text": "Inverse Matrix Theorem (also known as the Inverse Property): If A is an invertible matrix, then A^(-1) exists, and A^(-1)A = AA^(-1) = I, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8206851071870235
      },
      {
        "text": "Inverse Matrix: A matrix that, when multiplied by the original matrix, produces the identity matrix. It is denoted by the symbol A^(-1) or A^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9211862396770869
      },
      {
        "text": "A^(-1)A: The product of the inverse of matrix A and matrix A, which is equal to the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8031003956240694
      },
      {
        "text": "Inverse of a Matrix: The inverse of a matrix A is a matrix that can be used to undo the action of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9029238170842767
      },
      {
        "text": "Inverse Matrix: A matrix that, when multiplied by the original matrix, results in the identity matrix (I), representing the inverse operation of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9239362338151176
      },
      {
        "text": "Inverse of a Matrix: The inverse of a square matrix A is denoted as A^-1 and satisfies the property:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9050372513739163
      },
      {
        "text": "Inverse Matrix: A matrix that, when multiplied by the original matrix, results in the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9394456188680287
      },
      {
        "text": "Definition of an Inverse Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8964285678451569
      },
      {
        "text": "The inverse of a non-singular matrix A, denoted by A^(-1) or (A)^(-1), is a matrix that satisfies AA^(-1) = I, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8524933206507055
      },
      {
        "text": "Inverse of a Matrix: The inverse of a matrix A is a new matrix B such that the product AB is equal to the identity matrix I.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9081227320024259
      },
      {
        "text": "Matrix Inversion: The process of finding the inverse of a matrix, which is a matrix that, when multiplied by the original matrix, produces the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8993492162172338
      },
      {
        "text": "Inverse matrices: A matrix that, when multiplied by the original matrix, results in the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9037341174320362
      },
      {
        "text": "Matrix Inverse: A matrix that can be multiplied by a matrix to produce the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9191961051722621
      },
      {
        "text": "Inverse of a Matrix: A matrix that, when multiplied by the original matrix, results in the identity matrix. The existence and uniqueness of the inverse are guaranteed by the Invertibility Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9108389483571508
      },
      {
        "text": "Invertible Matrices: Invertible matrices are matrices that have an inverse, and the inverse of a matrix can be used to solve systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8185311992862812
      },
      {
        "text": "Theorem on the Inverse of a Matrix: The inverse of a matrix A is defined as the matrix A^(-1) such that AA^(-1) = I, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8946893624945346
      },
      {
        "text": "The inverse of a matrix is a matrix that, when multiplied by the original matrix, results in the identity matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9137534065792255
      }
    ]
  },
  {
    "representative_text": "Non-Invertible Matrix: A matrix that does not have an inverse, which means that the matrix cannot be used to \"undo\" the linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Non-Invertible Matrix: A matrix that does not have an inverse, which means that the matrix cannot be used to \"undo\" the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Non-Invertible Matrix: A square matrix that does not have an inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9379465640158604
      },
      {
        "text": "The concept of a \"non-invertible\" matrix: A matrix that does not have an inverse, which can be determined using the determinant of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8888310054305966
      },
      {
        "text": "Invertible Matrix and Non-Invertible Matrix: A matrix that has an inverse, and a matrix that does not have an inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8783345117965763
      },
      {
        "text": "Non-invertible matrix and determinants: A matrix that does not have an inverse can be determined using the determinant of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8623798236287822
      }
    ]
  },
  {
    "representative_text": "Linear Algebra: A branch of mathematics that deals with the study of vector spaces, linear transformations, and matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra: A branch of mathematics that deals with the study of vector spaces, linear transformations, and matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Physics: A field that uses linear algebra to describe the motion of objects, forces, and energies.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Physics: A field that uses linear algebra to describe the motion of objects, forces, and energies.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computer Science: A field that uses linear algebra to solve systems of equations, optimize functions, and analyze data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Computer Science: A field that uses linear algebra to solve systems of equations, optimize functions, and analyze data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The use of linear algebra in computer science: Linear algebra is used in a wide range of applications in computer science, including computer graphics, machine learning, and data analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8583132507718919
      },
      {
        "text": "Computer Graphics: Linear algebra is used extensively in computer graphics to perform tasks such as 3D modeling, texture mapping, and lighting.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8440951668139685
      },
      {
        "text": "Computer Science: Linear algebra is used in computer science to perform tasks such as graph theory, computer graphics, and machine learning.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9573424592373028
      }
    ]
  },
  {
    "representative_text": "Definition 2: A subspace W of a vector space V is a subset of V that is closed under addition and scalar multiplication.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Definition 2: A subspace W of a vector space V is a subset of V that is closed under addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace: A subset of a vector space that is itself a vector space under the same operations of vector addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732859714125393
      },
      {
        "text": "Subspace: A set of vectors that is closed under addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9231150983097877
      },
      {
        "text": "Subspace: A subspace of a vector space is a subset of vectors that also satisfies the properties of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9453546309998826
      },
      {
        "text": "The concept of a \"subspace\": A subset of a vector space that is closed under addition and scalar multiplication is called a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9257266392169236
      }
    ]
  },
  {
    "representative_text": "Scalar addition of vectors: The sum of two vectors u, v in V is defined as u + v, and it results in a vector in V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar addition of vectors: The sum of two vectors u, v in V is defined as u + v, and it results in a vector in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Definition 1: A linear transformation T from a vector space V to a vector space W is a function that satisfies the following properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Definition 1: A linear transformation T from a vector space V to a vector space W is a function that satisfies the following properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation: A function between vector spaces that preserves the operations of vector addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.828427416399586
      },
      {
        "text": "Linear Operator: A function $T: V \\to W$ between two vector spaces $V$ and $W$ over a field (usually $\\mathbb{R}$ or $\\mathbb{C}$) that satisfies the following properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8376708311860691
      },
      {
        "text": "Linear Transformation: A linear operator between two vector spaces that is both injective (one-to-one) and surjective (onto).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8954762365478235
      },
      {
        "text": "Linear Transformation: A function T: V → W between two vector spaces V and W that preserves the operations of vector addition and scalar multiplication, i.e., T(u + v) = T(u) + T(v) and T(cu) = cT(u) for all u, v in V and c in the underlying field.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9406821429083936
      },
      {
        "text": "T: A linear transformation between two vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.860097802287725
      },
      {
        "text": "Linear Transformation: A function T: V → W between two vector spaces V and W, where T is a linear operator that satisfies the following properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9702919043421373
      },
      {
        "text": "Linear Transformation: A linear transformation is a function between vector spaces that preserves linear combinations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9222804876533928
      },
      {
        "text": "Definition: A linear transformation is a function between vector spaces that preserves the operations of vector addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9405465123387642
      },
      {
        "text": "A linear transformation is a function from a vector space to itself that preserves the operations of vector addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8923982054168265
      },
      {
        "text": "Linear transformations: A function that maps vectors to other vectors while preserving certain properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8990097393237048
      },
      {
        "text": "Definition of linear transformation: A function that maps vectors from one vector space to another while preserving the operations of vector addition and scalar multiplication.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9576049347926681
      },
      {
        "text": "A linear transformation is a function that takes a vector to another vector while preserving linear combinations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9102850754715602
      }
    ]
  },
  {
    "representative_text": "Definition 2: A linear transformation T from a vector space V to a vector space W is said to be one-to-one if the only way to express the zero vector in W as a linear combination of the images of vectors in V is with all coefficients equal to zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Definition 2: A linear transformation T from a vector space V to a vector space W is said to be one-to-one if the only way to express the zero vector in W as a linear combination of the images of vectors in V is with all coefficients equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Definition 3: A linear transformation T from a vector space V to a vector space W is said to be onto if for every vector w in W, there exists a vector v in V such that T(v) = w.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Definition 3: A linear transformation T from a vector space V to a vector space W is said to be onto if for every vector w in W, there exists a vector v in V such that T(v) = w.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Definition 1: The matrix representation of a linear transformation T from a vector space V to a vector space W is a matrix A such that T(v) = Av for any vector v in V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Definition 1: The matrix representation of a linear transformation T from a vector space V to a vector space W is a matrix A such that T(v) = Av for any vector v in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Representation: A matrix $A$ that represents a linear operator $T: V \\to W$ such that $T(v) = Av$ for all $v \\in V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8065935586538283
      },
      {
        "text": "$A$: A matrix that represents a linear operator $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8197790100035081
      },
      {
        "text": "A: The matrix representing a linear transformation T: V → W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8779150691109012
      },
      {
        "text": "Matrix Representation: A matrix A representing a linear transformation T: V → W, where the columns of A are the images of the standard basis vectors of V under T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9138553933770975
      },
      {
        "text": "Linear Transformation Matrix: The matrix representation of a linear transformation T is a matrix A such that the product Ax is equal to the image of x under T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9020823249080971
      },
      {
        "text": "Linear Transformations and Matrices: A linear transformation T from a vector space V to a vector space W is a function that satisfies certain properties, such as linearity in both arguments. A matrix A representing T is a square matrix such that A(u) = T(u) for all u in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8785646810399225
      },
      {
        "text": "Linear Transformation and Linear Combinations: A linear transformation T from a vector space V to a vector space W can be represented by a matrix A such that T(u) = Au for all u in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8659996775624751
      }
    ]
  },
  {
    "representative_text": "Definition 2: The matrix A representing a linear transformation T from a vector space V to a vector space W is said to be invertible if there exists a matrix B such that AB = I, where I is the identity matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Definition 2: The matrix A representing a linear transformation T from a vector space V to a vector space W is said to be invertible if there exists a matrix B such that AB = I, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem 2: The matrix A representing a linear transformation T from a vector space V to a vector space W is said to be invertible if and only if there exists a matrix B such that AB = I, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8517343056977746
      }
    ]
  },
  {
    "representative_text": "Definition 1: An eigenvalue λ of a linear transformation T from a vector space V to a vector space W is a scalar such that there exists a non-zero vector v in V such that T(v) = λv.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Definition 1: An eigenvalue λ of a linear transformation T from a vector space V to a vector space W is a scalar such that there exists a non-zero vector v in V such that T(v) = λv.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Definition 2: An eigenvector v of a linear transformation T from a vector space V to a vector space W corresponding to an eigenvalue λ is a non-zero vector such that T(v) = λv.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8628107270780474
      },
      {
        "text": "Theorem 1: An eigenvalue λ of a linear transformation T from a vector space V to a vector space W is a scalar such that there exists a non-zero vector v in V such that T(v) = λv.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8544551433798859
      },
      {
        "text": "Theorem 2: An eigenvector v of a linear transformation T from a vector space V to a vector space W corresponding to an eigenvalue λ is a non-zero vector such that T(v) = λv.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.893339131231288
      },
      {
        "text": "Theorem of Eigenvalue and Eigenvector: In addition to Theorem 1, it's worth noting that an eigenvalue is a scalar such that there exists a non-zero vector v in V such that T(v) = λv. This is equivalent to saying that the transformation has a non-zero vector that is scaled by the eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8079501614080629
      },
      {
        "text": "Eigenvalues and Eigenvectors: For a linear transformation T from a vector space V to a vector space W, an eigenvector v of T is a non-zero vector in V such that T(v) = λv for some scalar λ, called the eigenvalue corresponding to v.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9124433032046637
      },
      {
        "text": "Theorem on the Eigenvalues and Eigenvectors of a Linear Transformation: This theorem states that eigenvalues and eigenvectors of a linear transformation T from a finite-dimensional vector space V to itself satisfy the equation T(v) = λv for some scalar λ.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8461779941039205
      },
      {
        "text": "Eigenvalue and Eigenvector Theorem: For a linear transformation T from a vector space V to itself, the eigenvalue theorem states that the eigenvalues of T are the scalars λ such that there exists a non-zero vector v in V such that T(v) = λv.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9206255895336376
      }
    ]
  },
  {
    "representative_text": "Definition 1: A linear transformation T from a vector space V to a vector space W is said to be diagonalizable if there exists a basis for V such that the matrix representation of T is a diagonal matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Definition 1: A linear transformation T from a vector space V to a vector space W is said to be diagonalizable if there exists a basis for V such that the matrix representation of T is a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem 1: A linear transformation T from a vector space V to a vector space W is said to be diagonalizable if there exists a basis for V such that the matrix representation of T is a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8465091175463724
      },
      {
        "text": "Diagonalization: A linear transformation T from a vector space V to a vector space W is diagonalizable if there exists a basis for V consisting of eigenvectors of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.919249129848827
      },
      {
        "text": "Jordan Canonical Form: A linear transformation T from a vector space V to a vector space W is diagonalizable if and only if its Jordan canonical form is diagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8220278245828099
      },
      {
        "text": "Diagonalization Theorem: A linear transformation T from a vector space V to itself is diagonalizable if and only if there exists a basis for V consisting of eigenvectors of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9195223024457364
      },
      {
        "text": "Jordan Canonical Form Theorem: A linear transformation T from a vector space V to itself is diagonalizable if and only if its Jordan canonical form is diagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8847700507610423
      }
    ]
  },
  {
    "representative_text": "Definition 2: A diagonal matrix is a matrix whose entries on the main diagonal are eigenvalues of the linear transformation.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Definition 2: A diagonal matrix is a matrix whose entries on the main diagonal are eigenvalues of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem 2: A diagonal matrix is a matrix whose entries on the main diagonal are eigenvalues of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8653524945257047
      }
    ]
  },
  {
    "representative_text": "Definition 1: A linear transformation T from a vector space V to a vector space W can be decomposed into the product of three matrices: U, Σ, and V^T, such that T(v) = UΣV^T(v) for any vector v in V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Definition 1: A linear transformation T from a vector space V to a vector space W can be decomposed into the product of three matrices: U, Σ, and V^T, such that T(v) = UΣV^T(v) for any vector v in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem 1: A linear transformation T from a vector space V to a vector space W can be decomposed into the product of three matrices: U, Σ, and V^T, such that T(v) = UΣV^T(v) for any vector v in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8508620737369212
      },
      {
        "text": "Singular Value Decomposition (SVD): A linear transformation T from a vector space V to a vector space W is represented by an SVD if there exist matrices U, Σ, and V such that T(x) = UΣV^T(x) for all x in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8564514328903609
      },
      {
        "text": "The Singular Value Decomposition (SVD) Theorem for Vector Spaces: This theorem states that a linear transformation T from a vector space V to itself can be represented by an SVD if and only if there exist matrices U, Σ, and V^T such that T(x) = UΣV^T(x) for all x in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8794306724381087
      }
    ]
  },
  {
    "representative_text": "Definition 2: The matrices U and V are said to be orthogonal, and the matrix Σ is said to be a diagonal matrix containing the singular values of T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Definition 2: The matrices U and V are said to be orthogonal, and the matrix Σ is said to be a diagonal matrix containing the singular values of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem 2: The matrices U and V are said to be orthogonal, and the matrix Σ is said to be a diagonal matrix containing the singular values of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9575578050096681
      },
      {
        "text": "Orthogonality of U and V: U and V are orthogonal matrices, i.e., U^T U = I and V^T V = I.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8283696365686994
      },
      {
        "text": "The U matrix represents the left singular vectors, the Σ matrix represents the singular values, and the V matrix represents the right singular vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8225821241364132
      },
      {
        "text": "The U matrix contains the left singular vectors, the Σ matrix contains the singular values, and the V matrix contains the right singular vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8830599030283808
      }
    ]
  },
  {
    "representative_text": "Definition 1: Two vectors u and v are said to be orthogonal if their dot product is zero, i.e., u · v = 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Definition 1: Two vectors u and v are said to be orthogonal if their dot product is zero, i.e., u · v = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem 1: Two vectors u and v are said to be orthogonal if their dot product is zero, i.e., u · v = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9019444830399675
      },
      {
        "text": "Two vectors u and v are orthogonal if their inner product is zero, i.e., <u, v> = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9021103366492558
      },
      {
        "text": "Orthogonal vectors: Two vectors are said to be orthogonal if their dot product is zero. In other words, if u and v are orthogonal, then u · v = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9177155368449026
      },
      {
        "text": "Orthogonality: If u and v are orthogonal, then u · v = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921740926568373
      },
      {
        "text": "Orthogonality condition: u · v = 0 if and only if u and v are orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8929768611380515
      },
      {
        "text": "Theorem of Orthogonality: If two vectors are orthogonal, then their dot product is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8909437980838238
      },
      {
        "text": "Theorem of Orthonormality: If a set of vectors is orthonormal, then the dot product of any two vectors in the set is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.861367189780911
      },
      {
        "text": "Orthogonality and Orthogonal Vectors: Two vectors u and v in a vector space V are said to be orthogonal if (u, v) = 0, where (u, v) denotes the inner product of u and v. A set of vectors in V is said to be orthogonal if every pair of vectors in the set is orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.872869913670061
      },
      {
        "text": "Theorem on the Orthogonality of Vectors: This theorem states that two vectors are orthogonal if and only if their inner product is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9048517834851956
      },
      {
        "text": "Definition of orthogonality: Two vectors are orthogonal if their dot product is zero.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9129696611080527
      },
      {
        "text": "Two vectors are orthogonal if their dot product is zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.93446432009317
      }
    ]
  },
  {
    "representative_text": "Definition 2: A projection onto a subspace W of a vector space V is a linear transformation that maps every vector in V to a vector in W.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Definition 2: A projection onto a subspace W of a vector space V is a linear transformation that maps every vector in V to a vector in W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem 2: A projection onto a subspace W of a vector space V is a linear transformation that maps every vector in V to a vector in W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8903597325729237
      }
    ]
  },
  {
    "representative_text": "Definition 2: An inner product space is said to be Hilbertian if it is complete with respect to the norm induced by the inner product.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Definition 2: An inner product space is said to be Hilbertian if it is complete with respect to the norm induced by the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem 2: An inner product space is said to be Hilbertian if it is complete with respect to the norm induced by the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9152451036416372
      }
    ]
  },
  {
    "representative_text": "Theorem 2: A set of vectors v1, v2, ..., vn is linearly independent if and only if the determinant of the matrix whose columns are the vectors v1, v2, ..., vn is non-zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Theorem 2: A set of vectors v1, v2, ..., vn is linearly independent if and only if the determinant of the matrix whose columns are the vectors v1, v2, ..., vn is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence Criterion: A set of vectors is linearly independent if and only if the determinant of the matrix formed by the vectors is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8871544195952532
      },
      {
        "text": "Linear Independence Criterion using Determinant: A set of vectors is linearly independent if and only if the determinant of the matrix formed by the vectors is non-zero. This criterion is equivalent to the determinant criterion mentioned earlier.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9195946147939087
      },
      {
        "text": "The Linear Independence Criterion using Determinants: A criterion for determining whether a set of vectors is linearly independent using determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.86927257021135
      }
    ]
  },
  {
    "representative_text": "Theorem 1: A linear transformation T from a vector space V to a vector space W can be represented by a matrix A if and only if T(v) = Av for some vector v in V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem 1: A linear transformation T from a vector space V to a vector space W can be represented by a matrix A if and only if T(v) = Av for some vector v in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The inner product must satisfy the following properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The inner product must satisfy the following properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Properties of inner product:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9051510765439372
      },
      {
        "text": "Inner Product Properties:",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9556545515399246
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement: The orthogonal complement of a subspace W in V is the set of all vectors in V that are orthogonal to every vector in W.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 18,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement: The orthogonal complement of a subspace W in V is the set of all vectors in V that are orthogonal to every vector in W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Complement: The orthogonal complement of a vector space W is the set of vectors that are orthogonal to every vector in W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9536288105138179
      },
      {
        "text": "Orthogonal Complement: The set of all vectors in the vector space that are orthogonal to every vector in a given subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9094065612771606
      },
      {
        "text": "Orthogonality: Every vector in the orthogonal complement of a subspace is orthogonal to every vector in the original subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8687363451364112
      },
      {
        "text": "Orthogonal Complement Theorem: For a subspace W of a vector space V, the orthogonal complement W^⊥ is a subspace of V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.872396072241757
      },
      {
        "text": "Orthogonal Complement Decomposition: For a subspace W of a vector space V, the vector space V can be decomposed as V = W ⊕ W^⊥, where ⊕ denotes the direct sum.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8157106675819169
      },
      {
        "text": "Orthogonal Complement Representation: The orthogonal complement of a subspace can be represented as a linear combination of the orthogonal complements of its basis vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8134700905538684
      },
      {
        "text": "Orthogonal Complement: A subspace that contains all vectors orthogonal to a given subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9207884472429859
      },
      {
        "text": "Orthogonal Vector: A vector that is orthogonal to every vector in a given subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8522189913295358
      },
      {
        "text": "Orthogonal Complement: The orthogonal complement of a subspace is a subspace that contains all vectors orthogonal to the original subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.945705732608502
      },
      {
        "text": "Orthogonal Complement: The subspace that is orthogonal to a given subspace, which can be found by taking the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9024068604188606
      },
      {
        "text": "Orthogonal Subspace: A subspace that is orthogonal to a given subspace, which can be found by taking the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8883141968539149
      },
      {
        "text": "Orthogonal Complement: Given a vector u and a subspace V, the orthogonal complement of u with respect to V is the set of all vectors v in V that are orthogonal to u, denoted by V ⊥ u.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.879375064945721
      },
      {
        "text": "Orthogonal Complement: The set of vectors that are orthogonal to a given set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9093476183200886
      },
      {
        "text": "Orthogonal Complement Theorem: The orthogonal complement of a subspace is the set of vectors that are orthogonal to all vectors in the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9476985529062982
      },
      {
        "text": "Orthogonal Complement of a Matrix: The orthogonal complement of a matrix is the set of all vectors that are orthogonal to every vector in the column space of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8521760190511409
      },
      {
        "text": "Orthogonal Complement of a Matrix: The orthogonal complement of a matrix can be defined as the set of all vectors orthogonal to every vector in the range of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8249996048944892
      },
      {
        "text": "Orthogonal Complement Theorem: The orthogonal complement of a subspace $U$ is the set of vectors that are orthogonal to every vector in $U$. The orthogonal complement of $U$ is denoted as $U^\\perp$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9147191944240521
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection: The orthogonal projection of a vector u onto a subspace W is the vector in W that is closest to u.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection: The orthogonal projection of a vector u onto a subspace W is the vector in W that is closest to u.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The orthogonal projection of a vector u onto a subspace W is given by:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8524144959281393
      },
      {
        "text": "The orthogonal projection of a vector u onto a subspace W is the vector in W that is closest to u.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9501520571068709
      },
      {
        "text": "Orthogonal projection: Given a vector u and a subspace V, an orthogonal projection of u onto V is a vector v in V such that u - v is orthogonal to V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9010121982436893
      },
      {
        "text": "Orthogonal projection formula: proj_V(u) = (u · v) / (v · v) v, where v is a unit vector in the subspace V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8476424341979972
      },
      {
        "text": "Orthogonal Projection Formula:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8276858571656178
      },
      {
        "text": "Orthogonal Projection Formula: The formula for projecting a vector onto a subspace, which involves finding the orthogonal complement of the subspace and taking the projection of the vector onto the orthogonal complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8388587067748944
      },
      {
        "text": "The Orthogonal Projections of a Vector: Given a vector u and a subspace V, the orthogonal projections of u onto V are the vectors in V that are closest to u.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8961164695462143
      },
      {
        "text": "The definition and properties of the orthogonal complement of a subspace, including the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8483566053768172
      },
      {
        "text": "The concept of an orthogonal vector, including the properties of an orthogonal vector and the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8189483322933608
      },
      {
        "text": "Orthogonal Projection: A subspace W of a vector space V is said to be orthogonal to a vector u if and only if the inner product of u and v is zero for all v in W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8657593696926371
      }
    ]
  },
  {
    "representative_text": "An orthogonal matrix is a square matrix whose columns and rows are orthonormal vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 21,
    "detailed_sources": [
      {
        "text": "An orthogonal matrix is a square matrix whose columns and rows are orthonormal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "An orthogonal matrix satisfies the following properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8464818446439654
      },
      {
        "text": "Orthogonality: A matrix A is orthogonal if A^T A = I, where A^T is the transpose of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8583982411831799
      },
      {
        "text": "Orthogonal Matrix: A square matrix whose transpose is its inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9046182467481205
      },
      {
        "text": "Orthogonal Matrix: An orthogonal matrix is a square matrix whose columns and rows are orthonormal vectors, i.e., the matrix satisfies the condition U^T U = I, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9006663596826978
      },
      {
        "text": "Orthogonal Matrix: A square matrix whose columns and rows are orthogonal vectors, meaning the dot product of any two different columns (or rows) is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9269870146178862
      },
      {
        "text": "Orthogonality of Orthogonal Matrices: The columns of an orthogonal matrix are orthogonal to each other, and the rows of an orthogonal matrix are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8393135669893187
      },
      {
        "text": "Orthogonal matrix: A square matrix A is said to be orthogonal if its transpose is its inverse, i.e., A^T = A^-1. This implies that the columns of A are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9363707405465846
      },
      {
        "text": "Orthogonal Matrix: A square matrix whose columns and rows are all orthogonal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9467920273748891
      },
      {
        "text": "Orthogonal Matrices:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8176161892200103
      },
      {
        "text": "Orthogonal Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8603670021438711
      },
      {
        "text": "Orthogonal Matrices and Householder Transformations: An orthogonal matrix is a square matrix whose columns and rows are orthonormal vectors, which means they are orthogonal to each other and have a length of 1. Householder transformations are a type of orthogonal matrix that can be used to diagonalize matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8220473134653639
      },
      {
        "text": "Orthogonal Matrix: An orthogonal matrix is a square matrix whose columns and rows are orthonormal vectors. Eigenvalues and eigenvectors can be used to analyze orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8903327026499874
      },
      {
        "text": "Orthogonal Matrices: A square matrix A is said to be orthogonal if its transpose is equal to its inverse, i.e., A^T = A^-1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9187812302482267
      },
      {
        "text": "Matrix Orthogonality and Orthogonal Matrices: The definition and properties of orthogonal matrices, including the concept of matrix orthogonality and its applications in eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8312639057332228
      },
      {
        "text": "The definition and properties of an orthogonal matrix, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8502364165025006
      },
      {
        "text": "Orthogonal Matrix Orthogonality: The columns of an orthogonal matrix are orthogonal to each other, and the rows of an orthogonal matrix are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572822820261057
      },
      {
        "text": "3. It is a orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8529868959999973
      },
      {
        "text": "Orthogonal Matrix Orthogonality Theorem: A theorem that states that two matrices are orthogonal if and only if their product is equal to the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8115589769267596
      },
      {
        "text": "Matrix Orthogonality: A property of matrices where the transpose of a matrix is its own inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8376071499901055
      },
      {
        "text": "Orthogonal Matrices: An orthogonal matrix is a matrix whose columns and rows are orthonormal vectors. Orthogonal matrices have many applications in linear algebra, including finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8711093887734591
      }
    ]
  },
  {
    "representative_text": "Every vector can be uniquely expressed as the sum of an orthogonal projection onto a subspace and a vector orthogonal to the subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Every vector can be uniquely expressed as the sum of an orthogonal projection onto a subspace and a vector orthogonal to the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Projection Theorem: The projection of a vector onto a subspace is unique.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.832260085656395
      },
      {
        "text": "Orthogonal projection theorem: If u is a vector in an inner product space V, and V is a subspace of V, then there exists an orthogonal projection of u onto V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8653061603323722
      },
      {
        "text": "Orthogonal Projection Theorem: Given a subspace V and a vector v in the ambient space, there exists a unique orthogonal projection matrix P onto V that satisfies the following equation:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8349733731821147
      },
      {
        "text": "Theorem of Projection: Given a vector and a subspace, there exists a unique vector in the subspace that is closest to the original vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8882002731714496
      },
      {
        "text": "Orthogonal Projection Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8960159385702032
      },
      {
        "text": "Orthogonal Projections: The orthogonal projection of a vector onto a subspace is unique.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9005096807139391
      },
      {
        "text": "Orthogonal Projection Theorem: The projection of a vector onto a subspace is unique and can be found using the orthogonal projection formula.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9540060405705386
      },
      {
        "text": "Orthogonal Projection Theorem (Generalized): The projection of a vector onto a subspace is unique and can be found using the orthogonal projection formula, which can be generalized to non-orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8717226553495907
      },
      {
        "text": "Linear Transformations and the Orthogonal Projection Theorem: The orthogonal projection theorem is a result that describes the projection of a vector onto the image of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8156876944913376
      },
      {
        "text": "Orthogonal Projection Theorem: A vector can be uniquely represented as the sum of an orthogonal projection onto a subspace and an orthogonal projection onto the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9230508444481679
      },
      {
        "text": "Orthogonal Complement Theorem: A theorem that states that a vector can be uniquely represented as the sum of an orthogonal projection onto a subspace and an orthogonal projection onto the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8676471329309557
      }
    ]
  },
  {
    "representative_text": "The orthogonal projection method is a method for solving systems of linear equations by projecting the right-hand side onto the solution space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The orthogonal projection method is a method for solving systems of linear equations by projecting the right-hand side onto the solution space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The orthogonal matrix method is a method for solving systems of linear equations by using an orthogonal matrix to transform the system into upper triangular form.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The orthogonal matrix method is a method for solving systems of linear equations by using an orthogonal matrix to transform the system into upper triangular form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonalization of a Matrix: A process for transforming a matrix into an orthogonal matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8482323032211613
      }
    ]
  },
  {
    "representative_text": "The Pythagorean theorem states that the square of the length of the hypotenuse of a right triangle is equal to the sum of the squares of the lengths of the other two sides.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Pythagorean theorem states that the square of the length of the hypotenuse of a right triangle is equal to the sum of the squares of the lengths of the other two sides.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvectors corresponding to distinct eigenvalues of a matrix are orthogonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Eigenvectors corresponding to distinct eigenvalues of a matrix are orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis vectors corresponding to distinct basis vectors of a vector space are orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.820701009781952
      },
      {
        "text": "Eigenvector Orthogonality: If λ1 and λ2 are distinct eigenvalues of A, then the corresponding eigenvectors are orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8487398682707334
      },
      {
        "text": "Orthogonality of Eigenvectors: If λ1, λ2, ..., λn are distinct eigenvalues of matrix A and v1, v2, ..., vn are the corresponding eigenvectors, then the set {v1, v2, ..., vn} is an orthogonal set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8963708836252637
      },
      {
        "text": "Orthogonality: Eigenvectors corresponding to distinct eigenvalues are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9253253712876657
      },
      {
        "text": "Eigenvector Orthogonality: If v1, v2, ..., vn are eigenvectors of A corresponding to distinct eigenvalues λ1, λ2, ..., λn, then v1, v2, ..., vn are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9517351980541215
      },
      {
        "text": "Orthogonality of Eigenvectors: The eigenvectors of a diagonalizable matrix are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8882215121995332
      },
      {
        "text": "Orthogonality of Eigenvectors:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9034788364245587
      },
      {
        "text": "Orthogonality of Basis Vectors:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8164488037819951
      },
      {
        "text": "The Orthogonality of Eigenvectors: Eigenvectors corresponding to distinct eigenvalues are orthogonal. This is because the matrix is diagonalizable, and the eigenvectors form a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9225080365907683
      }
    ]
  },
  {
    "representative_text": "Two vectors u and v are orthonormal if their inner product is zero, i.e., <u, v> = 0, and their norms are equal, i.e., ||u|| = ||v||.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Two vectors u and v are orthonormal if their inner product is zero, i.e., <u, v> = 0, and their norms are equal, i.e., ||u|| = ||v||.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scalar Multiplication Identity: Multiplying a vector by 1 leaves the vector unchanged.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar Multiplication Identity: Multiplying a vector by 1 leaves the vector unchanged.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector Addition: The sum of two vectors is another vector in the same space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Vector Addition: The sum of two vectors is another vector in the same space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Vector Addition: The sum of two vectors, denoted by v1 + v2, where v1 and v2 are the vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8492490434996804
      },
      {
        "text": "Vector Addition: The sum of two vectors is a new vector with a magnitude equal to the sum of the magnitudes of the original vectors and a direction equal to the sum of the directions of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8358353084785969
      },
      {
        "text": "Vector addition: the sum of two vectors is the vector obtained by adding their corresponding components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8868223202067784
      }
    ]
  },
  {
    "representative_text": "Intersection of Subspaces: The intersection of two subspaces is a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Intersection of Subspaces: The intersection of two subspaces is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of the Intersection of Subspaces: The span of the intersection of two subspaces is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9286206737054696
      },
      {
        "text": "Intersection of spans: The intersection of two or more spans is also a subspace, and it contains the common vectors of the individual spans.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8989049798720166
      },
      {
        "text": "Intersection of Subspaces Theorem: A theorem that states that the intersection of two subspaces is equal to the subspace spanned by the intersection of the two subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8529420696867018
      }
    ]
  },
  {
    "representative_text": "Null Space of a Linear Transformation: The null space of a linear transformation is a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Null Space of a Linear Transformation: The null space of a linear transformation is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Kernel of a Linear Transformation: The kernel of a linear transformation is the null space of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8186318921508325
      },
      {
        "text": "Null Space Theorem: The null space of a linear transformation is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9027313714110111
      },
      {
        "text": "Kernel Theorem: The kernel of a linear transformation is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8953405358422011
      },
      {
        "text": "Kernel (Null Space) of a Linear Transformation T:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8592115125511344
      },
      {
        "text": "Kernel (Null Space) of a Linear Transformation: The set of all vectors u in V such that T(u) = 0, denoted by N(T) or Ker(T).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8610092222266463
      },
      {
        "text": "Kernel of a Linear Transformation: The kernel of a linear transformation is the set of vectors that are mapped to the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8781201470185098
      },
      {
        "text": "Kernel and Image: The kernel of a linear transformation is the set of vectors that are mapped to the zero vector, while the image is the set of vectors that are mapped from the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8089892456925463
      },
      {
        "text": "The Null Space and Image of a Linear Transformation: The null space and image of a linear transformation are subspaces of the domain and codomain, respectively. The intersection of the null space and image is the null space of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8043678407879828
      },
      {
        "text": "Kernel and Image of a Linear Transformation: The set of vectors that are mapped to the zero vector, and the set of vectors that are mapped from the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8563119613844506
      },
      {
        "text": "Null Space of a Linear Transformation's Kernel: The kernel of a linear transformation is the set of all vectors that are mapped to the zero vector. The null space is a subspace of the kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9345384340439764
      },
      {
        "text": "Null Space of a Linear Transformation's Kernel: The null space of the kernel of a linear transformation, which is a subspace of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9216543199819587
      },
      {
        "text": "The kernel of a linear transformation is the set of vectors that are mapped to the zero vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9027193946100405
      }
    ]
  },
  {
    "representative_text": "Dimension Theorem: The dimension of a vector space is the same as the dimension of its subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension Theorem: The dimension of a vector space is the same as the dimension of its subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Image Theorem: The image of a linear transformation is a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Image Theorem: The image of a linear transformation is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Image (Range) of a Linear Transformation T:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8175231145439342
      },
      {
        "text": "Kernel and Image of a Linear Transformation:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8759761464390248
      }
    ]
  },
  {
    "representative_text": "Rank-Nullity Theorem: The rank of a linear transformation plus the nullity of the transformation equals the dimension of the original vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 45,
    "detailed_sources": [
      {
        "text": "Rank-Nullity Theorem: The rank of a linear transformation plus the nullity of the transformation equals the dimension of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rank-Nullity Theorem: For a linear operator $T: V \\to W$, the rank of $T$ (the dimension of its image) plus the nullity of $T$ (the dimension of its kernel) is equal to the dimension of the domain $V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8936597117224554
      },
      {
        "text": "Rank-Nullity Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732424006241324
      },
      {
        "text": "Rank-Nullity Theorem: For any linear transformation T: V → W, the sum of the rank and nullity of T is equal to the dimension of the domain V, i.e., rank(T) + nullity(T) = dim(V).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9513231875172447
      },
      {
        "text": "Rank-Nullity Theorem (Alternative Form): For any linear transformation T: V → W, the rank of T is equal to the number of linearly independent columns of the matrix representation A of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8303153655105368
      },
      {
        "text": "Rank-Nullity Theorem (Equivalent Form): For any linear transformation T: V → W, the nullity of T is equal to the number of linearly dependent columns of the matrix representation A of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8706127684201488
      },
      {
        "text": "Dimension Theorem: For any linear transformation T: V → W, the dimension of the image (range) of T is equal to the rank of T, and the dimension of the kernel (null space) of T is equal to the nullity of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9073443197656206
      },
      {
        "text": "Rank-Nullity Theorem: If T: V → W is a linear transformation, then the rank of T (the dimension of the range of T) plus the nullity of T (the dimension of the kernel of T) equals the dimension of V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.957389096485679
      },
      {
        "text": "Rank-Nullity Theorem: If A is a square matrix, then the rank of A (the dimension of the range of A) plus the nullity of A (the dimension of the kernel of A) equals the dimension of V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8852396878717805
      },
      {
        "text": "Statement: The sum of the rank and nullity of a matrix is equal to the number of columns of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8341887485788613
      },
      {
        "text": "Rank-Nullity Theorem: The rank of a matrix is equal to the dimension of its column space, and the nullity of a matrix is equal to the dimension of its null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9204539747800813
      },
      {
        "text": "Rank-Nullity Theorem: The rank of a matrix (i.e., the dimension of the column space) plus the nullity of a matrix (i.e., the dimension of the null space) is equal to the number of columns in the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9169902776394643
      },
      {
        "text": "Rank and Nullity: The rank of a matrix is equal to the dimension of the column space, and the nullity of a matrix is equal to the dimension of the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8449299260017419
      },
      {
        "text": "Rank-Nullity Theorem: A theorem that states the rank of a matrix A plus the nullity of A is equal to the number of columns of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9083059708754296
      },
      {
        "text": "Rank-Nullity Theorem: For a linear transformation, the rank of the transformation is equal to the dimension of the range, and the nullity of the transformation is equal to the dimension of the kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9310183893490417
      },
      {
        "text": "The Rank-Nullity Theorem: This theorem states that the rank of a matrix (the dimension of its row space) plus the nullity of the matrix (the dimension of its null space) is equal to the number of columns of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9323692564046504
      },
      {
        "text": "Rank-nullity theorem: The rank of a linear transformation is equal to the dimension of the range of the transformation, and the nullity of a linear transformation is equal to the dimension of the null space of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.933040607548924
      },
      {
        "text": "Rank-Nullity Theorem: The rank of a linear transformation $T$ is equal to the dimension of the range of $T$ minus the dimension of the null space of $T$. This theorem is closely related to the concept of basis and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8763674205561757
      },
      {
        "text": "Rank-Nullity Theorem: This theorem states that the sum of the rank and nullity of a linear transformation is equal to the dimension of the domain vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9437874172824057
      },
      {
        "text": "The rank-nullity theorem** : This theorem states that the sum of the rank and nullity of a matrix is equal to the number of columns of the matrix. This theorem is related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9015785128187935
      },
      {
        "text": "Rank-Nullity Theorem: The rank of a linear transformation $T: V \\to W$ is equal to the dimension of the image of $T$ minus the dimension of the null space of $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9412012887878027
      },
      {
        "text": "Rank-Nullity Theorem: A theorem that states that the rank of a linear transformation plus the nullity of the transformation is equal to the dimension of the domain vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9339811725468685
      },
      {
        "text": "The Rank-Nullity Theorem and Dimension: The rank-nullity theorem provides a fundamental connection between the rank of a linear transformation, the nullity of the transformation, and the dimension of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9113376868607252
      },
      {
        "text": "The Rank-Nullity Theorem and its Consequences: The rank-nullity theorem states that the rank of a linear transformation plus the nullity of the transformation is equal to the dimension of the domain. This theorem has many consequences, including the fact that the rank of a linear transformation is equal to the dimension of its image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9189329644165435
      },
      {
        "text": "The Rank-Nullity Theorem for Non-Square Matrices: The rank-nullity theorem for non-square matrices, which states that the rank of a non-square matrix plus the nullity of the matrix is equal to the number of rows of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.866539037574239
      },
      {
        "text": "Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces: The rank of a linear transformation is equal to the dimension of the image of the transformation, and the nullity of the transformation is equal to the dimension of the kernel of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9169542211329533
      },
      {
        "text": "The Rank-Nullity Theorem in the Context of Inner Product Spaces: This theorem states that the rank of a matrix (the dimension of its column space) plus the nullity of the matrix (the dimension of its null space) is equal to the number of columns in the matrix, even in inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8264462074980434
      },
      {
        "text": "Gibbs' Theorem: For a linear transformation T from a vector space V to a vector space W, the nullity of T is equal to the dimension of the null space of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8143143532618644
      },
      {
        "text": "Relationship between Span and Dimension using the Rank-Nullity Theorem: The dimension of a vector space is equal to the rank of a matrix, which is equal to the dimension of its column space, using the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8487608609626286
      },
      {
        "text": "The Rank-Nullity Theorem for Infinite-Dimensional Spaces: This theorem states that the rank of a matrix (the dimension of its column space) plus the nullity of the matrix (the dimension of its null space) is equal to the number of columns in the matrix, even in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8502588897135521
      },
      {
        "text": "Rank-Nullity Theorem and Finite-Dimensional Vector Spaces: The rank-nullity theorem provides a fundamental connection between the rank of a linear transformation, the nullity of the transformation, and the dimension of the original vector space. This theorem has important implications for linear algebra and its applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8727333470597325
      },
      {
        "text": "The Rank-Nullity Theorem: A theorem that describes the relationship between the rank and nullity of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.868934849187698
      },
      {
        "text": "Matrix Rank and Nullity Theorem: This theorem states that for a matrix A, the rank of A (the maximum number of linearly independent rows or columns) plus the nullity of A (the dimension of the null space of A) is equal to the number of columns of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9235712517204421
      },
      {
        "text": "Theorem on the Rank-Nullity Theorem: This theorem states that for a linear transformation T from a vector space V to itself, the rank of T plus the nullity of T is equal to the dimension of V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8909999697213924
      },
      {
        "text": "Rank-Nullity Theorem (continued): The rank-nullity theorem states that the rank of a linear transformation is equal to the dimension of the range of the transformation, and the nullity of a linear transformation is equal to the dimension of the null space of the transformation, where the rank is also equal to the number of linearly independent vectors in the range.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9181310337024922
      },
      {
        "text": "The Linear Operator Rank-Nullity Theorem: This theorem states that the rank of a linear transformation $T$ plus the nullity of $T$ is equal to the dimension of the domain of $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9247344732438822
      },
      {
        "text": "The Linear Operator Rank-Nullity Consequence: This consequence states that if the rank of a linear transformation $T$ is equal to the dimension of its image, then the nullity of $T$ is equal to the dimension of its kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8107647145938193
      },
      {
        "text": "Matrix Rank and Nullity: The rank-nullity theorem states that for a matrix A, the rank of A is equal to the dimension of the image of A and the nullity of A is equal to the dimension of the null space of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8726167781863152
      },
      {
        "text": "Rank-Nullity Theorem and Dimension: The rank-nullity theorem states that the rank of a linear transformation is equal to the dimension of the range of the transformation, and the nullity of a linear transformation is equal to the dimension of the null space of the transformation. This theorem implies that the dimension of a vector space is equal to the number of linearly independent vectors in the range of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8984457097044238
      },
      {
        "text": "Rank-Nullity Theorem and its Implications: Delving deeper into the Rank-Nullity Theorem, including its implications for understanding the relationship between the dimension of a vector space, the rank of a linear transformation, and the dimension of the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.872925295373487
      },
      {
        "text": "The Rank-Nullity Theorem for Complex Vector Spaces: The theorem that states that the sum of the rank and nullity of a linear transformation is equal to the dimension of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9164692158697485
      },
      {
        "text": "The Fundamental Theorem of Finite-Dimensional Vector Spaces and the Rank-Nullity Theorem: The fundamental theorem of finite-dimensional vector spaces and the rank-nullity theorem provide a fundamental connection between the dimension of a vector space and the rank and nullity of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8689398795583334
      },
      {
        "text": "Rank-Nullity Theorem (with non-standard basis): This theorem states that for a linear transformation T: V → W with a non-standard basis, the rank of T (the dimension of the range of T) plus the nullity of T (the dimension of the null space of T) is equal to the dimension of the domain V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8621714781056211
      },
      {
        "text": "Rank and Nullity Theorem for Vector Spaces: This theorem states that for a linear transformation T from a vector space V to itself, the rank of T plus the nullity of T is equal to the dimension of V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.938107696800299
      },
      {
        "text": "The rank-nullity theorem states that the rank of a matrix is equal to the dimension of its column space minus the nullity of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8860464936039625
      }
    ]
  },
  {
    "representative_text": "Direct Sum of Subspaces: The direct sum of two subspaces is a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Direct Sum of Subspaces: The direct sum of two subspaces is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Direct Sum: The direct sum of two subspaces is a subspace that contains both subspaces and is closed under addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8994368488127067
      }
    ]
  },
  {
    "representative_text": "Quotient Space of a Subspace: The quotient space of a subspace is a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Quotient Space of a Subspace: The quotient space of a subspace is a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace of a Quotient Space: The subspace of a quotient space is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9002604692819927
      },
      {
        "text": "Quotient Space: The quotient space of a vector space, which is a subspace obtained by identifying certain vectors as equivalent, and its relationship with linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8157813789609274
      },
      {
        "text": "Quotient Space of a Vector Space with a Non-Trivial Null Space: The quotient space of a vector space with a non-trivial null space is a vector space, but its dimension may not be the dimension of the original space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8098811515248157
      }
    ]
  },
  {
    "representative_text": "$T(u + v) = T(u) + T(v)$ for all $u, v \\in V$ (additivity)",
    "is_in_domain": false,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "$T(u + v) = T(u) + T(v)$ for all $u, v \\in V$ (additivity)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Additivity: If $T$ and $S$ are linear operators, then $T(u) + S(u) = (T + S)(u)$ for all $u \\in V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8400512027132516
      },
      {
        "text": "Linearity: If $T$ is a linear operator and $c, d \\in \\mathbb{F}$, then $T(cu + dv) = cT(u) + dT(v)$ for all $u, v \\in V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8607961137562282
      },
      {
        "text": "Linearity of T:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8289733568472353
      },
      {
        "text": "T(u + v) = T(u) + T(v)",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9086151293333388
      }
    ]
  },
  {
    "representative_text": "$T(cu) = cT(u)$ for all $c \\in \\mathbb{F}$ and $u \\in V$ (homogeneity)",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "$T(cu) = cT(u)$ for all $c \\in \\mathbb{F}$ and $u \\in V$ (homogeneity)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Homogeneity: If $T$ is a linear operator and $c \\in \\mathbb{F}$, then $T(cu) = cT(u)$ for all $u \\in V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9393567248343486
      },
      {
        "text": "T(au) = aT(u)",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8284263958423419
      }
    ]
  },
  {
    "representative_text": "Endomorphism: A linear operator from a vector space to itself.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Endomorphism: A linear operator from a vector space to itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Automorphism: A linear operator from a vector space to itself that is both injective and surjective, and has an inverse.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Automorphism: A linear operator from a vector space to itself that is both injective and surjective, and has an inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Automorphisms: A linear transformation that is an automorphism of its domain is a transformation that is bijective and has an inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8132103320743435
      }
    ]
  },
  {
    "representative_text": "Isomorphism: A bijective linear operator between two vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Isomorphism: A bijective linear operator between two vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Isomorphism between Vector Spaces: Two vector spaces are isomorphic if there exists a bijective linear transformation between them.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8545689835742429
      },
      {
        "text": "Isomorphism of Subspaces: Two subspaces are isomorphic if there exists a bijective linear transformation between them.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8725395467502315
      },
      {
        "text": "Vector Space Isomorphism: A bijective linear transformation between two vector spaces that preserves the operations of vector addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9140889004076399
      },
      {
        "text": "Isomorphism between V and W: The relationship between isomorphisms and linear transformations, including the concept of isomorphisms between vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8502685068394709
      },
      {
        "text": "Vector Space Isomorphism Theorem: A theorem that states that if two vector spaces have the same dimension, then there exists a bijective linear transformation between them.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8762122287327154
      },
      {
        "text": "Vector Space Isomorphism: A bijective linear transformation of a generalized the basis:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572558537664487
      },
      {
        "text": "Vector Space Homomorphisms: A bijective linear transformation between two vector spaces that preserves the operations of vector addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8846063451771561
      },
      {
        "text": "Linear Transformation and Isomorphism between Vector Spaces: The concept of isomorphism between vector spaces, which is a bijective linear transformation between two vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9441160510079307
      }
    ]
  },
  {
    "representative_text": "Bilinear Form: A function $B: V \\times W \\to \\mathbb{F}$ that is linear in each argument.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Bilinear Form: A function $B: V \\times W \\to \\mathbb{F}$ that is linear in each argument.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bilinear Form: A bilinear form on a vector space V is a function that satisfies the following properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8603822683076742
      },
      {
        "text": "Bilinear Forms: A bilinear form is a function from a vector space to the real numbers that is linear in each argument separately.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9006558627286372
      },
      {
        "text": "Definition: A bilinear form is a function that takes two vectors as input and returns a scalar value.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9073836576686267
      }
    ]
  },
  {
    "representative_text": "Injectivity: If $T$ is a linear operator, then $T(u) = T(v)$ implies $u = v$ for all $u, v \\in V$.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Injectivity: If $T$ is a linear operator, then $T(u) = T(v)$ implies $u = v$ for all $u, v \\in V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Surjectivity: If $T$ is a linear operator, then for every $w \\in W$, there exists $v \\in V$ such that $T(v) = w$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Surjectivity: If $T$ is a linear operator, then for every $w \\in W$, there exists $v \\in V$ such that $T(v) = w$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Combination Theorem: For a linear operator $T: V \\to W$ and vectors $v1, v2, \\ldots, vn \\in V$, if $T(v1) = T(v2) = \\ldots = T(vn) = 0$, then $T(a1v1 + a2v2 + \\ldots + anvn) = 0$ for any scalars $a1, a2, \\ldots, a_n$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Combination Theorem: For a linear operator $T: V \\to W$ and vectors $v1, v2, \\ldots, vn \\in V$, if $T(v1) = T(v2) = \\ldots = T(vn) = 0$, then $T(a1v1 + a2v2 + \\ldots + anvn) = 0$ for any scalars $a1, a2, \\ldots, a_n$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Theorem: For a linear operator $T: V \\to V$, there exists a basis for $V$ such that the matrix representation of $T$ with respect to this basis is diagonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Spectral Theorem: For a linear operator $T: V \\to V$, there exists a basis for $V$ such that the matrix representation of $T$ with respect to this basis is diagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Spectral Theorem and its Consequences: The spectral theorem states that every linear transformation between two vector spaces can be represented by a diagonal matrix. This theorem has many consequences, including the fact that the eigenvalues of a linear transformation are the diagonal entries of its matrix representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8070507945239379
      },
      {
        "text": "The Spectral Theorem: The spectral theorem states that every linear transformation between two vector spaces can be represented by a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9229063692620296
      },
      {
        "text": "Spectral Theorem: The spectral theorem states that a linear transformation T from a finite-dimensional vector space V to itself is diagonalizable if and only if it is similar to a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9221414050392311
      },
      {
        "text": "Spectral Theorem: A theorem that relates the rank and nullity of a linear transformation to its eigenvalues and eigenvectors, which is essential in understanding the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8067868506974385
      },
      {
        "text": "Spectral Theorem and Infinite-Dimensional Vector Spaces: The spectral theorem is a fundamental theorem in linear algebra that provides a connection between linear transformations and their eigenvalues and eigenvectors. This theorem has important implications for linear algebra and its applications in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8213792961904902
      },
      {
        "text": "The Spectral Theorem and Linear Algebra: The spectral theorem provides a fundamental connection between linear transformations and their eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8873325882944174
      }
    ]
  },
  {
    "representative_text": "Diagonalization Theorem: For a linear operator $T: V \\to V$, if $T$ is diagonalizable, then there exists a basis for $V$ such that the matrix representation of $T$ with respect to this basis is diagonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 27,
    "detailed_sources": [
      {
        "text": "Diagonalization Theorem: For a linear operator $T: V \\to V$, if $T$ is diagonalizable, then there exists a basis for $V$ such that the matrix representation of $T$ with respect to this basis is diagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Diagonalization Theorem: If matrix A is diagonalizable, then there exists an invertible matrix P and a diagonal matrix D such that A = PDP^(-1), where P is composed of eigenvectors of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.82588620378959
      },
      {
        "text": "Diagonalization Theorem: If a matrix A is diagonalizable, then there exists a basis of eigenvectors, and the matrix can be written in the form A = PDP^(-1), where D is a diagonal matrix and P is an invertible matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9539191400067619
      },
      {
        "text": "Diagonalization Theorem: A square matrix A can be diagonalized if and only if it has a full set of linearly independent eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8935669855919193
      },
      {
        "text": "Peano's Theorem: If A is a square matrix with distinct eigenvalues, then A can be diagonalized.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8280663408651876
      },
      {
        "text": "Lagrange's Theorem: If A is a square matrix with distinct eigenvalues, then A can be diagonalized.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8654600194489578
      },
      {
        "text": "Gelfand's Theorem: If A is a square matrix with distinct eigenvalues, then A can be diagonalized using a similarity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8527495996549852
      },
      {
        "text": "Lanczos Theorem: If A is a symmetric matrix with distinct eigenvalues, then A can be diagonalized using a similarity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8453656980372732
      },
      {
        "text": "Orthogonal Diagonalization Theorem: A square matrix can be orthogonally diagonalized if and only if it has a full set of linearly independent eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8317727601787878
      },
      {
        "text": "Statement: A square matrix can be diagonalized if and only if it has a full set of linearly independent eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8346790219607388
      },
      {
        "text": "The spectral theorem states that if A is a square matrix, then there exists a unitary matrix P such that P^(-1)AP = D, where D is a diagonal matrix containing the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8180143574210988
      },
      {
        "text": "Jacobi's Theorem: If A is a square matrix with distinct eigenvalues, then A can be diagonalized using a similarity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8808436968507733
      },
      {
        "text": "The Spectral Theorem: This theorem states that a matrix A can be diagonalized by an orthogonal matrix, i.e., A = UΛU^T, where U is an orthogonal matrix and Λ is a diagonal matrix containing the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8485824717208175
      },
      {
        "text": "The Spectral Theorem: A matrix can be diagonalized, and this theorem provides a fundamental connection between linear transformations and their representations as matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8534373383972407
      },
      {
        "text": "The Fundamental Theorem of Linear Algebra: This theorem states that every square matrix can be diagonalized using an orthogonal matrix, and its eigenvalues are the diagonal elements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8782344583074955
      },
      {
        "text": "Gelfand's Theorem: This theorem states that a matrix A can be diagonalized if and only if its characteristic polynomial can be factored into distinct linear factors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8213793316904658
      },
      {
        "text": "The Jordan-Chevalley Theorem: This theorem states that a matrix A can be diagonalized if and only if its Jordan canonical form is a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8927443196296457
      },
      {
        "text": "Spectral Theorem: A theorem that states that a square matrix can be orthogonally diagonalized if and only if it is a normal matrix, which is a matrix that commutes with its conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.816557233117624
      },
      {
        "text": "The Spectral Theorem for Normal Matrices: A theorem that states that a normal matrix can be orthogonally diagonalized if and only if it is a normal matrix, which is a matrix that commutes with its conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8241978122674302
      },
      {
        "text": "The Spectral Theorem for Hermitian Matrices: A theorem that states that a Hermitian matrix can be orthogonally diagonalized using only real eigenvalues and real eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8332921545240941
      },
      {
        "text": "The Spectral Theorem for Symmetric Matrices: A theorem that states that a symmetric matrix can be orthogonally diagonalized using only real eigenvalues and real eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8554852075356051
      },
      {
        "text": "Spectral Theorem: A theorem that states that every square matrix can be diagonalized using an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8699476308110192
      },
      {
        "text": "Theorem on the Jordan Canonical Form: This theorem states that a linear transformation T from a finite-dimensional vector space V to itself is diagonalizable if and only if its Jordan canonical form is diagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8471714867261753
      },
      {
        "text": "Theorem on the Diagonalization of a Linear Transformation: This theorem states that a linear transformation T from a finite-dimensional vector space V to itself is diagonalizable if and only if there exists a basis for V consisting of eigenvectors of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8810191051465148
      },
      {
        "text": "The Gelfand's Theorem (Alternative Proof): Gelfand's theorem states that a matrix can be diagonalized if and only if its characteristic polynomial can be factored into distinct linear factors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8155163711758817
      },
      {
        "text": "Diagonalization Theorem: A theorem that states that every square matrix can be diagonalized using an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8531049997216944
      },
      {
        "text": "The Spectral Theorem: This theorem states that a matrix A can be diagonalized by a unitary matrix U, such that U^(-1)AU is a diagonal matrix containing the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9067609490387936
      }
    ]
  },
  {
    "representative_text": "Schur's Theorem: For a linear operator $T: V \\to V$, if $T$ is diagonalizable, then there exists a basis for $V$ such that the matrix representation of $T$ with respect to this basis is upper triangular.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Schur's Theorem: For a linear operator $T: V \\to V$, if $T$ is diagonalizable, then there exists a basis for $V$ such that the matrix representation of $T$ with respect to this basis is upper triangular.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Schur's Theorem: A linear transformation T from a vector space V to a vector space W is diagonalizable if and only if it is similar to a diagonalizable matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8851329127201193
      },
      {
        "text": "Schur's Theorem: Schur's theorem states that a linear transformation T from a finite-dimensional vector space V to itself is diagonalizable if and only if it is similar to a diagonalizable matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9408172146539839
      },
      {
        "text": "Theorem on the Schur's Theorem: This theorem states that a linear transformation T from a finite-dimensional vector space V to itself is diagonalizable if and only if it is similar to a diagonalizable matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9359725208630971
      }
    ]
  },
  {
    "representative_text": "$\\mathbb{F}$: The field of scalars, usually $\\mathbb{R}$ or $\\mathbb{C}$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$\\mathbb{F}$: The field of scalars, usually $\\mathbb{R}$ or $\\mathbb{C}$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "$\\mathbb{V}$: A vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "$\\mathbb{V}$: A vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "$\\mathbb{W}$: A vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8851606460075032
      },
      {
        "text": "V: The vector space",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8700296223934784
      },
      {
        "text": "V: A vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9437763638983259
      },
      {
        "text": "S: A set of vectors in a vector space V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8215697711867793
      }
    ]
  },
  {
    "representative_text": "$T: V \\to W$: A linear operator from $V$ to $W$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$T: V \\to W$: A linear operator from $V$ to $W$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "$A \\in \\mathcal{M}_n(\\mathbb{F})$: A matrix of size $n \\times n$ with entries in $\\mathbb{F}$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$A \\in \\mathcal{M}_n(\\mathbb{F})$: A matrix of size $n \\times n$ with entries in $\\mathbb{F}$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "$\\mathcal{L}(V, W)$: The set of all linear operators from $V$ to $W$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "$\\mathcal{L}(V, W)$: The set of all linear operators from $V$ to $W$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "$\\mathcal{M}(V, W)$: The set of all linear transformations from $V$ to $W$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8482907227830894
      }
    ]
  },
  {
    "representative_text": "$\\mathcal{B}(V)$: A basis for $V$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$\\mathcal{B}(V)$: A basis for $V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "$\\operatorname{im}(T)$: The image of the linear operator $T$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "$\\operatorname{im}(T)$: The image of the linear operator $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "$\\operatorname{range}(T)$: The range of the linear operator $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.834378822922402
      }
    ]
  },
  {
    "representative_text": "$\\ker(T)$: The kernel of the linear operator $T$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$\\ker(T)$: The kernel of the linear operator $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "$\\operatorname{rank}(T)$: The rank of the linear operator $T$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$\\operatorname{rank}(T)$: The rank of the linear operator $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "$\\operatorname{nullity}(T)$: The nullity of the linear operator $T$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$\\operatorname{nullity}(T)$: The nullity of the linear operator $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "$\\operatorname{domain}(T)$: The domain of the linear operator $T$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$\\operatorname{domain}(T)$: The domain of the linear operator $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "$\\operatorname{adj}(T)$: The adjoint of the linear operator $T$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$\\operatorname{adj}(T)$: The adjoint of the linear operator $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "$\\operatorname{det}(T)$: The determinant of the matrix representation of $T$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "$\\operatorname{det}(T)$: The determinant of the matrix representation of $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix: A rectangular array of numbers, symbols, or expressions, arranged in rows and columns, with the property that the number of columns in the first row is equal to the number of rows in the second row.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Matrix: A rectangular array of numbers, symbols, or expressions, arranged in rows and columns, with the property that the number of columns in the first row is equal to the number of rows in the second row.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix: A rectangular array of numbers, symbols, or expressions, arranged in rows and columns. It is denoted by a capital letter (e.g., A, B, C).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9305642967367088
      },
      {
        "text": "Matrix: A rectangular array of numbers, symbols, or expressions, arranged in rows and columns. It can be represented as A = [aij], where aij is the element in the ith row and jth column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9512512071337657
      },
      {
        "text": "Matrix: A rectangular array of scalars, with rows and columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8808529747803
      },
      {
        "text": "Matrix: A rectangular array of numbers, symbols, or expressions, arranged in rows and columns. Matrices can be thought of as collections of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9374906110007133
      },
      {
        "text": "A matrix is a rectangular array of numbers, symbols, or expressions.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9152217170350291
      }
    ]
  },
  {
    "representative_text": "Matrix Multiplication: The process of multiplying two matrices to obtain a new matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Matrix Multiplication: The process of multiplying two matrices to obtain a new matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Multiplication: Two matrices A and B can be multiplied to obtain a new matrix C, denoted by A * B, where the element C(i, j) is calculated as the dot product of the ith row of A and the jth column of B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8206796699808858
      },
      {
        "text": "Matrix Multiplication: The process of multiplying two matrices to produce another matrix. The resulting matrix has the same number of rows as the first matrix and the same number of columns as the second matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9019919955300183
      },
      {
        "text": "Matrix Multiplication: Matrix multiplication is a way of combining two matrices to produce another matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9207789773324314
      },
      {
        "text": "Matrix Multiplication: The product of two matrices A and B is a new matrix whose elements are the dot products of the rows of A and the columns of B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9278587427216911
      },
      {
        "text": "Matrix multiplication: the product of two matrices is a matrix calculated by multiplying the rows of the first matrix by the columns of the second matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8790974053368791
      },
      {
        "text": "Theorem on the Product of Two Matrices: The product of two matrices A and B is defined as the matrix obtained by multiplying the rows of A by the columns of B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8497961335862398
      },
      {
        "text": "Matrix multiplication is defined by the dot product of rows and columns.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8818270443213212
      }
    ]
  },
  {
    "representative_text": "Matrix Multiplication Properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Matrix Multiplication Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Operations:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8283525232668782
      },
      {
        "text": "Advanced Matrix Operations:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8363512195586161
      },
      {
        "text": "Matrix Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9058418784947447
      },
      {
        "text": "**Properties of a matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8379671192034485
      }
    ]
  },
  {
    "representative_text": "Eigenvalues and Eigenvectors: A scalar value λ and a non-zero vector v such that Av = λv.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 16,
    "detailed_sources": [
      {
        "text": "Eigenvalues and Eigenvectors: A scalar value λ and a non-zero vector v such that Av = λv.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalues: The eigenvalues of a matrix A are the scalar values that satisfy the equation A v = λ v, where v is an eigenvector and λ is the corresponding eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8557194477294668
      },
      {
        "text": "Eigenvectors: The eigenvectors of a matrix A are the non-zero vectors that satisfy the equation A v = λ v, where λ is the corresponding eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8520219746291191
      },
      {
        "text": "The Eigenvector Equation: (A - λI)v = 0, where A is the matrix, λ is the eigenvalue, I is the identity matrix, and v is the eigenvector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.806874770172384
      },
      {
        "text": "Eigenvalues: The scalar values that are characteristic of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8294210507480702
      },
      {
        "text": "Statement: The eigenvalues of a matrix are the scalar values that satisfy the characteristic equation of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8111061551159583
      },
      {
        "text": "The eigenvalues of a matrix A are the roots of the characteristic equation det(A - λI) = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8161327331206394
      },
      {
        "text": "Eigenvector Equation: The equation Av = λv represents the eigenvector equation, where λ is the eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8192827301745715
      },
      {
        "text": "Eigenvalues and Eigenvectors: The scalar values and non-zero vectors that, when multiplied by a matrix, produce a scaled version of the same vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8600050177811754
      },
      {
        "text": "Eigenvalues and Eigenvectors: Scalars and vectors that satisfy the equation Av = λv, where A is a matrix, v is a vector, and λ is an eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9404503462817398
      },
      {
        "text": "Eigenvalue and Eigenvector: A scalar (eigenvalue) and a non-zero vector (eigenvector) that satisfy the equation $T(v) = λv$, where $λ$ is the eigenvalue and $v$ is the eigenvector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8441123681321593
      },
      {
        "text": "Eigenvalues and Eigenvectors: A pair of scalar and vector that satisfy the equation Ax = λx, where A is a square matrix and λ is an eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9286318535190339
      },
      {
        "text": "The Eigenvalue Equation: This equation is a linear equation in the variables λ and v, where λ is the eigenvalue and v is the corresponding eigenvector. It is given by Av = λv, where A is the linear transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8205312401397693
      },
      {
        "text": "The concept of eigenvalues and eigenvectors being a scalar (eigenvalue) and a non-zero vector (eigenvector) that satisfies the equation $T(v) = λv$",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8341541555194829
      },
      {
        "text": "Eigenvalues and Eigenvectors of a Linear Transformation: Eigenvalues and eigenvectors of a linear transformation T from a finite-dimensional vector space V to itself are scalars and vectors that satisfy the equation T(v) = λv for some scalar λ.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8307323596686733
      },
      {
        "text": "Eigenvalue and Eigenvector Pairs: The scalar and vector that satisfy the equation Av = λv, where A is a matrix, v is a vector, and λ is an eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9173076432789801
      }
    ]
  },
  {
    "representative_text": "Theorem 1: Existence of Matrix Representation",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem 1: Existence of Matrix Representation",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem 2: Uniqueness of Matrix Representation",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem 2: Uniqueness of Matrix Representation",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem 3: Invertibility",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem 3: Invertibility",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem 4: Eigenvalues and Eigenvectors",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem 4: Eigenvalues and Eigenvectors",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem 5: Diagonalization",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Theorem 5: Diagonalization",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "This theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.803234728840724
      },
      {
        "text": "Based on theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8419833974181696
      },
      {
        "text": "1. Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8505590422645157
      }
    ]
  },
  {
    "representative_text": "Rank: The number of linearly independent rows or columns in a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 16,
    "detailed_sources": [
      {
        "text": "Rank: The number of linearly independent rows or columns in a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rank of a Matrix: The rank of a matrix A is the maximum number of linearly independent rows or columns in A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9112220286014667
      },
      {
        "text": "Rank of a Matrix: The maximum number of linearly independent rows or columns in a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9699531303428179
      },
      {
        "text": "The Nullity of a Matrix: The number of linearly independent columns in a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8481406671310221
      },
      {
        "text": "Rank of a Matrix: The rank of a matrix is equal to the dimension of its column space, which is related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8700056638318754
      },
      {
        "text": "Rank of a Matrix: The rank of a matrix $A$ is equal to the dimension of the column space of $A$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8399170743292877
      },
      {
        "text": "Rank and Determinant: The rank of a matrix A is equal to the number of linearly independent columns or rows of A. This can be determined using the determinant, as det(A) ≠ 0 if and only if the rank of A is equal to the number of columns or rows.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8328129740694701
      },
      {
        "text": "The relationship between linear independence and the rank-nullity theorem: The rank of a matrix is equal to the dimension of the span of its linearly independent columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8624468370875921
      },
      {
        "text": "The Relationship between Linear Independence and the Rank of a Matrix: Examining the relationship between linear independence and the rank of a matrix, which involves the use of linear independence to define a notion of dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8481217343746712
      },
      {
        "text": "The Relationship between Linear Independence and the Rank of a Matrix: If a matrix has a non-zero rank, then its row space and column space are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8647060763290652
      },
      {
        "text": "The Relationship Between Linear Independence and the Rank of a Matrix: The rank of a matrix is closely related to linear independence. If the columns of a matrix are linearly independent, the rank of the matrix is equal to the number of linearly independent vectors in the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8948106519074597
      },
      {
        "text": "Theorem on the Rank of a Matrix: The rank of a matrix A is defined as the maximum number of linearly independent rows or columns of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8971456191461393
      },
      {
        "text": "The rank of a matrix: The maximum number of linearly independent rows or columns in a matrix, which is related to the determinant of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.910837747990645
      },
      {
        "text": "Rank of a Matrix and Linear Independence: Investigate the relationship between the rank of a matrix and linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9077060255783196
      },
      {
        "text": "Rank of a matrix: The maximum number of linearly independent rows or columns in a matrix, which can be used to determine whether a matrix has a solution to a system of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8925376635395137
      },
      {
        "text": "Rank and Nullity: The rank and nullity of a matrix are properties of the matrix that describe the number of linearly independent rows and columns of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8341271951083891
      }
    ]
  },
  {
    "representative_text": "A^(-1): The inverse matrix of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A^(-1): The inverse matrix of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A^T: The transpose of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A^T: The transpose of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "det(A): The determinant of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "det(A): The determinant of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "|A|: The determinant of matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8927533918005879
      }
    ]
  },
  {
    "representative_text": "λ: An eigenvalue of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "λ: An eigenvalue of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "v: An eigenvector of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "v: An eigenvector of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "P: A matrix whose columns are the eigenvectors of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "P: A matrix whose columns are the eigenvectors of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "D: A diagonal matrix containing the eigenvalues of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "D: A diagonal matrix containing the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "I: The identity matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "I: The identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "I: The identity matrix, which is a square matrix with 1s on the main diagonal and 0s elsewhere.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.908981773266472
      },
      {
        "text": "I_n: The identity matrix of size n x n.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9033782527067358
      }
    ]
  },
  {
    "representative_text": "Identity Matrix: A square matrix with 1's on the diagonal and 0's elsewhere.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Identity Matrix: A square matrix with 1's on the diagonal and 0's elsewhere.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Identity Matrix: A square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by the letter I.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9415123939974075
      },
      {
        "text": "Identity Matrix (I): A square matrix with ones on the main diagonal and zeros elsewhere. It is used to represent the identity transformation, which leaves a vector unchanged.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9080395633907298
      },
      {
        "text": "The identity matrix is the matrix with 1s on the diagonal and 0s elsewhere.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8626067369151696
      }
    ]
  },
  {
    "representative_text": "Zero Matrix: A square matrix with all entries equal to 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Zero Matrix: A square matrix with all entries equal to 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Zero Matrix: A matrix with all elements equal to zero, denoted by 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9397242219331113
      },
      {
        "text": "0: The zero matrix, which is a square matrix with all elements equal to 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9126609054922736
      },
      {
        "text": "O_n: The zero matrix of size n x n.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8150694625840043
      },
      {
        "text": "Zero Matrix: A matrix with zeros in every entry, denoted by 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9259815624117819
      },
      {
        "text": "Zero Matrix: A square matrix with zeros everywhere.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9512489211382824
      }
    ]
  },
  {
    "representative_text": "Symmetric Matrix: A square matrix that is equal to its transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Symmetric Matrix: A square matrix that is equal to its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Symmetric Matrix: A symmetric matrix is a square matrix that is equal to its transpose. Eigenvalues and eigenvectors can be used to analyze symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8211709311522548
      },
      {
        "text": "The concept of \"symmetric\" and \"skew-symmetric\" matrices: Symmetric matrices are matrices that are equal to their own transpose, while skew-symmetric matrices are matrices that are equal to the negative of their own transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8321016440691393
      },
      {
        "text": "Symmetric Matrices: A symmetric matrix is a matrix that is equal to its own transpose. Symmetric matrices have many applications in linear algebra, including finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9242395847000276
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD): A factorization of a matrix A into three matrices U, Σ, and V^T, such that A = U Σ V^T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 60,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD): A factorization of a matrix A into three matrices U, Σ, and V^T, such that A = U Σ V^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a factorization technique that decomposes a matrix into three matrices: U, Σ, and V, such that A = UΣV^T, where A is the original matrix and U, V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9346007061979793
      },
      {
        "text": "A = UΣV^T: The SVD decomposition of matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.813009762162618
      },
      {
        "text": "Singular value decomposition formula: A = U Σ V^T, where U and V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8803738341249623
      },
      {
        "text": "Singular Value Decomposition (SVD): The SVD of a matrix $A$ is a factorization of the form $A = U\\Sigma V^T$ where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix containing the singular values of $A$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9487388000477922
      },
      {
        "text": "Singular Value Decomposition (SVD): A factorization technique that decomposes a matrix into three matrices: U, Σ, and V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9019200501863587
      },
      {
        "text": "Singular Value Decomposition (SVD):",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8457318329401868
      },
      {
        "text": "SVD is a factorization technique that decomposes a matrix into three matrices: U, Σ, and V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.913870258985473
      },
      {
        "text": "Singular Value Decomposition (SVD): A factorization of a matrix into three matrices: U, Σ, and V, which can be used to analyze and solve linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9249443244377724
      },
      {
        "text": "Singular Value Decomposition (SVD): An algorithm for decomposing a matrix into a product of three matrices: an orthogonal matrix, a diagonal matrix, and an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9182040521466925
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a factorization of a matrix into three matrices: U, Σ, and V^T, where U and V are orthogonal matrices and Σ is a diagonal matrix containing the singular values of the matrix. SVD is used to find eigenvalues and eigenvectors of a matrix and can be used for matrix factorization and dimensionality reduction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9071443230732363
      },
      {
        "text": "Singular Value Decomposition (SVD): A decomposition of a matrix into three orthogonal matrices, which is useful in solving systems of linear equations and in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9017720261705602
      },
      {
        "text": "Singular Value Decomposition: A matrix can be decomposed into a sum of singular value matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8655818901301259
      },
      {
        "text": "Singular Value Decomposition (SVD): A factorization of a matrix into three matrices (U, Σ, V^T) that are used to decompose linear transformations into their rank-one components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9481384855333209
      },
      {
        "text": "Singular Value Decomposition (SVD): The SVD is a factorization of a linear transformation matrix that provides information about the transformation's properties, such as its rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8916169490044588
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a factorization of a matrix into three matrices: U, Σ, and V^T. The determinant of Σ is the product of the singular values of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8772578803416912
      },
      {
        "text": "Singular Value Decomposition (SVD): This is a factorization of a matrix into the product of three matrices: U, Σ, and V^T. SVD has many applications in linear algebra, machine learning, and computer graphics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9492559504901159
      },
      {
        "text": "Singular Value Decomposition (SVD): The SVD is a factorization of a matrix into the product of three matrices: U, Σ, and V^T. This decomposition is useful in understanding the properties of matrices and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9037554353442668
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a factorization of a matrix A into the product of three matrices: U, Σ, and V^T. Eigenvalues and eigenvectors can be used to analyze the SVD of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8648045483797178
      },
      {
        "text": "The Singular Value Decomposition (SVD) of a Matrix: This is a factorization of a matrix A into the product A = U Σ V^T, where U and V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.946991404164257
      },
      {
        "text": "Singular Value Decomposition (SVD): This is a factorization of a matrix into the product of three matrices: U, Σ, and V. The SVD can be used to find the inverse of a matrix and can also be used to solve systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8974275726286256
      },
      {
        "text": "Singular Value Decomposition (SVD): This is a factorization of a matrix A into the product A = UΣV^(-1), where U and V are unitary matrices and Σ is a diagonal matrix containing the singular values of A. The determinant of A can be computed using its SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8779877239504419
      },
      {
        "text": "Singular value decomposition: A matrix can be decomposed into a product of a diagonal matrix containing its singular values, a matrix containing the corresponding singular vectors, and a matrix containing the remaining singular vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8728935180076792
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a factorization technique that can be used to decompose matrices into their singular values and vectors, which is useful in computer graphics for tasks such as texture mapping and lighting.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8151656089127054
      },
      {
        "text": "Singular Value Decomposition (SVD): A matrix can be decomposed into a product of three matrices: an orthogonal matrix, a diagonal matrix, and a matrix of singular values.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9472105414156908
      },
      {
        "text": "The Singular Value Decomposition (SVD): The SVD is a factorization of a matrix into three matrices (U, Σ, V^T) that are used to decompose linear transformations into their rank-one components. This concept is essential in understanding the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8907532791153285
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD can be used to decompose a matrix into its rank, nullity, and other properties, which is essential in understanding the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8299984048282371
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix: SVD of a matrix is a factorization of the matrix into the product of three matrices: U, Σ, and V^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9580092308645662
      },
      {
        "text": "Singular Value Decomposition (SVD): A factorization technique that decomposes a matrix into the product of three matrices: U, Σ, and V. This can be used to find the inverse of a matrix and to analyze the properties of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9226326201910011
      },
      {
        "text": "Singular Value Decomposition (SVD): This is a factorization of a matrix into the product of three matrices, which can be used to describe the properties of the matrix and solve certain types of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9464098504044695
      },
      {
        "text": "Singular Value Decomposition (SVD) and Eigenvalue Decomposition: SVD is a factorization of a matrix A into the product of three matrices: U, Σ, and V^T. Eigenvalue decomposition is a factorization of a matrix A into the product of a diagonal matrix D and an orthogonal matrix P, where D contains the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.880152275391207
      },
      {
        "text": "Singular Value Decomposition (SVD) and its Applications: This involves the study of the SVD of a matrix and its applications in various fields, such as data compression, image processing, and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8295539149560709
      },
      {
        "text": "The concept of SVD being a factorization of a matrix into three matrices (U, Σ, V^T) that are used to decompose linear transformations into their rank-one components",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.872408244548305
      },
      {
        "text": "Singular Value Decomposition (SVD): The SVD is a factorization of a matrix into the product of three matrices: U, Σ, and V. It is closely related to eigenvalue decomposition and is used in many applications, including image compression and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9459593732348288
      },
      {
        "text": "The Singular Value Decomposition (SVD): The SVD is a factorization of a matrix into the product of three matrices: an orthogonal matrix, a diagonal matrix, and a transpose of an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9392284385720298
      },
      {
        "text": "Singular Value Decomposition (SVD) and Its Applications: SVD is a factorization of a matrix into the product of three matrices: U, Σ, and V. It has applications in image compression, data analysis, and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9285800266450851
      },
      {
        "text": "The Computation of Eigenvalues using the Singular Value Decomposition (SVD): The SVD is a factorization of a matrix into the product of three matrices: U, Σ, and V. It can be used to compute eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8733079007979796
      },
      {
        "text": "Singular Value Decomposition (continued): Singular value decomposition (SVD) is a factorization of a matrix into the product of three matrices: U, Σ, and V, where U and V are orthogonal matrices and Σ is a diagonal matrix containing the singular values.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9398951713182088
      },
      {
        "text": "Singular Value Decomposition (SVD): A method for decomposing a matrix into three matrices: U, Σ, and V, which is useful for determining linear independence and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.927851337930885
      },
      {
        "text": "Singular Value Decomposition (SVD) of Non-Symmetric Matrices: The SVD of a non-symmetric matrix is a factorization of the matrix into the product of three matrices: U, Σ, and V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8975326473612212
      },
      {
        "text": "The Singular Value Decomposition (SVD): SVD is a factorization technique that decomposes a matrix into three matrices: U, Σ, and V. It can be used to determine the rank of a matrix, find the eigenvalues and eigenvectors, and perform linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9427302643826967
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a factorization of a matrix into the product of three matrices: U, Σ, and V. It can be used to find the inverse of a matrix, solve systems of linear equations, and represent linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9288831041170763
      },
      {
        "text": "Singular Value Decomposition (SVD) of Complex Matrices: The factorization of complex matrices into U, Σ, and V^T, where U and V are orthogonal matrices and Σ is a diagonal matrix containing the singular values of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9124691990650711
      },
      {
        "text": "Singular Value Decomposition (SVD): The SVD is a factorization of a matrix into the product of three matrices: U, Σ, and V. It can be used to compute eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9421294770793109
      },
      {
        "text": "Singular Value Decomposition (SVD) and Linear Algebra: SVD provides a fundamental connection between linear transformations and their representations as matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8536280825304836
      },
      {
        "text": "Singular Value Decomposition (SVD): This is a factorization of a matrix into the product of three matrices: U, Σ, and V. The SVD is useful for solving systems of linear equations and for finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9577190009877006
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a technique used in computer graphics and game development to decompose matrices into their singular values and vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8508360076360694
      },
      {
        "text": "Singular Value Decomposition: A method for decomposing a matrix into a product of orthogonal matrices and diagonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8784074690966663
      },
      {
        "text": "Singular Value Decomposition (SVD) and its Applications: A factorization of a matrix into three matrices: U, Σ, and V^T, such that U is unitary, Σ is diagonal, and V^T is orthogonal. SVD has numerous applications in linear algebra, machine learning, and data analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9572580942596323
      },
      {
        "text": "Singular Value Decomposition (SVD): Explore SVD, which is a factorization of a matrix into the product of three matrices: U, Σ, and V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9190146018085522
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix: The SVD of a matrix is a factorization of the matrix into the product of three.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9161769004051818
      },
      {
        "text": "Linear Transformation and Singular Value Decomposition (SVD): The SVD of a linear transformation, which is a factorization of the transformation matrix into three matrices: U, Σ, and V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8961313280680524
      },
      {
        "text": "**Singular Value Decomposition (SVD) that are:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8520413824862841
      },
      {
        "text": "Singular value decomposition (SVD): A factorization of a matrix into three matrices: U (an orthogonal matrix), Σ (a diagonal matrix), and V^T (the transpose of an orthogonal matrix), which can be used to decompose a matrix into its singular values and their corresponding right and left singular vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9455271551897982
      },
      {
        "text": "Singular Value Decomposition (SVD) with Applications: The SVD is used in linear algebra to decompose a matrix into its singular values and their corresponding right and left singular vectors, and to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8999960158766451
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix with Applications: A factorization of a matrix into three matrices: U (an orthogonal matrix), Σ (a diagonal matrix), and V^T (the transpose of an orthogonal matrix), which can be used to decompose a matrix into its singular values and their corresponding right and left singular vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9235008169363838
      },
      {
        "text": "Singular Value Decomposition with Applications: The SVD is used in linear algebra to decompose a matrix into its singular values and their corresponding right and left singular vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9063634850958945
      },
      {
        "text": "Singular value decomposition (SVD) of a matrix: The SVD is a factorization of a matrix into three matrices: U, Σ, and V^T, which can be used to decompose a matrix into its singular values and their corresponding right and left singular vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9651868422613616
      },
      {
        "text": "SVD is a method for decomposing a matrix into three matrices: U, Σ, and V.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8896922893271417
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a factorization of a matrix into the product of three matrices: U, Σ, and V.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9600921204696562
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Decomposition: A factorization of a matrix A into three matrices P, D, and P^(-1), such that A = P D P^(-1).",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 22,
    "detailed_sources": [
      {
        "text": "Eigenvalue Decomposition: A factorization of a matrix A into three matrices P, D, and P^(-1), such that A = P D P^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Diagonalization: A factorization of a matrix A into three matrices P, D, and P^(-1), such that A = P D P^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8949868310691307
      },
      {
        "text": "The eigendecomposition of a matrix A is a diagonal matrix D and an invertible matrix P such that A = PDP^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8356265516621946
      },
      {
        "text": "Eigenvalue Decomposition: A factorization technique that decomposes a matrix into three matrices: V, Λ, and V^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8733324328293763
      },
      {
        "text": "Eigenvalue decomposition: An algorithm for decomposing a matrix into a product of a diagonal matrix and an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8373312810901841
      },
      {
        "text": "Eigenvalue Decomposition: A matrix can be decomposed into a sum of eigenvalue matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8463458750628948
      },
      {
        "text": "The Eigenvalue Decomposition: This is a factorization of a matrix A into the product A = UΛU^T, where U is an orthogonal matrix, Λ is a diagonal matrix containing the eigenvalues, and U^T is the transpose of U.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8904013279186785
      },
      {
        "text": "Eigenvalue decomposition: A matrix can be decomposed into a diagonal matrix containing its eigenvalues and a matrix containing the corresponding eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8878592442969844
      },
      {
        "text": "Eigendecomposition of Symmetric Matrices: A factorization technique that decomposes a symmetric matrix into its eigenvalues and eigenvectors, which are used to diagonalize the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8744555925609336
      },
      {
        "text": "Eigenvalue Decomposition: This is a technique that involves decomposing a matrix into a product of a diagonal matrix and an orthogonal matrix. The determinant of the original matrix is equal to the determinant of the diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8418225736572582
      },
      {
        "text": "Eigenvalue Decomposition: A method for decomposing a matrix into a product of orthogonal matrices and diagonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.920043649413553
      },
      {
        "text": "The Eigenvalue Decomposition of a Matrix: This decomposition states that any matrix A can be written as A = VΛV^T, where V is an orthogonal matrix and Λ is a diagonal matrix containing the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.87034930032782
      },
      {
        "text": "The concept of a matrix eigenvalue decomposition and its relation to the orthogonal decomposition: The matrix eigenvalue decomposition of a matrix is a factorization of the matrix into its eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.867700407538766
      },
      {
        "text": "Eigenvalue Decomposition: A factorization of a matrix A into three matrices: P, D, and P^(-1), such that A = P D P^(-1). Eigenvalue decomposition is closely related to the singular value decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8924856912221175
      },
      {
        "text": "Eigendecomposition of a Matrix: The eigendecomposition of a matrix is a factorization of the matrix into the product of a diagonal matrix and an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.914096507693843
      },
      {
        "text": "Eigenvalue Decomposition of Orthogonal Matrices: An orthogonal matrix can be decomposed into a product of an orthogonal matrix Q and a diagonal matrix D, where D contains the eigenvalues of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.850124425117581
      },
      {
        "text": "Eigenvalue Decomposition: This is a method for decomposing a matrix into a product of a diagonal matrix and an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9303961678697451
      },
      {
        "text": "Eigendecomposition of a Matrix with Complex Entries: The eigendecomposition of a matrix with complex entries is a factorization of the matrix into the product of a diagonal matrix and an orthogonal matrix, where the diagonal matrix contains the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8231629389794584
      },
      {
        "text": "Eigendecomposition of a Matrix with Non-Uniformly Distributed Eigenvalues: The eigendecomposition of a matrix with non-uniformly distributed eigenvalues is a factorization of the matrix into the product of a diagonal matrix and an orthogonal matrix, where the diagonal matrix contains the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8544849485397668
      },
      {
        "text": "Eigenvalue decomposition: A factorization of a matrix into a product of a diagonal matrix containing its eigenvalues and a matrix containing its corresponding eigenvectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9405082039030104
      },
      {
        "text": "Eigenvalue Decomposition of a Matrix: A factorization of a matrix into a product of a diagonal matrix containing its eigenvalues and a matrix containing its corresponding eigenvectors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8926640010574578
      },
      {
        "text": "Eigenvalue decomposition is a method for diagonalizing a matrix using its eigenvalues and eigenvectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9032234821622875
      }
    ]
  },
  {
    "representative_text": "Linear Transformation T:",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Linear Transformation T:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation with Finite-Dimensional Domain:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8444176122805445
      },
      {
        "text": "Linearity of T (Dual Transformation):",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8008388089401353
      },
      {
        "text": "Linear Transformation Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8522569150135066
      },
      {
        "text": "Linear Transformations:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9208506743122334
      },
      {
        "text": "A linear transformation can be represented by a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8443542522675991
      },
      {
        "text": "Linear Transformation Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9208118047829461
      },
      {
        "text": "Properties of linear transformations:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9214220388293453
      }
    ]
  },
  {
    "representative_text": "Linear Operator:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Operator:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "First Isomorphism Theorem:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "First Isomorphism Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation with Trivial Kernel:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformation with Trivial Kernel:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation with Trivial Image:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9301366127518351
      },
      {
        "text": "Linear Transformations with a Trivial Null Space: If the null space of a linear transformation is trivial (i.e., contains only the zero vector), the nullity of the transformation is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.822518150587703
      }
    ]
  },
  {
    "representative_text": "Isomorphism between V and W:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Isomorphism between V and W:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Isomorphism between V and V:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Isomorphism between V and V:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Row Echelon Form:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Row Echelon Form:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Reduced Row Echelon Form:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9310507783502986
      },
      {
        "text": "R (Row Echelon Form)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9176054578663069
      },
      {
        "text": "RREF (Reduced Row Echelon Form)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9380527148615846
      },
      {
        "text": "Row and Column Operations: Methods for transforming a matrix into row echelon form or reduced row echelon form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9109650290163223
      }
    ]
  },
  {
    "representative_text": "Row Operations:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Row Operations:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Row and Column Operations: How row and column operations can be used to simplify matrices and solve systems of equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8465917398494296
      }
    ]
  },
  {
    "representative_text": "Column Operations:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Column Operations:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalues and Eigenvectors:",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "Eigenvalues and Eigenvectors:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Definition of Eigenvectors and Eigenvalues:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572884503642566
      },
      {
        "text": "Properties of Eigenvectors and Eigenvalues:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9122561520398712
      },
      {
        "text": "Eigendecomposition of a Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8545673749438376
      },
      {
        "text": "Computing Eigenvectors and Eigenvalues:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9016507788938124
      },
      {
        "text": "The relationship between eigenvalues and the characteristic polynomial",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8088770201999328
      },
      {
        "text": "Eigenvalue Decomposition:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8936301170200568
      },
      {
        "text": "**Numerical Methods for Finding Eigenvalues and Eigenv",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8713100349561814
      },
      {
        "text": "Determinantigenvalue eigenvi.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8355932623082547
      },
      {
        "text": "* Determinantigenvalue eigenvi.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8206561480416816
      },
      {
        "text": "Here are eigenvalue Decomposition of a matrix of a,",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.845851059710228
      },
      {
        "text": "Linear Algebraic Eigenvectorialization of a resultants",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8014763336711054
      },
      {
        "text": "Eigenvalue Interllyap and Orthogonset of a Generalized eigenvectorialization of theore: :",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8009299880578307
      },
      {
        "text": "Properties of eigenvalues and eigenvectors:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9134864740636612
      }
    ]
  },
  {
    "representative_text": "Image (Range) of a Linear Transformation: The set of all possible outputs of T, denoted by Im(T) or R(T).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Image (Range) of a Linear Transformation: The set of all possible outputs of T, denoted by Im(T) or R(T).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Operator Range: This concept is related to the image of the linear transformation and states that the linear operator range of a linear transformation $T$ is the set of all possible images of $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8240397652992273
      },
      {
        "text": "Range and Span: The range of a linear transformation is equal to the span of the image of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8281955586714622
      },
      {
        "text": "Span of a Linear Transformation's Image: The span of the image of a linear transformation, which is the set of all possible outputs of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8902590221525195
      },
      {
        "text": "Span of a Linear Transformation: The span of a linear transformation is the image of the domain vector space, which is a subspace of the codomain vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8599869465582974
      }
    ]
  },
  {
    "representative_text": "Rank of a Linear Transformation: The dimension of the image (range) of T, denoted by rank(T) or dim(Im(T)).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Rank of a Linear Transformation: The dimension of the image (range) of T, denoted by rank(T) or dim(Im(T)).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Rank of a Linear Transformation: The rank of a linear transformation T is the dimension of its image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9316299025564396
      },
      {
        "text": "Dimension of the Range: The dimension of the range of a linear transformation T: V → W is equal to the rank of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8816688821728556
      },
      {
        "text": "Dimension of the Range: The dimension of the range of a linear transformation is equal to the rank of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9151882715038053
      },
      {
        "text": "Dimension of the Range (with non-standard basis): The dimension of the range of a linear transformation T: V → W with a non-standard basis is equal to the rank of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8453626898857285
      }
    ]
  },
  {
    "representative_text": "Nullity of a Linear Transformation: The dimension of the kernel (null space) of T, denoted by nullity(T) or dim(Ker(T)).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Nullity of a Linear Transformation: The dimension of the kernel (null space) of T, denoted by nullity(T) or dim(Ker(T)).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Nullity of a Linear Transformation: The nullity of a linear transformation T is the dimension of its kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.896753723404198
      },
      {
        "text": "The Linear Operator Nullity: This concept is related to the kernel of the linear transformation and states that the linear operator nullity of a linear transformation $T$ is the dimension of its kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8778642765126041
      }
    ]
  },
  {
    "representative_text": "Orthogonality of Kernel and Image: The kernel and image of a linear transformation are orthogonal, i.e., the dot product of any vector in the kernel and any vector in the image is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Orthogonality of Kernel and Image: The kernel and image of a linear transformation are orthogonal, i.e., the dot product of any vector in the kernel and any vector in the image is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Kernel: The kernel of a linear transformation is the orthogonal complement of its range.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8338813757711301
      },
      {
        "text": "Range: The range of a linear transformation is the orthogonal complement of its kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8833604347236854
      },
      {
        "text": "Linear Transformation and Orthogonality: The relationship between linear transformations and orthogonality, including the concept of orthogonal complements and the orthogonality of images and kernels.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8265505130363577
      },
      {
        "text": "Orthogonal Complement of a Linear Transformation: Given a linear transformation T: V → W, the orthogonal complement of T is the set of all vectors in V that are orthogonal to every vector in the range of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8421935658663848
      },
      {
        "text": "Orthogonal Complement of a Linear Transformation for Matrices: The orthogonal complement of a linear transformation can be defined as the set of all vectors orthogonal to every vector in the range of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.885002680829726
      },
      {
        "text": "Orthogonal Complement of a Linear Transformation for Matrices using the Schmidt Process and Gram-Schmidt Process: The orthogonal complement of a linear transformation can be defined as the set of all vectors orthogonal to every vector in the range of the transformation, and the Schmidt process and Gram-Schmidt process can be used to investigate its properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8349586035879064
      }
    ]
  },
  {
    "representative_text": "Dimension of Image and Kernel: The dimension of the image (range) of a linear transformation is equal to the rank of the transformation, and the dimension of the kernel (null space) of a linear transformation is equal to the nullity of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 22,
    "detailed_sources": [
      {
        "text": "Dimension of Image and Kernel: The dimension of the image (range) of a linear transformation is equal to the rank of the transformation, and the dimension of the kernel (null space) of a linear transformation is equal to the nullity of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of Kernel and Image: The span of the kernel of a linear transformation is equal to the null space of the transformation, and the span of the image of a linear transformation is equal to the range of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8431254541896473
      },
      {
        "text": "Rank and Nullity: The rank of a linear transformation is the dimension of its image, while the nullity is the dimension of its kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8295253224613156
      },
      {
        "text": "Linear Independence and Dimension of the Kernel: The dimension of the kernel of a linear transformation is equal to the nullity of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8565857421259198
      },
      {
        "text": "Linear Independence and Dimension of the Range: The dimension of the range of a linear transformation is equal to the rank of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.870262451486963
      },
      {
        "text": "Dimension of the Null Space: The dimension of the null space of a linear transformation $T: V \\to W$ is equal to the nullity of $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8510850689582409
      },
      {
        "text": "Rank and Nullity: For a linear transformation T from a vector space V to a vector space W, the rank of T is the dimension of the column space of T, and the nullity of T is the dimension of the null space of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8509632964470037
      },
      {
        "text": "Rank and Nullity of Linear Transformations with a Degenerate Image or Kernel: The rank and nullity of a linear transformation can be determined even when the image or kernel is degenerate, meaning it has zero dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8505239893745019
      },
      {
        "text": "Rank and Nullity of Linear Transformations with a Non-Standard Basis: The rank and nullity of a linear transformation can be determined even when the basis is non-standard, meaning it is not an orthonormal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8130979992811854
      },
      {
        "text": "Rank and Nullity of Linear Transformations with a Non-Standard Metric: The rank and nullity of a linear transformation can be determined even when the metric is non-standard, meaning it is not the standard Euclidean metric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8296332219284215
      },
      {
        "text": "The rank of a linear transformation is equal to the dimension of the image of the transformation, and the nullity of the transformation is equal to the dimension of the kernel of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8827000853499547
      },
      {
        "text": "Rank and Nullity for Linear Transformations with Degenerate Image or Kernel: The concept of analyzing the rank and nullity of a linear transformation when its image or kernel is degenerate, which is essential in understanding the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8538427121798731
      },
      {
        "text": "Linear Transformation and Null Space Dimension: The dimension of the null space of a linear transformation is equal to the nullity of the transformation, which can be determined using the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8562531845227664
      },
      {
        "text": "Dimension of the Range of a Linear Transformation: Developing a deeper understanding of how the dimension of the range of a linear transformation relates to the rank of the transformation and the dimension of the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8565985607781551
      },
      {
        "text": "Dimension of the Null Space using Linear Transformations: Develop a deeper understanding of how the dimension of the null space of a linear transformation relates to the rank of the transformation and the dimension of the range.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.886358065689884
      },
      {
        "text": "Dimension of the Range of a Linear Transformation using Rank-Nullity Theorem: Develop a deeper understanding of how the dimension of the range of a linear transformation relates to the rank of the transformation and the dimension of the null space using the Rank-Nullity Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.894556328076819
      },
      {
        "text": "Dimension of the Null Space: The dimension of the null space of a linear transformation is equal to the nullity of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8844045120969677
      },
      {
        "text": "Dimension of the Null Space using Rank-Nullity Theorem: Developing a deeper understanding of how the dimension of the null space of a linear transformation relates to the rank of the transformation and the dimension of the range using the Rank-Nullity Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9045220949735228
      },
      {
        "text": "Dimension of the Null Space (with non-standard basis): The dimension of the null space of a linear transformation T: V → W with a non-standard basis is equal to the nullity of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8361541760647477
      },
      {
        "text": "Dimension of the Null Space and Span for Infinite-Dimensional Vector Spaces: The dimension of the null space is equal to the nullity of the linear transformation, and the dimension of the span is equal to the rank of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8914130688869448
      },
      {
        "text": "Dimension of the Null Space using Rank-Nullity Theorem: This concept is crucial in understanding the relationship between the dimension of the null space and the rank of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.892159902422005
      },
      {
        "text": "Rank-Nullity Theorem and its Implications for Dimension of the Range: This theorem has implications for understanding the dimension of the range of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572945669449392
      }
    ]
  },
  {
    "representative_text": "Linear Independence of Kernel and Image: The kernel and image of a linear transformation are linearly independent.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence of Kernel and Image: The kernel and image of a linear transformation are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Calculating Rank: To calculate the rank of a linear transformation, find the number of linearly independent columns of the matrix representation A of T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Calculating Rank: To calculate the rank of a linear transformation, find the number of linearly independent columns of the matrix representation A of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Calculating Rank and Nullity: To calculate the rank and nullity of a linear transformation, use the Rank-Nullity Theorem or the Dimension Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8354562202890303
      },
      {
        "text": "Rank of a Linear Transformation with a Non-Full-Rank Matrix Representation: If the matrix representation of a linear transformation has fewer than full rank (i.e., fewer than the dimension of the domain), the rank of the transformation is equal to the rank of the matrix representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8165173349166479
      }
    ]
  },
  {
    "representative_text": "Calculating Nullity: To calculate the nullity of a linear transformation, find the number of linearly dependent columns of the matrix representation A of T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Calculating Nullity: To calculate the nullity of a linear transformation, find the number of linearly dependent columns of the matrix representation A of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence: The rank-nullity theorem can be used to determine the linear independence of vectors in a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 27,
    "detailed_sources": [
      {
        "text": "Linear Independence: The rank-nullity theorem can be used to determine the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span and Basis: The rank-nullity theorem can be used to determine the span and basis of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8914371748142389
      },
      {
        "text": "Dimensional Analysis: The rank-nullity theorem can be used to analyze the dimensions of vector spaces and subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8984892741163979
      },
      {
        "text": "Linear Transformations and Matrices: The rank-nullity theorem can be used to analyze the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8737229686869763
      },
      {
        "text": "Linear Transformations and Eigenvalues: The rank-nullity theorem can be used to analyze the eigenvalues of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9054952261066628
      },
      {
        "text": "Linear Transformations and Determinants: The rank-nullity theorem can be used to analyze the determinants of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9302586333817798
      },
      {
        "text": "Linear Transformations and Inverse Transformations: The rank-nullity theorem can be used to analyze the inverse of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9063181441774423
      },
      {
        "text": "Linear Transformations and Linear Combinations: The rank-nullity theorem can be used to determine the linear independence of vectors in a vector space, which is crucial in understanding the properties of linear combinations of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8663759493669974
      },
      {
        "text": "Projection Matrices: Projection matrices can be used to find the image and kernel of a linear transformation, and the rank-nullity theorem can be applied to analyze their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8329332193797898
      },
      {
        "text": "Eigenvalues and Eigenvectors: The rank-nullity theorem can be used to analyze the eigenvalues and eigenvectors of a linear transformation, which is crucial in understanding its properties and behavior.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9001683545780356
      },
      {
        "text": "Relationship between Rank and Nullity and the Null Space and Image: The rank-nullity theorem can be used to relate the rank and nullity of a linear transformation to the null space and image, which is essential in understanding the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9060937343066312
      },
      {
        "text": "Linear Transformations and Quadratic Forms: The rank-nullity theorem can be used to analyze the quadratic forms associated with a linear transformation, which is crucial in understanding its properties and behavior.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8657817389568179
      },
      {
        "text": "Linear Transformations and Orthogonal Decompositions: The rank-nullity theorem can be used to analyze the orthogonal decompositions of a linear transformation, which is essential in understanding its properties and behavior.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9037168464415888
      },
      {
        "text": "Linear Transformations with Non-Linear Transformations: The rank-nullity theorem can be used to analyze the properties of linear transformations that are composed with non-linear transformations, which is crucial in understanding their behavior and applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8726298216683303
      },
      {
        "text": "Linear Transformations and Manifolds: The rank-nullity theorem can be used to analyze the properties of linear transformations on manifolds, which are essential in understanding the behavior of linear transformations in higher-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8832107215318659
      },
      {
        "text": "Linear Transformations and Representation Theory: The rank-nullity theorem can be used to analyze the properties of linear transformations in the context of representation theory, which is crucial in understanding the behavior of linear transformations in group theory and other areas of mathematics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8746791477701126
      },
      {
        "text": "Linear Transformations and Measure Theory: The rank-nullity theorem can be used to analyze the properties of linear transformations in the context of measure theory, which is essential in understanding the behavior of linear transformations in measure spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8676671957918756
      },
      {
        "text": "Linear Invariant Spaces: The concept of linear invariant spaces, which are subspaces that are invariant under the linear transformation, can be used to analyze the rank and nullity of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8354104031678461
      },
      {
        "text": "Null Space and Image Decomposition: The concept of decomposing the null space and image of a linear transformation into subspaces, which can be used to analyze the rank and nullity of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8421333958519055
      },
      {
        "text": "Linear Transformations and Measure-Theoretic Invariants: The concept of analyzing the rank and nullity of linear transformations using measure-theoretic invariants, which can be used to analyze the properties of linear transformations in measure spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8541348234887575
      },
      {
        "text": "Linear Transformations and Projective Geometry: The rank-nullity theorem can be used to analyze the properties of linear transformations in projective geometry, which is essential in understanding the behavior of linear transformations in higher-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8932047185387322
      },
      {
        "text": "Linear Transformations and Group Theory: The rank-nullity theorem can be used to analyze the properties of linear transformations in the context of group theory, which is essential in understanding the behavior of linear transformations in group actions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8851679757582065
      },
      {
        "text": "Rank and Nullity of Linear Transformations with Non-Standard Scaling: The rank-nullity theorem can be used to analyze the properties of linear transformations with non-standard scaling, which is essential in understanding the behavior of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9113822350326852
      },
      {
        "text": "Rank and Nullity of Linear Transformations with Non-Standard Completeness: The rank-nullity theorem can be used to analyze the properties of linear transformations with non-standard completeness, which is essential in understanding the behavior of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9000246797101392
      },
      {
        "text": "Linear Transformations and Homology: The rank-nullity theorem can be used to analyze the properties of linear transformations in the context of homology, which is essential in understanding the behavior of linear transformations in topological spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9070819615673602
      },
      {
        "text": "Linear Transformations and the Null Space: The null space of a linear transformation can be used to analyze the properties of the transformation, including the rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9021144607314238
      },
      {
        "text": "Linear Transformations and the Image: The image of a linear transformation can be used to analyze the properties of the transformation, including the rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8523278457944887
      }
    ]
  },
  {
    "representative_text": "Determinant of a Product: If A and B are square matrices, then det(AB) = det(A)det(B).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Determinant of a Product: If A and B are square matrices, then det(AB) = det(A)det(B).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant Theorem: The determinant of the product of two matrices is equal to the product of their determinants: det(AB) = det(A)det(B).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8999504569041474
      },
      {
        "text": "Determinant of a Product of Matrices: The determinant of a product of matrices is equal to the product of their determinants, i.e., det(AB) = det(A)det(B).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.963066064494041
      },
      {
        "text": "Determinant of a Product: The determinant of the product of two square matrices A and B is equal to the product of their determinants:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9488346399667952
      },
      {
        "text": "Associativity: The determinant of the product of two matrices is equal to the product of their determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8917619378991349
      },
      {
        "text": "Associativity: det(ABC) = det(A)det(B)det(C).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.805164829540078
      },
      {
        "text": "Determinant of a Product: det(AB) = det(A)det(B).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9446472188950136
      }
    ]
  },
  {
    "representative_text": "Determinant of a Sum: If A and B are square matrices, then det(A + B) ≠ det(A) + det(B) in general. However, the following identities hold:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Sum: If A and B are square matrices, then det(A + B) ≠ det(A) + det(B) in general. However, the following identities hold:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Transpose: If A is a square matrix, then det(A^T) = det(A).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Determinant of a Transpose: If A is a square matrix, then det(A^T) = det(A).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Transpose: The determinant of the transpose of a matrix is equal to the determinant of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9318813451361865
      },
      {
        "text": "Determinant and Rank of a Matrix: The determinant of a matrix is equal to the determinant of its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.912087036746888
      },
      {
        "text": "Determinant and Orthogonality: The determinant of the transpose of a matrix A is equal to the determinant of A, i.e., det(A^T) = det(A). This is a fundamental property of determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9053837200180445
      },
      {
        "text": "The relationship between determinants and the determinant of a matrix's transpose: The determinant of a matrix's transpose is equal to the determinant of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9469342051018399
      },
      {
        "text": "Determinant Theorem: A theorem that states that the determinant of a matrix is equal to the determinant of its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8613324954921506
      }
    ]
  },
  {
    "representative_text": "Determinant of a Scalar Multiple: If A is a square matrix and c is a scalar, then det(cA) = c^n det(A), where n is the dimension of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Determinant of a Scalar Multiple: If A is a square matrix and c is a scalar, then det(cA) = c^n det(A), where n is the dimension of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Scalar Multiplication: det(cA) = c^n det(A) for an n x n matrix A and scalar c.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9287153190090547
      },
      {
        "text": "Determinant of a Scalar Multiple of a Matrix: The determinant of a scalar multiple of a matrix is equal to the scalar raised to the power of the dimension of the matrix times the determinant of the original matrix, i.e., det(kA) = k^n det(A) for an n x n matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9269519628341101
      },
      {
        "text": "Scalar Multiplication: The determinant of a matrix multiplied by a scalar is equal to the scalar raised to the power of the dimension of the matrix times the determinant of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9423829416603492
      }
    ]
  },
  {
    "representative_text": "Non-Singularity: A square matrix A is invertible if and only if det(A) ≠ 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 29,
    "detailed_sources": [
      {
        "text": "Non-Singularity: A square matrix A is invertible if and only if det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Invertible Matrix: A square matrix A is invertible if and only if det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9291001374801628
      },
      {
        "text": "Inverse of a Matrix: A matrix A has an inverse if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8931279162334895
      },
      {
        "text": "Invertibility: A matrix A is invertible if and only if its determinant is non-zero, denoted as det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9583868777150467
      },
      {
        "text": "Invertibility Criterion: A matrix A is invertible if and only if its determinant is non-zero, det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.927440024423063
      },
      {
        "text": "Invertibility Theorem: A matrix A is invertible if and only if the matrix A and its adjugate matrix adj(A) are both invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8393227472756408
      },
      {
        "text": "Existence of Inverse: A square matrix has an inverse if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9015393790663199
      },
      {
        "text": "Uniqueness of Inverse: The inverse of a matrix is unique if and only if the matrix is invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8288777020711088
      },
      {
        "text": "Inverse Matrix Theorem: The inverse matrix theorem states that a square matrix A is invertible if and only if its determinant is non-zero, i.e., det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9121644477435518
      },
      {
        "text": "Determinant of an Invertible Matrix: The determinant of an invertible matrix A is non-zero, i.e., det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9028038408071045
      },
      {
        "text": "Inverse Matrix Theorem: A theorem stating that a square matrix A is invertible if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9047755886808384
      },
      {
        "text": "Uniqueness of Inverse: A square matrix has a unique inverse if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8796119420310007
      },
      {
        "text": "Existence: A square matrix has an inverse if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9106542318792319
      },
      {
        "text": "Invertibility: A matrix is invertible if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9503814825081482
      },
      {
        "text": "Inverse of a Non-Invertible Matrix: The inverse of a non-invertible matrix does not exist.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8331748647854191
      },
      {
        "text": "Statement: A square matrix is invertible if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8847144775919076
      },
      {
        "text": "The Inverse Matrix Theorem: A square matrix A has an inverse if and only if it is invertible, i.e., det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9247071462183182
      },
      {
        "text": "The Inverse of a Matrix with a Non-Zero Determinant: The inverse of a matrix with a non-zero determinant exists.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9209421065957812
      },
      {
        "text": "Invertible Matrix and Determinant Relationship: The relationship between the invertibility of a matrix and its determinant is given by the following equation: if A is invertible, then det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8766203011721929
      },
      {
        "text": "Invertibility and Determinant: A matrix A is invertible if and only if det(A) ≠ 0. This can be used to determine whether a matrix is invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9315866977170773
      },
      {
        "text": "Invertibility Criterion using the Null Space: A matrix A is invertible if and only if its null space contains only the zero vector. This criterion can be used to determine the invertibility of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8432473534920333
      },
      {
        "text": "The Invertibility Criterion using the Rank-Nullity Theorem: A matrix A is invertible if and only if its rank is equal to its nullity. This criterion can be used to determine the invertibility of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.841466908511551
      },
      {
        "text": "The Invertibility Criterion using the Inverse of the Adjugate Matrix: A matrix A is invertible if and only if its inverse is invertible. This criterion can be used to determine the invertibility of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8459042212257895
      },
      {
        "text": "Determinant of a Matrix with a Non-Unique Inverse: This is a subtle nuance that states that a matrix can have a non-unique inverse if and only if its determinant is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8204712673223574
      },
      {
        "text": "The relationship between the eigenvalues of a matrix and its invertibility: A matrix is invertible if and only if all its eigenvalues are non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8469325197203376
      },
      {
        "text": "Invertibility of a Matrix with a Zero Determinant: Understanding the relationship between the determinant and invertibility of a matrix is crucial in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8223407943699905
      },
      {
        "text": "Browner's Theorem: This theorem states that if a matrix has a non-zero determinant, then it has an inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8314044577525526
      },
      {
        "text": "Theorem on the Determinant of a Matrix: The determinant of a matrix A is defined as the scalar value det(A) such that A is invertible if and only if det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8687610719716238
      },
      {
        "text": "Invertibility using the rank-nullity theorem: A matrix A is invertible if and only if its rank is equal to its nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.879370563049102
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Linearly Dependent Columns: If the columns of a square matrix A are linearly dependent, then det(A) = 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 18,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Linearly Dependent Columns: If the columns of a square matrix A are linearly dependent, then det(A) = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Linearly Independent Columns or Rows: If a matrix A has linearly independent columns or rows, then det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.892958347442907
      },
      {
        "text": "Determinants and Linear Independence: The relationship between the determinant of a matrix and the linear independence of its columns or rows.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8439640006816083
      },
      {
        "text": "The Linear Independence of the Column Space: The column space of a matrix is linearly independent if and only if the matrix has a non-zero determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8767830870636975
      },
      {
        "text": "The Linear Independence of the Row Space: The row space of a matrix is linearly independent if and only if the matrix has a non-zero determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.884173966196452
      },
      {
        "text": "Linear Transformation and Linear Independence: The linear independence of the columns or rows of a matrix A can be determined using the determinant. If det(A) ≠ 0, then the columns or rows of A are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9030081928182432
      },
      {
        "text": "Determinant and Linear Dependence: The linear dependence of the columns or rows of a matrix A can be determined using the determinant. If det(A) = 0, then the columns or rows of A are linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9088285031265775
      },
      {
        "text": "Determinant and Linearly Independent Columns or Rows: The determinant of a matrix A with linearly independent columns or rows is not equal to 0. This is a subtle nuance of determinants and linearly independent columns or rows.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9081017699611398
      },
      {
        "text": "Determinants and Linear Independence of Eigenvectors: The relationship between the determinant of a matrix and the linear independence of its eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8046448693911499
      },
      {
        "text": "The Role of the Determinant in Linear Independence: The determinant is an important concept in linear algebra, and it plays a crucial role in understanding linear independence. The determinant of a matrix is zero if and only if the matrix is singular (i.e., not invertible). If the determinant is non-zero, then the matrix is invertible, and the corresponding vectors are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8608880687661215
      },
      {
        "text": "Linear Independence of the Row Space and Column Space: The row space and column space of a matrix are linearly independent if and only if the matrix has a non-zero determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9033666643898779
      },
      {
        "text": "The Linear Independence of the Columns of a Matrix: This concept involves determining whether the columns of a matrix are linearly independent. If the columns are linearly independent, the matrix is invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8217172841067871
      },
      {
        "text": "The Relationship Between Linear Independence and the Determinant of a Matrix: The determinant of a matrix is closely related to linear independence. If the columns of a matrix are linearly independent, the determinant of the matrix is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9516542190028692
      },
      {
        "text": "The Relationship Between Linear Dependence and the Determinant of a Matrix: If the columns of a matrix are linearly dependent, the determinant of the matrix is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9060657975213737
      },
      {
        "text": "Determinant and Linear Independence of Vectors: The determinant of a matrix can be used to determine the linear independence of vectors. Specifically, the determinant of a matrix is equal to zero if and only if the vectors are linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9006383818807107
      },
      {
        "text": "Determinant and Linear Algebraic Independence: The determinant of a matrix can be used to determine the linear algebraic independence of vectors. Specifically, the determinant of a matrix is equal to zero if and only if the vectors are linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9113492571966828
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space of a Matrix: If a matrix has a non-zero determinant, then its row space and column space are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9379824498651981
      },
      {
        "text": "**The Linear Independence of a matrix can be a matrix,",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8152275550022763
      }
    ]
  },
  {
    "representative_text": "LU Decomposition: A method for computing the determinant of a square matrix by decomposing it into lower triangular and upper triangular matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 21,
    "detailed_sources": [
      {
        "text": "LU Decomposition: A method for computing the determinant of a square matrix by decomposing it into lower triangular and upper triangular matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "LU Decomposition: An algorithm for decomposing a matrix into a product of a lower triangular matrix and an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9004048170686414
      },
      {
        "text": "LU Decomposition Method:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.82996507101416
      },
      {
        "text": "LU Decomposition: A factorization of a matrix into the product of a lower triangular matrix and an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9242971197199957
      },
      {
        "text": "The LU Decomposition Method: A method for finding the inverse of a matrix by decomposing it into the product of a lower triangular matrix and an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8782418280411426
      },
      {
        "text": "The LU Decomposition Method for 2x2 Matrices: A simplified version of the LU decomposition method for 2x2 matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8698300338234659
      },
      {
        "text": "The LU Decomposition Method for 3x3 Matrices: A simplified version of the LU decomposition method for 3x3 matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8884127194938597
      },
      {
        "text": "LU Factorization: A method for factorizing a matrix into a product of a lower triangular matrix (L) and an upper triangular matrix (U), which can be used to calculate the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8482846454737649
      },
      {
        "text": "Computing the Determinant of a Matrix using LU Decomposition: LU decomposition is a factorization of a matrix into the product of a lower triangular matrix and an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9267332605588564
      },
      {
        "text": "The LU Decomposition of a Matrix: This is a factorization of a matrix into the product of a lower triangular matrix L and an upper triangular matrix U.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8971573982444823
      },
      {
        "text": "LU Decomposition for Large Matrices: LU decomposition is a factorization of a matrix into the product of a lower triangular matrix and an upper triangular matrix. It can be used to compute the inverse of a matrix for large matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.850957123853118
      },
      {
        "text": "LU Decomposition: The LU decomposition is a factorization of a matrix into the product of a lower triangular matrix L and an upper triangular matrix U. This decomposition is useful for solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9216933201671476
      },
      {
        "text": "LU Decomposition with Partial Pivoting: LU decomposition with partial pivoting is a method used to factorize a matrix into the product of a lower triangular matrix and an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8896237949464585
      },
      {
        "text": "Computing Determinants using LU Decomposition with a Given Initial Guess: The LU decomposition method can be used to compute the determinant of a matrix with a given initial guess, which can be useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8095648827018049
      },
      {
        "text": "LU Decomposition of Complex Matrices: The factorization of complex matrices into the product of a lower triangular matrix L and an upper triangular matrix U.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8553490314121976
      },
      {
        "text": "LU Decomposition for Non-Symmetric Matrices: LU decomposition is a method for solving systems of linear equations by decomposing the matrix into a lower triangular matrix and an upper triangular matrix and an upper triangular matrix and upper triangular matrix and upper triangular matrix is not only for Non-symmetric matrices. While it is not only one of a way of Matrix Orthogonal matrices: LU decomposition: LU decomposition: LU decomposition: LU Decomposition: This is a matrix inversion methods for Non-standardized matrices: These are some matrices: These are not only.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8118751854267895
      },
      {
        "text": "LU Decomposition with Partial Pivoting: An algorithm for decomposing a matrix into its lower triangular and upper triangular factors, which is used to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9124064291666613
      },
      {
        "text": "LU Decomposition: LU decomposition is a method for decomposing a matrix into the product of a lower triangular matrix and an upper triangular matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9507499933067538
      },
      {
        "text": "Definition: LU decomposition is a method for decomposing a matrix into the product of a lower triangular matrix and an upper triangular matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9021218030717665
      },
      {
        "text": "Doolittle's Algorithm: Doolittle's algorithm is a method for computing the LU decomposition of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8922252616906554
      },
      {
        "text": "LU Decomposition with Partial Pivoting: LU decomposition with partial pivoting is a method for decomposing a matrix into the product of a lower triangular matrix and an upper triangular matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9066703095788728
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination: A method for computing the determinant of a square matrix by performing row operations to transform it into upper triangular form.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 21,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination: A method for computing the determinant of a square matrix by performing row operations to transform it into upper triangular form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gaussian Elimination: An algorithm for solving systems of linear equations using row operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8276988352204547
      },
      {
        "text": "Gauss-Jordan Elimination: A method for transforming a matrix into row-echelon form using elementary row operations, which can be used to determine linear independence of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8175782814667073
      },
      {
        "text": "Gaussian Elimination: A method for reducing a matrix to row echelon form using elementary row operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9359896384103541
      },
      {
        "text": "Gaussian Elimination: A method for determining linear independence by transforming the matrix of coefficients into row echelon form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8982946967380818
      },
      {
        "text": "Gaussian Elimination: A method for solving systems of linear equations by reducing the augmented matrix to row echelon form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9288039868888286
      },
      {
        "text": "Row Reduction: A method for reducing a matrix to row echelon form by performing elementary row operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8667196460188304
      },
      {
        "text": "Gaussian elimination is a method for solving a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8667253868609177
      },
      {
        "text": "Gaussian Elimination: A method for transforming a matrix into row echelon form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9541787840272458
      },
      {
        "text": "Gaussian Elimination: An algorithm for solving systems of linear equations by transforming the augmented matrix into upper triangular form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.933065847555661
      },
      {
        "text": "Gaussian Elimination: Gaussian elimination is a method for solving systems of linear equations by transforming the matrix representation of the system into upper triangular form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8998187014620262
      },
      {
        "text": "Row Reduction: This method is used to transform a matrix into row echelon form or reduced row echelon form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8707392601961674
      },
      {
        "text": "Gaussian Elimination: A method for finding a basis for a vector space by row reducing a matrix to row echelon form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9041287296849769
      },
      {
        "text": "Gaussian Elimination: A method used to reduce a matrix to row echelon form, which helps in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8615988780934061
      },
      {
        "text": "Gaussian Elimination: A method for solving systems of linear equations, used in various applications such as optimization and control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8320651024185555
      },
      {
        "text": "Gaussian Elimination with Partial Pivoting: Gaussian elimination with partial pivoting is a method used to solve systems of linear equations by transforming the matrix into upper triangular form using elementary row operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8535625173423038
      },
      {
        "text": "Gaussian Elimination with Partial Pivoting: An algorithm for solving systems of linear equations using Gaussian elimination, which is used to improve numerical stability.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8081014229102228
      },
      {
        "text": "Gauss-Jordan Elimination: A method for solving systems of linear equations using row operations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.945537484302408
      },
      {
        "text": "Gaussian elimination is a method for solving linear systems using row operations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9225279799331013
      },
      {
        "text": "Definition: Gaussian elimination is a method for solving systems of linear equations using row operations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8968871134529106
      },
      {
        "text": "Gaussian Elimination with Partial Pivoting: Gaussian elimination with partial pivoting is a method for solving systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8543177496967833
      }
    ]
  },
  {
    "representative_text": "Determinant of a Linear Transformation: The determinant of a linear transformation T: V → W is equal to the determinant of its matrix representation A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Determinant of a Linear Transformation: The determinant of a linear transformation T: V → W is equal to the determinant of its matrix representation A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of the Image: If T: V → W is a linear transformation, then the determinant of the image of T (the image of the standard basis vectors of V under T) is equal to the determinant of the matrix representation A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8887863952661736
      }
    ]
  },
  {
    "representative_text": "Determinant of the Inverse: If T: V → W is a linear transformation, then the determinant of its inverse T^(-1) is equal to the reciprocal of the determinant of T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of the Inverse: If T: V → W is a linear transformation, then the determinant of its inverse T^(-1) is equal to the reciprocal of the determinant of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence: Determinants can be used to determine whether a set of vectors is linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Independence: Determinants can be used to determine whether a set of vectors is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span: Determinants can be used to determine the span of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.869049078927036
      }
    ]
  },
  {
    "representative_text": "Eigenvalues: Determinants can be used to compute the eigenvalues of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Eigenvalues: Determinants can be used to compute the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalues and Eigenvectors: Determinants can be used to find the eigenvalues and eigenvectors of a matrix by calculating the characteristic equation and solving for the roots.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9387567157304507
      },
      {
        "text": "Eigenvalues and Eigenvectors: The determinant of a matrix can be used to determine the eigenvalues and eigenvectors of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9680155982117306
      },
      {
        "text": "The Determinant of a Matrix: The determinant of a matrix can be expressed in terms of its eigenvalues. This is known as the eigenvalue decomposition of the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8378973255260108
      },
      {
        "text": "Matrix Determinant: The determinant of a matrix A can be used to analyze the eigenvalues of A. Specifically, the determinant of A is equal to the product of its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8614089949037437
      },
      {
        "text": "Determinant and Spectral Decomposition: The determinant of a matrix is related to the spectral decomposition of the matrix. Specifically, the determinant of a matrix is equal to the product of its eigenvalues, which are the roots of the characteristic polynomial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8295455633070503
      },
      {
        "text": "The Determinant of a Matrix using Eigenvalues: The determinant of a matrix can be expressed in terms of its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9128715742154306
      },
      {
        "text": "Relationship between determinants and eigenvalues of a matrix: The determinant of a matrix is related to its eigenvalues, which can be used to determine the linear transformation described by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8906772361311687
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition: Determinants can be used to compute the singular values of a matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition: Determinants can be used to compute the singular values of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a factorization method that can be used to decompose a matrix into the product of three matrices: U, Σ, and V. The determinant of a matrix can be related to the singular values in the SVD decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.845763997689978
      },
      {
        "text": "Singular Value Decomposition (SVD) and Determinants: The relationship between SVD and determinants, including the concept of singular values and the properties of determinants in the context of SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8878935712148821
      },
      {
        "text": "Singular Value Decomposition (SVD) and Determinant: The SVD of a matrix can be used to determine the determinant of the matrix. The determinant of the SVD is equal to the determinant of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9321434971286873
      },
      {
        "text": "Linear Transformation and Singular Value Decomposition with Non-Orthogonal Linear Transformations and Non-Constant Coefficients: The relationship between linear transformations and singular value decomposition with non-orthogonal linear transformations and non-constant coefficients can be studied using determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8598517118864267
      },
      {
        "text": "Singular Value Decomposition and Determinants: Singular value decomposition (SVD) is a factorization of a matrix into the product of three matrices: U, Σ, and V, where U and V are orthogonal matrices and Σ is a diagonal matrix containing the singular values. The determinant of a matrix can be used to find the singular values of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9409221879566747
      },
      {
        "text": "Singular Value Decomposition (SVD) and Dimension: Singular value decomposition (SVD) is a factorization of a matrix into the product of three matrices: U, Σ, and V, where U and V are orthogonal matrices and Σ is a diagonal matrix containing the singular values. The determinant of a matrix can be used to find the singular values of the matrix, which can also be used to find the dimension of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9131630962924091
      }
    ]
  },
  {
    "representative_text": "Sylvester's Law of Inertia: If A is a real square matrix, then the number of positive eigenvalues of A is equal to the number of negative eigenvalues of A^T.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Sylvester's Law of Inertia: If A is a real square matrix, then the number of positive eigenvalues of A is equal to the number of negative eigenvalues of A^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Sylvester's Law of Inertia: Sylvester's law of inertia states that the number of positive, negative, and zero eigenvalues of a matrix is equal to the number of positive, negative, and zero eigenvalues of the transpose of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9283563533885153
      }
    ]
  },
  {
    "representative_text": "Gershgorin's Theorem: If A is a square matrix, then the absolute value of each eigenvalue of A is less than or equal to the maximum of the absolute values of the diagonal entries of A.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gershgorin's Theorem: If A is a square matrix, then the absolute value of each eigenvalue of A is less than or equal to the maximum of the absolute values of the diagonal entries of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Definition: A matrix is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns, with each entry denoted by a pair of indices (i, j).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Matrix Definition: A matrix is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns, with each entry denoted by a pair of indices (i, j).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Definition: A matrix is a mathematical object that consists of rows and columns of numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8905421359261081
      }
    ]
  },
  {
    "representative_text": "Order of a Matrix: The number of rows in a matrix is called the order of the matrix, denoted by m, and the number of columns is called the order of the matrix, denoted by n.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Order of a Matrix: The number of rows in a matrix is called the order of the matrix, denoted by m, and the number of columns is called the order of the matrix, denoted by n.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Notation: A matrix is typically denoted by a capital letter (e.g., A, B, C), with rows and columns separated by commas or semicolons.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Matrix Notation: A matrix is typically denoted by a capital letter (e.g., A, B, C), with rows and columns separated by commas or semicolons.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrices: A matrix is typically denoted by uppercase letters (A, B, C), while vectors are denoted by lowercase letters (a, b, c).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8770512434673079
      },
      {
        "text": "Notation: Matrices are often denoted using capital letters, such as A or B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.908138185418275
      }
    ]
  },
  {
    "representative_text": "Matrix Addition: Two matrices A and B with the same order can be added element-wise, resulting in a new matrix C, where C(i, j) = A(i, j) + B(i, j).",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Matrix Addition: Two matrices A and B with the same order can be added element-wise, resulting in a new matrix C, where C(i, j) = A(i, j) + B(i, j).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Addition: The sum of two matrices A and B is denoted by A + B and is defined as the matrix obtained by adding corresponding elements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8342281543108194
      },
      {
        "text": "Matrix addition: the sum of two matrices is the matrix obtained by adding their corresponding elements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9123608013806355
      },
      {
        "text": "Matrix addition is defined element-wise.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8924467483068489
      }
    ]
  },
  {
    "representative_text": "Matrix Subtraction: The difference between two matrices A and B is denoted by A - B and is defined as the matrix obtained by subtracting corresponding elements.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Subtraction: The difference between two matrices A and B is denoted by A - B and is defined as the matrix obtained by subtracting corresponding elements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scalar Multiplication: A scalar (a single number) can be multiplied with a matrix A to obtain a new matrix A * c, where each element is multiplied by the scalar.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar Multiplication: A scalar (a single number) can be multiplied with a matrix A to obtain a new matrix A * c, where each element is multiplied by the scalar.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Transpose: The transpose of a matrix A, denoted by A^T, is obtained by interchanging the rows and columns of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Matrix Transpose: The transpose of a matrix A, denoted by A^T, is obtained by interchanging the rows and columns of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Transpose: The transpose of a matrix A is a new matrix whose rows are the columns of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9168306336888943
      },
      {
        "text": "Transpose: The operation of swapping the rows and columns of a matrix to produce a new matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8929857315791656
      },
      {
        "text": "Matrix transpose: the transpose of a matrix is the matrix obtained by interchanging its rows and columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.922965113392424
      }
    ]
  },
  {
    "representative_text": "Matrix Conjugate: The conjugate of a matrix A, denoted by A^*, is obtained by taking the complex conjugate of each element in A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Conjugate: The conjugate of a matrix A, denoted by A^*, is obtained by taking the complex conjugate of each element in A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Exponentiation: A matrix A can be raised to a power n, denoted by A^n, using the binomial expansion or other methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Exponentiation: A matrix A can be raised to a power n, denoted by A^n, using the binomial expansion or other methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Exponentiation Properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Matrix Exponentiation Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Matrix Exponential: The function that can be used to compute the exponential of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8140380883824638
      },
      {
        "text": "**Matrix Exponent of theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9450000936592111
      }
    ]
  },
  {
    "representative_text": "Lower Triangular Matrix: A matrix with all elements below the main diagonal equal to zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Lower Triangular Matrix: A matrix with all elements below the main diagonal equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Lower Triangular Matrix: A lower triangular matrix is a matrix that has zeros above the diagonal.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9404072684091185
      }
    ]
  },
  {
    "representative_text": "Upper Triangular Matrix: A matrix with all elements above the main diagonal equal to zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Upper Triangular Matrix: A matrix with all elements above the main diagonal equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Upper Triangular Matrix: An upper triangular matrix is a square matrix with non-zero entries only below the main diagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9289119192434239
      },
      {
        "text": "Upper Triangular Matrix: An upper triangular matrix is a matrix that has zeros below the diagonal.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.955082862437834
      }
    ]
  },
  {
    "representative_text": "Lower Triangular Inversion: A matrix A can be inverted using the lower triangular inversion method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Lower Triangular Inversion: A matrix A can be inverted using the lower triangular inversion method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Upper Triangular Inversion: A matrix A can be inverted using the upper triangular inversion method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9293505187867165
      }
    ]
  },
  {
    "representative_text": "Cholesky Decomposition: A matrix A can be decomposed into a product of a lower triangular matrix L and its transpose L^T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Cholesky Decomposition: A matrix A can be decomposed into a product of a lower triangular matrix L and its transpose L^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cholesky Decomposition: A factorization technique that decomposes a symmetric positive definite matrix into the product of a lower triangular matrix and its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8467958265236574
      },
      {
        "text": "Cholesky Decomposition: Cholesky decomposition is a factorization technique that can be used to decompose symmetric positive-definite matrices, which is useful in computer graphics for tasks such as simulating realistic physics and collisions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8051634359136839
      },
      {
        "text": "Matrix Inversion by Cholesky Decomposition: Cholesky decomposition is a factorization of a symmetric matrix into the product of a lower triangular matrix and its transpose. It can be used to compute the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8466207924946201
      },
      {
        "text": "Cholesky Decomposition: Cholesky decomposition is a method used to factorize a symmetric matrix into the product of a lower triangular matrix and its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9494762540230555
      },
      {
        "text": "Cholesky Decomposition: Explore Cholesky decomposition, which is a factorization of a symmetric positive definite matrix into the product of a lower triangular matrix and its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9292507166614273
      },
      {
        "text": "Cholesky Decomposition: A factorization of a symmetric matrix into its lower triangular factors, which is used to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9103910097229693
      },
      {
        "text": "Cholesky Decomposition: Cholesky decomposition is a method for decomposing a symmetric positive definite matrix into the product of a lower triangular matrix and its transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9543358925166803
      },
      {
        "text": "Definition: Cholesky decomposition is a method for decomposing a symmetric positive definite matrix into the product of a lower triangular matrix and its transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8985793048922851
      },
      {
        "text": "Cholesky's Algorithm: Cholesky's algorithm is a method for computing the Cholesky decomposition of a symmetric positive definite matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.882073562112835
      },
      {
        "text": "Cholesky Decomposition with Partial Pivoting: Cholesky decomposition with partial pivoting is a method for decomposing a symmetric positive definite matrix into the product of a lower triangular matrix and its transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.884538561271945
      }
    ]
  },
  {
    "representative_text": "LU Decomposition: A matrix A can be decomposed into a product of a lower triangular matrix L and an upper triangular matrix U.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "LU Decomposition: A matrix A can be decomposed into a product of a lower triangular matrix L and an upper triangular matrix U.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant Properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 27,
    "detailed_sources": [
      {
        "text": "Determinant Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Multilinearity: det(A) = det(A^T), where A^T is the transpose of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8467750444248547
      },
      {
        "text": "Calculating Determinants:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8479155551561698
      },
      {
        "text": "Determinant of a Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9197110503610928
      },
      {
        "text": "Determinant of a 2x2 Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8835890003575145
      },
      {
        "text": "Determinant of a 3x3 Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9105310731096459
      },
      {
        "text": "Eigenvalues and Determinants:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8695285615685862
      },
      {
        "text": "Determinants:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9029878255549477
      },
      {
        "text": "Determinants and inverses: A way to analyze the behavior of linear transformations using determinants and inverses.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8239627610095819
      },
      {
        "text": "Advanced Determinant Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8756245598910315
      },
      {
        "text": "The Determinant of a Matrix with a Given Rank: The properties of the determinant of a matrix with a given rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.862605685199662
      },
      {
        "text": "The Determinant of a Matrix using the LU Decomposition: The properties of the determinant of a matrix computed using the LU decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8695505629330343
      },
      {
        "text": "The Determinant of an Orthogonal Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8717681309624425
      },
      {
        "text": "Determinant of a Linear Transformation:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8731339028648926
      },
      {
        "text": "The Properties of Determinants for Matrices with Non-Standard Linear Transformation: A method for computing the determinant of a matrix with a non-standard linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8948395483856384
      },
      {
        "text": "The Properties of Determinants for Matrices with Non-Standard Eigenvalues: A method for computing the determinant of a matrix with non-standard eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.887768772070044
      },
      {
        "text": "The Properties of Determinants for Matrices with Non-Standard Singular Values: A method for computing the determinant of a matrix with non-standard singular values.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8807486541225114
      },
      {
        "text": "The Properties of Determinants for Matrices with Non-Standard Jordan Canonical Form: A method for computing the determinant of a matrix with a non-standard Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8792250342871805
      },
      {
        "text": "The Properties of Determinants for Matrices with Non-Standard Spectral Theorem: A method for computing the determinant of a matrix with a non-standard spectral theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.85821709025979
      },
      {
        "text": "**Determinant of a matrix with a matrix using theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8394522736972032
      },
      {
        "text": "The relationship between determinants and eigenvalues of a matrix: A mathematical relationship between the determinant of a matrix and its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8432289617442372
      },
      {
        "text": "Here are some of matrices are some of Determinantiberal",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8399716004227169
      },
      {
        "text": "Here are not only for Determinantil specification of Determinant Matrices, and eigenv",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8425615118766553
      },
      {
        "text": "Determinant of a Matrix with a Zero Row or Column: A method for calculating the determinant of a matrix with a zero row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9015194516340277
      },
      {
        "text": "Determinant of a Matrix with Non-Symmetric Transformation Matrix and Non-Linear Jacobian Matrix: The determinant of a matrix with a matrix can be calculated using theorelated matrix can be calculated using theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8609523641432448
      },
      {
        "text": "**Determinant of a Matrix: The final Answer Agent:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8957706377210526
      },
      {
        "text": "Properties of determinants:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9167445175928912
      }
    ]
  },
  {
    "representative_text": "Inverse of a Matrix Properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Inverse of a Matrix Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Calculating Inverse Matrices:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8842054771102967
      },
      {
        "text": "Inverse of a 2x2 Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9142447576975415
      },
      {
        "text": "Inverse of a 3x3 Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9064722058283974
      },
      {
        "text": "Matrix Inversion Theorems:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8341606545403146
      },
      {
        "text": "The Inverse of a Product of Matrices: The properties of the inverse of a product of matrices, such as the product of two inverses.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8829947927140132
      },
      {
        "text": "The Inverse of a Matrix with a Non-Trivial Null Space: The properties of the inverse of a matrix with a non-trivial null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8520690545215932
      },
      {
        "text": "The Inverse of a Matrix using the Gauss-Jordan Elimination Method: The properties of the inverse of a matrix computed using the Gauss-Jordan elimination method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9059738237747499
      },
      {
        "text": "The Inverse of a Matrix using the QR Decomposition: The properties of the inverse of a matrix computed using the QR decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8634013297055153
      },
      {
        "text": "The Inverse of an Orthogonal Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8901979237424664
      },
      {
        "text": "The Moore-Penrose Inverse of a Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8591989945169886
      },
      {
        "text": "Inverse of a Matrix using the Adjoint and Determinant: A method for computing the inverse of a square matrix using the adjoint and determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8929680429627597
      },
      {
        "text": "Matrix Multiplication and Inverse: A thorough.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.854692278469551
      }
    ]
  },
  {
    "representative_text": "Cramer's Rule: A system of linear equations can be solved using Cramer's rule, which involves calculating the determinants of matrices formed by replacing the coefficients of the variables with the constants.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 25,
    "detailed_sources": [
      {
        "text": "Cramer's Rule: A system of linear equations can be solved using Cramer's rule, which involves calculating the determinants of matrices formed by replacing the coefficients of the variables with the constants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cramer's Rule: A determinant can be used to find the solution to a system of linear equations, where the determinant is used to calculate the coefficients of the solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8807075616576667
      },
      {
        "text": "Cramer's Rule: The solution to a system of linear equations can be expressed in terms of the cofactors of the coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8877441447126098
      },
      {
        "text": "Cramer's Rule: Cramer's rule states that the solution to a system of linear equations Ax = b is given by xj = (det(Aj)) / det(A), where A_j is the matrix obtained by replacing the jth column of A with the vector b.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8641741214389003
      },
      {
        "text": "Cramer's Rule: A method for solving systems of linear equations using determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.899279347947057
      },
      {
        "text": "Cramer's Rule: The solution to a system of linear equations Ax = b can be found using Cramer's rule, which involves calculating the determinant of the coefficient matrix A and the determinants of the matrices formed by replacing each column of A with the column vector b.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9324179678523685
      },
      {
        "text": "Cramer's Rule: A formula for solving systems of linear equations by computing the determinants of matrices formed by replacing the coefficient matrix with the constant matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9056790972961184
      },
      {
        "text": "Statement: The value of a variable in a system of linear equations can be found using the determinant of a matrix constructed by replacing the coefficient of the variable with the constant term.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8199625746262138
      },
      {
        "text": "Cramer's Rule Formula: The formula for computing the value of a variable in a system of linear equations using the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8630893304411118
      },
      {
        "text": "Cramer's Rule:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8484992888872585
      },
      {
        "text": "Cramer's rule is a theorem that provides a formula for solving a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9024839445695135
      },
      {
        "text": "The theorem states that the solution to a system of linear equations can be found by replacing each variable in the coefficient matrix with the constant term and calculating the resulting determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8503939003550054
      },
      {
        "text": "Cramer's Rule: Cramer's rule is a method for solving systems of linear equations using determinants. It states that the value of a variable in a system can be found by replacing the corresponding column in the coefficient matrix with the constant vector and taking the ratio of the determinant of the resulting matrix to the determinant of the coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9297138537295182
      },
      {
        "text": "Cramer's Rule: A method for finding the values of the variables in a linear system by using determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9246030769228937
      },
      {
        "text": "Cramer's Rule: This rule states that if a system of linear equations has a unique solution, then the value of a variable can be found by taking the determinant of the matrix of coefficients and dividing it by the determinant of the matrix of coefficients with the variable removed.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9348475412974884
      },
      {
        "text": "The Cramer's Rule: This rule states that for any square matrix A, the value of each element of the determinant matrix of A is equal to the value of the corresponding element in the matrix obtained by replacing the column containing the element with the constant vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8739284625991961
      },
      {
        "text": "Cramer's Rule and its Limitations: The method of Cramer's rule for solving systems of linear equations and its limitations, including cases where the method fails.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8219612101145071
      },
      {
        "text": "Cramer's Rule for Systems of Linear Equations: Cramer's rule is a method for solving systems of linear equations by using determinants. It involves finding the determinants of the coefficient matrix and the augmented matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9463117941987391
      },
      {
        "text": "The Cramer's Rule for Systems of Linear Equations with a Rank-Deficient Matrix: Cramer's rule can be modified to handle systems of linear equations with a rank-deficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8193990449197641
      },
      {
        "text": "Cramer's Rule: This rule is used to solve systems of linear equations by finding the determinants of matrices formed by replacing columns with constant vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9514066402405275
      },
      {
        "text": "Cramer's Rule: This rule is used to find the values of the variables in a system of linear equations, and it is related to the eigenvalues of the coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8553667859058626
      },
      {
        "text": "Cramer's Rule for Non-Square Matrices: Cramer's rule can be extended to non-square matrices by using the cofactor expansion along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8153038935953916
      },
      {
        "text": "Cramer's Rule with Pivoting: A method for finding the solution to a system of linear equations by using determinants, using pivoting to avoid division by zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8703135173675186
      },
      {
        "text": "Cramer's Rule: A method for solving systems of linear equations, used in various applications such as optimization and control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8399514325177011
      },
      {
        "text": "Cramer's Rule with Pivoting (continued): Cramer's rule with pivoting is a method for finding the solution to a system of linear equations by using determinants, using pivoting to avoid division by zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8775391517582549
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination: A system of linear equations can be solved using Gaussian elimination, which involves transforming the augmented matrix into upper triangular form.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination: A system of linear equations can be solved using Gaussian elimination, which involves transforming the augmented matrix into upper triangular form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The method involves transforming the augmented matrix into upper triangular form using elementary row operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8001183946390121
      },
      {
        "text": "The method involves transforming the augmented matrix into upper triangular form.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9277331610058404
      }
    ]
  },
  {
    "representative_text": "Row Echelon Form: A matrix is in row echelon form if and only if it satisfies certain properties, such as having all zeros below the leading entries.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Row Echelon Form: A matrix is in row echelon form if and only if it satisfies certain properties, such as having all zeros below the leading entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Row Echelon Form: A matrix is said to be in row echelon form if all the entries below the leading entry in each row are zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8909652217979103
      },
      {
        "text": "Row Echelon Form: A matrix that has been transformed into a form where the leading entries of each row are to the right of the leading entries of the rows above it.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9107566457589189
      }
    ]
  },
  {
    "representative_text": "Cofactor (Cij): The determinant of the submatrix formed by removing the ith row and jth column of the original matrix A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Cofactor (Cij): The determinant of the submatrix formed by removing the ith row and jth column of the original matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Minor (Mij): The determinant of the submatrix formed by removing the ith row and jth column of the original matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8273987268418374
      },
      {
        "text": "Cofactor Matrix: A matrix where each element Cij is the cofactor of the corresponding element aij in the original matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.821889215546552
      },
      {
        "text": "Cij: The cofactor of the element a_ij in a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8764158668701256
      },
      {
        "text": "Mij: The minor of the element aij in a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8234111592226444
      },
      {
        "text": "Cofactor Matrix: A matrix whose entries are the cofactors of the corresponding entries in the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8224559568129621
      },
      {
        "text": "Cofactor Matrix: A matrix constructed by replacing each element of the original matrix with its cofactor.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8694474757277679
      },
      {
        "text": "Matrix of Minors and Matrix of Cofactors: A matrix where each element is the minor of the corresponding element in the original matrix, and a matrix where each element is the cofactor of the corresponding element in the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8700560148417655
      }
    ]
  },
  {
    "representative_text": "Adjugate Matrix (adj(A)): The transpose of the cofactor matrix, often used to find the inverse of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Adjugate Matrix (adj(A)): The transpose of the cofactor matrix, often used to find the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Adjugate Matrix: The transpose of the matrix of cofactors of a given matrix, denoted by adj(A).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9384631855637265
      },
      {
        "text": "Adjugate Method: A method for finding the inverse of a matrix by computing the adjugate matrix and dividing it by the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.808461031176373
      },
      {
        "text": "Adjoint Formula: A formula for computing the adjugate matrix of a given matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8570279444734843
      },
      {
        "text": "Inverse Formula: A formula for computing the inverse of a matrix using the adjugate matrix and the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8792920216000333
      },
      {
        "text": "Inverse Matrix Formula: The formula for computing the inverse of a square matrix using the adjoint and determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8724314810671769
      },
      {
        "text": "Statement: The formula for computing the inverse of a square matrix using the adjoint and determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8430763287177601
      },
      {
        "text": "Adjugate Matrix: The adjugate matrix of A, denoted by adj(A), is a matrix whose entries are the cofactors of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.832821088968572
      },
      {
        "text": "The Adjoint Method: A method for finding the inverse of a matrix by using the adjoint matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8926089054061104
      },
      {
        "text": "The Inverse of a Matrix using the Adjugate: The process of computing the inverse of a matrix using the adjoint (also known as the classical adjugate) and determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8993041214694075
      }
    ]
  },
  {
    "representative_text": "Linearity: det(αA + βB) = αdet(A) + βdet(B) for matrices A and B and scalars α and β.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linearity: det(αA + βB) = αdet(A) + βdet(B) for matrices A and B and scalars α and β.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linearity: The determinant of a sum of matrices is equal to the sum of their determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8807862233033755
      }
    ]
  },
  {
    "representative_text": "Permutation: det(P) = (-1)^(i+j) det(Aij) for a permutation matrix P and an n x n matrix A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Permutation: det(P) = (-1)^(i+j) det(Aij) for a permutation matrix P and an n x n matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cyclic Permutation: det(P) = (-1)^(i+j+k) det(A) for a cyclic permutation matrix P and an n x n matrix A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cyclic Permutation: det(P) = (-1)^(i+j+k) det(A) for a cyclic permutation matrix P and an n x n matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cofactor Rule: The cofactor of an element aij is equal to the determinant of the submatrix formed by removing the ith row and jth column, multiplied by (-1)^(i+j).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Cofactor Rule: The cofactor of an element aij is equal to the determinant of the submatrix formed by removing the ith row and jth column, multiplied by (-1)^(i+j).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cofactor of an Element: The cofactor of an element aij in a matrix A is calculated as Cij = (-1)^(i+j)Mij, where Mij is the minor of the element a_ij.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8664105336462837
      }
    ]
  },
  {
    "representative_text": "Sarrus Rule: A determinant can be calculated using the Sarrus rule, which involves summing the products of the elements in each row and column, with alternating signs.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Sarrus Rule: A determinant can be calculated using the Sarrus rule, which involves summing the products of the elements in each row and column, with alternating signs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Sarrus' Rule: Sarrus' rule states that the determinant of a 3x3 matrix A can be calculated using the formula det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8427976414065725
      },
      {
        "text": "Sarrus' Rule: A method for calculating the determinant of a 3x3 matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8861643227421129
      },
      {
        "text": "Sarrus Rule for Block Matrices: A generalization of the Sarrus rule for block matrices, which involves summing the products of the elements in each row and column, with alternating signs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8475575516166127
      }
    ]
  },
  {
    "representative_text": "Eigenvalue: A scalar value that represents how much a matrix stretches or compresses a vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Eigenvalue: A scalar value that represents how much a matrix stretches or compresses a vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue (λ): A scalar value that represents how much change occurs in a linear transformation. It is a non-zero scalar value that is a root of the characteristic equation det(A - λI) = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8712012425183563
      },
      {
        "text": "Eigenvalue: A scalar value, denoted by λ (lambda), that represents how much change occurs in a linear transformation when the input is scaled by a factor of λ.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.951550030449658
      },
      {
        "text": "Eigenvalues are scalar values that describe the amount of stretching or shrinking that occurs when a linear transformation is applied to a vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8547835201066826
      },
      {
        "text": "Definition of eigenvalue: A scalar that, when multiplied by a vector, results in the same vector scaled by the same scalar.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8767926726001313
      },
      {
        "text": "An eigenvalue of a matrix is a scalar that represents the amount of scaling applied to the eigenvector.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8859538483440252
      }
    ]
  },
  {
    "representative_text": "Eigenvector: A non-zero vector that, when multiplied by the matrix, results in a scaled version of itself.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Eigenvector: A non-zero vector that, when multiplied by the matrix, results in a scaled version of itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvector (v): A non-zero vector that, when the linear transformation represented by matrix A is applied to it, results in a scaled version of itself. Mathematically, Av = λv.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9155879828894902
      },
      {
        "text": "Statement: The eigenvectors of a matrix are the non-zero vectors that, when multiplied by the matrix, result in a scaled version of themselves.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8711590340749784
      },
      {
        "text": "An eigenvector of a matrix A is a non-zero vector v that satisfies Av = λv, where λ is a scalar.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8856026386469042
      },
      {
        "text": "Definition: An eigenvector of a matrix A is a non-zero vector v such that the matrix A multiplied by v results in a scaled version of v.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9545820930871535
      },
      {
        "text": "Eigenvectors are non-zero vectors that, when multiplied by a linear transformation, result in a scaled version of the vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9005817530383472
      },
      {
        "text": "Eigenvector and Eigenvalue: This refers to a vector that, when multiplied by a linear transformation, results in a scaled version of the same vector (the eigenvalue).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8838645340128365
      },
      {
        "text": "Definition of eigenvector: A non-zero vector that, when multiplied by a scalar, results in the same vector scaled by the same scalar.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.921720287199072
      },
      {
        "text": "An eigenvector of a matrix is a non-zero vector that, when multiplied by the matrix, results in a scaled version of itself.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9376154948123581
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Theorem: If A is a square matrix, then A has a set of eigenvalues, which are the scalar values that represent how much A stretches or compresses a vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Eigenvalue Theorem: If A is a square matrix, then A has a set of eigenvalues, which are the scalar values that represent how much A stretches or compresses a vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvector Theorem: If A is a square matrix with eigenvalues λ, then A has a set of eigenvectors, which are the non-zero vectors that, when multiplied by A, result in a scaled version of themselves.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9098569645457021
      },
      {
        "text": "Fundamental Theorem of Eigenvalues: If λ is an eigenvalue of matrix A and v is the corresponding eigenvector, then Av = λv.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8746674767814173
      },
      {
        "text": "Fundamental Theorem of Eigenvalues: If A is a square matrix and λ is an eigenvalue of A, then λ is a root of the characteristic equation det(A - λI) = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8758484299570997
      },
      {
        "text": "Rayleigh's Theorem: If A is a square matrix and v is an eigenvector of A corresponding to eigenvalue λ, then |Av| = λ|v|.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8086225471889644
      }
    ]
  },
  {
    "representative_text": "Inverse of a Product: If A and B are invertible matrices, then (AB)^(-1) = B^(-1)A^(-1).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Inverse of a Product: If A and B are invertible matrices, then (AB)^(-1) = B^(-1)A^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Inverse of a Product: The inverse of a product of matrices is equal to the product of their inverses in reverse order.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9102548085367107
      },
      {
        "text": "Inverse of a Product: (AB)^(-1) = B^(-1)A^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9470901734091284
      },
      {
        "text": "The Inverse of a Product of Two Matrices: The inverse of the product of two matrices A and B is equal to the product of the inverses of B and A in reverse order, i.e., (AB)^(-1) = B^(-1)A^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9431534091929087
      },
      {
        "text": "Properties of the Inverse of a Matrix: The inverse of a matrix A satisfies the following properties: (AB)^(-1) = B^(-1)A^(-1), (A + B)^(-1) = A^(-1) + B^(-1) - A^(-1)B^(-1)A^(-1)B, and A^(-1)A = I.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.801222127304902
      },
      {
        "text": "Inverse of a Product of Matrices with Different Dimensions: The inverse of a product of matrices with different dimensions can be calculated using the formula (A ∘ B)^-1 = B^-1 ∘ A^-1, where A and B are invertible matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8552379937956721
      }
    ]
  },
  {
    "representative_text": "Inverse of a Transpose: If A is a square matrix, then (A^T)^(-1) = (A^(-1))^T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inverse of a Transpose: If A is a square matrix, then (A^T)^(-1) = (A^(-1))^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inverse of a Sum: If A and B are square matrices, then (A + B)^(-1) ≠ A^(-1) + B^(-1) in general.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inverse of a Sum: If A and B are square matrices, then (A + B)^(-1) ≠ A^(-1) + B^(-1) in general.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Multiplicativity: If λ is an eigenvalue of A, then λ^n is an eigenvalue of A^n.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Multiplicativity: If λ is an eigenvalue of A, then λ^n is an eigenvalue of A^n.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvector Decomposition: If A has eigenvalues λ1 and λ2, then the matrix A can be decomposed into A = PDP^(-1), where P is a matrix whose columns are the eigenvectors of A, and D is a diagonal matrix containing the eigenvalues of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvector Decomposition: If A has eigenvalues λ1 and λ2, then the matrix A can be decomposed into A = PDP^(-1), where P is a matrix whose columns are the eigenvectors of A, and D is a diagonal matrix containing the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Decomposition: The eigenvectors can be used to decompose the matrix A into the product A = PDP^(-1), where P is composed of eigenvectors and D is a diagonal matrix containing eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.824465478230608
      }
    ]
  },
  {
    "representative_text": "Solving Systems of Linear Equations: Inverse matrices are used to solve systems of linear equations by isolating the variables.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Solving Systems of Linear Equations: Inverse matrices are used to solve systems of linear equations by isolating the variables.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Solving Systems of Linear Equations: Determinants can be used to solve systems of linear equations by finding the inverse of the coefficient matrix and multiplying it by the constant matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8467620899124262
      },
      {
        "text": "Solving Systems of Linear Equations: The determinant of a coefficient matrix can be used to determine the existence and uniqueness of the solution to a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.857653276686273
      },
      {
        "text": "The relationship between determinants and linear algebraic equations: Determinants are used to solve linear algebraic equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8499146509077651
      }
    ]
  },
  {
    "representative_text": "Finding the Determinant: Determinants are used to find the volume scaling factor of a linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 15,
    "detailed_sources": [
      {
        "text": "Finding the Determinant: Determinants are used to find the volume scaling factor of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations: Determinants can be used to describe linear transformations and their properties, such as the determinant of the transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8602621576821767
      },
      {
        "text": "Linear Transformations: The determinant of a matrix can be used to determine the volume scaling factor of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9360557328866653
      },
      {
        "text": "Linear Algebra: The determinant of a matrix is used extensively in linear algebra, including in the calculation of inverse matrices, eigenvalues, and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8311223706858613
      },
      {
        "text": "Cofactor Expansion and Determinant: The determinant of a matrix A can be computed using the cofactors of each element, and it can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.854663602292534
      },
      {
        "text": "The Determinant of a Linear Transformation: The determinant of a linear transformation is a scalar value that can be used to compute the volume scaling factor of the linear transformation. This concept is essential in understanding the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.862565043741906
      },
      {
        "text": "Determinant of a Linear Transformation: The determinant of a linear transformation can be used to determine its invertibility and the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9173545978136267
      },
      {
        "text": "The Determinant of a Matrix: The determinant of a matrix can be used to determine its invertibility and the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8817497555369131
      },
      {
        "text": "Determinant and Toeplitz Matrices: The determinant of a Toeplitz matrix can be used to determine the behavior of the linear transformation represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8305963237488998
      },
      {
        "text": "The concept of a matrix determinant and its relation to the orthogonal decomposition: The matrix determinant of a matrix is a scalar value that can be used to determine the invertibility of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8508022938823326
      },
      {
        "text": "The Determinant of a Matrix: This concept is related to the rank-nullity theorem and states that the determinant of a matrix is equal to the volume scaling factor of the linear transformation represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8581217403132538
      },
      {
        "text": "Interpretation of Determinants: The determinant of a matrix can be interpreted as a scalar value that represents the scaling factor by which the matrix transforms a unit vector. This interpretation can be useful in various applications, such as linear transformations and eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8043082403940363
      },
      {
        "text": "Linear Transformations and the Determinant of a Linear Transformation: The determinant of a linear transformation is a measure of the transformation's \"area\" or \"volume\" scaling factor. Understanding the properties of the determinant can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8720034450511753
      },
      {
        "text": "Linear Transformations and the Determinant: The determinant of a matrix can be used to analyze the properties of linear transformations, including the rank and nullity of a transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9272902646989862
      },
      {
        "text": "Linear Transformation and Determinant: Discussing the determinant of a linear transformation, which provides a measure of the transformation's \"area\" or \"volume\" scaling factor.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8841772438747872
      }
    ]
  },
  {
    "representative_text": "Finding Eigenvalues and Eigenvectors: Eigenvalues and eigenvectors are used to diagonalize a matrix, which can be useful in many applications, such as solving systems of linear equations and finding the eigenvalues of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Finding Eigenvalues and Eigenvectors: Eigenvalues and eigenvectors are used to diagonalize a matrix, which can be useful in many applications, such as solving systems of linear equations and finding the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A^n: The matrix A raised to the power n.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A^n: The matrix A raised to the power n.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Power Iteration: An algorithm for finding the dominant eigenvalue and corresponding eigenvector of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 16,
    "detailed_sources": [
      {
        "text": "Eigenvalue Power Iteration: An algorithm for finding the dominant eigenvalue and corresponding eigenvector of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Power Method: An iterative algorithm for finding eigenvalues and eigenvectors of a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9355812425301836
      },
      {
        "text": "Power Method: The power method is an iterative technique for computing the dominant eigenvalue and eigenvector of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9102664711750528
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using the Power Method: The power method is an iterative technique for computing the dominant eigenvalue and eigenvector of a matrix. It works by repeatedly multiplying the matrix by a vector and normalizing the result.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9116480081439139
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the Power Method: The power method is a method for computing the eigenvalues of a matrix by iteratively multiplying the matrix by a vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9471172868308648
      },
      {
        "text": "Computing the Eigenvectors of a Matrix using the Power Method: The power method can also be used to compute the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8564625686268184
      },
      {
        "text": "Computation of Eigenvalues using the Power Method: The power method is an iterative algorithm for computing eigenvalues and eigenvectors of a matrix A. It involves repeatedly multiplying the matrix A by a vector v and normalizing the result.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9427058813757627
      },
      {
        "text": "Power Method: This is an algorithm used to find the dominant eigenvalue and corresponding eigenvector of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9366248620483929
      },
      {
        "text": "The Computation of Eigenvalues using the Power Iteration Method: The power iteration method is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves repeatedly multiplying the matrix by a vector and normalizing the result.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9170986873556563
      },
      {
        "text": "The Power Method: An algorithm for computing the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.881591308480868
      },
      {
        "text": "Power Method with Shifts: An extension of the power method that uses shifts to improve its convergence rate.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8256087216239739
      },
      {
        "text": "The Power Method (Alternative Method): The power method can be used to find the dominant eigenvalue and eigenvector of a matrix, even if the matrix is not symmetric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8444763437721022
      },
      {
        "text": "The Power Method: The power method can be used to find the dominant eigenvalue and eigenvector of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9175720426256304
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Power Iteration: An algorithm for computing eigenvalues and eigenvectors using power iteration, which is a method for finding the dominant eigenvalue and eigenvector of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9430131705474316
      },
      {
        "text": "The Power Method: The power method is an algorithm used to find the dominant eigenvalue and corresponding eigenvector of a matrix. It involves iteratively multiplying the matrix by a vector and normalizing the result.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9257361721196169
      },
      {
        "text": "The Diagonalization of a Matrix using the Power Method: The power method is an iterative method for finding the dominant eigenvalue and eigenvector of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9310502709344763
      }
    ]
  },
  {
    "representative_text": "QR Decomposition: An algorithm for decomposing a matrix into a product of an orthogonal matrix and an upper triangular matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 19,
    "detailed_sources": [
      {
        "text": "QR Decomposition: An algorithm for decomposing a matrix into a product of an orthogonal matrix and an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Matrix Decomposition: A factorization of a matrix into an orthogonal matrix and an orthogonal matrix, denoted as A = QR, where Q and R are orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8029966076277022
      },
      {
        "text": "The QR Decomposition: This is a factorization of a matrix A into the product A = QR, where Q is an orthogonal matrix and R is an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9141787634327847
      },
      {
        "text": "The QR Decomposition of a Matrix: A decomposition of a matrix into an orthogonal matrix and an upper triangular matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.922594189125196
      },
      {
        "text": "The QR Decomposition: The QR decomposition is a factorization of a matrix into the product of an orthogonal matrix and an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9351720599384902
      },
      {
        "text": "The QR Decomposition of a Matrix: This decomposition states that any matrix A can be written as A = QR, where Q is an orthogonal matrix and R is an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9089645706928009
      },
      {
        "text": "QR Decomposition: The QR decomposition is a factorization of a matrix into the product of an orthogonal matrix Q and an upper triangular matrix R. This decomposition is useful for finding eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9186904264335904
      },
      {
        "text": "The Use of QR Decomposition in Numerical Computations: QR decomposition is a technique used to factorize a matrix into the product of an orthogonal matrix and an upper triangular matrix, and understanding its application is essential in ensuring the accuracy of numerical computations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8182133062659573
      },
      {
        "text": "Orthogonal Decomposition of a Matrix using the QR Decomposition: The QR decomposition can be used to orthogonalize a matrix, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.889681496270479
      },
      {
        "text": "QR Decomposition of Complex Matrices: The factorization of complex matrices into the product of an orthogonal matrix Q and an upper triangular matrix R.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8901585515373871
      },
      {
        "text": "The QR Decomposition of a Matrix with Partial Pivoting: This is a method for orthogonalizing a matrix using Givens rotations and Householder reflections, while also ensuring that the matrix is upper triangular.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8585319299365606
      },
      {
        "text": "The Orthogonalization of a Matrix using the QR Decomposition: This is a method for orthogonalizing a matrix using the QR decomposition, which is a factorization of a matrix into the product of an orthogonal matrix and an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8661493656281308
      },
      {
        "text": "Orthogonal Matrix Decomposition Algorithms: Algorithms for decomposing an orthogonal matrix into its constituent parts, including the Gram-Schmidt process and Householder transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8147211809315147
      },
      {
        "text": "QR Decomposition: Discuss QR decomposition, which is a factorization of a matrix into the product of an orthogonal matrix and an upper triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9433384124149992
      },
      {
        "text": "Orthogonal Matrix Decomposition using QR Algorithm: The QR algorithm is an iterative method for finding the eigenvalues and eigenvectors of a matrix, which can be used for orthogonal matrix decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8565283979368545
      },
      {
        "text": "QR Decomposition: A factorization of a matrix into its orthogonal and triangular factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8942874790656012
      },
      {
        "text": "QR Decomposition with Applications: A factorization of a matrix into its orthogonal and triangular factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9295500474489409
      },
      {
        "text": "QR Decomposition: QR decomposition is a factorization of a matrix into the product of an orthogonal matrix and an upper triangular matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.954212400145694
      },
      {
        "text": "QR Decomposition with SVD: QR decomposition is a factorization of a matrix into the product of an orthogonal matrix and an upper triangular matrix. SVD (Singular Value Decomposition) is a factorization of a matrix into the product of three matrices: U, Σ, and V. QR decomposition with SVD is used in numerical linear algebra to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8443804203464715
      }
    ]
  },
  {
    "representative_text": "Vector: An ordered set of numbers, arranged in a single row or column. It is denoted by a lowercase letter with an arrow above it (e.g., a, b, c).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Vector: An ordered set of numbers, arranged in a single row or column. It is denoted by a lowercase letter with an arrow above it (e.g., a, b, c).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Vector: A one-dimensional array of numbers, often used as a column of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8346951584544021
      },
      {
        "text": "Row Vector: A column of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8045540715809001
      },
      {
        "text": "Row Vector: A one-dimensional vector represented as a row of scalars.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8822369133812017
      },
      {
        "text": "Column Vector: A one-dimensional vector represented as a column of scalars.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9112368695755161
      },
      {
        "text": "Row Matrix: A matrix with one row and multiple columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8232117261496104
      },
      {
        "text": "Column Matrix: A matrix with multiple rows and one column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8491094812526766
      }
    ]
  },
  {
    "representative_text": "Gauss-Jordan Elimination Method:",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Gauss-Jordan Elimination Method:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "G (Gaussian Elimination)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8650777665362017
      },
      {
        "text": "Gaussian Elimination:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9577555896451277
      },
      {
        "text": "The Gauss-Jordan Elimination Method for 2x2 Matrices: A simplified version of the Gauss-Jordan elimination method for 2x2 matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8938258690113425
      },
      {
        "text": "The Gauss-Jordan Elimination Method for 3x3 Matrices: A simplified version of the Gauss-Jordan elimination method for 3x3 matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.919355494978187
      },
      {
        "text": "This is a row and Gauss-Jordan Elimination.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8640303450326399
      },
      {
        "text": "Gaussian Elimination Properties:",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8584958668327942
      }
    ]
  },
  {
    "representative_text": "Adjoint Method:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Adjoint Method:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Adjoint Method for 2x2 Matrices: A simplified version of the adjoint method for 2x2 matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8461946370155036
      },
      {
        "text": "The Adjoint Method for 3x3 Matrices: A simplified version of the adjoint method for 3x3 matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.911332180861983
      }
    ]
  },
  {
    "representative_text": "Inverse of a Sum: The inverse of a sum of matrices is equal to the sum of their inverses.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inverse of a Sum: The inverse of a sum of matrices is equal to the sum of their inverses.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Distributive Property: The inverse of a product of matrices can be expressed as a sum of their inverses, using the distributive property.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Distributive Property: The inverse of a product of matrices can be expressed as a sum of their inverses, using the distributive property.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "adj(A): The adjoint matrix of matrix A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "adj(A): The adjoint matrix of matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vectors: Vectors are often denoted by boldface letters (A, B, C), while matrices are denoted by normal letters.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Vectors: Vectors are often denoted by boldface letters (A, B, C), while matrices are denoted by normal letters.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Notation: Vectors are often denoted using boldface letters, such as v or a.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8633434381613895
      }
    ]
  },
  {
    "representative_text": "Sums and products: The sum of matrices is denoted by the plus sign (+), while the product of matrices is denoted by the asterisk (*).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sums and products: The sum of matrices is denoted by the plus sign (+), while the product of matrices is denoted by the asterisk (*).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Multiplication: Matrix multiplication is denoted by the asterisk (*), while scalar multiplication is denoted by the multiplication symbol (×).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Multiplication: Matrix multiplication is denoted by the asterisk (*), while scalar multiplication is denoted by the multiplication symbol (×).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Values: The singular values of a matrix A are the square roots of the eigenvalues of A^T A or A A^T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Singular Values: The singular values of a matrix A are the square roots of the eigenvalues of A^T A or A A^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Values and Singular Value Decomposition (SVD): The singular values of a matrix A are the square roots of the eigenvalues of the matrix A^T A. The SVD of a matrix A is a factorization of A into the product of three matrices U, Σ, and V^T, such that A = U Σ V^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9173495505027154
      },
      {
        "text": "Singular Value Spectrum: The singular value spectrum of a matrix A is the set of singular values of A, which are the square roots of the eigenvalues of A^T A or A A^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8435057833072025
      }
    ]
  },
  {
    "representative_text": "Diagonal Matrix: A diagonal matrix is a square matrix whose non-zero entries are confined to the main diagonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Diagonal Matrix: A diagonal matrix is a square matrix whose non-zero entries are confined to the main diagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Diagonal Matrix: A square matrix with all non-zero elements on the main diagonal (from the top-left to the bottom-right) and all other elements equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9319950367611133
      },
      {
        "text": "Diagonal Matrix: A square matrix where all non-zero elements are on the main diagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9645245433951444
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition Theorem: Given a matrix A, there exists an orthogonal matrix U and a diagonal matrix Σ such that A = UΣV^T, where V is also an orthogonal matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition Theorem: Given a matrix A, there exists an orthogonal matrix U and a diagonal matrix Σ such that A = UΣV^T, where V is also an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Decomposition (SVD) Theorem: A matrix can be decomposed into the product of an orthogonal matrix, a diagonal matrix, and a transpose of an orthogonal matrix, where the orthogonal matrix represents the projection onto the range of the matrix, the diagonal matrix represents the singular values, and the transpose of the orthogonal matrix represents the left singular vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.881579511300222
      },
      {
        "text": "Orthogonal decomposition theorem: If A is a matrix, then there exists an orthogonal decomposition of A into the product A = U Σ V^T, where U and V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9212292363086929
      },
      {
        "text": "The definition and properties of the SVD of a matrix, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8056911390053003
      },
      {
        "text": "The Singular Value Decomposition (SVD) of a Matrix: This decomposition states that any matrix A can be written as A = UΣV^T, where U and V are orthogonal matrices and Σ is a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9227807569107991
      },
      {
        "text": "Orthogonal Decomposition of a Matrix using the Singular Value Decomposition (SVD): The SVD can be used to orthogonalize a matrix, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8741328282630909
      }
    ]
  },
  {
    "representative_text": "Unique Factorization: The SVD is unique, i.e., if A = UΣV^T and B = UΣV^T, then U = V, Σ = I.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Unique Factorization: The SVD is unique, i.e., if A = UΣV^T and B = UΣV^T, then U = V, Σ = I.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of U and V: The span of U and V are equal, i.e., span(U) = span(V).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span of U and V: The span of U and V are equal, i.e., span(U) = span(V).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of Σ: The span of Σ is equal to the range of A, i.e., span(Σ) = range(A).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Span of Σ: The span of Σ is equal to the range of A, i.e., span(Σ) = range(A).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Range of A: The range of A is equal to the span of Σ, i.e., range(A) = span(Σ).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.943269176578138
      }
    ]
  },
  {
    "representative_text": "Null Space of A: The null space of A is equal to the orthogonal complement of the range of A, i.e., null(A) = (range(A))^⊥.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Null Space of A: The null space of A is equal to the orthogonal complement of the range of A, i.e., null(A) = (range(A))^⊥.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Null Space: The null space of an orthogonal complement is the orthogonal complement of its null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8239918753502138
      },
      {
        "text": "Null Space: The null space of the projection matrix is equal to the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8379470976046878
      },
      {
        "text": "Range and Null Space of an Orthogonal Projection Matrix: These are the subspaces of the domain vector space that consist of all projected vectors and all vectors orthogonal to the projected vectors, respectively.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8085319281441021
      },
      {
        "text": "Theorem on the Null Space of an Orthogonal Projection Matrix: This theorem states that the null space of an orthogonal projection matrix is equal to the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8702168878031077
      },
      {
        "text": "Null space and range: The null space of a linear transformation is equal to the orthogonal complement of the range of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.888690859839786
      },
      {
        "text": "The Orthogonal Complement of a Matrix: The matrix that represents the orthogonal complement of a given matrix, which can be used to find the null space of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8368125699753367
      },
      {
        "text": "The Orthogonal Complement of a Matrix: The orthogonal complement of a matrix is the matrix that represents the orthogonal complement of the given matrix, which can be used to find the null space of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8467995747331958
      },
      {
        "text": "Null Space Theorem: A theorem that states that the null space of a linear transformation is equal to the orthogonal complement of the range of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8514955295648957
      },
      {
        "text": "Null Space and Span: The null space of a linear transformation is equal to the orthogonal complement of the range of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8932893009920437
      }
    ]
  },
  {
    "representative_text": "QR Algorithm: The QR algorithm is an iterative method for computing the SVD of a matrix A.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "QR Algorithm: The QR algorithm is an iterative method for computing the SVD of a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Connection between SVD and QR Algorithm: The SVD and QR algorithm are connected, as the QR algorithm can be used to compute the SVD of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8132534546142229
      },
      {
        "text": "Computing the SVD of a Matrix with High Rank: The SVD of a matrix with high rank can be computed using iterative methods such as the QR algorithm or the Householder transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8016635299033236
      },
      {
        "text": "Numerical Methods for SVD Computation: While the SVD is a factorization technique, it is often computationally expensive to compute directly. Numerical methods, such as the QR algorithm, Householder transformations, and Givens rotations, can be used to approximate the SVD of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8029339392742416
      },
      {
        "text": "Computing the SVD of a matrix with a large number of rows or columns: The SVD of a matrix with a large number of rows or columns can be computed using specialized algorithms, such as the block QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8620855657598606
      },
      {
        "text": "Block QR Algorithm: This algorithm is used to compute the SVD of a matrix with a large number of rows or columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8251147462499202
      },
      {
        "text": "Numerical Methods for SVD Computation: Numerical methods, such as the QR algorithm, Householder transformations, and Givens rotations, can be used to approximate the SVD of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8989593586549776
      }
    ]
  },
  {
    "representative_text": "Householder Transformations: Householder transformations are a method for applying orthogonal transformations to a matrix to reduce its rank.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 20,
    "detailed_sources": [
      {
        "text": "Householder Transformations: Householder transformations are a method for applying orthogonal transformations to a matrix to reduce its rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Householder Transformations: A method for orthogonalizing a matrix using a series of Householder transformations, which are orthogonal matrices with a specific structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8557989137974917
      },
      {
        "text": "The Householder Transformation: This is a mathematical operation used to orthogonalize a vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8576223695701874
      },
      {
        "text": "Householder Transformations: These are a class of orthogonal transformations that are used to orthogonalize vectors and matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9310778755997455
      },
      {
        "text": "Orthogonalization of a Matrix using Householder Transformations: A process for transforming a matrix into an orthogonal matrix using Householder transformations, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8938911181401166
      },
      {
        "text": "The Orthogonalization of a Matrix using Householder Transformations: A method for orthogonalizing a matrix using Householder transformations, which are orthogonal matrices with a specific structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8897447139410074
      },
      {
        "text": "The Role of the Householder Transformations: Householder transformations are a class of orthogonal transformations that can be used to orthogonalize vectors and matrices in the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8936759029696604
      },
      {
        "text": "The Householder Transformation: The Householder transformation is a method for orthogonalizing a matrix using a series of Householder transformations, which are orthogonal matrices with a specific structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9479130772522665
      },
      {
        "text": "The Householder Transformation: This transformation is used to orthogonalize a vector in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9311180237419142
      },
      {
        "text": "Householder Transformations and Orthogonalization: Householder transformations are a type of orthogonal transformation that can be used to orthogonalize a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9398243868450107
      },
      {
        "text": "Orthogonalization of Matrices using the Householder Transformation: The Householder transformation can be generalized to orthogonalize matrices, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.916655763982544
      },
      {
        "text": "Householder Transformations: The properties and applications of Householder transformations, including their use in diagonalizing matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8656174001428595
      },
      {
        "text": "Householder Transformations and Their Applications: Understanding Householder transformations, and their properties and their applications of orthogonal transformations, and their applications of Householder transformations, and their applications of Householder transformations, and their applications of complex eigenvalue decomposition of Householder transformations: Studying Transformations: Studying of Complex Matrix**: Studying theoreilly",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8324024864974731
      },
      {
        "text": "The Orthogonalization of a Matrix using Householder Transformations: This is a method for orthogonalizing a matrix using Householder transformations, which are a type of linear transformation used to orthogonalize a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9098355784033485
      },
      {
        "text": "Householder Transformation for Matrices: The Householder transformation can be generalized to orthogonalize matrices, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8982904826361962
      },
      {
        "text": "The Role of Householder Transformations: Householder transformations are a class of orthogonal transformations that can be used to orthogonalize vectors and matrices in the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9176240011593707
      },
      {
        "text": "Householder Transformations and Givens Rotations: This involves studying the properties of Householder transformations and Givens rotations, including the definition of Householder transformations, Givens rotations properties, and the characterization of Householder transformation and Givens rotation spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8500067026848195
      },
      {
        "text": "Householder Transformations and their Applications: The study of Householder Transformations and their applications of Householder Transformations: Theorem and its Applications of its Applications of a fundamental concepts in linear algebraic properties of a set of a differentiables Vector Space: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem, which is a set of linear algebraic properties of a fundamental concept of a vector spaces**: Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8161619557726929
      },
      {
        "text": "Householder Transformations for Orthogonal Matrix Decomposition: Householder transformations are a method for orthogonalizing a matrix, which can be used for orthogonal matrix decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9213018643422389
      },
      {
        "text": "Orthogonalization of a Matrix using the QR Decomposition and Householder Transformation: The QR decomposition can be used to orthogonalize a matrix, and the Householder transformation can be used to further orthogonalize the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.867061274152429
      }
    ]
  },
  {
    "representative_text": "Givens Rotations: Givens rotations are a method for applying orthogonal transformations to a matrix to reduce its rank.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Givens Rotations: Givens rotations are a method for applying orthogonal transformations to a matrix to reduce its rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Givens Rotations: A method for orthogonalizing a matrix using a series of Givens rotations, which are orthogonal matrices with a specific structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8727286623340771
      },
      {
        "text": "Givens Rotations: These are a type of orthogonal transformation that is used to orthogonalize vectors and matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.924451801538877
      },
      {
        "text": "Givens Rotation: This is a technique used to eliminate elements of a matrix by applying a rotation, which is useful in numerical computations of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.866889088816787
      },
      {
        "text": "The Orthogonalization of a Matrix using Givens Rotations: A method for orthogonalizing a matrix using Givens rotations, which are orthogonal matrices with a specific structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8669195349458321
      },
      {
        "text": "Givens Rotations: Givens rotations are a type of orthogonal transformation that can be used to orthogonalize vectors and matrices in the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.906997993523444
      },
      {
        "text": "The Givens Rotation: The Givens rotation is a method for orthogonalizing a matrix using a series of Givens rotations, which are orthogonal matrices with a specific structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9451883047873882
      },
      {
        "text": "Givens Rotations in Gram-Schmidt Process: Givens rotations are a type of orthogonal transformation that can be used to orthogonalize vectors and matrices in the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9229127835961424
      },
      {
        "text": "The Givens Rotations: The Givens rotations are a set of linear transformations used to orthogonalize a matrix. These transformations are essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8515019425655952
      },
      {
        "text": "The Givens Rotation: The Givens rotation is a linear transformation that rotates a vector by a certain angle. It is a fundamental tool in linear algebra and is used in various algorithms for orthogonalization and diagonalization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8244002719562569
      },
      {
        "text": "Orthogonalization of a Matrix using Givens Rotations: This is a method for orthogonalizing a matrix using Givens rotations, which are a type of linear transformation used to orthogonalize a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8989816703169011
      },
      {
        "text": "Givens Rotations for Orthogonal Matrix Decomposition: Givens rotations are a method for orthogonalizing a matrix, which can be used for orthogonal matrix decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9275548024840645
      }
    ]
  },
  {
    "representative_text": "Image Compression: SVD is used in image compression algorithms such as JPEG and MPEG.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Image Compression: SVD is used in image compression algorithms such as JPEG and MPEG.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Data Analysis: SVD is used in data analysis techniques such as principal component analysis (PCA).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Data Analysis: SVD is used in data analysis techniques such as principal component analysis (PCA).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Machine Learning: SVD is used in machine learning algorithms such as singular value regression and singular value machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.825985221016033
      }
    ]
  },
  {
    "representative_text": "A^T A: The product of the transpose of matrix A with itself.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "A^T A: The product of the transpose of matrix A with itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "A A^T: The product of matrix A with its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9261747679370824
      },
      {
        "text": "U^T U: The product of the transpose of matrix U with itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8114259885643146
      },
      {
        "text": "V^T V: The product of the transpose of matrix V with itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.857386668274006
      }
    ]
  },
  {
    "representative_text": "Σ: The diagonal matrix containing the singular values of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Σ: The diagonal matrix containing the singular values of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Σ_i: The i-th singular value of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Σ_i: The i-th singular value of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "U_i: The i-th column of matrix U.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "U_i: The i-th column of matrix U.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "V_i: The i-th column of matrix V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.833624694137425
      }
    ]
  },
  {
    "representative_text": "Matrix Transpose: The transpose of a matrix A is a matrix that can be used to combine A with its conjugate transpose.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Transpose: The transpose of a matrix A is a matrix that can be used to combine A with its conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Equations: A linear equation is an equation in which the highest power of the variable is 1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Equations: A linear equation is an equation in which the highest power of the variable is 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "A linear equation is an equation in which the highest power of the variable(s) is 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9285053215639537
      }
    ]
  },
  {
    "representative_text": "Linear System: A linear system is a set of linear equations in which the coefficients and variables are real numbers.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear System: A linear system is a set of linear equations in which the coefficients and variables are real numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "A system of linear equations is a set of linear equations in which the variables appear only in the linear terms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8560917213637379
      },
      {
        "text": "A linear system is a system of linear equations that can be solved using matrix methods.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8751450777862562
      }
    ]
  },
  {
    "representative_text": "Nullity of a Matrix: The nullity of a matrix A is the dimension of the null space of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Nullity of a Matrix: The nullity of a matrix A is the dimension of the null space of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Nullity: The dimension of the null space of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8130125120793441
      },
      {
        "text": "Nullity: The dimension of the null space of a matrix A, which represents the number of linearly independent vectors in the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9078048421891196
      },
      {
        "text": "Dimension of the Null Space using the Rank-Nullity Theorem: The dimension of the null space of a matrix can be found using the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8479301089427222
      }
    ]
  },
  {
    "representative_text": "Characteristic Equation: A polynomial equation det(A - λI) = 0, where A is the matrix, λ is the eigenvalue, and I is the identity matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Characteristic Equation: A polynomial equation det(A - λI) = 0, where A is the matrix, λ is the eigenvalue, and I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Equation: The equation det(A - λI) = 0 represents the eigenvalue equation, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8177734827458076
      }
    ]
  },
  {
    "representative_text": "Eigenvalue-Multiplicity Theorem: If λ is an eigenvalue of matrix A with algebraic multiplicity m and geometric multiplicity k, then m ≥ k.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue-Multiplicity Theorem: If λ is an eigenvalue of matrix A with algebraic multiplicity m and geometric multiplicity k, then m ≥ k.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Solving Eigenvector Equations: The eigenvectors can be found by solving the equation (A - λI)v = 0, which can be done using various methods such as Gaussian elimination, LU decomposition, or numerical methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Solving Eigenvector Equations: The eigenvectors can be found by solving the equation (A - λI)v = 0, which can be done using various methods such as Gaussian elimination, LU decomposition, or numerical methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence: Eigenvectors are linearly independent if the geometric multiplicity of each eigenvalue is equal to its algebraic multiplicity.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Independence: Eigenvectors are linearly independent if the geometric multiplicity of each eigenvalue is equal to its algebraic multiplicity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvector Linear Independence: If v1, v2, ..., vn are eigenvectors of A corresponding to distinct eigenvalues λ1, λ2, ..., λn, then v1, v2, ..., vn are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8858706702956418
      },
      {
        "text": "The Linear Independence of Eigenvectors: Eigenvectors are linearly independent if the matrix is diagonalizable. This is because the eigenvectors form a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9034964621872967
      },
      {
        "text": "The Linear Independence of Eigenvectors: Eigenvectors are linearly independent if the matrix is diagonalizable.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.958071278159556
      }
    ]
  },
  {
    "representative_text": "Span: The set of eigenvectors forms a subspace of R^n, where R^n is the vector space of n-dimensional vectors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span: The set of eigenvectors forms a subspace of R^n, where R^n is the vector space of n-dimensional vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Algebraic Multiplicity: The number of times an eigenvalue appears as a root of the characteristic equation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Algebraic Multiplicity: The number of times an eigenvalue appears as a root of the characteristic equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Multiplicity: The multiplicity of an eigenvalue is the number of times it appears as a root of the characteristic equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8984632572339536
      },
      {
        "text": "Algebraic Multiplicity: The algebraic multiplicity of an eigenvalue is the multiplicity of the eigenvalue as a root of the characteristic equation. It is an important concept in linear algebra, as it can affect the diagonalization of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8597365302218924
      },
      {
        "text": "Algebraic and Geometric Multiplicity: The algebraic multiplicity of an eigenvalue is the number of times it appears as a root of the characteristic equation, while the geometric multiplicity is the number of linearly independent eigenvectors corresponding to that eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9047898665340144
      }
    ]
  },
  {
    "representative_text": "Geometric Multiplicity: The number of linearly independent eigenvectors corresponding to an eigenvalue.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Geometric Multiplicity: The number of linearly independent eigenvectors corresponding to an eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Geometric Multiplicity: The geometric multiplicity of an eigenvalue is the number of linearly independent eigenvectors corresponding to that eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9408282284546615
      },
      {
        "text": "Geometric Multiplicity: The geometric multiplicity of an eigenvalue is the number of linearly independent eigenvectors corresponding to the eigenvalue. It is an important concept in linear algebra, as it can affect the diagonalization of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8600335817997901
      }
    ]
  },
  {
    "representative_text": "Non-Diagonalizable Matrix: A matrix that does not have a diagonalizing matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Diagonalizable Matrix: A matrix that does not have a diagonalizing matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Diagonalizable Matrix: A matrix that can be diagonalized, i.e., it has a diagonalizing matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Diagonalizable Matrix: A matrix that can be diagonalized, i.e., it has a diagonalizing matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Diagonalizable matrix: A square matrix that can be transformed into a diagonal matrix using a similarity transformation, i.e., A = PDP^(-1), where D is a diagonal matrix and P is an invertible matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8500066075353694
      },
      {
        "text": "Diagonalizable Matrix: A square matrix A is diagonalizable if and only if it has a basis of eigenvectors, i.e., there exists a matrix P such that P^(-1)AP is a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8779225333453057
      },
      {
        "text": "Diagonalizable Matrix: A square matrix that can be transformed into a diagonal matrix using a similarity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.942754157925108
      }
    ]
  },
  {
    "representative_text": "Existence: Every square matrix has at least one eigenvalue, which can be found by solving the characteristic equation det(A - λI) = 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Existence: Every square matrix has at least one eigenvalue, which can be found by solving the characteristic equation det(A - λI) = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Uniqueness: If a matrix A has a real eigenvalue λ, then λ is unique, i.e., there is only one value of λ that satisfies the characteristic equation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Uniqueness: If a matrix A has a real eigenvalue λ, then λ is unique, i.e., there is only one value of λ that satisfies the characteristic equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Real vs. Complex Eigenvalues: Eigenvalues can be either real or complex. Real eigenvalues have a multiplicity of 1, while complex eigenvalues have a multiplicity of 2 (counted with multiplicity).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Real vs. Complex Eigenvalues: Eigenvalues can be either real or complex. Real eigenvalues have a multiplicity of 1, while complex eigenvalues have a multiplicity of 2 (counted with multiplicity).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Multiplicity and the Determinant: The determinant of a matrix is equal to the product of its eigenvalues, i.e., det(A) = ∏(λi), where λi are the eigenvalues of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Eigenvalue Multiplicity and the Determinant: The determinant of a matrix is equal to the product of its eigenvalues, i.e., det(A) = ∏(λi), where λi are the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant: The determinant of a matrix is equal to the product of its eigenvalues, i.e., det(A) = ∏(λi).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9372822105631125
      },
      {
        "text": "Determinant and Rank: The determinant of a matrix is equal to the product of its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9192330348704114
      },
      {
        "text": "Statement: The determinant of a matrix is equal to the product of its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8838946774575323
      },
      {
        "text": "Determinant as a Product of Eigenvalues:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8623360778109472
      },
      {
        "text": "The determinant of a matrix A is equal to the product of its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9375285933508404
      },
      {
        "text": "det(A) = ∏[λi], where λi are the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8814158598612778
      },
      {
        "text": "The determinant of A can be computed using the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8782483075382708
      },
      {
        "text": "Determinant and Eigenvalues: The determinant of a matrix A is equal to the product of its eigenvalues. This is a fundamental property of determinants and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9631767165416708
      },
      {
        "text": "Determinant and Singular Value Decomposition: The determinant of a matrix A is equal to the product of the singular values of A. This is a fundamental property of determinants and singular value decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8201175496560585
      },
      {
        "text": "The Relationship between Eigenvalues and the Determinant: The determinant of a matrix is equal to the product of its eigenvalues. This relationship can be used to compute the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9227345047481732
      },
      {
        "text": "Determinant of a Matrix: The determinant of a matrix is equal to the product of the eigenvalues of the matrix, which is related to the concept of span in terms of eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8357121823868545
      },
      {
        "text": "Determinant of a Matrix with a Jordan Block: The determinant of a matrix with a Jordan block is the product of the eigenvalues of the block.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.812760979015557
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Multiplicity and the Rank: The rank of a matrix is equal to the number of non-zero eigenvalues, i.e., rank(A) = number of non-zero eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Eigenvalue Multiplicity and the Rank: The rank of a matrix is equal to the number of non-zero eigenvalues, i.e., rank(A) = number of non-zero eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rank: The rank of a matrix is equal to the number of non-zero eigenvalues, i.e., rank(A) = number of non-zero eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9374716182045864
      },
      {
        "text": "The Rank of a Matrix: The rank of a matrix is equal to the number of non-zero eigenvalues. This is known as the eigenvalue decomposition of the rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9362093942940781
      },
      {
        "text": "The relationship between the eigenvalues of a matrix and its rank: The rank of a matrix is equal to the number of non-zero eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9414207805902943
      },
      {
        "text": "The Rank of a Matrix using Eigenvalues: The rank of a matrix is equal to the number of non-zero eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9639193992365196
      },
      {
        "text": "Rank of a matrix and determinants: The rank of a matrix is equal to the number of non-zero eigenvalues (or the number of linearly independent rows/columns), which is related to the determinant of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8949255563837395
      }
    ]
  },
  {
    "representative_text": "Nullity: The nullity of a matrix is equal to the number of zero eigenvalues, i.e., nullity(A) = number of zero eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Nullity: The nullity of a matrix is equal to the number of zero eigenvalues, i.e., nullity(A) = number of zero eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Nullity of a Matrix: The nullity of a matrix is equal to the number of zero eigenvalues. This is known as the eigenvalue decomposition of the nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9544083043935503
      },
      {
        "text": "The relationship between the eigenvalues of a matrix and its nullity: The nullity of a matrix is equal to the number of zero eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9257497516571062
      },
      {
        "text": "The Nullity of a Matrix using Eigenvalues: The nullity of a matrix is equal to the number of zero eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9729319292328095
      }
    ]
  },
  {
    "representative_text": "Gerschgorin's Theorem: Every eigenvalue of a matrix A lies within at least one of the Gerschgorin discs, which are defined as Di = {z: |z - aij| ≤ Ri}, where aij are the elements of the matrix and R_i are the sum of the absolute values of the elements in the i-th row.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Gerschgorin's Theorem: Every eigenvalue of a matrix A lies within at least one of the Gerschgorin discs, which are defined as Di = {z: |z - aij| ≤ Ri}, where aij are the elements of the matrix and R_i are the sum of the absolute values of the elements in the i-th row.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gershgorin's Theorem: If A is a square matrix and Aij ≠ 0, then there exists an eigenvalue λ of A such that 1 - λ ≤ λij ≤ 1 + λij, where λij is the element in the ith row and jth column of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9264249388957181
      },
      {
        "text": "Gershgorin's Circle Theorem: This theorem states that if A is a square matrix and Aij are the entries of A, then every eigenvalue of A lies within one of the Gershgorin discs centered at Aii and having radius Rii = ∑ |Aij|.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.964499304528309
      },
      {
        "text": "Gerschgorin's Theorem: A theorem that describes the location of eigenvalues of a matrix based on the elements of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8397555722795891
      },
      {
        "text": "Gerschgorin Circle Theorem: This theorem provides a method for approximating the eigenvalues of a matrix by considering the Gerschgorin discs, which are discs centered at the diagonal elements with radii equal to the sum of the absolute values of the non-diagonal elements in the same row.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9136799391208686
      },
      {
        "text": "Gershgorin Circle Theorem: A theorem that describes the properties of the eigenvalues of a matrix based on the Gershgorin discs, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8661177585991953
      },
      {
        "text": "Gershgorin's Circle Theorem: This theorem states that if A is a square matrix and Aij are the entries of A, then every eigenvalue of A lies within one of the Gershgorin discs centered at Aii and having radius Rii = ∑ |Aij|. This is an important theorem that is related to the SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8651585495459748
      },
      {
        "text": "Gerschgorin's Theorem: This theorem states that every matrix has at least one eigenvalue that lies within each of its Gerschgorin discs. These discs are defined by the sum of the absolute values of the elements in each row (or column) of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9449290521425499
      },
      {
        "text": "Gerschgorin's Theorem: Explore Gerschgorin's theorem, which states that every matrix has at least one eigenvalue that lies within each of its Gerschgorin discs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9341202608842459
      }
    ]
  },
  {
    "representative_text": "Perron-Frobenius Theorem: The largest eigenvalue of a positive definite matrix is real and positive, and the corresponding eigenvector is a non-negative vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Perron-Frobenius Theorem: The largest eigenvalue of a positive definite matrix is real and positive, and the corresponding eigenvector is a non-negative vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Perron-Frobenius Theorem: This theorem states that a square matrix with real entries has a largest eigenvalue, and its corresponding eigenvector is a non-negative vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9074286343508137
      }
    ]
  },
  {
    "representative_text": "Jordan Decomposition: Every square matrix can be decomposed into a product of a diagonalizable matrix and a nilpotent matrix, i.e., A = PDP^(-1), where D is a diagonal matrix and N is a nilpotent matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Jordan Decomposition: Every square matrix can be decomposed into a product of a diagonalizable matrix and a nilpotent matrix, i.e., A = PDP^(-1), where D is a diagonal matrix and N is a nilpotent matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Jordan Decomposition of a Matrix: A matrix can be decomposed into a product of a diagonalizable matrix and a nilpotent matrix. This is known as the Jordan decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8856042190721969
      },
      {
        "text": "The Jordan Decomposition Theorem: A matrix can be decomposed into a direct sum of Jordan blocks, and this theorem provides a fundamental connection between linear transformations and their representations as matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8271790158893535
      },
      {
        "text": "The Schur's Decomposition: This decomposition states that a matrix A can be decomposed into a product of a diagonalizable matrix and a nilpotent matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8739780166606791
      },
      {
        "text": "The relationship between eigenvalues and the Jordan decomposition: The Jordan decomposition of a matrix is a way of expressing the matrix as a product of a diagonalizable matrix and a nilpotent matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8523543750182532
      },
      {
        "text": "The Diagonalization of a Matrix using the Jordan Decomposition: The Jordan decomposition of a matrix can be used to diagonalize the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8507919965996773
      },
      {
        "text": "The concept of Jordan decomposition: A method used to diagonalize a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8420836389668578
      }
    ]
  },
  {
    "representative_text": "Block Diagonalization: A square matrix A can be block diagonalized if and only if all its eigenvalues are distinct, i.e., A = PDP^(-1), where D is a diagonal matrix and P is a permutation matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Block Diagonalization: A square matrix A can be block diagonalized if and only if all its eigenvalues are distinct, i.e., A = PDP^(-1), where D is a diagonal matrix and P is a permutation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Block Diagonalization of a Matrix: A square matrix A can be block diagonalized if and only if all its eigenvalues are distinct.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9395112329728947
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Algorithm: An algorithm for finding the eigenvalues of a matrix, such as the QR algorithm or the power method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 31,
    "detailed_sources": [
      {
        "text": "Eigenvalue Algorithm: An algorithm for finding the eigenvalues of a matrix, such as the QR algorithm or the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvector Algorithm: An algorithm for finding the eigenvectors of a matrix, such as the QR algorithm or the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9302273584996552
      },
      {
        "text": "QR Algorithm: A popular algorithm for finding eigenvalues and eigenvectors of a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8764087498627654
      },
      {
        "text": "Jacobi Algorithm: An iterative algorithm for finding eigenvalues and eigenvectors of a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8593069599797412
      },
      {
        "text": "QR Iterative Algorithm: An iterative algorithm for finding eigenvalues and eigenvectors of a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8651108255069201
      },
      {
        "text": "QR Algorithm: The QR algorithm is an iterative technique for computing the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9214307220647151
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using the QR Algorithm: The QR algorithm is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by decomposing the matrix into the product of an orthogonal matrix Q and an upper triangular matrix R.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8721170165638359
      },
      {
        "text": "Lanczos Algorithm: An algorithm for finding the eigenvalues and eigenvectors of a matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8673933275964137
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the QR Algorithm: The QR algorithm is a method for computing the eigenvalues of a matrix by transforming the matrix into a tridiagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9037438908901593
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the QR Algorithm with Shift and Invert: This method is an improvement over the QR algorithm and can be used to compute the eigenvalues of a matrix more efficiently.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8340482285926407
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the Arnoldi Method: The Arnoldi method is a method for computing the eigenvalues of a matrix by iteratively applying a series of Householder transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8157141817639295
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the Krylov Subspace Method: The Krylov subspace method is a method for computing the eigenvalues of a matrix by iteratively applying a series of Householder transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8016153587011587
      },
      {
        "text": "Eigenvalue Computation using the QR Algorithm: The QR algorithm is a popular method for computing eigenvalues and eigenvectors of a matrix A. It involves repeatedly applying the Gram-Schmidt process to the columns of A to obtain an orthogonal matrix Q and a diagonal matrix R.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8902511668084253
      },
      {
        "text": "QR Iteration: This is an algorithm used to find the eigenvalues and eigenvectors of a matrix by iteratively applying a QR decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9346201127762344
      },
      {
        "text": "Lanczos Algorithm: This is an algorithm used to find the eigenvalues and eigenvectors of a matrix by iteratively applying a QR decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9278317525426152
      },
      {
        "text": "The QR Algorithm: This is an algorithm used to find the eigenvalues and eigenvectors of a matrix by iteratively applying a QR decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9608714095352083
      },
      {
        "text": "The Computation of Eigenvectors using the QR Algorithm: The QR algorithm is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves decomposing the matrix into an orthogonal matrix Q and an upper triangular matrix R.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9098701167916943
      },
      {
        "text": "The Computation of Eigenvalues using the QR Algorithm with Shift: The QR algorithm with shift is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves decomposing the matrix into an orthogonal matrix Q and an upper triangular matrix R, and shifting the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8916546336154969
      },
      {
        "text": "The QR Algorithm: An iterative method for finding the eigenvalues and eigenvectors of a matrix by applying a sequence of orthogonal transformations and orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9362782712140075
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Davidson Algorithm: The Davidson algorithm is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively applying a Davidson rotation to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8441650709953242
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using QR Algorithm with Shifts: The QR algorithm with shifts is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively applying a shift to the matrix and computing its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8986900763532073
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Davidson Algorithm with Shifts: The Davidson algorithm with shifts is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively applying a Davidson rotation and a shift to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8226584474708185
      },
      {
        "text": "The QR Algorithm: An algorithm for computing the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8922054164856823
      },
      {
        "text": "Eigenvalue Computation using the QR Algorithm: The QR algorithm is a popular method for computing eigenvalues and eigenvectors of a matrix A. This is an important concept in linear algebra, as it can be used to estimate the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9134758194554284
      },
      {
        "text": "QR Algorithm for Large Matrices: The QR algorithm is an iterative algorithm for finding the eigenvalues and eigenvectors of a matrix. It can be used to compute the inverse of a matrix for large matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8179648806592001
      },
      {
        "text": "The Diagonalization of a Matrix using the QR Algorithm: The QR algorithm is an iterative method for finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9217287786745727
      },
      {
        "text": "The Diagonalization of a Matrix using the QR Algorithm (Alternative Method): The QR algorithm can be used to diagonalize a matrix, even if the matrix is not symmetric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8022252112877447
      },
      {
        "text": "The Diagonalization of a Matrix using the QR Algorithm: The QR algorithm can be used to diagonalize a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.867993567087427
      },
      {
        "text": "The Lanczos Algorithm: The Lanczos algorithm is an algorithm used to find the eigenvalues and eigenvectors of a matrix. It involves iteratively applying a QR decomposition and using the resulting matrices to estimate the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8838368167275582
      },
      {
        "text": "The QR Algorithm: The QR algorithm is an algorithm used to find the eigenvalues and eigenvectors of a matrix. It involves iteratively applying a QR decomposition and using the resulting matrices to estimate the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9364572864617291
      },
      {
        "text": "QR Algorithm: An algorithm for computing the eigenvalues and eigenvectors of a matrix, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8957297294792723
      }
    ]
  },
  {
    "representative_text": "Jordan Form: The Jordan form of a matrix is a block diagonal matrix, where each block is a Jordan block, which is a matrix with a single eigenvalue on the diagonal and ones on the superdiagonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 26,
    "detailed_sources": [
      {
        "text": "Jordan Form: The Jordan form of a matrix is a block diagonal matrix, where each block is a Jordan block, which is a matrix with a single eigenvalue on the diagonal and ones on the superdiagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Jordan Canonical Form: A canonical form of a matrix that represents a linear transformation, which is used to diagonalize the matrix and understand its behavior.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8071190049860176
      },
      {
        "text": "Jordan Canonical Form: The Jordan canonical form is a block diagonal matrix that represents a square matrix in a canonical form. This form is useful in understanding the properties of eigenvalues and eigenvectors, particularly for matrices with repeated eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9344853547872642
      },
      {
        "text": "Jordan Canonical Form: This is a block diagonal matrix that can be used to represent a matrix that is not diagonalizable. It is a generalization of the diagonalization theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8782087158818727
      },
      {
        "text": "Jordan Canonical Form for Complex Matrices: The Jordan canonical form is a matrix that has the same eigenvalues as the original matrix, but with Jordan blocks instead of diagonal entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8933979533333765
      },
      {
        "text": "The Jordan Canonical Form: The Jordan canonical form is a canonical form of a matrix that represents a linear transformation. This form is essential in understanding the behavior of linear transformations and has many applications in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9031627007395356
      },
      {
        "text": "Jordan Canonical Form: A canonical form of a matrix that is similar to the diagonal matrix D, but can have Jordan blocks instead of diagonal entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9440469104464606
      },
      {
        "text": "Jordan Canonical Form: The Jordan canonical form is a canonical form for matrices that can be used to diagonalize them.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9321399708962671
      },
      {
        "text": "The Jordan Normal Form: The Jordan normal form is a block diagonal matrix that represents a matrix in a canonical form. It is related to the Jordan canonical form and can be used to analyze the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.870933368712333
      },
      {
        "text": "The Jordan Canonical Form for Complex Matrices: This is a matrix that has the same eigenvalues as the original matrix, but with Jordan blocks instead of diagonal entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8917806684094512
      },
      {
        "text": "The Jordan Canonical Form: The Jordan canonical form is a canonical form of a matrix that represents a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9145877759862145
      },
      {
        "text": "The Jordan canonical form of a matrix: The Jordan canonical form of a matrix is a block diagonal matrix where each block is a Jordan block. This is related to the concept of diagonalization and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9065341175653976
      },
      {
        "text": "Jordan Canonical Form: This is a block diagonal matrix that represents a matrix in a canonical form, which is a fundamental concept in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9256696615318407
      },
      {
        "text": "Jordan Canonical Form: The Jordan canonical form is a way of representing a diagonalizable linear transformation T from a finite-dimensional vector space V to itself as a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8899427908267947
      },
      {
        "text": "Diagonalization of Non-Diagonalizable Matrices: The Jordan canonical form is a block diagonal matrix that can be used to represent a matrix that is not diagonalizable. Understanding the properties of non-diagonalizable matrices and their Jordan canonical forms is essential in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8382729730850522
      },
      {
        "text": "Jordan Canonical Form: The Jordan canonical form is a block diagonal matrix that represents a matrix in a canonical form. This is an important concept in linear algebra, as it can be used to diagonalize matrices with repeated eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9594153687600269
      },
      {
        "text": "Jordan Canonical Form for Complex Matrices: The Jordan canonical form is a matrix that has the same eigenvalues as the original matrix, but with Jordan blocks instead of diagonal entries. This form is useful for finding eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9032374731628974
      },
      {
        "text": "The Jordan Canonical Form: This concept is related to the eigenvalues and states that the Jordan canonical form is a canonical form of a matrix that represents a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.891387795566857
      },
      {
        "text": "Jordan Normal Form: A canonical form of a matrix that is similar to the diagonal matrix D, but can have Jordan blocks instead of diagonal entries. It's an extension of the Jordan canonical form for square matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8983412538642646
      },
      {
        "text": "Canonical Form of a Matrix: This is a fundamental concept in linear algebra that states that every square matrix A can be transformed into a canonical form, which is a block diagonal matrix containing Jordan blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8767184774616957
      },
      {
        "text": "The Jordan Canonical Form for Non-Symmetric Matrices: A canonical form of a non-symmetric matrix that is similar to the diagonal matrix D, but can have Jordan blocks instead of diagonal entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8470139302426077
      },
      {
        "text": "Jordan Canonical Form: This is a representation of a matrix as a block diagonal matrix, where each block corresponds to an eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9184974097049821
      },
      {
        "text": "Jordan Canonical Form: The Jordan canonical form is a block diagonal matrix that represents a matrix in a canonical form. It is useful for finding eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9611274171201164
      },
      {
        "text": "Jordan Normal Form: This is a canonical form of a matrix that is similar to the matrix, but has a simpler structure. The Jordan normal form is useful for solving systems of linear equations and for finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9118863043297014
      },
      {
        "text": "Jordan Normal Form: Discuss Jordan normal form, which is a canonical form of a matrix that is similar to the matrix, but has a simpler structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8844978786741504
      },
      {
        "text": "Jordan canonical form: A block diagonal matrix that represents the eigenvalues and their corresponding eigenvectors of a matrix, which can be used to simplify calculations involving eigenvalues and eigenvectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8761001610175063
      }
    ]
  },
  {
    "representative_text": "Diagonalization: The process of transforming a matrix into a diagonal matrix using a similarity transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "Diagonalization: The process of transforming a matrix into a diagonal matrix using a similarity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Diagonalization: A process of diagonalizing a matrix using orthogonal matrices, resulting in a diagonal matrix containing the eigenvalues of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8018044926324482
      },
      {
        "text": "Orthogonal Diagonalization: A method for diagonalizing a matrix using an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9035748403930985
      },
      {
        "text": "Diagonalization: A method for transforming a matrix into a diagonal matrix, which can help in finding eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9009175588068215
      },
      {
        "text": "Orthogonal Diagonalization: A process for diagonalizing a matrix using orthogonal matrices, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.900240083605713
      },
      {
        "text": "Diagonalization and Orthogonalization: A method for diagonalizing a matrix by applying an orthogonal transformation to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9389911730138145
      },
      {
        "text": "Diagonalization: This refers to the process of finding a basis of eigenvectors for a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8588822599995102
      },
      {
        "text": "Diagonalization: Diagonalization of a matrix is a process of transforming a matrix into a diagonal matrix, which can be useful in solving certain types of linear equations and systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8548728518311451
      },
      {
        "text": "Diagonalization: A process of finding a matrix that is similar to another matrix, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8422765032121551
      },
      {
        "text": "Diagonalization of Complex Matrices: Diagonalization is a process of finding a basis of eigenvectors of a complex matrix and representing the matrix as a diagonal matrix. This process is useful for finding eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8204400423612548
      },
      {
        "text": "The process of diagonalizing a matrix: A method used to find a matrix that is similar to another matrix, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8587616193356391
      },
      {
        "text": "Diagonalization of Complex Matrices: The process of finding a basis of eigenvectors of a complex matrix and representing the matrix as a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8841207409400349
      },
      {
        "text": "Diagonalization of Infinite-Dimensional Matrices: The process of finding a basis of eigenvectors for an infinite-dimensional matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8199914935022833
      },
      {
        "text": "Orthogonal diagonalization: A method for diagonalizing a matrix using an orthogonal matrix, which is a matrix whose columns are orthonormal vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.881484573138885
      }
    ]
  },
  {
    "representative_text": "Similar Matrix: Two square matrices A and B are similar if there exists an invertible matrix P such that A = PBP^(-1).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Similar Matrix: Two square matrices A and B are similar if there exists an invertible matrix P such that A = PBP^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Multiplicativity: If λ and μ are eigenvalues of A, then λμ is an eigenvalue of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvalue Multiplicativity: If λ and μ are eigenvalues of A, then λμ is an eigenvalue of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Product: If λ and μ are eigenvalues of A, then λμ is an eigenvalue of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9398490361182995
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Sum: If λ1, λ2, ..., λn are eigenvalues of A, then λ1 + λ2 + ... + λn is the sum of the diagonal elements of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Sum: If λ1, λ2, ..., λn are eigenvalues of A, then λ1 + λ2 + ... + λn is the sum of the diagonal elements of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Inverse: If λ is an eigenvalue of A, then 1/λ is an eigenvalue of A^(-1).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Inverse: If λ is an eigenvalue of A, then 1/λ is an eigenvalue of A^(-1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvector Scaling: If v is an eigenvector of A corresponding to eigenvalue λ, then kv is an eigenvector of A corresponding to eigenvalue λk.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvector Scaling: If v is an eigenvector of A corresponding to eigenvalue λ, then kv is an eigenvector of A corresponding to eigenvalue λk.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Diagonalization Steps: Diagonalization involves finding the eigenvalues and eigenvectors of A, constructing the eigenvector matrix P, and computing the diagonal matrix D = P^(-1)AP.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Diagonalization Steps: Diagonalization involves finding the eigenvalues and eigenvectors of A, constructing the eigenvector matrix P, and computing the diagonal matrix D = P^(-1)AP.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Pseudo-Eigenvectors: If A does not have a full set of linearly independent eigenvectors, then we can construct pseudo-eigenvectors by normalizing the eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pseudo-Eigenvectors: If A does not have a full set of linearly independent eigenvectors, then we can construct pseudo-eigenvectors by normalizing the eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Solving Systems of Linear Equations: Diagonalization can be used to solve systems of linear equations by transforming the coefficient matrix into a diagonal matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Solving Systems of Linear Equations: Diagonalization can be used to solve systems of linear equations by transforming the coefficient matrix into a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Systems of Equations: Orthogonal diagonalization can be used to solve systems of linear equations by reducing the coefficient matrix to a diagonal matrix containing the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.917685846247184
      },
      {
        "text": "Linear Transformations: Orthogonal diagonalization can be used to find the eigenvalues and eigenvectors of a linear transformation, which can be used to solve systems of linear equations and to find the range and null space of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8678114500312304
      },
      {
        "text": "Data Analysis: Orthogonal diagonalization can be used to reduce the dimensionality of high-dimensional data by projecting it onto a lower-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8000238105757602
      },
      {
        "text": "Signal Processing: Orthogonal diagonalization can be used to filter out noise from signals by projecting the signal onto a subspace that preserves the desired information.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8544885755598031
      },
      {
        "text": "Applications of Orthogonal Diagonalization: Various applications of orthogonal diagonalization, such as solving systems of linear equations, finding eigenvalues and eigenvectors, and reducing the dimensionality of high-dimensional data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8910618110617141
      },
      {
        "text": "Orthogonal Diagonalization of Block Matrices: The diagonalization of block matrices using orthogonal matrices, which is useful in solving systems of linear equations and in linear algebra applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.839674244972951
      },
      {
        "text": "Orthogonal Diagonalization and Linear Systems of Equations: The relationship between orthogonal diagonalization and linear systems of equations, including the fact that orthogonal diagonalization can be used to solve systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9042577272637033
      }
    ]
  },
  {
    "representative_text": "Linear Transformations: Eigenvalues and eigenvectors can be used to describe linear transformations in a more compact and efficient way.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Linear Transformations: Eigenvalues and eigenvectors can be used to describe linear transformations in a more compact and efficient way.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalues and eigenvectors: A way to analyze the behavior of linear transformations using scalar values and vector directions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8360809829425113
      },
      {
        "text": "Eigenvalues and eigenvectors: A way to analyze the behavior of linear transformations using eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9218225809680497
      },
      {
        "text": "Eigenvalues and Eigenvectors of a Matrix: The properties and behavior of eigenvalues and eigenvectors of a matrix, including how they relate to the linear transformation represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8752100469095894
      },
      {
        "text": "Eigenvalues and Eigenvectors: These are scalar values and vectors, respectively, that are used to describe the properties of a matrix, particularly in the context of linear transformations and stability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8205080685833515
      }
    ]
  },
  {
    "representative_text": "Data Analysis: Eigenvalues and eigenvectors can be used in data analysis to identify patterns and structures in data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Data Analysis: Eigenvalues and eigenvectors can be used in data analysis to identify patterns and structures in data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Annihilator: The set of all vectors that annihilate a given subspace, meaning that when they are dotted with any vector in the subspace, the result is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Annihilator: The set of all vectors that annihilate a given subspace, meaning that when they are dotted with any vector in the subspace, the result is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonalizer: A process or operation that finds the orthogonal complement of a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Orthogonalizer: A process or operation that finds the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gram-Schmidt Process: A method for finding the orthogonal complement of a subspace by applying the Gram-Schmidt process to an orthonormal basis of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.806936762455154
      },
      {
        "text": "Householder Transformation: A method for finding the orthogonal complement of a subspace by applying a Householder transformation to a basis of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8335069819401022
      },
      {
        "text": "Orthogonal Projections: A method for finding the orthogonal complement of a subspace by applying an orthogonal projection to a basis of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8997045060195035
      },
      {
        "text": "Orthogonal Complement Basis: A basis for the orthogonal complement of a subspace can be obtained by applying the Gram-Schmidt process to an orthonormal basis of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8141565295275424
      }
    ]
  },
  {
    "representative_text": "W: The subspace",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "W: The subspace",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "W^⊥: The orthogonal complement of W (also denoted as W^⊥ or (W)⊥)",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "W^⊥: The orthogonal complement of W (also denoted as W^⊥ or (W)⊥)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "V^⊥: The orthogonal complement of V (also denoted as V^⊥ or (V)⊥)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9318526522963289
      }
    ]
  },
  {
    "representative_text": "Ann(V): The annihilator of V (also denoted as Ann(V) or (V)⊥)",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Ann(V): The annihilator of V (also denoted as Ann(V) or (V)⊥)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linearity: The orthogonal complement of a subspace is a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linearity: The orthogonal complement of a subspace is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8061928610217539
      },
      {
        "text": "The definition and properties of a basis for a subspace, including the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8416109155462232
      },
      {
        "text": "The definition and properties of the dimension of a subspace, including the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9313248914126585
      }
    ]
  },
  {
    "representative_text": "Annihilation: Every vector in the annihilator of a subspace is orthogonal to every vector in the subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Annihilation: Every vector in the annihilator of a subspace is orthogonal to every vector in the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Annihilator Theorem: For a subspace W of a vector space V, the annihilator Ann(W) is equal to the orthogonal complement W^⊥.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8555066437213479
      },
      {
        "text": "Orthogonalizer Theorem: For a subspace W of a vector space V, the orthogonalizer of W is equal to the annihilator Ann(W).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8740581853037804
      }
    ]
  },
  {
    "representative_text": "Intersection: The intersection of two orthogonal complements is the orthogonal complement of their intersection.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Intersection: The intersection of two orthogonal complements is the orthogonal complement of their intersection.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Sum: The sum of two orthogonal complements is the orthogonal complement of their sum.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sum: The sum of two orthogonal complements is the orthogonal complement of their sum.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scalar Multiplication: The scalar multiplication of an orthogonal complement by a scalar results in an orthogonal complement.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar Multiplication: The scalar multiplication of an orthogonal complement by a scalar results in an orthogonal complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span: The span of an orthogonal complement is the orthogonal complement of its span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Span: The span of an orthogonal complement is the orthogonal complement of its span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of the orthogonal complement: The span of the orthogonal complement of a subspace is equal to the original subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9018803071826855
      },
      {
        "text": "Relationship between Span and Basis with respect to Orthogonal Complement: The span of a set of vectors is equal to the orthogonal complement of the orthogonal complement of the span of the set, while a basis for a vector space is a set of linearly independent vectors that spans the entire space and is also a basis for the orthogonal complement of the span of the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8371199826756379
      },
      {
        "text": "Complement of a Span: The complement of a span is the set of vectors that are orthogonal to all vectors in the span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8573811922433984
      },
      {
        "text": "Span Implies Orthogonal Complement: The span of a set of vectors is equal to the orthogonal complement of the orthogonal complement of the span of the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9188782954464021
      },
      {
        "text": "Orthogonal Complement Implies Span: The orthogonal complement of a subspace is spanned by a set of vectors if and only if the set of vectors is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9015229388489887
      },
      {
        "text": "Relationship between Span and Orthogonal Complement: Delve deeper into the relationship between the span of a set of vectors and its orthogonal complement, including the implications for understanding the dimension of the span and the existence of a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8368454602730163
      },
      {
        "text": "Span of the Orthogonal Complement: The span of the orthogonal complement of a subspace is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9233788701838224
      },
      {
        "text": "Span of a Linearly Independent Set and its Orthogonal Complement: The span of a linearly independent set and its orthogonal complement is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8859764476816665
      }
    ]
  },
  {
    "representative_text": "Image: The image of an orthogonal complement is the orthogonal complement of its image.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Image: The image of an orthogonal complement is the orthogonal complement of its image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement Dimension: The dimension of the orthogonal complement of a subspace is equal to the dimension of the vector space minus the dimension of the subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement Dimension: The dimension of the orthogonal complement of a subspace is equal to the dimension of the vector space minus the dimension of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of the Complement of a Set: The dimension of the complement of a set of vectors is equal to the dimension of the entire vector space minus the dimension of the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.838571609174996
      },
      {
        "text": "Dimension of the Complement of a Set using Orthogonal Complement: The dimension of the complement of a set of vectors can be found using the orthogonal complement of the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9080347350483571
      },
      {
        "text": "Dimension Implies Orthogonal Complement: The dimension of a vector space is equal to the number of vectors in a basis for the space if and only if the orthogonal complement of the subspace has a basis with the same number of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.840574032962141
      },
      {
        "text": "Dimension of the Orthogonal Complement of a Free Basis: The dimension of the orthogonal complement of a free basis is equal to the dimension of the entire vector space minus the dimension of the free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.891094790752494
      },
      {
        "text": "Dimension of the Orthogonal Decomposition of a Free Basis: The dimension of the orthogonal decomposition of a free basis is equal to the dimension of the entire vector space minus the dimension of the free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8330515747357778
      },
      {
        "text": "Orthogonal Complement Dimension Formula: A formula that relates the dimension of the orthogonal complement of a subspace to the dimension of the vector space and the dimension of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.863899804502872
      },
      {
        "text": "Dimension Theorem for Orthogonal Complement: The dimension of the orthogonal complement of a subspace $U$ is equal to the dimension of the vector space minus the dimension of $U$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9133736808145314
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection: A linear transformation that projects a vector onto a subspace, preserving the length of the vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection: A linear transformation that projects a vector onto a subspace, preserving the length of the vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Projection Matrix: A square matrix that projects any vector onto a subspace while preserving the orthogonality between the projected vector and the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8406689289961748
      },
      {
        "text": "Projection: A linear transformation that maps a vector to its closest approximation in a given subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8255192693951865
      },
      {
        "text": "Projection Matrix: A square matrix that projects any vector onto a subspace while preserving the orthogonality between the projected vector and the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8975252794734004
      },
      {
        "text": "Orthogonal Projection Matrix with respect to a Basis: A square matrix that projects any vector onto a subspace using a given orthonormal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8963788735339857
      },
      {
        "text": "Projection Matrix: A square matrix that projects vectors onto a subspace, preserving the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9050870637978771
      },
      {
        "text": "Orthogonal Projection of a Matrix onto a Subspace: This involves finding the projection matrix that projects any matrix onto a subspace while preserving the orthogonality between the projected matrix and the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8268196967446386
      },
      {
        "text": "Orthogonal Projection: The projection of a vector onto a subspace using an orthogonal projection matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9126419818836229
      }
    ]
  },
  {
    "representative_text": "Orthonormal Basis: A set of orthogonal vectors that are also normalized to have a length of 1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Orthonormal Basis: A set of orthogonal vectors that are also normalized to have a length of 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthonormal basis: A set of orthogonal vectors that are also orthonormal (i.e., have a norm of 1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.930989259385878
      },
      {
        "text": "Orthonormal Vectors: A set of orthogonal vectors with each vector having a length of 1 (norm equal to 1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9132389555080369
      },
      {
        "text": "Orthonormal Basis: A set of orthonormal vectors that span a vector space, denoted as {u1, u2, ..., un}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8777114727113422
      },
      {
        "text": "Orthonormal Basis: A basis consisting of orthogonal and normalized vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9237994392324336
      },
      {
        "text": "Orthonormal Vectors: Vectors that are both orthogonal and normalized (i.e., have a magnitude of 1).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9059172807613902
      },
      {
        "text": "Orthonormal Bases: An orthonormal basis is a basis of orthogonal vectors with norm 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.906885754488332
      },
      {
        "text": "Orthonormal Bases: An orthonormal basis of a vector space V is a basis for V whose vectors are orthogonal to each other and have a norm of 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9054063564248489
      },
      {
        "text": "Orthonormal Basis: A basis of orthonormal vectors, which are vectors with a length of 1 and orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9481855576647258
      }
    ]
  },
  {
    "representative_text": "Orthogonal Basis: A set of orthogonal vectors that may or may not be normalized.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Orthogonal Basis: A set of orthogonal vectors that may or may not be normalized.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal basis: A set of orthogonal vectors that may not be orthonormal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9490218296125225
      },
      {
        "text": "Orthogonal Vectors: A set of vectors that are all orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8295960063127807
      },
      {
        "text": "Orthogonal Basis: A basis consisting of orthogonal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9205470629806043
      },
      {
        "text": "Orthogonal Basis: A basis for a vector space where the vectors are mutually orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9145986945588309
      }
    ]
  },
  {
    "representative_text": "Polar Decomposition Theorem: A matrix can be decomposed into the product of an orthogonal matrix and a diagonal matrix, where the orthogonal matrix represents the projection onto the range of the matrix, and the diagonal matrix represents the scaling factor.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 20,
    "detailed_sources": [
      {
        "text": "Polar Decomposition Theorem: A matrix can be decomposed into the product of an orthogonal matrix and a diagonal matrix, where the orthogonal matrix represents the projection onto the range of the matrix, and the diagonal matrix represents the scaling factor.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Polar decomposition: A factorization of a matrix A into the product A = U ||A|| V, where U is an orthogonal matrix, ||A|| is the norm of A, and V is a vector in the range of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8095547869503007
      },
      {
        "text": "Polar Decomposition: The polar decomposition of a matrix is a factorization of the matrix into a product of a positive semi-definite matrix and an orthogonal matrix. This is useful for finding eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8091341794894056
      },
      {
        "text": "Polar Decomposition: A factorization of a matrix into a product of an orthogonal matrix and a positive semi-definite matrix, which is used to decompose linear transformations into their component parts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9053907506255345
      },
      {
        "text": "The Polar Decomposition: The polar decomposition of a matrix is a factorization of the matrix into a product of an orthogonal matrix and a positive semi-definite matrix. This concept is essential in understanding the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.918616087644459
      },
      {
        "text": "The Polar Decomposition of a Matrix: This is a factorization of a matrix into a product of a positive semi-definite matrix and an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8858209579612025
      },
      {
        "text": "The Polar Decomposition: The polar decomposition of a matrix is a factorization of the matrix into a product of an orthogonal matrix and a positive semi-definite matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9617634902071606
      },
      {
        "text": "Polar Decomposition of a Matrix: The polar decomposition of a matrix is the product of an orthogonal matrix and a diagonal matrix, where the orthogonal matrix represents the projection onto the range of the matrix, and the diagonal matrix represents the scaling factor.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.905730066913029
      },
      {
        "text": "Polar Decomposition: Polar decomposition is a mathematical technique that can be used to decompose a matrix into a product of an orthogonal matrix and a positive semidefinite matrix. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8593637565816228
      },
      {
        "text": "The Polar Decomposition: This concept is related to the singular value decomposition and states that the polar decomposition of a matrix is a factorization of the matrix into a product of an orthogonal matrix and a positive semi-definite matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8881451951354
      },
      {
        "text": "Polar Decomposition Theorem: This theorem states that every square matrix A can be uniquely decomposed into the product of a positive semi-definite matrix P and an orthogonal matrix Q, i.e., A = P Q.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8512165092714767
      },
      {
        "text": "Properties of the Polar Decomposition for Matrices: The polar decomposition can be extended to matrices, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.82890370186363
      },
      {
        "text": "The Polar Decomposition of a Matrix: The factorization of a matrix into a product of a positive semi-definite matrix and an orthogonal matrix, which can be used to find eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9085539858845637
      },
      {
        "text": "The Polar Decomposition Theorem: The polar decomposition theorem states that a matrix can be decomposed into the product of an orthogonal matrix and a diagonal matrix. This decomposition is useful in various applications of orthogonal diagonalization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8529717428577
      },
      {
        "text": "Polar Decomposition: The polar decomposition of a matrix A is a factorization of the form A = UP, where U is an orthogonal matrix and P is a positive semi-definite matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9429990766789367
      },
      {
        "text": "Orthogonal Matrix Decomposition Theorems: Theorems that state the existence and uniqueness of orthogonal matrix decompositions, including the polar decomposition theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.825473353440149
      },
      {
        "text": "Polar Decomposition: Explore polar decomposition, which is a factorization of a matrix into the product of a positive semidefinite matrix and an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9075610956943636
      },
      {
        "text": "Polar Decomposition for Non-Square Matrices: Polar decomposition is a factorization of a matrix into the product of a positive semidefinite matrix and an orthogonal matrix. It is a useful technique for solving systems of linear equations for non-square matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8979527189989913
      },
      {
        "text": "Polar decomposition: A factorization of a matrix into a product of an orthogonal matrix and a positive semidefinite matrix, which can be used to decompose a matrix into its projection and scaling components.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9469098127480737
      },
      {
        "text": "Polar Decomposition of a Matrix with Applications: A factorization of a matrix into a product of an orthogonal matrix and a positive semidefinite matrix, which can be used to decompose a matrix into its projection and scaling components.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9227117790383523
      }
    ]
  },
  {
    "representative_text": "Orthogonality of Diagonal Matrices: The columns of a diagonal matrix are orthogonal to each other, and the rows of a diagonal matrix are orthogonal to each other.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonality of Diagonal Matrices: The columns of a diagonal matrix are orthogonal to each other, and the rows of a diagonal matrix are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Diagonalization of Hermitian Matrices: A Hermitian matrix can be orthogonally diagonalized using only real eigenvalues and real eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Diagonalization of Hermitian Matrices: A Hermitian matrix can be orthogonally diagonalized using only real eigenvalues and real eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Diagonalization of a Hermitian Matrix: A Hermitian matrix can be diagonalized by a unitary matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8517997836907361
      }
    ]
  },
  {
    "representative_text": "Diagonalization of Symmetric Matrices: A symmetric matrix can be orthogonally diagonalized using only real eigenvalues and real eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Diagonalization of Symmetric Matrices: A symmetric matrix can be orthogonally diagonalized using only real eigenvalues and real eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "QR Decomposition: A matrix can be decomposed into the product of an orthogonal matrix and an upper triangular matrix, where the orthogonal matrix represents the projection onto the range of the matrix, and the upper triangular matrix represents the scaling factor.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "QR Decomposition: A matrix can be decomposed into the product of an orthogonal matrix and an upper triangular matrix, where the orthogonal matrix represents the projection onto the range of the matrix, and the upper triangular matrix represents the scaling factor.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The definition and properties of the QR decomposition of a matrix, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8209067179164713
      }
    ]
  },
  {
    "representative_text": "Jacobi Method: A method for diagonalizing a symmetric matrix by iteratively applying a series of orthogonal transformations to the matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Jacobi Method: A method for diagonalizing a symmetric matrix by iteratively applying a series of orthogonal transformations to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Givens Rotation: A method for diagonalizing a symmetric matrix by applying a series of orthogonal rotations to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.85027350643119
      },
      {
        "text": "Orthogonal Diagonalization of Symmetric Matrices: A method for diagonalizing a symmetric matrix using an orthogonal matrix, which is used in linear algebra to solve systems of linear equations and to compute eigenvalues and eigenvectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8254699210630976
      }
    ]
  },
  {
    "representative_text": "Orthogonal decomposition: A decomposition of a vector or matrix into its orthogonal components, where each component is orthogonal to the others.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Orthogonal decomposition: A decomposition of a vector or matrix into its orthogonal components, where each component is orthogonal to the others.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Decomposition: A process for decomposing a vector or a subspace into a sum of orthogonal subspaces, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.837067369582347
      },
      {
        "text": "Orthogonal Decomposition: A vector space can be decomposed into a direct sum of subspaces, each of which is orthogonal to the others.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.838955026392675
      },
      {
        "text": "Orthogonal Decomposition: A method for decomposing a vector into a sum of orthogonal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9295980072629062
      },
      {
        "text": "Vector Space Decomposition: A method for decomposing a vector space into a direct sum of subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8626328328260817
      },
      {
        "text": "Orthogonal Decomposition and Projection: Orthogonal decomposition is a method for decomposing a vector into a sum of orthogonal vectors. Projection is a method for finding the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8612516043390963
      },
      {
        "text": "Schmidt's Decomposition: A method for expressing a vector as a sum of a projection onto a subspace and a component orthogonal to that subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8599166956858204
      },
      {
        "text": "Linear Independence and the Orthogonal Decomposition: The orthogonal decomposition of a vector space into a direct sum of subspaces is a way of representing the space as a linear combination of its orthogonal complements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8567034408314064
      },
      {
        "text": "Linear Transformation and Orthogonal Decomposition: A concept in linear algebra that provides a way to decompose a vector into a sum of orthogonal vectors, useful in understanding the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8340750830060923
      },
      {
        "text": "Orthogonal Decomposition: The decomposition of a vector space into a direct sum of a subspace and its orthogonal complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9320551567862858
      },
      {
        "text": "Orthogonal Decomposition: A decomposition of a vector into the sum of two orthogonal vectors, one of which is orthogonal to the other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.909128908862572
      },
      {
        "text": "Linear Transformation and Orthogonal Decomposition of the Image: The concept of orthogonal decomposition of the image of a linear transformation, which is a way to decompose the image into a direct sum of orthogonal subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8263336165294703
      },
      {
        "text": "Orthogonal Decomposition: The decomposition of a vector space into a direct sum of a subspace and its Implies the same as mentioned so far more",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8194652216632031
      }
    ]
  },
  {
    "representative_text": "Orthogonality of a vector and a subspace: If u is orthogonal to a subspace V, then u is orthogonal to every vector in V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Orthogonality of a vector and a subspace: If u is orthogonal to a subspace V, then u is orthogonal to every vector in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonality of a Vector to a Subspace: This is a fundamental concept in linear algebra, which states that if u is orthogonal to a subspace V, then u is orthogonal to every vector in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.820013586494858
      },
      {
        "text": "Theorem on the Orthogonal Projection: This theorem states that a subspace W of a vector space V is said to be orthogonal to a vector u if and only if the inner product of u and v is zero for all v in W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8428439923937512
      },
      {
        "text": "Projection Theorem for Orthogonal Complement: If u is a vector in an inner product space V, and V is a subspace of V, then the orthogonal complement of u with respect to V is the set of all vectors v in V such that u and v are orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8425055952826814
      }
    ]
  },
  {
    "representative_text": "Orthogonality of a vector to itself: If u is orthogonal to itself, then u = 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonality of a vector to itself: If u is orthogonal to itself, then u = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal eigendecomposition: A decomposition of a matrix A into its orthogonal eigenvectors and eigenvalues, i.e., A = U Λ U^T, where U is an orthogonal matrix, Λ is a diagonal matrix containing the eigenvalues, and U^T is the transpose of U.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal eigendecomposition: A decomposition of a matrix A into its orthogonal eigenvectors and eigenvalues, i.e., A = U Λ U^T, where U is an orthogonal matrix, Λ is a diagonal matrix containing the eigenvalues, and U^T is the transpose of U.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal diagonalization: A decomposition of a matrix A into its orthogonal eigenvectors and eigenvalues, i.e., A = U Λ U^T, where U is an orthogonal matrix, Λ is a diagonal matrix containing the eigenvalues, and U^T is the transpose of U.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9354861681473137
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt process: A method for orthonormalizing a set of orthogonal vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 27,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt process: A method for orthonormalizing a set of orthogonal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gram-Schmidt Process: A method for constructing an orthonormal basis of a subspace using an orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.894867934608169
      },
      {
        "text": "Gram-Schmidt Orthogonalization: A process for orthogonalizing a set of vectors, denoted as {v1, v2, ..., vn}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9074508401040418
      },
      {
        "text": "Gram-Schmidt Normalization: A process for normalizing a set of orthogonal vectors, denoted as {u1, u2, ..., un}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9019809144626405
      },
      {
        "text": "Orthogonalization: A process of transforming a set of linearly dependent vectors into a set of orthogonal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.851749430880757
      },
      {
        "text": "Gram-Schmidt Process: A method for orthonormalizing a set of linearly independent vectors, which is useful in constructing an orthonormal basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9370620715678133
      },
      {
        "text": "Gram-Schmidt Process: The Gram-Schmidt process is an algorithm for constructing an orthonormal basis from a given basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9163876452913088
      },
      {
        "text": "Orthogonalization: A process of transforming a set of vectors into an orthonormal set using an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8832728056005376
      },
      {
        "text": "Orthonormalization: This is a process of transforming a set of vectors to an orthonormal set, which is a set of vectors that are orthogonal to each other and have a norm of 1. Orthonormalization is closely related to the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.858833728808987
      },
      {
        "text": "The Gram-Schmidt Process for Orthogonalization: This process is used to orthonormalize a set of orthogonal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9423111552146665
      },
      {
        "text": "The Orthogonalization of a Matrix using the Gram-Schmidt Process: This is a method used to orthogonalize a matrix by iteratively applying the Gram-Schmidt process to its columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8369680199266221
      },
      {
        "text": "The QR Algorithm: This algorithm is used to orthogonalize a matrix and is closely related to the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8028675353129926
      },
      {
        "text": "Orthogonalization of Tensors: This is an extension of the Gram-Schmidt process to higher-dimensional spaces, such as tensors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8342956786023348
      },
      {
        "text": "Gram-Schmidt Process for High-Dimensional Spaces: This includes the development of algorithms and techniques for orthogonalizing vectors and matrices in high-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8182039929212663
      },
      {
        "text": "Basis Extension using the Gram-Schmidt Process: This process is a method for constructing a basis for a vector space from a given set of linearly independent vectors. It involves iteratively subtracting the projection of each new vector onto the previous vectors to ensure linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8255885723120724
      },
      {
        "text": "The Orthogonalization of a Matrix using the Gram-Schmidt Process: A method for orthogonalizing a matrix using the Gram-Schmidt process, which is a process for orthogonalizing a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9109858429634685
      },
      {
        "text": "Theorem on the Gram-Schmidt Process: This theorem states that the Gram-Schmidt process is a method for constructing an orthonormal basis from a set of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.849188280643556
      },
      {
        "text": "Gram-Schmidt Process and Orthogonalization: The Gram-Schmidt process is a method for constructing an orthogonal basis from a set of linearly independent vectors. Orthogonalization is the process of making a set of vectors orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9181004152281461
      },
      {
        "text": "Orthogonalization: A process of transforming a set of vectors into an orthonormal set, which is a set of vectors with a length of 1 and orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8592877963536193
      },
      {
        "text": "The Gram-Schmidt Process: The Gram-Schmidt process is a method for orthonormalizing a set of linearly independent vectors. This process is essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9022196945946751
      },
      {
        "text": "Gram-Schmidt Process for Orthogonalization of Matrices: The Gram-Schmidt process can be extended to orthogonalize matrices, not just vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8769770362575315
      },
      {
        "text": "Orthogonalization of a Matrix using the Gram-Schmidt Process: The Gram-Schmidt process can be used to orthogonalize a matrix, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9035104635643996
      },
      {
        "text": "Orthogonalization of a Matrix using the Gram-Schmidt Process: The Gram-Schmidt process is an algorithm for orthogonalizing a set of vectors. It is widely used in linear algebra and is a fundamental technique for orthogonalizing a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9354758549123605
      },
      {
        "text": "The Gram-Schmidt Process for Complex Vector Spaces: This is a method for orthonormalizing a set of linearly independent vectors in a complex vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8927512471312353
      },
      {
        "text": "The Orthogonalization of a Matrix using the Gram-Schmidt Process: This is a method for orthogonalizing a matrix using the Gram-Schmidt process, which is a method for orthonormalizing a set of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9272227542690766
      },
      {
        "text": "Orthogonalization and its Applications: A method for orthogonalizing a set of vectors, which is a process for finding an orthonormal basis for a vector space. Orthogonalization has numerous applications in linear algebra, including the study of eigenvectors and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8347035345983976
      },
      {
        "text": "Definition of Gram-Schmidt process: A method for orthonormalizing a set of vectors in a vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8761420227066685
      }
    ]
  },
  {
    "representative_text": "Signal processing: Orthogonal decomposition is used in signal processing to decompose signals into their orthogonal components.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Signal processing: Orthogonal decomposition is used in signal processing to decompose signals into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Image compression: Orthogonal decomposition is used in image compression to decompose images into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8336149750322132
      },
      {
        "text": "Data analysis: Orthogonal decomposition is used in data analysis to decompose data into its orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.867638751205013
      },
      {
        "text": "Machine learning: Orthogonal decomposition is used in machine learning to decompose data into its orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8686103768842783
      },
      {
        "text": "Orthogonal Decomposition: Orthogonal decomposition is a technique used to decompose a matrix into the sum of orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8571514564637353
      },
      {
        "text": "Orthogonal Matrix Decomposition Applications: Applications of orthogonal matrix decomposition in various fields, including data analysis, signal processing, and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8516832229015188
      }
    ]
  },
  {
    "representative_text": "Norm of a vector: ||u|| = √(u · u) if and only if u is a unit vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Norm of a vector: ||u|| = √(u · u) if and only if u is a unit vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product: A scalar value that represents the dot product of two vectors, often denoted by the symbol (•, •).",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Inner Product: A scalar value that represents the dot product of two vectors, often denoted by the symbol (•, •).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Definition of inner product: A function that takes two vectors as input and returns a scalar value that represents the amount of \"similarity\" between the two vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.82305142251883
      }
    ]
  },
  {
    "representative_text": "Orthogonality: A relationship between two vectors where their inner product is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Orthogonality: A relationship between two vectors where their inner product is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonality: Two vectors are orthogonal if their inner product is zero, meaning they have no component in the same direction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732876239349431
      },
      {
        "text": "Orthogonal Vectors: Vectors that are perpendicular to each other, meaning their dot product is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8706054481729272
      },
      {
        "text": "Definition of Orthogonality:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8484390400229767
      },
      {
        "text": "Orthogonality: A property of vectors where the dot product of two vectors is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9231154191437776
      },
      {
        "text": "The Concept of Orthogonality: This concept involves determining whether two vectors are orthogonal (perpendicular) to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8167483878110309
      },
      {
        "text": "Vector Orthogonality: A concept used to describe the relationship between two vectors, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8290475089596984
      },
      {
        "text": "Orthogonality and Orthonormality: Orthogonality refers to the property of vectors that their dot product is zero. Orthonormality refers to the property of vectors that they are orthogonal and have a length of 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8344829032381622
      },
      {
        "text": "Orthogonality of Functions: A property of functions that states that two functions are orthogonal if and only if their inner product is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8003547935833739
      }
    ]
  },
  {
    "representative_text": "Orthogonality Preservation: The projection matrix preserves the orthogonality between the projected vector and the orthogonal complement of the subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonality Preservation: The projection matrix preserves the orthogonality between the projected vector and the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonality of Projection Matrix and Orthogonal Complement: The projection matrix P satisfies the following equation:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8289464195941838
      }
    ]
  },
  {
    "representative_text": "Span: The range of the projection matrix is equal to the subspace onto which it projects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span: The range of the projection matrix is equal to the subspace onto which it projects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Householder Transformations: A method for constructing an orthogonal projection matrix using a Householder matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Householder Transformations: A method for constructing an orthogonal projection matrix using a Householder matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Projection using QR Decomposition: A method for constructing an orthogonal projection matrix using the QR decomposition of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8121235720484293
      },
      {
        "text": "Orthogonal Projection Method:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8425698412208986
      },
      {
        "text": "Orthogonal Matrix Method:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8749266713532575
      },
      {
        "text": "Orthogonal Projection:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8805036093778262
      },
      {
        "text": "Projections onto Subspaces",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8234550617815233
      },
      {
        "text": "Householder Transformations",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8330221027761471
      },
      {
        "text": "Orthogonal Decomposition",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8903697264275171
      },
      {
        "text": "The properties of orthogonal projections",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8965970544455026
      },
      {
        "text": "Orthogonal Projections:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9371745446373492
      },
      {
        "text": "Orthogonal Matrix Decomposition Variations: Variations of orthogonal matrix decomposition, including the use of orthogonal projections and orthogonal complements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8242706511915209
      },
      {
        "text": "Orthogonal Projection Matrices and the Orthogonal Decomposition of Tensor Fields: Decomposing tensor fields into their orthogonal components, with applications in physics and engineering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8291539245537318
      },
      {
        "text": "Orthogonal Projection Matrices and the Connection to Image Analysis: Using orthogonal projection matrices in image analysis, such as image filtering and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.82586294191954
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement Matrix: A square matrix that represents the orthogonal complement of a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement Matrix: A square matrix that represents the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Orthogonal Complement of a Matrix: The orthogonal complement of a matrix is the matrix that represents the orthogonal complement of the given matrix. It is a fundamental tool in linear algebra and is used in various applications of orthogonal diagonalization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8416981508290647
      }
    ]
  },
  {
    "representative_text": "Image Processing: Orthogonal projection matrices are used in image processing techniques such as image filtering and image compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Image Processing: Orthogonal projection matrices are used in image processing techniques such as image filtering and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Data Analysis: Orthogonal projection matrices are used in data analysis techniques such as principal component analysis (PCA) and singular value decomposition (SVD).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8325340218966402
      },
      {
        "text": "Machine Learning: Orthogonal projection matrices are used in machine learning techniques such as neural networks and support vector machines.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8745232305112662
      },
      {
        "text": "Orthogonal Projection and Machine Learning: This involves understanding how orthogonal projection is used in machine learning techniques such as neural networks and support vector machines.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8442806098376486
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Signal Processing and Image Analysis: This involves understanding how orthogonal projection matrices are used in signal processing and image analysis to filter signals, reconstruct images, and perform other image analysis tasks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8672651816623134
      },
      {
        "text": "The Role of Orthogonal Projections in Image Compression: Orthogonal projections are used in image compression algorithms such as JPEG to reduce the number of pixels required to represent an image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8406614734750295
      },
      {
        "text": "The use of orthogonal projection matrices in machine learning: This includes studying the use of orthogonal projection matrices in machine learning, such as dimensionality reduction and feature extraction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8712316006326938
      },
      {
        "text": "The use of orthogonal projection matrices in signal processing: This includes studying the use of orthogonal projection matrices in signal processing, such as filtering and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8873172703450241
      },
      {
        "text": "Block Orthogonal Projection Matrices: These are matrices that project vectors onto subspaces with non-standard orthonormal bases. They are used in various applications, including signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.806704501251272
      },
      {
        "text": "Orthogonal Projection Matrices and the Fourier Transform: The Fourier transform is a way to decompose a function into its frequency components. It has applications in orthogonal projection matrices, particularly when dealing with signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8294041415248672
      },
      {
        "text": "Block Orthogonal Projection Matrices: Matrices that project vectors onto subspaces with non-standard orthonormal bases, used in signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8752367660973508
      },
      {
        "text": "Orthogonal Projection Matrices and the Connection to Signal Processing: Using orthogonal projection matrices in signal processing, such as filtering and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.886245857350918
      }
    ]
  },
  {
    "representative_text": "Principal Component Analysis (PCA): A technique for reducing the dimensionality of a dataset using orthogonal projection matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Principal Component Analysis (PCA): A technique for reducing the dimensionality of a dataset using orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Principal Component Analysis (PCA): PCA is a method for finding the span of the columns of a matrix $A$ and is used in data analysis and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.836504791294804
      },
      {
        "text": "Principal Component Analysis (PCA): A dimensionality reduction technique that projects high-dimensional data onto a lower-dimensional space using orthonormal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9193528597181703
      },
      {
        "text": "Principal Component Analysis (PCA) for High-Dimensional Data: A technique for reducing the dimensionality of high-dimensional data using PCA, which involves selecting the top k eigenvectors corresponding to the largest eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.844678214815724
      },
      {
        "text": "Principal Component Analysis (PCA): A linear dimensionality reduction technique that transforms high-dimensional data into lower-dimensional data by identifying the principal components with the largest variance.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9359873618318009
      },
      {
        "text": "Principal Component Analysis (PCA) for High-Dimensional Data: This technique is used to reduce the dimensionality of high-dimensional data while preserving its underlying structure and is widely used in machine learning and data science.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9104484366841578
      },
      {
        "text": "Principal Component Analysis (PCA): a class of algorithms for reducing the dimensionality of high-dimensional data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9031745411058447
      },
      {
        "text": "Principal Component Analysis (PCA) with Linear Algebra: A class of algorithms for reducing the dimensionality of high-dimensional data using linear algebra and PCA.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8998769630789916
      },
      {
        "text": "Principal Component Analysis (PCA): PCA is a technique used in computer graphics and game development to reduce the dimensionality of data and extract the most important features.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8357594649730536
      }
    ]
  },
  {
    "representative_text": "Step 1: Initialize the Basis: Choose an initial set of linearly independent vectors, denoted as {v1, v2, ..., vn}.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Step 1: Initialize the Basis: Choose an initial set of linearly independent vectors, denoted as {v1, v2, ..., vn}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Step 2: Orthogonalize the First Vector: Set u1 = v1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Step 2: Orthogonalize the First Vector: Set u1 = v1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Step 3: Orthogonalize the Remaining Vectors: For each remaining vector vi, compute:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Step 3: Orthogonalize the Remaining Vectors: For each remaining vector vi, compute:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Step 4: Repeat Step 3: Repeat the process for each remaining vector vi until all vectors are orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9021604024603169
      },
      {
        "text": "Step 5: Normalize the Orthogonal Vectors: For each orthogonal vector ui, compute:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8845839496380812
      }
    ]
  },
  {
    "representative_text": "Orthogonality of the Gram-Schmidt Process: The vectors produced by the Gram-Schmidt process are orthogonal to each other.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Orthogonality of the Gram-Schmidt Process: The vectors produced by the Gram-Schmidt process are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthonormality of the Gram-Schmidt Process: If the initial set of vectors is orthonormal, the vectors produced by the Gram-Schmidt process are also orthonormal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.921842280477878
      },
      {
        "text": "Orthogonal Projection: The Gram-Schmidt process can be used to compute orthogonal projections, where the resulting vectors are the orthogonal projections of the original vectors onto the span of the orthogonal set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8077568300796547
      },
      {
        "text": "Gram-Schmidt Theorem: If {v1, v2, ..., vn} is a set of linearly independent vectors, then the Gram-Schmidt process produces a set of orthogonal vectors {u1, u2, ..., un} that span the same subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8320664775812119
      },
      {
        "text": "Orthonormality Theorem: If {v1, v2, ..., vn} is an orthonormal set of vectors, then the Gram-Schmidt process produces an orthonormal set of vectors {u1, u2, ..., un} that span the same subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.92412724567153
      },
      {
        "text": "Projection Theorem: If A is a projection matrix, then the Gram-Schmidt process produces a set of orthogonal vectors {u1, u2, ..., un} that span the same subspace as A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8773759266385188
      },
      {
        "text": "Orthogonality of the Gram-Schmidt Process: The Gram-Schmidt process preserves orthogonality, meaning that the resulting vectors are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9003286245182094
      },
      {
        "text": "Properties of Gram-Schmidt process:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8381784620856298
      }
    ]
  },
  {
    "representative_text": "Linear Independence: The Gram-Schmidt process preserves linear independence, meaning the resulting set of vectors is also linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Independence: The Gram-Schmidt process preserves linear independence, meaning the resulting set of vectors is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship Between Linear Independence and the Gram-Schmidt Process: The Gram-Schmidt process is closely related to linear independence. If the columns of a matrix are linearly independent, the Gram-Schmidt process produces an orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8548334400329365
      }
    ]
  },
  {
    "representative_text": "Span: The span of the vectors produced by the Gram-Schmidt process is the same as the span of the original set of vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span: The span of the vectors produced by the Gram-Schmidt process is the same as the span of the original set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Complexity: The Gram-Schmidt process has a computational complexity of O(n^3), where n is the number of vectors in the initial set.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Complexity: The Gram-Schmidt process has a computational complexity of O(n^3), where n is the number of vectors in the initial set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Stability: The Gram-Schmidt process is sensitive to numerical errors, which can lead to inaccurate results.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Numerical Stability: The Gram-Schmidt process is sensitive to numerical errors, which can lead to inaccurate results.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Stability of Gram-Schmidt Process: This topic explores the sensitivity of the Gram-Schmidt process to numerical errors, which can lead to inaccurate results.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.893361403887136
      },
      {
        "text": "Condition Number of Gram-Schmidt Process: This topic explores the condition number of the Gram-Schmidt process, which can be used to determine the stability of the algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8712896436210467
      },
      {
        "text": "Condition Number of the Gram-Schmidt Process: Analyze the condition number of the Gram-Schmidt process, which measures the sensitivity of the algorithm to small changes in the input vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8401204293107197
      },
      {
        "text": "Stability Analysis: Analyze the stability of the Gram-Schmidt process, including the effect of numerical errors and the behavior of the algorithm in the presence of ill-conditioned matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8639477728173126
      },
      {
        "text": "Numerical Stability: The Gram-Schmidt process is sensitive to numerical errors, which can lead to inaccurate results. Methods to improve numerical stability include using more robust algorithms or reducing the number of iterations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8688977298614584
      }
    ]
  },
  {
    "representative_text": "Computer Graphics: The Gram-Schmidt process is used to compute orthogonal projections, which are essential in computer graphics for tasks such as rendering and animation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Computer Graphics: The Gram-Schmidt process is used to compute orthogonal projections, which are essential in computer graphics for tasks such as rendering and animation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonalization Techniques: Orthogonalization techniques like Gram-Schmidt orthogonalization are used to orthogonalize vectors and matrices, which is essential in many computer graphics and game development applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8108195176858788
      }
    ]
  },
  {
    "representative_text": "Machine Learning: The Gram-Schmidt process is used in machine learning for tasks such as feature extraction and dimensionality reduction.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 19,
    "detailed_sources": [
      {
        "text": "Machine Learning: The Gram-Schmidt process is used in machine learning for tasks such as feature extraction and dimensionality reduction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Signal Processing: The Gram-Schmidt process is used in signal processing for tasks such as filtering and spectral analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8404999733667015
      },
      {
        "text": "Linear Algebra: The Gram-Schmidt process is used in linear algebra for tasks such as finding orthonormal bases and computing orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8699642124429483
      },
      {
        "text": "The Gram-Schmidt Process as a Tool for Data Analysis: This includes the application of the Gram-Schmidt process in data analysis, such as dimensionality reduction and feature extraction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8639444570892738
      },
      {
        "text": "The Gram-Schmidt Process in Machine Learning: This includes the use of the Gram-Schmidt process in machine learning, such as feature learning and dimensionality reduction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.897785235049528
      },
      {
        "text": "Gram-Schmidt Process for High-Dimensional Lattices: The Gram-Schmidt process can be used to orthogonalize vectors in high-dimensional lattices, which are discrete sets of points in a high-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8302017359403167
      },
      {
        "text": "Orthogonality and Orthogonalization: Orthogonalization techniques, such as the Gram-Schmidt process, are essential in signal processing and image analysis for tasks like filtering and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8345945506404583
      },
      {
        "text": "Gram-Schmidt Process for High-Dimensional Lattices: This topic examines the use of the Gram-Schmidt process to orthogonalize vectors in high-dimensional lattices, which can be used in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8473267938833782
      },
      {
        "text": "The Gram-Schmidt Process for High-Dimensional Data Analysis: The Gram-Schmidt process can be used to reduce the dimensionality of high-dimensional data, such as in the analysis of gene expression data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9224663649966628
      },
      {
        "text": "The Role of the Gram-Schmidt Process in Machine Learning: The Gram-Schmidt process is used in machine learning to reduce the dimensionality of high-dimensional data, and to compute orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9193070987141978
      },
      {
        "text": "The Gram-Schmidt Process for Non-Linear Data Analysis: The Gram-Schmidt process can be used to analyze non-linear data, such as in the analysis of non-linear relationships between variables.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8959313013757655
      },
      {
        "text": "The Gram-Schmidt Process for Time-Series Data Analysis: The Gram-Schmidt process can be used to analyze time-series data, such as in the analysis of stock prices or weather patterns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8978721401706222
      },
      {
        "text": "The Role of the Gram-Schmidt Process in Signal Processing: The Gram-Schmidt process is used in signal processing to reduce the dimensionality of high-dimensional signals, and to compute orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9452859973316589
      },
      {
        "text": "The Role of the Gram-Schmidt Process in Data Visualization: The Gram-Schmidt process can be used to reduce the dimensionality of high-dimensional data, and to compute orthogonal projections, which can be used in data visualization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9273858590733579
      },
      {
        "text": "Gram-Schmidt Process for Partially Separable Data: Explore the application of the Gram-Schmidt process to partially separable data, where the data can be separated into a set of linearly independent subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.815397220916914
      },
      {
        "text": "Gram-Schmidt Process for High-Dimensional Data Analysis: Investigate the application of the Gram-Schmidt process to high-dimensional data analysis, where the data can be represented by high-dimensional vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8883681109325348
      },
      {
        "text": "Orthogonal Decomposition using the Schmidt Process: This involves decomposing a vector space into an orthogonal basis using the Schmidt process. It has applications in signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8615401395270259
      },
      {
        "text": "Gram-Schmidt Process for High-Dimensional Data Analysis: Use the Gram-Schmidt process to analyze high-dimensional data, which can be challenging to analyze using traditional techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9375427454397851
      },
      {
        "text": "Orthogonalization of Matrices: The Gram-Schmidt process can be used to orthogonalize matrices, which is essential in many applications, such as linear algebra, computer graphics, and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8948799483364465
      }
    ]
  },
  {
    "representative_text": "Square Matrix: A matrix with the same number of rows and columns, i.e., a square matrix of size n x n.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Square Matrix: A matrix with the same number of rows and columns, i.e., a square matrix of size n x n.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Square Matrix: A matrix with the same number of rows and columns, denoted by a capital letter (e.g., A, B, C).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8874467457042291
      }
    ]
  },
  {
    "representative_text": "Skew-Symmetric Matrix: A square matrix where the element at position (i, j) is equal to the negative of the element at position (j, i).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Skew-Symmetric Matrix: A square matrix where the element at position (i, j) is equal to the negative of the element at position (j, i).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hermitian Matrix: A square matrix that is equal to its conjugate transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hermitian Matrix: A square matrix that is equal to its conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a 2x2 Matrix: The determinant of a 2x2 matrix A = [a, b; c, d] is calculated as det(A) = ad - bc.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Determinant of a 2x2 Matrix: The determinant of a 2x2 matrix A = [a, b; c, d] is calculated as det(A) = ad - bc.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "det(A) = a{11}a{22} - a{12}a{21} for a 2x2 matrix A = [a{11}, a{12}; a{21}, a{22}].",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8869200451343375
      },
      {
        "text": "The determinant of a 2x2 matrix is calculated as ad - bc.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9214176080735867
      }
    ]
  },
  {
    "representative_text": "Determinant of a 3x3 Matrix: The determinant of a 3x3 matrix A = [[a, b, c]; [d, e, f]; [g, h, i]] is calculated using the formula det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Determinant of a 3x3 Matrix: The determinant of a 3x3 matrix A = [[a, b, c]; [d, e, f]; [g, h, i]] is calculated using the formula det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a 3x3 Matrix: The determinant of a 3x3 matrix [a, b, c; d, e, f; g, h, i] is calculated using the formula:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9180448308306269
      },
      {
        "text": "det(A) = a{11}(a{22}a{33} - a{23}a{32}) - a{12}(a{21}a{33} - a{23}a{31}) + a{13}(a{21}a{32} - a{22}a{31}) for a 3x3 matrix A = [a{11}, a{12}, a{13}; a{21}, a{22}, a{23}; a{31}, a{32}, a{33}].",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8808843507073624
      }
    ]
  },
  {
    "representative_text": "Determinant of a Sum of Matrices: The determinant of a sum of matrices is equal to the sum of their determinants, i.e., det(A + B) = det(A) + det(B).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Determinant of a Sum of Matrices: The determinant of a sum of matrices is equal to the sum of their determinants, i.e., det(A + B) = det(A) + det(B).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Sum: The determinant of the sum of two square matrices A and B is equal to the sum of their determinants:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9348070112322467
      }
    ]
  },
  {
    "representative_text": "Orthogonality and Orthogonal Matrices: Determinants can be used to test for orthogonality and find orthogonal matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonality and Orthogonal Matrices: Determinants can be used to test for orthogonality and find orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Calculus and Optimization: Determinants can be used to solve optimization problems and find the maximum or minimum of a function.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Calculus and Optimization: Determinants can be used to solve optimization problems and find the maximum or minimum of a function.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "e: The matrix of ones, which is a matrix with all elements equal to 1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "e: The matrix of ones, which is a matrix with all elements equal to 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "E_n: The matrix of ones of size n x n.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9292024293601977
      }
    ]
  },
  {
    "representative_text": "det(A)^-1: The inverse of the determinant of a matrix A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "det(A)^-1: The inverse of the determinant of a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of Inverse: The determinant of the inverse of a square matrix A is the reciprocal of the determinant of A:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8437449079034094
      },
      {
        "text": "Multiplicative Inverse: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8897236786236192
      },
      {
        "text": "The relationship between determinants and the determinant of a matrix's inverse: The determinant of a matrix's inverse is equal to the reciprocal of the determinant of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9220635930435093
      }
    ]
  },
  {
    "representative_text": "Non-Square Matrix: A matrix with a different number of rows and columns.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Square Matrix: A matrix with a different number of rows and columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a nxn Matrix: The determinant of an nxn matrix can be calculated using various methods, including expansion by minors and LU decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Determinant of a nxn Matrix: The determinant of an nxn matrix can be calculated using various methods, including expansion by minors and LU decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "LU Decomposition: The determinant of an nxn matrix can be calculated using LU decomposition, which factors the matrix into a product of a lower triangular matrix (L) and an upper triangular matrix (U).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8627098997973031
      }
    ]
  },
  {
    "representative_text": "Inverse of a Matrix: The inverse of a square matrix A can be calculated using the formula:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Inverse of a Matrix: The inverse of a square matrix A can be calculated using the formula:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Inversion Formula: A^(-1) = (1/det(A)) * adj(A).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8445957294613071
      },
      {
        "text": "If A = [a{11}, a{12}; a{21}, a{22}] is a 2x2 matrix, then A^(-1) = (1/det(A)) * [a{22}, -a{12}; -a{21}, a{11}].",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8068782753798069
      },
      {
        "text": "If A = [a{11}, a{12}, a{13}; a{21}, a{22}, a{23}; a{31}, a{32}, a{33}] is a 3x3 matrix, then A^(-1) = (1/det(A)) * [a{22}a{33} - a{23}a{32}, -a{12}a{33} + a{13}a{32}, a{12}a{23} - a{13}a{22}; a{21}a{33} - a{23}a{31}, -a{11}a{33} + a{13}a{31}, a{11}a{23} - a{13}a{21}; a{21}a{32} - a{22}a{31}, -a{11}a{32} + a{12}a{31}, a{11}a{22} - a{12}a_{21}].",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8400213745662611
      }
    ]
  },
  {
    "representative_text": "Determinant of a Quotient: The determinant of the quotient of two square matrices A and B is equal to the quotient of their determinants:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Quotient: The determinant of the quotient of two square matrices A and B is equal to the quotient of their determinants:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix: The determinant of a square matrix A can be used to determine the existence and uniqueness of its inverse:",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix: The determinant of a square matrix A can be used to determine the existence and uniqueness of its inverse:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix: The determinant of a matrix can be used to determine the linear independence of its columns or rows, as well as to find the inverse of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8565146257125812
      },
      {
        "text": "Determinant of a matrix: The determinant of a matrix can be used to determine the invertibility of the matrix and the existence of solutions to systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9366163036102917
      },
      {
        "text": "Determinants are used to calculate the inverse of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8614974289891999
      },
      {
        "text": "The determinant of a matrix is a scalar value that can be used to calculate the inverse of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9023249412993903
      },
      {
        "text": "The inverse of a matrix can be calculated using the determinant and the adjugate matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8626795667264897
      }
    ]
  },
  {
    "representative_text": "Uniqueness: If a matrix A has an inverse, then the inverse is unique.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Uniqueness: If a matrix A has an inverse, then the inverse is unique.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Inverse of an Invertible Matrix: The inverse of an invertible matrix is unique.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.916584078981279
      },
      {
        "text": "The Inverse of a Matrix is Unique if and only if the Matrix has a Non-Zero Determinant: The inverse of a matrix is unique if and only if the matrix has a non-zero determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9303439444886292
      }
    ]
  },
  {
    "representative_text": "Associativity: The product of three matrices A, B, and C satisfies (A ∘ B) ∘ C = A ∘ (B ∘ C).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Associativity: The product of three matrices A, B, and C satisfies (A ∘ B) ∘ C = A ∘ (B ∘ C).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Distributivity: The product of a matrix A and a sum of matrices B and C satisfies A ∘ (B + C) = A ∘ B + A ∘ C.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Distributivity: The product of a matrix A and a sum of matrices B and C satisfies A ∘ (B + C) = A ∘ B + A ∘ C.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gauss-Jordan Method: An algorithm for finding the inverse of a matrix by transforming the matrix into reduced row echelon form using elementary row operations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Gauss-Jordan Method: An algorithm for finding the inverse of a matrix by transforming the matrix into reduced row echelon form using elementary row operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gaussian Elimination: An algorithm for calculating the determinant of a matrix using a series of row operations, which can be used to determine the existence and uniqueness of the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.857533795443715
      },
      {
        "text": "Gaussian Elimination and its Applications: The method of Gaussian elimination and its applications in solving systems of linear equations and finding the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8296712526616401
      },
      {
        "text": "Computing the Inverse of a Matrix using Gauss-Jordan Elimination: Gauss-Jordan elimination is a method for solving systems of linear equations by transforming the augmented matrix into reduced row echelon form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9099133721175149
      },
      {
        "text": "Matrix Inversion using Gaussian Elimination: A method for computing the inverse of a matrix using Gaussian elimination.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8965320504526133
      },
      {
        "text": "Inverse of a Matrix using Gauss-Jordan Elimination: A method for finding the inverse of a matrix by transforming the matrix into the identity matrix using Gauss-Jordan elimination.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9431737980327628
      },
      {
        "text": "**Matrix Inversion using Gaussian Elimination: A detailed analysis of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.887529362505479
      }
    ]
  },
  {
    "representative_text": "Finding Eigenvalues and Eigenvectors: Inverse matrices can be used to find eigenvalues and eigenvectors of a matrix by solving the equation AX = λX, where X is the eigenvector and λ is the eigenvalue.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Finding Eigenvalues and Eigenvectors: Inverse matrices can be used to find eigenvalues and eigenvectors of a matrix by solving the equation AX = λX, where X is the eigenvector and λ is the eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Inverse of a Matrix and Eigenvalues: The inverse of a matrix can be used to compute the eigenvalues of a matrix. This can be used to solve systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8584908082196938
      },
      {
        "text": "The Relationship between Eigenvalues and the Matrix Inverse: The eigenvalues of a matrix are related to the inverse of the matrix. This can be used to analyze the behavior of linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8456170281970925
      },
      {
        "text": "The Relationship between Eigenvalues and the Matrix Inverse: The eigenvalues of a matrix are related to the inverse of the matrix, which is a fundamental property in many applications, including control theory and stability analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8831207569498081
      },
      {
        "text": "The Computation of Eigenvalues using the Inverse of a Matrix: The inverse of a matrix can be used to compute eigenvalues of a matrix. This is not explicitly mentioned in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9022737266964369
      },
      {
        "text": "The Relationship between Eigenvalues and the Inverse of a Matrix: The eigenvalues of a matrix are related to the inverse of the matrix. This relationship is crucial in is crucial in a crucial in many be used in the inverse is crucial in matrix is crucial in matrix. This can be used in the theorem: Theorem: Theorem: Theorem: Eigenvector Decomposition of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8542934878368718
      }
    ]
  },
  {
    "representative_text": "Representing Linear Transformations: Inverse matrices can be used to represent linear transformations by multiplying the transformation matrix by the inverse matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Representing Linear Transformations: Inverse matrices can be used to represent linear transformations by multiplying the transformation matrix by the inverse matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Inversion of Matrices: In computer graphics, the inverse of a matrix can be used to calculate the projection of a point onto a plane or the reflection of a point across a plane.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8189204912540479
      }
    ]
  },
  {
    "representative_text": "Solving Linear Systems with Non-Integer Coefficients: Inverse matrices can be used to solve systems of linear equations with non-integer coefficients by transforming the system into a system with integer coefficients.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Solving Linear Systems with Non-Integer Coefficients: Inverse matrices can be used to solve systems of linear equations with non-integer coefficients by transforming the system into a system with integer coefficients.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Formula: A formula for finding the eigenvalues of a matrix by solving the equation AX = λX.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvalue Formula: A formula for finding the eigenvalues of a matrix by solving the equation AX = λX.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalues Formula: The formula for computing the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8150749366936502
      }
    ]
  },
  {
    "representative_text": "Matrix Inversion Algorithms: Various algorithms for computing the inverse of a matrix, including the Gauss-Jordan method, LU decomposition, and the adjugate method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Matrix Inversion Algorithms: Various algorithms for computing the inverse of a matrix, including the Gauss-Jordan method, LU decomposition, and the adjugate method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Inverse Matrices: This is a fundamental problem in linear algebra, and various algorithms such as the Gauss-Jordan elimination, the LU decomposition, and the Cholesky decomposition can be used to compute inverse matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8633057309567866
      },
      {
        "text": "Computational Methods for Inverse Matrices: There are several computational methods available for finding the inverse of a matrix, including Gauss-Jordan elimination, LU decomposition, and Cholesky decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8844729654693356
      }
    ]
  },
  {
    "representative_text": "Numerical Stability: The study of the numerical stability of matrix inversion algorithms, including the effects of rounding errors and precision limitations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 17,
    "detailed_sources": [
      {
        "text": "Numerical Stability: The study of the numerical stability of matrix inversion algorithms, including the effects of rounding errors and precision limitations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Stability: A discussion of the numerical stability of matrix operations, such as matrix inversion and eigenvalue decomposition, and how to minimize the effects of rounding errors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8528602557805138
      },
      {
        "text": "Error Analysis of Matrix Inversion Algorithms: This topic deals with the analysis of the errors that can occur when using matrix inversion algorithms, such as the Gauss-Jordan method and LU decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8162969333932077
      },
      {
        "text": "Sensitivity Analysis of Matrix Inversion Algorithms: This topic deals with the analysis of the sensitivity of matrix inversion algorithms to small changes in the input.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8655483673073484
      },
      {
        "text": "Ill-Conditioned Matrices: A discussion of matrices that are ill-conditioned, which can lead to numerical instability in linear algebra algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8243250265852076
      },
      {
        "text": "Numerical Stability of Matrix Inversion Algorithms: The numerical stability of matrix inversion algorithms can be analyzed using various techniques such as the conditioning number and the sensitivity analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9055743938194083
      },
      {
        "text": "Error Analysis of Matrix Inversion Algorithms: The error analysis of matrix inversion algorithms can be done using various techniques such as the error bounds and the sensitivity analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8758501647609553
      },
      {
        "text": "Error Analysis of Matrix Inversion Algorithms: In addition to the error bounds and sensitivity analysis, other error analysis techniques such as the Krylov subspace method and the residual-based method can be used to analyze the errors in matrix inversion algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8602621287606806
      },
      {
        "text": "Error Analysis of Matrix Inversion Algorithms: Error analysis techniques such as the error bounds, sensitivity analysis, and Krylov subspace method can be used to analyze the errors in matrix inversion algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8747692043461963
      },
      {
        "text": "Numerical Stability of Matrix Operations: Understanding the numerical stability of matrix operations, such as matrix multiplication and inversion, is essential in ensuring the accuracy of numerical computations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8247424833072172
      },
      {
        "text": "Numerical Stability of Matrix Inversions: Discuss the numerical stability of matrix inversions, including the effects of round-off errors and the impact of different numerical methods on stability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8749549232281051
      },
      {
        "text": "Approximation and Error Analysis of Matrix Inversion Methods: Discuss the approximation and error analysis of matrix inversion methods, including the analysis of errors and the development of error bounds.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8167316920994392
      },
      {
        "text": "Sensitivity Analysis of Matrix Inversion Algorithms: Sensitivity analysis of matrix inversion algorithms is a technique used to study the behavior of a matrix inversion algorithm when small changes are made to the input.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8330760856453197
      },
      {
        "text": "Numerical Stability of Matrix Inversion Methods: This is an important topic in numerical linear algebra, as many matrix inversion methods can be sensitive to round-off errors. Techniques such as regularization, preconditioning, and iterative methods can be used to improve the numerical stability of matrix inversion methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8841016218457453
      },
      {
        "text": "Approximation and Error Analysis of Matrix Inversion Methods: This is an important topic in numerical linear algebra, as many matrix inversion methods can have errors and approximations. Understanding the approximation and error analysis of a matrix inversion method is crucial in determining its accuracy and reliability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8865237756450026
      },
      {
        "text": "Sensitivity Analysis of Matrix Inversion Algorithms in High-Dimensional Spaces: Sensitivity analysis of matrix inversion algorithms in high-dimensional spaces can be done using techniques such as the conditioning number and the sensitivity analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8667557571305349
      },
      {
        "text": "Sensitivity Analysis of Matrix Inversion Algorithms: Sensitivity analysis of matrix inversion algorithms can be done using techniques such as the conditioning number and the sensitivity analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9054348591943836
      }
    ]
  },
  {
    "representative_text": "Condition Number: A measure of the condition number of a matrix, which can be used to estimate the sensitivity of the matrix inversion algorithm to small changes in the input.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 15,
    "detailed_sources": [
      {
        "text": "Condition Number: A measure of the condition number of a matrix, which can be used to estimate the sensitivity of the matrix inversion algorithm to small changes in the input.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Condition Number: A measure of how sensitive a matrix is to small changes in its input. It is related to the determinant of the matrix and can be used to determine the stability of numerical methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572900795192118
      },
      {
        "text": "Condition Number of a Matrix: This is a measure of the sensitivity of a matrix inversion algorithm to small changes in the input. A matrix with a small condition number is more numerically stable than a matrix with a large condition number.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9265155107668492
      },
      {
        "text": "Condition Number: A measure of how sensitive a matrix is to changes in its input, which is closely related to numerical stability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9234142302419612
      },
      {
        "text": "Condition Number of a Matrix: A measure of the sensitivity of a matrix inversion algorithm to small changes in the input, which is related to the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9063520587700797
      },
      {
        "text": "Condition Number of a Matrix and Its Applications: The condition number of a matrix is a measure of the sensitivity of a matrix inversion algorithm to small changes in the input. It has applications in numerical linear algebra and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8883652247704736
      },
      {
        "text": "Condition Number: A measure of how sensitive a linear transformation is to small changes in the input data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8861454053979815
      },
      {
        "text": "Condition Number (continued): The condition number of a linear transformation is a measure of how sensitive the transformation is to small changes in the input data, and it is equal to the ratio of the largest to smallest singular value of the transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.815558136677065
      },
      {
        "text": "Condition Number of a Matrix: The condition number of a matrix is a measure of how sensitive the matrix is to small changes in the input. A matrix with a large condition number is more sensitive to small changes in the input.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9176044535952936
      },
      {
        "text": "Condition Number and Singular Values: The condition number of a linear transformation is a measure of how sensitive the transformation is to small changes in the input data. The condition number is equal to the ratio of the largest to smallest singular value of the transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8345661748874791
      },
      {
        "text": "Condition Number: This is a scalar value that describes the \"conditioning\" of a matrix. The condition number is the ratio of the largest to smallest eigenvalue of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.831365153377054
      },
      {
        "text": "Condition Number of a Matrix: The condition number of a matrix A is a measure of how sensitive the solution of a linear system Ax = b is to changes in the input matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9399179903060713
      },
      {
        "text": "Condition Number of Matrix Inversions for Non-Invertible Rank: The condition number of a matrix is a scalar value that describes the \"conditioning\" of a matrix. It is an important concept in numerical linear algebra, and can be used to estimate the sensitivity of the solution to small changes in the input matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8847875069059703
      },
      {
        "text": "Condition Number of a Matrix: The condition number of a matrix A is a measure of how sensitive the solution of a linear system Ax = b is to changes to changes is to changes is to changes in theorectal matrix A is a matrix. It is not only applies to compute theore the SVD of a matrix. The condition number of a matrix can be used in SVD: This is not only exists, The condition number of a matrix, but does not only in 'Additional Points of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.824175414804178
      },
      {
        "text": "Condition Number: The condition number of a matrix is a measure of its sensitivity to small changes in the input data.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9570644152010721
      }
    ]
  },
  {
    "representative_text": "Determinant of a Zero Matrix: The determinant of a zero matrix is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Determinant of a Zero Matrix: The determinant of a zero matrix is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Zero Eigenvalues: If a matrix A has zero eigenvalues, then det(A) = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8788039074479392
      },
      {
        "text": "The Determinant of a Matrix with a Zero Eigenvalue: The determinant of a matrix with a zero eigenvalue is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9533315713775735
      },
      {
        "text": "Determinant of a Matrix with a Nilpotent Matrix: The determinant of a matrix with a nilpotent matrix is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8672908752760229
      },
      {
        "text": "Determinant of a Matrix with a Degenerate Eigenvalue: The determinant of a matrix with a degenerate eigenvalue is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9104562734407624
      }
    ]
  },
  {
    "representative_text": "Determinant of an Identity Matrix: The determinant of an identity matrix is one.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of an Identity Matrix: The determinant of an identity matrix is one.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvectors Formula: The formula for computing the eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvectors Formula: The formula for computing the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Diagonalization Formula: The formula for diagonalizing a square matrix using eigenvectors and eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Diagonalization Formula: The formula for diagonalizing a square matrix using eigenvectors and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvectors and eigenvalues are used to diagonalize a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.837877788975717
      }
    ]
  },
  {
    "representative_text": "It is a measure of the volume scaling factor of the linear transformation represented by the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "It is a measure of the volume scaling factor of the linear transformation represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of the determinant of a linear transformation being a scalar value that can be used to compute the volume scaling factor of the linear transformation",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8294037480333027
      }
    ]
  },
  {
    "representative_text": "Non-Singular Matrix: A matrix A is said to be non-singular if det(A) ≠ 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Non-Singular Matrix: A matrix A is said to be non-singular if det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Matrix: A matrix A is said to be singular if det(A) = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8471322567931492
      }
    ]
  },
  {
    "representative_text": "Inverse of a Sum: (A + B)^(-1) = A^(-1) + B^(-1) - A^(-1)B^(-1)A^(-1)B.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inverse of a Sum: (A + B)^(-1) = A^(-1) + B^(-1) - A^(-1)B^(-1)A^(-1)B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The scalar λ is called the eigenvalue of v.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The scalar λ is called the eigenvalue of v.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Correspondence: Each eigenvalue λ corresponds to a unique eigenvector v.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Correspondence: Each eigenvalue λ corresponds to a unique eigenvector v.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Uniqueness: If λ and v are eigenvalues and eigenvectors of A, then λ = μ and v = cv for any scalar μ and vector c.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Uniqueness: If λ and v are eigenvalues and eigenvectors of A, then λ = μ and v = cv for any scalar μ and vector c.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Jacobi Method: The Jacobi method is an iterative technique for computing the eigenvalues and eigenvectors of a symmetric matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Jacobi Method: The Jacobi method is an iterative technique for computing the eigenvalues and eigenvectors of a symmetric matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using the Jacobi Method: The Jacobi method is an iterative technique for computing the eigenvalues and eigenvectors of a symmetric matrix. It works by iteratively transforming the matrix into a diagonal matrix using a series of Givens rotations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9022785105777643
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the Jacobi Method: The Jacobi method is a method for computing the eigenvalues of a matrix by iteratively applying a series of Householder transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9494493622420239
      },
      {
        "text": "Computing the Eigenvectors of a Matrix using the Jacobi Method: The Jacobi method can also be used to compute the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8807212078122192
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the Jacobi Method with Shift: This involves the study of the Jacobi method with shift, which is an extension of the Jacobi method for computing the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8396705796071151
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Jacobi-Davidson Algorithm: The Jacobi-Davidson algorithm is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively applying a Jacobi rotation to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8908633515190993
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Jacobi-Davidson Algorithm with Shifts: The Jacobi-Davidson algorithm with shifts is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively applying a Jacobi rotation and a shift to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8954723666697686
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using the Jacobi-Davidson Algorithm: The Jacobi-Davidson algorithm is an iterative method for computing eigenvalues and eigenvectors, which is an extension of the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8971229300393094
      }
    ]
  },
  {
    "representative_text": "Spectral Theorem:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Spectral Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Hilbert-Schmidt Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8509498784086058
      },
      {
        "text": "The spectral theorem for linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.923222903128862
      },
      {
        "text": "Spectral Theorem for Linear Operators on Infinite-Dimensional Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9254815382681635
      },
      {
        "text": "Linear Operator's spectral mapping of linear operator:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8106821655912694
      }
    ]
  },
  {
    "representative_text": "Eigendecomposition and Determinants:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Eigendecomposition and Determinants:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The eigendecomposition of a matrix A can be used to compute its determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8848876518605605
      },
      {
        "text": "The determinant of A can be computed using its eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9247287702709551
      },
      {
        "text": "Computing Determinants using Eigendecomposition: The determinant of a matrix A can be computed using its eigendecomposition as det(A) = ∏[λi], where λi are the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9087904694399229
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination Theorem: If a matrix has a non-zero determinant, then the matrix has a non-trivial solution to the homogeneous system of equations represented by the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination Theorem: If a matrix has a non-zero determinant, then the matrix has a non-trivial solution to the homogeneous system of equations represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Row Echelon Form Test: If the matrix formed by the vectors has a non-zero row, then the vectors are linearly dependent. If the matrix has a zero row, then the vectors are linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Row Echelon Form Test: If the matrix formed by the vectors has a non-zero row, then the vectors are linearly dependent. If the matrix has a zero row, then the vectors are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Reduced Row Echelon Form Test: If the matrix formed by the vectors has a non-zero entry in the last row, then the vectors are linearly dependent. If the matrix has a zero entry in the last row, then the vectors are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9540657280371805
      },
      {
        "text": "Row Reduction Test: A set of vectors is linearly independent if and only if the equation $a1v1 + a2v2 + \\cdots + anvn = 0$ has only the trivial solution, which can be determined by row reducing the matrix formed by the vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732036740031568
      },
      {
        "text": "Gaussian Elimination Test: A set of vectors is linearly independent if and only if the equation $a1v1 + a2v2 + \\cdots + anvn = 0$ has only the trivial solution, which can be determined by using Gaussian elimination to reduce the matrix formed by the vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.90169980821206
      },
      {
        "text": "Gaussian Elimination Method: This method involves using row reduction to determine if a set of vectors is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8304021802926289
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Span: If a set of vectors is linearly independent, then the set spans the vector space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 31,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Span: If a set of vectors is linearly independent, then the set spans the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence Implies Span: If a set of vectors $\\{v1, v2, ..., v_n\\}$ is linearly independent, then its span is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9565295426322498
      },
      {
        "text": "Span Implies Linear Independence: If the span of a set of vectors $\\{v1, v2, ..., v_n\\}$ is the entire vector space, then the vectors are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9552867482802183
      },
      {
        "text": "Span Property: If a set of vectors is linearly independent, then the span of that set is equal to the set of all linear combinations of those vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9097051649752903
      },
      {
        "text": "Linearly independent set: A set of vectors that is linearly independent spans the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8767431596003572
      },
      {
        "text": "Span of a Linearly Independent Set: If a set of vectors is linearly independent, then its span is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9660649803488244
      },
      {
        "text": "Span Theorem: The span of a set of linearly independent vectors is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9354416860745376
      },
      {
        "text": "Linear Independence Theorem: A set of vectors is linearly independent if and only if its span is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9174093104746313
      },
      {
        "text": "Span of Linearly Independent Vectors: If a set of vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n\\}$ is linearly independent, then its span is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.959911597804549
      },
      {
        "text": "Linear Independence Theorem: Suppose that $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n\\}$ is a set of vectors. Then, the span of the set is the entire vector space if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9508923132218593
      },
      {
        "text": "Linear Independence Theorem: If a set of vectors is linearly independent, then it spans a subspace of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9194919183957297
      },
      {
        "text": "Span Theorem: If a set of vectors spans a subspace of the vector space, then it is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9070236738465236
      },
      {
        "text": "Spanning Implies Linear Independence: If a set of vectors spans the space, then it is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9237737760715039
      },
      {
        "text": "Linear Independence and Span: A set of vectors is linearly independent if and only if it spans a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.949924258069152
      },
      {
        "text": "Linear Independence Implies Spanning: A set of linearly independent vectors spans the entire vector space if and only if the vectors are linearly independent and the vector space is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9506944901961061
      },
      {
        "text": "Spanning Set Implies Linear Independence: A set of vectors that spans the entire vector space is linearly independent if and only if the set is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9426322542095024
      },
      {
        "text": "Definition: A set of vectors spans a vector space if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9013986698618774
      },
      {
        "text": "Span and Linear Independence are Equivalent: A set of vectors is linearly independent if and only if the span of that set is equal to the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9402090985278826
      },
      {
        "text": "Linear Independence of Spanning Sets: A set of vectors is said to be linearly independent if it spans a vector space and is a spanning set for that space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9116425403260999
      },
      {
        "text": "Linear Independence of a Span: A set of vectors $\\{v1, v2, \\ldots, vn\\}$ is linearly independent if and only if the span of this set is a subspace with dimension equal to the number of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9145975203082544
      },
      {
        "text": "The Linear Independence of the Span of a Set of Vectors: The span of a set of vectors is linearly independent if and only if the set of vectors is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9284766482858814
      },
      {
        "text": "Linear Independence Implies Span Theorem: The Span Theorem states that the span of a set of vectors is equal to the smallest subspace that contains the set. However, it is also true that if a set of vectors is linearly independent, then the span of the set is equal to the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8815175289920881
      },
      {
        "text": "Linear Independence Implies Span Theorem for Vector Spaces with a Non-Empty Span: We need to consider the implications of having a non-empty span on linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8542005998133476
      },
      {
        "text": "Linear Independence Implies Dimension: A set of vectors is linearly independent if and only if the dimension of the span of the set is equal to the number of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9123591195091687
      },
      {
        "text": "Spanning Property Implies Linear Independence: A set of vectors spans the vector space if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9552645711814592
      },
      {
        "text": "Linear Independence Implies Spanning Property (with dependent vectors): A set of vectors is linearly independent if and only if the set spans the vector space with dependent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9141013088431571
      },
      {
        "text": "Linear Independence Implies Spanning Property (with Dependent Vectors and Non-Standard Bases): A set of vectors is linearly independent if and only if the set spans the vector space with dependent vectors and a non-standard basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9069986356159354
      },
      {
        "text": "Span of a Set of Vectors with Linearly Independent Vectors: The span of a set of vectors with linearly independent vectors is a subspace, and its dimension is equal to the number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9021907099975062
      },
      {
        "text": "Spanning Property Implies Linear Independence (with non-standard bases): A set of vectors spans the vector space with a non-standard basis if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9052218632432236
      },
      {
        "text": "Linear Independence Implies Span with Respect to a Given Basis: A set of vectors is linearly independent if and only if its span with respect to a given basis is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9416673974738237
      },
      {
        "text": "Linear Independence Implies Span with Respect to a Given Subspace: A set of vectors is linearly independent if and only if its span with respect to a given subspace is the entire subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9431458777236303
      }
    ]
  },
  {
    "representative_text": "Additive Property: The span of a set of vectors is equal to the span of the union of the set.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Additive Property: The span of a set of vectors is equal to the span of the union of the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Additivity: The span of a union of sets is equal to the union of the spans of the individual sets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9334335561232783
      }
    ]
  },
  {
    "representative_text": "Finite-Dimensional Vector Spaces: A finite-dimensional vector space is linearly independent if and only if the vectors are linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Finite-Dimensional Vector Spaces: A finite-dimensional vector space is linearly independent if and only if the vectors are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Infinite-Dimensional Vector Spaces: An infinite-dimensional vector space is linearly independent if and only if the vectors are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8714556041673309
      }
    ]
  },
  {
    "representative_text": "Linear Independence of the Kernel: The kernel of a linear transformation is linearly independent if and only if the transformation is one-to-one.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence of the Kernel: The kernel of a linear transformation is linearly independent if and only if the transformation is one-to-one.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Null Space of a Linear Transformation: The null space of a linear transformation is the set of vectors that are mapped to the zero vector. The kernel of a linear transformation is linearly independent if and only if the transformation is one-to-one.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9108320833654537
      },
      {
        "text": "Linear Independence of a Linear Transformation: A linear transformation is said to be linearly independent if its kernel is trivial, i.e., contains only the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8849372142871395
      }
    ]
  },
  {
    "representative_text": "Row-Reduced Echelon Form (RREF): A matrix is in RREF if it is in row-echelon form and each row with a non-zero entry has zeros below it.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Row-Reduced Echelon Form (RREF): A matrix is in RREF if it is in row-echelon form and each row with a non-zero entry has zeros below it.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Reduced Row Echelon Form (RREF): A matrix is in RREF if it has been reduced to a form where all rows consisting entirely of zeros are grouped at the bottom of the matrix, and each row that is not entirely zeros has a 1 as its first nonzero entry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9588542970092512
      },
      {
        "text": "Row Echelon Form (REF): A matrix is in REF if it has been reduced to a form where all rows consisting entirely of zeros are above all rows that are not entirely zeros, and each row that is not entirely zeros has a 1 as its first nonzero entry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9042816268320295
      },
      {
        "text": "Reduced Row Echelon Form: A matrix in which the rows are linearly independent and the columns are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.830525488124848
      },
      {
        "text": "Reduced Row Echelon Form (RREF): A form of a matrix where all the rows are in row echelon form and the leading entry of each row is to the right of the leading entry of the row above it, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9014649718893603
      },
      {
        "text": "The concept of reduced row echelon form (RREF): A form of a matrix where all the rows are in row echelon form and the leading entry of each row is to the right of the leading entry of the row above it.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9333822739871724
      },
      {
        "text": "Reduced Row Echelon Form (RREF): A canonical form for matrices, which is used to solve systems of linear equations and to determine the rank of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9046153271731339
      }
    ]
  },
  {
    "representative_text": "Pivot Column: A column of a matrix containing a pivot element (a non-zero entry) is called a pivot column.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Pivot Column: A column of a matrix containing a pivot element (a non-zero entry) is called a pivot column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Pivot Element: The pivot element is the element in the first column of the augmented matrix that is not equal to zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8095877793658965
      }
    ]
  },
  {
    "representative_text": "Pivot Row: A row of a matrix containing a pivot element (a non-zero entry) is called a pivot row.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pivot Row: A row of a matrix containing a pivot element (a non-zero entry) is called a pivot row.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Free Variable: A variable that does not appear in a linear combination of vectors is called a free variable.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Free Variable: A variable that does not appear in a linear combination of vectors is called a free variable.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dependent Vectors: Two or more vectors are said to be dependent if one can be expressed as a linear combination of the others.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dependent Vectors: Two or more vectors are said to be dependent if one can be expressed as a linear combination of the others.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence Test: A method for determining whether a set of vectors is linearly independent using row operations or other techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Independence Test: A method for determining whether a set of vectors is linearly independent using row operations or other techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence Criterion: A criterion for determining whether a set of vectors is linearly independent, such as the Linear Independence Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8244319714777102
      }
    ]
  },
  {
    "representative_text": "Linear Dependence Criterion: A criterion for determining whether a set of vectors is linearly dependent, such as the fact that if one vector can be expressed as a linear combination of the others, then the vectors are dependent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Dependence Criterion: A criterion for determining whether a set of vectors is linearly dependent, such as the fact that if one vector can be expressed as a linear combination of the others, then the vectors are dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linearity: If a set of vectors is linearly independent, then any linear combination of those vectors is also linearly independent.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linearity: If a set of vectors is linearly independent, then any linear combination of those vectors is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Homogeneous Property: If a set of vectors is linearly independent, then any linear combination of the vectors is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9045400614580317
      },
      {
        "text": "Homogeneous Property: If a set of vectors is linearly dependent, then any linear combination of the vectors is also linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9152509131574484
      }
    ]
  },
  {
    "representative_text": "Computer Graphics: Linear independence is used to create smooth curves and surfaces in computer graphics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computer Graphics: Linear independence is used to create smooth curves and surfaces in computer graphics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Data Analysis: Linear independence is used to analyze and visualize data in statistics and data science.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Data Analysis: Linear independence is used to analyze and visualize data in statistics and data science.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Physics: Linear independence is used to describe the motion of objects in physics.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Physics: Linear independence is used to describe the motion of objects in physics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spanning Basis Theorem: A set of vectors is a spanning basis for a vector space if and only if it is linearly independent and spans the entire vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 30,
    "detailed_sources": [
      {
        "text": "Spanning Basis Theorem: A set of vectors is a spanning basis for a vector space if and only if it is linearly independent and spans the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Spanning Set Theorem (Converse): If a set of vectors spans a vector space, then the set is linearly independent if and only if the set is a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9009481586424986
      },
      {
        "text": "Linearly Independent Spanning Set: If a set of vectors is linearly independent and spans a vector space, then the set is a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9136305146894851
      },
      {
        "text": "Span Basis Theorem: If a set of vectors spans a vector space, then it is a basis for that space if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9582559345850427
      },
      {
        "text": "Span Implies Linear Independence Implies Basis: If a set of vectors spans a vector space and is linearly independent, then it is a basis for that space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9297478171076986
      },
      {
        "text": "Linear Independence Implies Spanning Theorem: If V is a finite-dimensional vector space and S is a set of linearly independent vectors, then S spans V if and only if S is a free basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8885789411127285
      },
      {
        "text": "Free Basis is a Basis: If a set of vectors is linearly independent, then it is also a basis for the vector space if and only if the vector space is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8526351059635552
      },
      {
        "text": "Free Basis is Spanning: If a set of vectors is linearly independent, then it is also a spanning set for the vector space if and only if the vector space is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9392675024161212
      },
      {
        "text": "Linear Independence of Spanning Set: If a set of vectors spans a vector space, then the set is linearly independent if and only if it is a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9542620582283152
      },
      {
        "text": "Existence: If a set of vectors spans a vector space, then the set is linearly independent if and only if it is a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9347416124205405
      },
      {
        "text": "Linear Independence and Basis: A set of linearly independent vectors is a basis of a vector space if and only if the span of that set is equal to the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9422666263180965
      },
      {
        "text": "Span and Basis: A set of vectors that span a vector space must be linearly independent, and a set of linearly independent vectors is a basis of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9436177509281044
      },
      {
        "text": "Basis Implies Span: A set of vectors $\\{v1, v2, \\ldots, vn\\}$ is a basis for a vector space $V$ if and only if the span of this set is equal to $V$, i.e., $\\text{span}(\\{v1, v2, \\ldots, vn\\}) = V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.864661954157619
      },
      {
        "text": "Linear Independence and Basis for Finite-Dimensional Vector Spaces: A set of vectors is linearly independent if and only if the set is a basis for a finite-dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9138225123115887
      },
      {
        "text": "Existence of Basis: A set of linearly independent vectors is said to be a basis for a vector space if and only if its span is the entire vector space. This concept is crucial in understanding the fundamental properties of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.899401163758492
      },
      {
        "text": "Linear Independence Implies Spanning Set: A set of linearly independent vectors spans the entire vector space if and only if the set is a free basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9212581347329734
      },
      {
        "text": "Spanning Set Implies Linear Independence Implies Basis Implies Linear Independence Implies Spanning Set Implies Basis Implies Linear Independence Theorem: If V is a finite-dimensional vector space and S is a set of vectors that span V, then S is linearly independent if and only if S is a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9213361378611054
      },
      {
        "text": "The relationship between linear independence and span: There is a close relationship between linear independence and span. A set of vectors is linearly independent if and only if it is a basis for the space, and a set of vectors spans a space if and only if it is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8988909440778341
      },
      {
        "text": "Theorem of Linear Independence and Basis: A set of vectors is a basis for a vector space if and only if it is linearly independent and spans the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9356111591370808
      },
      {
        "text": "Free Basis Implies Basis Implies Linear Independence Implies Spanning Set Implies Basis Implies Linear Independence Implies Spanning Set Implies Basis Implies Linear Independence Theorem: If V is a finite-dimensional vector space and S is a set of linearly independent vectors, then S spans V if and only if S is a free basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9222902456133991
      },
      {
        "text": "**Linear Independence Implies Spanning Set Implies Basis Implies Linear Independence Implies Spanning Set Implies Basis Implies Linear Independence Implies Spanning Set",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8530759569498624
      },
      {
        "text": "Linear Independence Implies Span Implies Basis Implies Dimension: A set of vectors is linearly independent if and only if the span of the set is equal to the entire vector space, which is also equal to the basis for the span of the set, which is also equal to the dimension of the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9153503747676326
      },
      {
        "text": "If V is a finite-dimensional vector space and S is a set of linearly independent vectors, then S spans V if and only if S is a free basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9098473848101682
      },
      {
        "text": "If V is a finite-dimensional vector space and S is a set of vectors that span V, then S is linearly independent if and only if S is a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9315901425107895
      },
      {
        "text": "If a set of vectors is linearly independent, then it is also a basis for the vector space if and only if the vector space is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8885386100328445
      },
      {
        "text": "Weak Basis Theorem: If {v1, v2, ..., vn} is a spanning set for V, then {v1, v2, ..., vn} is a basis for V if and only if {v1, v2, ..., vn} is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8833127470004101
      },
      {
        "text": "Free Basis Implies Linear Independence Implies Spanning Set Implies Basis Implies Linear Independence Implies Spanning Set Implies Basis Implies Linear Independence Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8586951334711269
      },
      {
        "text": "**Free Basis Implies Linear Independence Implies Spanning Set Implies Basis Implies",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8801484133064996
      },
      {
        "text": "Linear Independence Implies Span and Span Implies Linear Independence with Respect to a Given Basis: This concept is a more general version of the Fundamental Theorem of Linear Independence and is essential to understand in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8033916822454823
      },
      {
        "text": "Linear Independence Implies Span Implies Basis Implies Linear Independence (Continuous): This theorem states that if a set of vectors is linearly independent, then the span of the set is equal to the entire vector space, which is also equal to the basis for the span of the set, which is also equal to the set of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9022786784375629
      }
    ]
  },
  {
    "representative_text": "Distributivity: The span of a set of vectors is distributive over the addition of vectors in the set.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Distributivity: The span of a set of vectors is distributive over the addition of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Distributivity of Span over Vector Addition: The span of a set of vectors is distributive over the addition of vectors in the set, meaning that the span of the sum of two sets is equal to the sum of the spans of the individual sets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9464152828615104
      }
    ]
  },
  {
    "representative_text": "Example 1: Let V be a 2-dimensional vector space with a basis {v1, v2}. The span of the set {v1, v2} is equal to the entire vector space V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Example 1: Let V be a 2-dimensional vector space with a basis {v1, v2}. The span of the set {v1, v2} is equal to the entire vector space V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Example 2: Let V be a 3-dimensional vector space with a basis {v1, v2, v3}. The span of the set {v1, v2} is a 2-dimensional subspace of V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9563830342958801
      },
      {
        "text": "Example 3: Let V be a 2-dimensional vector space with a basis {v1, v2}. The span of the set {v1, v2, v3} is equal to the entire vector space V, since {v1, v2} already spans V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.958174926998978
      }
    ]
  },
  {
    "representative_text": "Linear transformations: The concept of span is used in linear transformations to find the range of a transformation and the null space of a transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear transformations: The concept of span is used in linear transformations to find the range of a transformation and the null space of a transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalues and eigenvectors: The concept of span is used in eigenvalues and eigenvectors to find the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8552157813375489
      },
      {
        "text": "Singular value decomposition: The concept of span is used in singular value decomposition to find the singular values of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8655664361326756
      }
    ]
  },
  {
    "representative_text": "Span is Closed Under Addition: The span of a set of vectors is closed under addition, meaning that the sum of any two linear combinations of the vectors is also a linear combination of the vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span is Closed Under Addition: The span of a set of vectors is closed under addition, meaning that the sum of any two linear combinations of the vectors is also a linear combination of the vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span is Not Necessarily Closed Under Multiplication: The span of a set of vectors is not necessarily closed under multiplication, meaning that the product of any two linear combinations of the vectors may not be a linear combination of the vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span is Not Necessarily Closed Under Multiplication: The span of a set of vectors is not necessarily closed under multiplication, meaning that the product of any two linear combinations of the vectors may not be a linear combination of the vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Combination Notation: A linear combination of vectors is denoted as $a1\\mathbf{v}1 + a2\\mathbf{v}2 + ... + an\\mathbf{v}n$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Linear Combination Notation: A linear combination of vectors is denoted as $a1\\mathbf{v}1 + a2\\mathbf{v}2 + ... + an\\mathbf{v}n$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Combination: A linear combination of vectors is a way to express one vector as a sum of scalar multiples of other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8041363790405713
      },
      {
        "text": "Linear Combination of Vectors: An expression of the form a1v1 + a2v2 + ... + anvn, where a1, a2, ..., an are scalars and v1, v2, ..., vn are vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9156845502833646
      },
      {
        "text": "Linear Combination: A linear combination of vectors is a linear combination of their corresponding coefficients.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8973284903539503
      },
      {
        "text": "Linear Combination of Vectors: A linear combination of vectors $\\{v1, v2, \\ldots, vn\\}$ is an expression of the form $a1v1 + a2v2 + \\ldots + anvn$ where $ai \\in \\mathbb{R}$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9462356679611004
      },
      {
        "text": "Linear Combination of a Basis: An expression of the form a1v1 + a2v2 + ... + anvn, where vi is a basis vector and ai is a scalar.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9131097216710754
      },
      {
        "text": "Linear Combination of Vectors: A linear combination of vectors is a sum of scalar multiples of the vectors, which is used to find the span of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9070731414912092
      },
      {
        "text": "A linear combination of vectors is a linear combination of vectors with coefficients from a field (usually the real or complex numbers).",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8417958601945671
      }
    ]
  },
  {
    "representative_text": "Scalar Notation: A scalar is denoted as $c$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar Notation: A scalar is denoted as $c$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of the Standard Basis: The span of the standard basis vectors $\\{\\mathbf{e}1, \\mathbf{e}2, ..., \\mathbf{e}_n\\}$ is the entire vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span of the Standard Basis: The span of the standard basis vectors $\\{\\mathbf{e}1, \\mathbf{e}2, ..., \\mathbf{e}_n\\}$ is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of Linearly Dependent Vectors: If a set of vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n\\}$ is linearly dependent, then its span is a proper subspace of the vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Span of Linearly Dependent Vectors: If a set of vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n\\}$ is linearly dependent, then its span is a proper subspace of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of a Linearly Dependent Set: The span of a linearly dependent set is a subspace of the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8884649299050766
      }
    ]
  },
  {
    "representative_text": "Span Theorem: Suppose that $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}n\\}$ is a set of linearly independent vectors. Then, for any vector $\\mathbf{x}$ in the span of the set, we can write $\\mathbf{x} = a1\\mathbf{v}1 + a2\\mathbf{v}2 + ... + an\\mathbf{v}n$ for some scalars $a1, a2, ..., an$. Since the vectors are linearly independent, we must have $a1 = a2 = ... = a_n = 0$. Therefore, $\\mathbf{x} = \\mathbf{0}$, and the span of the set is the entire vector space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span Theorem: Suppose that $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}n\\}$ is a set of linearly independent vectors. Then, for any vector $\\mathbf{x}$ in the span of the set, we can write $\\mathbf{x} = a1\\mathbf{v}1 + a2\\mathbf{v}2 + ... + an\\mathbf{v}n$ for some scalars $a1, a2, ..., an$. Since the vectors are linearly independent, we must have $a1 = a2 = ... = a_n = 0$. Therefore, $\\mathbf{x} = \\mathbf{0}$, and the span of the set is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence of a Basis: If a set of vectors is a basis, then it is linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Linear Independence of a Basis: If a set of vectors is a basis, then it is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence Property: A basis is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.930658050831185
      },
      {
        "text": "Basis is a Linearly Independent Set: A basis is always a linearly independent set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9440682939619607
      },
      {
        "text": "Properties: The linear independence of a set of vectors is equivalent to the set being a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.876855219697791
      },
      {
        "text": "Linear Independence of a Basis: A basis is linearly independent if none of the vectors can be expressed as a linear combination of the others.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9317189122327467
      },
      {
        "text": "Basis Implies Linear Independence: A basis for a vector space is a set of linearly independent vectors, but it is not necessarily true that a set of linearly independent vectors is a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.846468567435156
      },
      {
        "text": "Relationship between Linear Independence and Basis: A basis of a vector space is a set of linearly independent vectors that spans the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.922694119727873
      },
      {
        "text": "Basis Implies Linear Independence Implies Dimension: A basis for a vector space is linearly independent if and only if the dimension of the basis is equal to the number of vectors in the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8856871374511766
      },
      {
        "text": "Existence of a Linearly Independent Basis: A vector space has a linearly independent basis if and only if the vector space has a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8686067187692041
      }
    ]
  },
  {
    "representative_text": "Basis is Unique: If a vector space has a basis, then it is unique up to scalar multiples.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Basis is Unique: If a vector space has a basis, then it is unique up to scalar multiples.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Uniqueness of Basis: If a set of vectors is a basis for a vector space, then it is the unique basis for that space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.92416170080267
      },
      {
        "text": "Basis is Unique: A basis is always unique, meaning that there is only one basis for a given vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.933600303201593
      },
      {
        "text": "Dimension is Unique: The dimension of a vector space is unique, meaning that there is only one basis for a given vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9056778928752862
      },
      {
        "text": "Uniqueness of Bases: The theorems imply that there is a unique basis for each vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9043726158052783
      }
    ]
  },
  {
    "representative_text": "Row Echelon Form: A matrix in which the rows are linearly independent and the columns are linearly dependent.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Row Echelon Form: A matrix in which the rows are linearly independent and the columns are linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Combination of Vectors: A linear combination of vectors can be used to construct a basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Combination of Vectors: A linear combination of vectors can be used to construct a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linearly Independent Vectors: A set of linearly independent vectors can be used to construct a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8816320295955026
      },
      {
        "text": "Basis for a Linear Combination: A set of vectors that can be used to express a linear combination of other vectors in a specific way.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8290737437935762
      }
    ]
  },
  {
    "representative_text": "Greatest Common Divisor: The greatest common divisor of a set of vectors can be used to construct a basis.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Greatest Common Divisor: The greatest common divisor of a set of vectors can be used to construct a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence: If a set of vectors is linearly independent, then it spans a subspace of the vector space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Independence: If a set of vectors is linearly independent, then it spans a subspace of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of Transformations: If a set of linear transformations is linearly independent, then it spans a subspace of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8787582974601631
      },
      {
        "text": "Linear Independence of Span: If a set of vectors is linearly independent, then its span is a subspace of the vector space. This concept is related to the concept of basis and dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.891637216933269
      },
      {
        "text": "Linear Independence of a Vector Space: This concept involves determining whether a vector space is linearly independent. If a vector space is linearly independent, it implies that no vector in the space can be expressed as a linear combination of other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8313465184632667
      }
    ]
  },
  {
    "representative_text": "Span of a Linear Transformation: The span of a linear transformation is the set of all transformed vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Span of a Linear Transformation: The span of a linear transformation is the set of all transformed vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of the Image: The span of the image of a linear transformation is equal to the image of the span of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8459912018126328
      },
      {
        "text": "Dimension of the Span of a Linear Transformation: We need to consider the relationship between the dimension of the span of a linear transformation and the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8310970791912078
      },
      {
        "text": "Dimension of the Span of a Linear Transformation for Infinite-Dimensional Vector Spaces: We need to consider the relationship between the dimension of the span of a linear transformation and the original vector space for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8881427596418561
      },
      {
        "text": "Dimension of the Image of a Linear Transformation: We need to explore the relationship between the dimension of the image of a linear transformation and the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8754911641142133
      },
      {
        "text": "Span of a Linear Transformation's Image: This concept is related to the image of a linear transformation, which is the set of all possible outputs of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8479627569221903
      }
    ]
  },
  {
    "representative_text": "dim(V) (dimension of a vector space V)",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "dim(V) (dimension of a vector space V)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of a Vector Space",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.920895240476295
      }
    ]
  },
  {
    "representative_text": "basis(V) (basis of a vector space V)",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "basis(V) (basis of a vector space V)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of a Basis: A vector space has a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8494355368545148
      },
      {
        "text": "B: A basis of a vector space V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8555881199076447
      },
      {
        "text": "Span and Basis:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8477461983779666
      },
      {
        "text": "Basis for a Vector Space",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9462114832047424
      },
      {
        "text": "Existence of a Basis for a Vector Space:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9066198564647281
      },
      {
        "text": "Properties of basis:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8694084936366323
      }
    ]
  },
  {
    "representative_text": "w (vector in V)",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "w (vector in V)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "v (vector in V)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9204185014403069
      }
    ]
  },
  {
    "representative_text": "a (scalar)",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "a (scalar)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spanning Matrix: A matrix whose columns form a basis for the vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spanning Matrix: A matrix whose columns form a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Free Variables: Vectors in a basis that can be expressed as a linear combination of other vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Free Variables: Vectors in a basis that can be expressed as a linear combination of other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dependent Variables: Vectors in a basis that are dependent on other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8287838983630678
      },
      {
        "text": "Dependent and Free Variables: In a basis, the dependent variables are vectors that can be expressed as a linear combination of other vectors, while the free variables are vectors that cannot be expressed as a linear combination of other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9096210835658851
      }
    ]
  },
  {
    "representative_text": "Trivial Basis Property: A basis with one vector is not a basis for a vector space with more than one dimension.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Trivial Basis Property: A basis with one vector is not a basis for a vector space with more than one dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis is Complete: A basis is always complete, meaning that it contains all the linearly independent vectors in the space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Basis is Complete: A basis is always complete, meaning that it contains all the linearly independent vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Completeness of the Basis Theorem: A basis for a vector space is said to be complete if it spans the entire space and is linearly independent. This theorem states that every vector space has a complete basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8768890569552068
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination: Gaussian elimination can be used to find a basis for a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination: Gaussian elimination can be used to find a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Row Reduction: Row reduction can be used to find a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.892684863177299
      }
    ]
  },
  {
    "representative_text": "Spanning Set Construction: A spanning set can be constructed by finding a set of linearly independent vectors that span the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spanning Set Construction: A spanning set can be constructed by finding a set of linearly independent vectors that span the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations: Bases are used to define linear transformations between vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformations: Bases are used to define linear transformations between vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Role of Basis in Linear Transformations: Bases play a crucial role in defining linear transformations between vector spaces. A basis for the domain vector space determines the basis for the codomain vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8742414335664664
      },
      {
        "text": "Change of Basis and Linear Transformations: When changing the basis of a vector space, the linear transformation defined by the original basis is transformed accordingly. This is crucial for understanding the behavior of linear transformations under basis changes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8561082825328225
      }
    ]
  },
  {
    "representative_text": "Change of Basis: Bases are used to change the basis of a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Change of Basis: Bases are used to change the basis of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Change of Basis: A method for transforming a basis of one vector space into a basis of another vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.840915986656371
      },
      {
        "text": "Change of Basis and Linear Transformations: How changing the basis of a vector space affects the representation of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8099151171351373
      },
      {
        "text": "Matrix Representation of Linear Transformations using the Change of Basis Formula: This involves the study of the change of basis formula, which is a formula for transforming a linear transformation from one basis to another.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8005325071711258
      },
      {
        "text": "Change of Basis: A process of transforming a basis of a vector space to another basis, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8692404420203159
      },
      {
        "text": "Matrix Representation of Linear Transformations using the Change of Basis Formula with Non-Real Entries: This involves the study of the change of basis formula with non-real entries, which is a formula for transforming a linear transformation from one basis to another.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8119737139649991
      },
      {
        "text": "Matrix Representation of Linear Transformations using the Change of Basis Formula with Non-Real Entries: The study of the change of basis formula with non-real entries, which is a formula for transforming a linear transformation from one basis to another.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8780636629140495
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Spanning: The concepts of linear independence and spanning are closely related and are used to define the concept of a basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and Spanning: The concepts of linear independence and spanning are closely related and are used to define the concept of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fundamental Theorem of Linear Independence: If a set of vectors is linearly independent, then it is a basis for the vector space spanned by those vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 47,
    "detailed_sources": [
      {
        "text": "Fundamental Theorem of Linear Independence: If a set of vectors is linearly independent, then it is a basis for the vector space spanned by those vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of Orthogonal Sets: This theorem states that if a set of orthogonal vectors is linearly independent, then it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8390856677472198
      },
      {
        "text": "Linear Independence of Trigonometric Functions: This theorem states that if a set of trigonometric functions (such as sine and cosine) is linearly independent, then it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8066805637000372
      },
      {
        "text": "Linear Independence of a Set of Matrices: This theorem states that if a set of matrices is linearly independent, then it is also a basis for the space of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9004828417555208
      },
      {
        "text": "Linear Independence of a Set of Rational Functions: This theorem states that if a set of rational functions is linearly independent, then it is also a basis for the space of rational functions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732665470707454
      },
      {
        "text": "Linear Independence of a Set of Trigonometric Series: This theorem states that if a set of trigonometric series is linearly independent, then it is also a basis for the space of trigonometric series.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8872595524905571
      },
      {
        "text": "Basis Theorem for Vector Spaces with Non-Standard Operations: This theorem states that a set of vectors is a basis for a vector space with non-standard operations (e.g., matrix multiplication) if and only if it is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8094008603304159
      },
      {
        "text": "Theorem of Linear Independence of Spanning Sets: This theorem states that if a set of vectors spans a vector space, then the set is linearly independent if and only if it is a basis for the space. This theorem is a generalization of the concept of linear independence of a set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8671495625387018
      },
      {
        "text": "Frobenius' Theorem: This theorem states that if a set of vectors is linearly independent, then it is also a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8777846085478094
      },
      {
        "text": "The Fundamental Theorem of Linear Independence: A theorem that establishes the equivalence between linear independence and the span of a set of vectors. This theorem is a generalization of the Linear Independence Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8418002591682918
      },
      {
        "text": "The Fundamental Theorem of Linear Independence: A theorem that establishes the equivalence between linear independence and the span of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8778374429554281
      },
      {
        "text": "Basis Theorem for Vector Spaces with Non-Standard Vector Addition: This theorem states that a set of vectors is a basis for a vector space with non-standard vector addition if and only if it is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8483737454985062
      },
      {
        "text": "Relationship between Linear Independence and the Basis Theorem: The basis theorem states that a set of vectors is a basis of a vector space if and only if it is linearly independent and spans the entire vector space. Understanding the relationship between linear independence and the basis theorem is crucial in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8805133981859793
      },
      {
        "text": "The Fundamental Theorem of Linear Independence with Respect to a Given Basis: This theorem establishes the equivalence between linear independence and the span of a set of vectors with respect to a given basis, which is a more general version of the Fundamental Theorem of Linear Independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8745049966300567
      },
      {
        "text": "The Fundamental Theorem of Linear Algebra: A theorem that states that every vector space has a basis, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.885784583933938
      },
      {
        "text": "The Relationship between the Basis Theorem and the Inner Product: The basis theorem states that a set of vectors is a basis of a vector space if and only if it is linearly independent and spans the entire vector space. The inner product can be used to understand this relationship, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8285709228373865
      },
      {
        "text": "The Fundamental Theorem of Linear Independence with Respect to a Given Basis: This theorem establishes the equivalence between linear independence and the span of a set of vectors with respect to a given basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9023514456740371
      },
      {
        "text": "The Fundamental Theorem of Linear Independence in Infinite-Dimensional Vector Spaces: This theorem establishes the equivalence between linear independence and the span of a set of vectors in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8500688303339994
      },
      {
        "text": "The Null Space Theorem: This theorem states that if V is a finite-dimensional vector space and S is a set of linearly independent vectors, then S is a basis for V if and only if the null space of the linear transformation T: V → V induced by S is trivial, i.e., N(T) = {0}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8595020888436715
      },
      {
        "text": "Linear Independence and the Fundamental Theorem of Linear Algebra: The Fundamental Theorem of Linear Algebra states that every vector space has a basis, and linear independence is a necessary condition for a set of vectors to be a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8981084501392618
      },
      {
        "text": "Theorem of Linear Independence in Fréchet Spaces: This theorem states that if a set of vectors is linearly independent in a Fréchet space, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8460932833019229
      },
      {
        "text": "Theorem of Linear Independence in Banach Spaces: This theorem states that if a set of vectors is linearly independent in a Banach space, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8791313097609958
      },
      {
        "text": "Theorem of Linear Independence in Hilbert Spaces: This theorem states that if a set of vectors is linearly independent in a Hilbert space, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8664806407591851
      },
      {
        "text": "Theorem of Linear Independence in Orthogonal Spaces: This theorem states that if a set of vectors is linearly independent in an orthogonal space, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8991700390048321
      },
      {
        "text": "Theorem of Linear Independence in Trigonometric Spaces: This theorem states that if a set of trigonometric functions is linearly independent, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8745800798711701
      },
      {
        "text": "Theorem of Linear Independence in Rational Function Spaces: This theorem states that if a set of rational functions is linearly independent, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8718735545487386
      },
      {
        "text": "Theorem of Linear Independence in Polynomial Spaces: This theorem states that if a set of polynomials is linearly independent, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9018859195218372
      },
      {
        "text": "Theorem of Linear Independence in Matrix Spaces: This theorem states that if a set of matrices is linearly independent, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.90381926690838
      },
      {
        "text": "Theorem of Linear Independence in Function Spaces: This theorem states that if a set of functions is linearly independent, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.912783777134712
      },
      {
        "text": "Theorem of Linear Independence in Vector Spaces with Non-Standard Bases: This theorem states that if a set of vectors is linearly independent in a vector space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8972338729109406
      },
      {
        "text": "Theorem of Linear Independence in Vector Spaces with Non-Standard Scalar Multiplication: This theorem states that if a set of vectors is linearly independent in a vector space with non-standard scalar multiplication, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8677652396667705
      },
      {
        "text": "Theorem of Linear Independence in Vector Spaces with Non-Standard Linear Transformations: This theorem states that if a set of vectors is linearly independent in a vector space with a non-standard linear transformation, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8546610550820548
      },
      {
        "text": "Theorem of Linear Independence in Vector Spaces with Non-Standard Dual Spaces: This theorem states that if a set of vectors is linearly independent in a vector space with a non-standard dual space, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8682635883292924
      },
      {
        "text": "Theorem of Linear Independence in Vector Spaces with Non-Standard Topologies: This theorem states that if a set of vectors is linearly independent in a vector space with a non-standard topology, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8795512036494173
      },
      {
        "text": "Theorem of Linear Independence in Vector Spaces with Non-Standard Structures: This theorem states that if a set of vectors is linearly independent in a vector space with a non-standard structure, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8954944920650388
      },
      {
        "text": "Theorem of Linear Independence in Vector Spaces with Non-Standard Boundedness: This theorem states that if a set of vectors is linearly independent in a vector space with non-standard boundedness, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8774895953244113
      },
      {
        "text": "Theorem of Linear Independence in Fréchet Spaces with a Non-Standard Basis: This theorem states that if a set of vectors is linearly independent in a Fréchet space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9052620083556744
      },
      {
        "text": "Theorem of Linear Independence in Banach Spaces with a Non-Standard Basis: This theorem states that if a set of vectors is linearly independent in a Banach space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9249404815595086
      },
      {
        "text": "Theorem of Linear Independence in Hilbert Spaces with a Non-Standard Basis: This theorem states that if a set of vectors is linearly independent in a Hilbert space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9168042707866336
      },
      {
        "text": "Theorem of Linear Independence in Orthogonal Spaces with a Non-Standard Basis: This theorem states that if a set of vectors is linearly independent in an orthogonal space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9400925402937512
      },
      {
        "text": "Theorem of Linear Independence in Trigonometric Spaces with a Non-Standard Basis: This theorem states that if a set of trigonometric functions is linearly independent in a trigonometric space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9018111688751143
      },
      {
        "text": "Theorem of Linear Independence in Rational Function Spaces with a Non-Standard Basis: This theorem states that if a set of rational functions is linearly independent in a rational function space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8949339293713892
      },
      {
        "text": "Theorem of Linear Independence in Polynomial Spaces with a Non-Standard Basis: This theorem states that if a set of polynomials is linearly independent in a polynomial space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9245540062156639
      },
      {
        "text": "Theorem of Linear Independence in Matrix Spaces with a Non-Standard Basis: This theorem states that if a set of matrices is linearly independent in a matrix space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9358784683330296
      },
      {
        "text": "Theorem of Linear Independence in Function Spaces with a Non-Standard Basis: This theorem states that if a set of functions is linearly independent in a function space with a non-standard basis, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.933612844090721
      },
      {
        "text": "Theorem of Linear Independence in Fréchet Spaces with a Non-Standard Basis: This theorem states that if a set of vectors is linearly independent in a Fréchet space with a non-standard basis, then theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8556868065406784
      },
      {
        "text": "Gelfand's Theorem: A theorem that states that a set of vectors is a basis for a vector space if and only if it is linearly independent and spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8414190954158393
      }
    ]
  },
  {
    "representative_text": "Span and Dimension: The span of a set of vectors is equal to the vector space spanned by that set, and the dimension of the span is equal to the dimension of the original vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Span and Dimension: The span of a set of vectors is equal to the vector space spanned by that set, and the dimension of the span is equal to the dimension of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span Implies Dimension: The span of a set of vectors is equal to the dimension of the span of the set, which is also equal to the number of vectors in a basis for the span of the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9072571818663511
      },
      {
        "text": "Span Implies Dimension: If a set of vectors spans a vector space, then the dimension of the span of the set is equal to the number of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9402157502188616
      }
    ]
  },
  {
    "representative_text": "Existence of Bases: The theorems guarantee the existence of bases for any vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Existence of Bases: The theorems guarantee the existence of bases for any vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem on the Existence of a Basis for a Vector Space: This theorem states that every vector space has a basis, but it does not provide a method for finding the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8197846978450539
      }
    ]
  },
  {
    "representative_text": "Computing Dimensions: The theorems provide a way to compute the dimension of a vector space by finding a basis for that space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computing Dimensions: The theorems provide a way to compute the dimension of a vector space by finding a basis for that space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computer Graphics: Basis is used to represent objects in 3D space, allowing for efficient rendering and manipulation of objects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computer Graphics: Basis is used to represent objects in 3D space, allowing for efficient rendering and manipulation of objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Machine Learning: Basis is used in techniques such as principal component analysis (PCA) to reduce the dimensionality of high-dimensional data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Machine Learning: Basis is used in techniques such as principal component analysis (PCA) to reduce the dimensionality of high-dimensional data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Physics: Basis is used to describe the motion of objects in terms of their position, velocity, and acceleration.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Physics: Basis is used to describe the motion of objects in terms of their position, velocity, and acceleration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector Spaces in Other Fields: Vector spaces are used in many fields, including calculus, differential equations, and topology.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector Spaces in Other Fields: Vector spaces are used in many fields, including calculus, differential equations, and topology.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations in Other Fields: Linear transformations are used in many fields, including calculus, differential equations, and functional analysis.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations in Other Fields: Linear transformations are used in many fields, including calculus, differential equations, and functional analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis in Other Fields: Basis is used in many fields, including computer graphics, machine learning, and physics.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis in Other Fields: Basis is used in many fields, including computer graphics, machine learning, and physics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Free Basis Theorem: A vector space has a free basis if and only if it is finite-dimensional.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Free Basis Theorem: A vector space has a free basis if and only if it is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Fundamental Theorem of Free Bases: If V is a finite-dimensional vector space, then V has a free basis if and only if V is spanned by a set of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9081315771078335
      },
      {
        "text": "Free Basis Theorem for Infinite-Dimensional Vector Spaces: If V is an infinite-dimensional vector space, then V does not have a free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8777494185390696
      },
      {
        "text": "Free Basis is a Basis for a Vector Space: A free basis is a basis for a vector space if and only if the vector space is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8935060649420317
      },
      {
        "text": "Existence of a Free Basis for a Vector Space: If V is a finite-dimensional vector space, then V has a free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8926872577868332
      },
      {
        "text": "Basis Theorem: If V is a finite-dimensional vector space, then V has a basis if and only if V has a free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9368861230699836
      }
    ]
  },
  {
    "representative_text": "Free Basis is Unique: If a vector space has a free basis, then there exists a unique free basis for the vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Free Basis is Unique: If a vector space has a free basis, then there exists a unique free basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A set of vectors is a free basis for V if and only if:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A set of vectors is a free basis for V if and only if:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "R^2: The vector space R^2 has a free basis {e1, e2}, where e1 and e2 are the standard basis vectors for R^2.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "R^2: The vector space R^2 has a free basis {e1, e2}, where e1 and e2 are the standard basis vectors for R^2.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "R^3: The vector space R^3 has a free basis {e1, e2, e3}, where e1, e2, and e3 are the standard basis vectors for R^3.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572948307121836
      }
    ]
  },
  {
    "representative_text": "R^∞: The vector space R^∞ does not have a free basis, since it is infinite-dimensional.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "R^∞: The vector space R^∞ does not have a free basis, since it is infinite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Free Basis: A free basis of a vector space is a basis that is linearly independent but may not span the entire vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Free Basis: A free basis of a vector space is a basis that is linearly independent but may not span the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Definition: A free basis of a vector space is a basis that is linearly independent but may not span the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9345058077765147
      },
      {
        "text": "Free Basis: A set of vectors that are linearly independent, but not necessarily spanning the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9282341544148354
      }
    ]
  },
  {
    "representative_text": "Span of a Subspace: The span of a subspace is the subspace itself.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Span of a Subspace: The span of a subspace is the subspace itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of a Free Basis: The span of a free basis is a subspace of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.817088423575335
      },
      {
        "text": "Existence: The span of a vector space is a subspace of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8554232466842071
      },
      {
        "text": "Theorem of Span: If a set of vectors spans a subspace, then the span of that set is equal to the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8655227106554222
      },
      {
        "text": "Subspace Spanning Theorem: A subset of a vector space is said to span the vector space if the span of the subset equals the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8734161550133648
      },
      {
        "text": "Subspace Spanning Theorem: If S is a subspace of a vector space V, then S is a spanning set for V if and only if S is a free basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8696901611136274
      },
      {
        "text": "Subspace Spanning Theorem for Infinite-Dimensional Vector Spaces: If S is a subspace of a vector space V, then S is a spanning set for V if and only if S is a free basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8548202914300203
      },
      {
        "text": "If S is a subspace of a vector space V, then S is a spanning set for V if and only if S is a free basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.884848099742964
      },
      {
        "text": "Corner Theorem: A subspace is spanned by a set of vectors if and only if the span of the set is equal to the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8501415793062174
      },
      {
        "text": "Subspace Span Theorem for Infinite-Dimensional Vector Spaces: This theorem states that the span of a set of vectors is equal to the subspace spanned by the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8836271803435818
      },
      {
        "text": "Subspace Spanning Theorem: This theorem states that if S is a subspace of a vector space V:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8712480958494584
      },
      {
        "text": "The span of a set of vectors is a subspace of the vector space.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8774594077102648
      }
    ]
  },
  {
    "representative_text": "Free Basis Theorem: A set of vectors is a free basis of a vector space if and only if it is linearly independent and the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has a non-trivial solution.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Free Basis Theorem: A set of vectors is a free basis of a vector space if and only if it is linearly independent and the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has a non-trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Free Basis Theorem with Null Space: The Free Basis Theorem states that a set of vectors is a free basis of a vector space if and only if it is linearly independent and the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has a non-trivial solution. A subtle nuance is that the null space of the free basis is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9169347953357982
      }
    ]
  },
  {
    "representative_text": "Spanning Set: A set of vectors that spans the entire vector space, but may not be linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Spanning Set: A set of vectors that spans the entire vector space, but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of a Set of Infinite Vectors: The span of a set of infinite vectors {v1, v2, ...} is the set of all linear combinations of these vectors, but it may not be a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8011751214352807
      },
      {
        "text": "Span of a Set of Polynomials: This refers to the set of all linear combinations of a set of polynomials.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8330543305443467
      },
      {
        "text": "Span of a Set of Matrices: The span of a set of matrices is the set of all linear combinations of the matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8505627374148669
      }
    ]
  },
  {
    "representative_text": "Existence of a Basis: If a vector space has a finite basis, then every vector in the space can be expressed as a linear combination of the basis vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Existence of a Basis: If a vector space has a finite basis, then every vector in the space can be expressed as a linear combination of the basis vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of a Basis for a Vector Space: If V is a finite-dimensional vector space, then V has a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8692336784918224
      },
      {
        "text": "Finite Dimensionality Implies Basis Existence: Every finite-dimensional vector space has a basis, but this is not explicitly mentioned in the provided knowledge points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8026601903177054
      }
    ]
  },
  {
    "representative_text": "Dimension of a Free Basis: The dimension of a free basis is the number of vectors in the basis minus one.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension of a Free Basis: The dimension of a free basis is the number of vectors in the basis minus one.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Uniqueness: Every basis for a vector space has the same dimension.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Uniqueness: Every basis for a vector space has the same dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Existence: Every vector space has a free basis, which is a linearly independent set of vectors that does not span the entire space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Existence: Every vector space has a free basis, which is a linearly independent set of vectors that does not span the entire space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "W: A subspace of a vector space V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "W: A subspace of a vector space V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "c: A coefficient in a linear combination.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "c: A coefficient in a linear combination.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Example of a Linearly Independent Set: Consider the set of vectors {(1, 0), (0, 1), (1, 1)} in R^2. This set is linearly independent because none of the vectors can be expressed as a linear combination of the other vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Example of a Linearly Independent Set: Consider the set of vectors {(1, 0), (0, 1), (1, 1)} in R^2. This set is linearly independent because none of the vectors can be expressed as a linear combination of the other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Example of a Linearly Dependent Set: Consider the set of vectors {(1, 1), (2, 2), (3, 3)} in R^2. This set is linearly dependent because the vectors are multiples of each other.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Example of a Linearly Dependent Set: Consider the set of vectors {(1, 1), (2, 2), (3, 3)} in R^2. This set is linearly dependent because the vectors are multiples of each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Row Space: The row space of a matrix $A$ is the set of all linear combinations of the rows of $A$, i.e., $\\text{row}(A) = \\{a1^T \\mid a1 \\in \\mathbb{R}^n\\}$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Row Space: The row space of a matrix $A$ is the set of all linear combinations of the rows of $A$, i.e., $\\text{row}(A) = \\{a1^T \\mid a1 \\in \\mathbb{R}^n\\}$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Row Space and Null Space: The row space of a matrix A is the span of the rows of A, and the null space of A is the set of vectors x such that Ax = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8428075144762279
      },
      {
        "text": "Row Space of a Matrix: The row space of a matrix $A$ is equal to the span of the rows of $A$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9020603968975331
      },
      {
        "text": "The Row Space and Null Space Theorem for Vector Spaces: This theorem states that the row space of a matrix A is the span of the rows of A, and the null space of A is the set of vectors x such that Ax = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8568292931731739
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Span: A set of vectors $\\{v1, v2, \\ldots, vn\\}$ is linearly independent if and only if the span of this set is equal to the zero vector, i.e., $\\text{span}(\\{v1, v2, \\ldots, vn\\}) = \\{0\\}$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Span: A set of vectors $\\{v1, v2, \\ldots, vn\\}$ is linearly independent if and only if the span of this set is equal to the zero vector, i.e., $\\text{span}(\\{v1, v2, \\ldots, vn\\}) = \\{0\\}$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span Implies Linear Independence: A set of vectors $\\{v1, v2, \\ldots, vn\\}$ spans a subspace $W$ if and only if the only linear combination of these vectors that equals the zero vector is the trivial solution, i.e., $\\{a1v1 + a2v2 + \\ldots + anvn = 0 \\mid ai \\in \\mathbb{R}\\} \\implies a_i = 0$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9265618718646593
      }
    ]
  },
  {
    "representative_text": "Linear Transformations: Linear transformations are linear combinations of basis vectors, and the span of the basis vectors is the image of the linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Transformations: Linear transformations are linear combinations of basis vectors, and the span of the basis vectors is the image of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span and Basis for Linear Transformations: If a set of vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}n\\}$ is a basis for a vector space, then the set of vectors $\\{T(\\mathbf{v}1), T(\\mathbf{v}2), ..., T(\\mathbf{v}n)\\}$ is a basis for the range of the linear transformation $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8027315526382302
      },
      {
        "text": "Relationship between Span and Basis with respect to Linear Transformations: The span of a set of vectors is equal to the range of the linear transformation associated with the set, while a basis for a vector space is a set of linearly independent vectors that spans the entire space and is also a basis for the range of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8923572765856469
      },
      {
        "text": "Existence of a Basis for a Vector Space using Linear Transformations: Explore the conditions under which a basis for a vector space can be constructed using linear transformations, including the role of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8158211870341141
      }
    ]
  },
  {
    "representative_text": "Eigenvalues and Eigenvectors: Eigenvalues and eigenvectors are related to the span of the matrix, and the linear independence of the eigenvectors is crucial in determining the dimension of the eigenspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Eigenvalues and Eigenvectors: Eigenvalues and eigenvectors are related to the span of the matrix, and the linear independence of the eigenvectors is crucial in determining the dimension of the eigenspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalues and Eigenvectors: The concept of eigenvalues and eigenvectors is closely related to linear independence and span, as eigenvectors are linearly independent and span the corresponding eigenspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9250113321637786
      },
      {
        "text": "Linear Independence and the Eigenvector and Eigenvalue: The concept of linear independence can be related to eigenvectors and eigenvalues, particularly in the context of diagonalization and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8830476529048875
      },
      {
        "text": "Eigenvalues and Eigenvectors: The scalar values and the corresponding vectors that remain unchanged under a linear transformation, which are related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8298238252575452
      },
      {
        "text": "Eigenvector and Eigenvalue for Infinite-Dimensional Matrices: The eigenvector and eigenvalue for an infinite-dimensional matrix can be used to determine linear independence and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8347230967837936
      }
    ]
  },
  {
    "representative_text": "Linear Dependence: A set of vectors is said to be linearly dependent if at least one vector in the set can be expressed as a linear combination of the other vectors in the set. In other words, if there exist non-trivial coefficients (i.e., not all zero) that can express the zero vector as a linear combination of the vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Dependence: A set of vectors is said to be linearly dependent if at least one vector in the set can be expressed as a linear combination of the other vectors in the set. In other words, if there exist non-trivial coefficients (i.e., not all zero) that can express the zero vector as a linear combination of the vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and Dependence: A set of vectors is said to be linearly independent if the only way to express the zero vector as a linear combination of these vectors is with all coefficients equal to zero. On the other hand, a set of vectors is said to be linearly dependent if the only way to express the zero vector as a linear combination of these vectors is with at least one coefficient equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9186436342678965
      },
      {
        "text": "Definition of linear dependence: A set of vectors is linearly dependent if at least one vector in the set can be expressed as a linear combination of the other vectors in the set.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.926421498573426
      },
      {
        "text": "A set of vectors is linearly dependent if at least one vector can be expressed as a linear combination of the others.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8816627948889024
      }
    ]
  },
  {
    "representative_text": "Trivial Basis Theorem: Every vector space has a trivial basis, which consists of a single vector. This vector is unique up to scalar multiples.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Trivial Basis Theorem: Every vector space has a trivial basis, which consists of a single vector. This vector is unique up to scalar multiples.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Trivial Basis of a Vector Space: Every vector space has a trivial basis, which consists of a single vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9185847322434997
      }
    ]
  },
  {
    "representative_text": "Reduced Row Echelon Form: A matrix is said to be in reduced row echelon form if all the entries below the leading entry in each row are zero, and the leading entry in each row is one.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Reduced Row Echelon Form: A matrix is said to be in reduced row echelon form if all the entries below the leading entry in each row are zero, and the leading entry in each row is one.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Reduced Row Echelon Form: A matrix that has been transformed into a form where the leading entries of each row are to the right of the leading entries of the rows above it, and the leading entries of each row are 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9052545846513806
      },
      {
        "text": "Reduced Row Echelon Form: The reduced row echelon form is a matrix that is in row echelon form and has the additional property that all the entries below the pivot element are zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.883872994460389
      }
    ]
  },
  {
    "representative_text": "Additive Property: If a set of vectors is linearly independent, then the sum of any two vectors in the set is also linearly independent.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Additive Property: If a set of vectors is linearly independent, then the sum of any two vectors in the set is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Additive Property: If a set of vectors is linearly dependent, then the sum of any two vectors in the set is also linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9330370296011763
      }
    ]
  },
  {
    "representative_text": "Multiplicative Property: If a set of vectors is linearly independent, then any scalar multiple of a vector in the set is also linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Multiplicative Property: If a set of vectors is linearly independent, then any scalar multiple of a vector in the set is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Multiplicative Property: If a set of vectors is linearly dependent, then any scalar multiple of a vector in the set is also linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9274000951033395
      }
    ]
  },
  {
    "representative_text": "Linear Dependence Theorem: If a set of vectors is linearly dependent, then it is not a basis for the vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Dependence Theorem: If a set of vectors is linearly dependent, then it is not a basis for the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Dependence of a Set of Vectors: The concept of linear dependence can be used to determine when a set of vectors is not linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8331904512488402
      }
    ]
  },
  {
    "representative_text": "Dot Product (Scalar Product): The dot product of two vectors u and v is a scalar value that represents the amount of \"similarity\" between the two vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Dot Product (Scalar Product): The dot product of two vectors u and v is a scalar value that represents the amount of \"similarity\" between the two vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dot product (or inner product): the product of two vectors is a scalar calculated as the sum of the products of their corresponding components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8128631774986885
      },
      {
        "text": "The dot product (or scalar product) of two vectors results in a scalar value.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8686241069091546
      }
    ]
  },
  {
    "representative_text": "Cross Product: The cross product of two vectors u and v is a new vector that is perpendicular to both u and v.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Cross Product: The cross product of two vectors u and v is a new vector that is perpendicular to both u and v.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The cross product of two vectors results in a vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8433773954889232
      }
    ]
  },
  {
    "representative_text": "Magnitude (Length) of a Vector: The magnitude of a vector v is the distance from the origin to the point (x, y, z) that represents the vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Magnitude (Length) of a Vector: The magnitude of a vector v is the distance from the origin to the point (x, y, z) that represents the vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rotation Matrix: A rotation matrix R is a matrix that represents a rotation of a vector v by an angle θ.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Rotation Matrix: A rotation matrix R is a matrix that represents a rotation of a vector v by an angle θ.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Reflection Matrix: A reflection matrix F is a matrix that represents a reflection of a vector v across a plane.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Reflection Matrix: A reflection matrix F is a matrix that represents a reflection of a vector v across a plane.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Definition: A quaternion is a mathematical object that consists of four components: a scalar and three vector components.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Definition: A quaternion is a mathematical object that consists of four components: a scalar and three vector components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quaternionic Rotation: A quaternionic rotation is a rotation of a vector v using a quaternion q.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quaternionic Rotation: A quaternionic rotation is a rotation of a vector v using a quaternion q.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Point in 3D Space: A point in 3D space is a mathematical object that consists of three coordinates (x, y, z).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Point in 3D Space: A point in 3D space is a mathematical object that consists of three coordinates (x, y, z).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Line in 3D Space: A line in 3D space is a mathematical object that consists of two points (x1, y1, z1) and (x2, y2, z2).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Line in 3D Space: A line in 3D space is a mathematical object that consists of two points (x1, y1, z1) and (x2, y2, z2).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Plane in 3D Space: A plane in 3D space is a mathematical object that consists of three points (x1, y1, z1), (x2, y2, z2), and (x3, y3, z3).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Plane in 3D Space: A plane in 3D space is a mathematical object that consists of three points (x1, y1, z1), (x2, y2, z2), and (x3, y3, z3).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Intersection of Two Lines: The intersection of two lines in 3D space is a point that lies on both lines.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Intersection of Two Lines: The intersection of two lines in 3D space is a point that lies on both lines.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Intersection of Two Planes: The intersection of two planes in 3D space is a line that lies on both planes.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Intersection of Two Planes: The intersection of two planes in 3D space is a line that lies on both planes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lighting Equation: The lighting equation is a mathematical equation that represents the way light interacts with an object in 3D space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Lighting Equation: The lighting equation is a mathematical equation that represents the way light interacts with an object in 3D space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Physics-Based Rendering Equation: The physics-based rendering equation is a mathematical equation that represents the way light interacts with objects in 3D space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8956204138198902
      },
      {
        "text": "Global Illumination Equation: The global illumination equation is a mathematical equation that represents the way light interacts with an object in 3D space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9139713599957975
      },
      {
        "text": "Light Transport Equation: The light transport equation is a mathematical equation that represents the way light interacts with an object in 3D space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9141274930966548
      }
    ]
  },
  {
    "representative_text": "Shading Equation: The shading equation is a mathematical equation that represents the way an object is shaded based on the lighting equation.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Shading Equation: The shading equation is a mathematical equation that represents the way an object is shaded based on the lighting equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Collision Detection Equation: The collision detection equation is a mathematical equation that represents the way two objects in 3D space interact.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Collision Detection Equation: The collision detection equation is a mathematical equation that represents the way two objects in 3D space interact.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Collision Response Equation: The collision response equation is a mathematical equation that represents the way an object responds to a collision.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Collision Response Equation: The collision response equation is a mathematical equation that represents the way an object responds to a collision.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Game Engine: A game engine is a software framework that provides the tools and infrastructure for building games.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Game Engine: A game engine is a software framework that provides the tools and infrastructure for building games.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Graphics Pipeline: The graphics pipeline is a sequence of stages that transform 3D models into 2D images that can be displayed on a screen.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Graphics Pipeline: The graphics pipeline is a sequence of stages that transform 3D models into 2D images that can be displayed on a screen.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Animation System: An animation system is a software framework that provides the tools and infrastructure for creating and playing back animations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Animation System: An animation system is a software framework that provides the tools and infrastructure for creating and playing back animations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Data Visualization: Data visualization is the process of creating graphical representations of data to help understand and communicate insights.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Data Visualization: Data visualization is the process of creating graphical representations of data to help understand and communicate insights.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Statistical Analysis: Statistical analysis is the process of using mathematical techniques to analyze and interpret data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Statistical Analysis: Statistical analysis is the process of using mathematical techniques to analyze and interpret data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Machine Learning: Machine learning is the process of using mathematical techniques to train models on data and make predictions or decisions.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Machine Learning: Machine learning is the process of using mathematical techniques to train models on data and make predictions or decisions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Image Processing: Image processing is the process of applying mathematical techniques to images to extract features and perform tasks such as object detection and recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Image Processing: Image processing is the process of applying mathematical techniques to images to extract features and perform tasks such as object detection and recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Object Recognition: Object recognition is the process of identifying objects in an image or video stream.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Object Recognition: Object recognition is the process of identifying objects in an image or video stream.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tracking: Tracking is the process of following the motion of objects in a video stream.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tracking: Tracking is the process of following the motion of objects in a video stream.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scene Rendering: Scene rendering is the process of creating a 2D image of a 3D scene using physics-based rendering.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scene Rendering: Scene rendering is the process of creating a 2D image of a 3D scene using physics-based rendering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem of Linear Independence: If a set of vectors is linearly independent, then the span of that set is a subspace of the vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem of Linear Independence: If a set of vectors is linearly independent, then the span of that set is a subspace of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem of Principal Component Analysis (PCA): The principal components of a dataset are the orthonormal vectors that maximize the variance of the data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem of Principal Component Analysis (PCA): The principal components of a dataset are the orthonormal vectors that maximize the variance of the data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "t-Distributed Stochastic Neighbor Embedding (t-SNE): A dimensionality reduction technique that maps high-dimensional data to a lower-dimensional space using non-linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "t-Distributed Stochastic Neighbor Embedding (t-SNE): A dimensionality reduction technique that maps high-dimensional data to a lower-dimensional space using non-linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Regression: A regression technique that predicts a continuous output variable based on one or more input features.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Regression: A regression technique that predicts a continuous output variable based on one or more input features.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Regression: A linear algebra-based technique for modeling the relationship between a dependent variable and one or more independent variables, which is widely used in data analysis and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8135764860731001
      },
      {
        "text": "Linear regression: A statistical technique for estimating the relationship between a dependent variable and one or more independent variables, which can be used to model real-world phenomena.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8700147010558792
      },
      {
        "text": "Linear regression is a statistical method for predicting a continuous output variable based on one or more input variables.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8859173372874405
      }
    ]
  },
  {
    "representative_text": "Logistic Regression: A regression technique that predicts a binary output variable based on one or more input features.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Logistic Regression: A regression technique that predicts a binary output variable based on one or more input features.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "K-Means Clustering: A clustering technique that partitions data into K clusters based on similarity.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "K-Means Clustering: A clustering technique that partitions data into K clusters based on similarity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Neural Networks: A machine learning model that uses multiple layers of interconnected nodes (neurons) to learn and represent complex patterns in data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Neural Networks: A machine learning model that uses multiple layers of interconnected nodes (neurons) to learn and represent complex patterns in data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Deep Learning: A subfield of machine learning that uses neural networks with multiple layers to learn and represent complex patterns in data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8595743061383551
      }
    ]
  },
  {
    "representative_text": "Convolutional Neural Networks (CNNs): A type of neural network that uses convolutional and pooling layers to learn and represent patterns in image data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Convolutional Neural Networks (CNNs): A type of neural network that uses convolutional and pooling layers to learn and represent patterns in image data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Recurrent Neural Networks (RNNs): A type of neural network that uses recurrent connections to learn and represent patterns in sequential data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Recurrent Neural Networks (RNNs): A type of neural network that uses recurrent connections to learn and represent patterns in sequential data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Support Vector Machines (SVMs): A machine learning model that uses a kernel function to classify and regress data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Support Vector Machines (SVMs): A machine learning model that uses a kernel function to classify and regress data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Support Vector Machines (SVMs) with Non-Linear Kernels: A type of kernel method that uses non-linear kernels to perform SVMs, which can be used for classification and regression tasks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8368083278185625
      }
    ]
  },
  {
    "representative_text": "Gradient Boosting: A machine learning model that uses an ensemble of decision trees to classify and regress data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gradient Boosting: A machine learning model that uses an ensemble of decision trees to classify and regress data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scatter Plots: A type of plot that displays the relationship between two variables.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Scatter Plots: A type of plot that displays the relationship between two variables.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Heatmaps: A type of plot that displays the correlation between two variables.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8596582683669021
      }
    ]
  },
  {
    "representative_text": "Bar Charts: A type of plot that displays the distribution of a variable.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Bar Charts: A type of plot that displays the distribution of a variable.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Histograms: A type of plot that displays the distribution of a variable.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8906975040812605
      }
    ]
  },
  {
    "representative_text": "Network Diagrams: A type of plot that displays the relationships between variables.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Network Diagrams: A type of plot that displays the relationships between variables.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Real and Complex Numbers:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Real and Complex Numbers:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Real numbers: a set of numbers that can be represented on the real number line.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Real numbers: a set of numbers that can be represented on the real number line.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Complex numbers: a set of numbers that can be represented as a + bi, where a and b are real numbers and i is the imaginary unit (i = √(-1)).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Complex numbers: a set of numbers that can be represented as a + bi, where a and b are real numbers and i is the imaginary unit (i = √(-1)).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Complex conjugate: the conjugate of a complex number a + bi is a - bi.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Complex conjugate: the conjugate of a complex number a + bi is a - bi.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Modulus (or magnitude): the distance of a complex number from the origin, calculated as |a + bi| = √(a^2 + b^2).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Modulus (or magnitude): the distance of a complex number from the origin, calculated as |a + bi| = √(a^2 + b^2).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector Operations:",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Vector Operations:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Operations on vectors:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9260727253538863
      },
      {
        "text": "Vector addition and scalar multiplication are defined.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8503209196960604
      }
    ]
  },
  {
    "representative_text": "Vector subtraction: the difference of two vectors is the vector obtained by subtracting their corresponding components.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector subtraction: the difference of two vectors is the vector obtained by subtracting their corresponding components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cross product (or outer product): the product of two vectors is a vector calculated as the sum of the products of their corresponding components, taken two at a time.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cross product (or outer product): the product of two vectors is a vector calculated as the sum of the products of their corresponding components, taken two at a time.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Equations:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Equations:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A linear transformation is said to be invertible if it has an inverse, which is a linear transformation that \"reverses\" the original transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "A linear transformation is said to be invertible if it has an inverse, which is a linear transformation that \"reverses\" the original transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations: A linear transformation T: V → W is said to be invertible if there exists a linear transformation S: W → V such that TS = ST = I, where I is the identity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8424694852938243
      },
      {
        "text": "Invertibility of Linear Transformations: A linear transformation T is invertible if and only if its null space is trivial, i.e., the only vector x such that Tx = 0 is x = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.861603610963434
      },
      {
        "text": "The Invertibility of Linear Transformations: A linear transformation is invertible if and only if it is both injective and surjective. This is a crucial concept in understanding the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8664011666541758
      },
      {
        "text": "Determinant and Invertibility: A linear transformation T from a vector space V to a vector space W is invertible if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8529843802961115
      },
      {
        "text": "The Inverse of a Linear Transformation: The inverse of a linear transformation T, denoted as T^-1, is a linear transformation from the image of T to the original space V, such that T^-1 ∘ T = I, where I is the identity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8510845691295916
      },
      {
        "text": "Invertibility of Linear Transformations: A linear transformation T from a finite-dimensional vector space V to itself is invertible if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9276171579263166
      },
      {
        "text": "Theorem on the Invertibility of a Linear Transformation: This theorem states that a linear transformation T from a vector space V to itself is invertible if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8625559577015116
      },
      {
        "text": "The Inverse of a Linear Transformation: This theorem states that if a linear transformation $T$ is invertible, then there exists a unique linear transformation $T^{-1}$ such that $T^{-1} \\circ T = I$ and $T \\circ T^{-1} = I$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8802237698831165
      },
      {
        "text": "The Linear Operator Invertibility Criterion: This criterion states that a linear transformation $T$ is invertible if and only if it is both injective and surjective.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572867224095317
      }
    ]
  },
  {
    "representative_text": "Fourier Analysis:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Fourier Analysis:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Fourier Analysis: a class of algorithms for analyzing signals using the Fourier transform.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8126708886793759
      },
      {
        "text": "Fourier Analysis with Linear Algebra: A class of algorithms for analyzing signals using the Fourier transform and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8327588734008984
      }
    ]
  },
  {
    "representative_text": "The Fourier transform is a mathematical tool for decomposing a function into its constituent frequencies.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Fourier transform is a mathematical tool for decomposing a function into its constituent frequencies.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Fourier transform of a signal is a function that describes the distribution of frequencies in the signal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9294878772093235
      },
      {
        "text": "The Fourier transform inversion formula is a mathematical tool for reconstructing a signal from its frequency representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8734125118742563
      }
    ]
  },
  {
    "representative_text": "The inverse Fourier transform is used to reconstruct the original signal from its frequency representation.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The inverse Fourier transform is used to reconstruct the original signal from its frequency representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The formula states that the original signal can be reconstructed by taking the inverse Fourier transform of the frequency representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8656135668187757
      }
    ]
  },
  {
    "representative_text": "Filtering:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Filtering:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A filter is a linear transformation that removes unwanted frequencies from a signal.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A filter is a linear transformation that removes unwanted frequencies from a signal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A low-pass filter removes high-frequency components from a signal, while a high-pass filter removes low-frequency components.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A low-pass filter removes high-frequency components from a signal, while a high-pass filter removes low-frequency components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Convolution:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Convolution:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Convolution is a linear transformation that combines two signals by sliding one signal over the other and calculating the point-wise sum of the resulting signals.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Convolution is a linear transformation that combines two signals by sliding one signal over the other and calculating the point-wise sum of the resulting signals.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The convolution of two signals is denoted by the symbol ∗.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The convolution of two signals is denoted by the symbol ∗.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Image Representation:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Image Representation:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "An image is a two-dimensional representation of a three-dimensional scene, often represented as a matrix of pixels.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "An image is a two-dimensional representation of a three-dimensional scene, often represented as a matrix of pixels.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Each pixel is typically represented by a vector of color values (red, green, blue).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Each pixel is typically represented by a vector of color values (red, green, blue).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Image Filtering:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Image Filtering:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Image filtering is the application of filters to an image to enhance or modify its features.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.835888814678494
      }
    ]
  },
  {
    "representative_text": "Common image filters include the Gaussian filter, the Sobel filter, and the Laplacian filter.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Common image filters include the Gaussian filter, the Sobel filter, and the Laplacian filter.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Image Compression:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Image Compression:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Image compression is the process of reducing the amount of data required to represent an image.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Image compression is the process of reducing the amount of data required to represent an image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Common image compression algorithms include the Discrete Cosine Transform (DCT) and the Wavelet Transform.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Common image compression algorithms include the Discrete Cosine Transform (DCT) and the Wavelet Transform.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fourier Transform Inversion Formula:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Fourier Transform Inversion Formula:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Optimization: The process of finding the best solution among a set of possible solutions, subject to certain constraints and objectives.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Optimization: The process of finding the best solution among a set of possible solutions, subject to certain constraints and objectives.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Control Systems: The study of systems that can be controlled and manipulated to achieve a desired outcome or behavior.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Control Systems: The study of systems that can be controlled and manipulated to achieve a desired outcome or behavior.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Farkas' Lemma: A statement that characterizes the solvability of linear programs and the existence of solutions to linear inequalities.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Farkas' Lemma: A statement that characterizes the solvability of linear programs and the existence of solutions to linear inequalities.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Strong Duality Theorem: A statement that establishes the equivalence between the optimal values of a linear program and the optimal values of its dual.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Strong Duality Theorem: A statement that establishes the equivalence between the optimal values of a linear program and the optimal values of its dual.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kuhn-Tucker Theorems: A set of theorems that provide necessary and sufficient conditions for the optimality of solutions to linear programming problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kuhn-Tucker Theorems: A set of theorems that provide necessary and sufficient conditions for the optimality of solutions to linear programming problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Carathéodory's Theorem: A statement that characterizes the convex hull of a set of points in n-dimensional space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Carathéodory's Theorem: A statement that characterizes the convex hull of a set of points in n-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Carathéodory's Theorem: A theorem that states that a point in a convex set can be represented as a convex combination of a finite number of points in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9028537523105338
      }
    ]
  },
  {
    "representative_text": "Bilinear Control Theory: A framework for analyzing and designing control systems that involve bilinear systems, which are systems where the control input and state variables are related by a bilinear relationship.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Bilinear Control Theory: A framework for analyzing and designing control systems that involve bilinear systems, which are systems where the control input and state variables are related by a bilinear relationship.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bilinear System Theory: A branch of linear algebra that studies the behavior of bilinear systems, which are systems where the control input and state variables are related by a bilinear relationship.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8829733963258847
      },
      {
        "text": "Bilinear Control Theory: a framework for analyzing and designing control systems that involve bilinear systems, which are systems where the control input and state variables are related by a bilinear relationship.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9032305679022185
      }
    ]
  },
  {
    "representative_text": "Linear Quadratic Regulator (LQR) Theorem: A theorem that provides a necessary and sufficient condition for the optimality of a linear quadratic regulator (LQR) control law.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Quadratic Regulator (LQR) Theorem: A theorem that provides a necessary and sufficient condition for the optimality of a linear quadratic regulator (LQR) control law.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Quadratic Regulator (LQR) Algorithm: An algorithm for designing a control law that optimizes a linear quadratic regulator (LQR) objective function.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8095917245532211
      },
      {
        "text": "Linear-Quadratic-Regulator (LQR) Theory: A theory that provides a framework for designing and analyzing control systems using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8650032202860913
      }
    ]
  },
  {
    "representative_text": "Kalman Filter Theorem: A theorem that provides a necessary and sufficient condition for the optimality of a Kalman filter, which is a widely used estimator for linear systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kalman Filter Theorem: A theorem that provides a necessary and sufficient condition for the optimality of a Kalman filter, which is a widely used estimator for linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lyapunov Theorem: A theorem that establishes the existence and uniqueness of solutions to linear systems with a specific stability property.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lyapunov Theorem: A theorem that establishes the existence and uniqueness of solutions to linear systems with a specific stability property.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gradient Descent: An algorithm for minimizing a function by iteratively updating the parameters of a model.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Gradient Descent: An algorithm for minimizing a function by iteratively updating the parameters of a model.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Newton's Method: An algorithm for minimizing a function by iteratively updating the parameters of a model using the gradient and Hessian of the function.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8535250169997712
      }
    ]
  },
  {
    "representative_text": "Quasi-Newton Methods: A class of algorithms for minimizing a function by iteratively updating the parameters of a model using an approximation of the Hessian of the function.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quasi-Newton Methods: A class of algorithms for minimizing a function by iteratively updating the parameters of a model using an approximation of the Hessian of the function.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Conjugate Gradient Method: An algorithm for solving systems of linear equations using a conjugate gradient approach.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Conjugate Gradient Method: An algorithm for solving systems of linear equations using a conjugate gradient approach.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Conjugate Gradient Method",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8143377505247757
      },
      {
        "text": "Conjugate Gradient Method: An iterative method for solving systems of linear equations, used in various applications such as optimization and control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8594058802208626
      },
      {
        "text": "Conjugate Gradient Method: An iterative algorithm for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9105933220555666
      }
    ]
  },
  {
    "representative_text": "Kalman Filter Algorithm: An algorithm for estimating the state of a linear system using a Kalman filter.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Kalman Filter Algorithm: An algorithm for estimating the state of a linear system using a Kalman filter.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Kalman Filter Variants: Variants of the Kalman filter algorithm that can be used to estimate the state of a linear system in the presence of uncertainty or noise.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8914969151922558
      }
    ]
  },
  {
    "representative_text": "pole placement algorithm: An algorithm for designing a control system by placing the poles of the system at desired locations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "pole placement algorithm: An algorithm for designing a control system by placing the poles of the system at desired locations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Encryption: The process of converting plaintext into ciphertext using an encryption algorithm.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Encryption: The process of converting plaintext into ciphertext using an encryption algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Decryption: The process of converting ciphertext back into plaintext using a decryption algorithm.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Decryption: The process of converting ciphertext back into plaintext using a decryption algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Ciphertext: The encrypted data that is unreadable without the decryption algorithm.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Ciphertext: The encrypted data that is unreadable without the decryption algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Plaintext: The original, unencrypted data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Plaintext: The original, unencrypted data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Key: A secret value used to encrypt or decrypt data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Key: A secret value used to encrypt or decrypt data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hash function: A one-way function that maps input data to a fixed-size output, often used in digital signatures.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Hash function: A one-way function that maps input data to a fixed-size output, often used in digital signatures.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Hash functions: A method for verifying the integrity of data by computing a fixed-size hash value.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8370329806752304
      }
    ]
  },
  {
    "representative_text": "Digital signature: A type of encryption that verifies the authenticity and integrity of data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Digital signature: A type of encryption that verifies the authenticity and integrity of data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Symmetric-key algorithm: An algorithm that uses the same key for both encryption and decryption.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Symmetric-key algorithm: An algorithm that uses the same key for both encryption and decryption.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Asymmetric-key algorithm: An algorithm that uses a pair of keys (public and private) for encryption and decryption.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Asymmetric-key algorithm: An algorithm that uses a pair of keys (public and private) for encryption and decryption.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Public-key cryptography: A method for encrypting and decrypting data using a pair of keys (public and private).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8234341206638014
      }
    ]
  },
  {
    "representative_text": "The Shannon-Hartley Theorem: A fundamental theorem in information theory that relates the capacity of a communication channel to the bandwidth and signal-to-noise ratio.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Shannon-Hartley Theorem: A fundamental theorem in information theory that relates the capacity of a communication channel to the bandwidth and signal-to-noise ratio.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Gilbert-Varshamov Bound: A bound on the minimum distance between codewords in a linear code.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Gilbert-Varshamov Bound: A bound on the minimum distance between codewords in a linear code.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Singleton Bound: A bound on the minimum distance between codewords in a linear code.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Singleton Bound: A bound on the minimum distance between codewords in a linear code.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Kruskal's Theorem: A theorem that states that the minimum distance between codewords in a linear code is equal to the minimum weight of a codeword.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Kruskal's Theorem: A theorem that states that the minimum distance between codewords in a linear code is equal to the minimum weight of a codeword.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Berlekamp-Massey Algorithm: An algorithm for finding the minimum weight of a codeword in a linear code.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Berlekamp-Massey Algorithm: An algorithm for finding the minimum weight of a codeword in a linear code.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Viterbi Algorithm: An algorithm for finding the most likely sequence of codewords in a linear code.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Viterbi Algorithm: An algorithm for finding the most likely sequence of codewords in a linear code.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear programming: An algorithm for solving optimization problems using linear equations and inequalities.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear programming: An algorithm for solving optimization problems using linear equations and inequalities.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Programming: Linear programming is a method for optimizing a linear function subject to linear constraints.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8832585404490245
      }
    ]
  },
  {
    "representative_text": "Secure multi-party computation: A method for performing computations on private data without revealing the data itself.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Secure multi-party computation: A method for performing computations on private data without revealing the data itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Homomorphic encryption: A method for performing computations on encrypted data without decrypting it first.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8187957411112883
      }
    ]
  },
  {
    "representative_text": "Digital signatures: A method for verifying the authenticity and integrity of data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Digital signatures: A method for verifying the authenticity and integrity of data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear independence and span: A way to analyze the properties of vectors and matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear independence and span: A way to analyze the properties of vectors and matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8373855525533828
      },
      {
        "text": "Properties of linear independence:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8727010872497103
      },
      {
        "text": "Properties of linear dependence:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8962884029705307
      }
    ]
  },
  {
    "representative_text": "QR decomposition and SVD: Algorithms for decomposing matrices into products of orthogonal and triangular matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "QR decomposition and SVD: Algorithms for decomposing matrices into products of orthogonal and triangular matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The QR Decomposition of a Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8929880545380104
      },
      {
        "text": "QR Decomposition of a Matrix",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9315369049184207
      },
      {
        "text": "SVD of a Matrix",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8339494485461674
      },
      {
        "text": "Singular Values",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8373987747866055
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.907811496972809
      },
      {
        "text": "Shur decomposition",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8507184910089944
      },
      {
        "text": "Polar decomposition",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732305321975524
      },
      {
        "text": "Orthogonalization of Matrices using the QR Decomposition: Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8693742869754261
      },
      {
        "text": "The SVD for linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8081518310377883
      }
    ]
  },
  {
    "representative_text": "Dimension and Dimensionality:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Dimension and Dimensionality:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of dimensionality, including the relationship between dimension and the number of basis vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8316786868696547
      },
      {
        "text": "Dimensionality:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8696212711458764
      },
      {
        "text": "Dimensionality",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9380161190443104
      }
    ]
  },
  {
    "representative_text": "Orthogonality Properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Orthogonality Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthonormality",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.888104678780743
      },
      {
        "text": "Properties of orthogonality:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9257324965165237
      }
    ]
  },
  {
    "representative_text": "Pythagorean Theorem:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pythagorean Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Vectors:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Orthogonal Vectors:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthonormal Vectors:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9281387836418693
      },
      {
        "text": "Orthogonal Matrices and Vectors:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9066155322572265
      }
    ]
  },
  {
    "representative_text": "Rank of a Linear Transformation with Infinite-Dimensional Domain or Codomain: The rank of a linear transformation with an infinite-dimensional domain or codomain can be difficult to determine directly. However, the rank-nullity theorem can still be applied by considering the dimension of the domain or codomain and the rank of the transformation on a finite-dimensional subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 60,
    "detailed_sources": [
      {
        "text": "Rank of a Linear Transformation with Infinite-Dimensional Domain or Codomain: The rank of a linear transformation with an infinite-dimensional domain or codomain can be difficult to determine directly. However, the rank-nullity theorem can still be applied by considering the dimension of the domain or codomain and the rank of the transformation on a finite-dimensional subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Nullity of a Linear Transformation with Infinite-Dimensional Domain or Codomain: Similar to the rank, the nullity of a linear transformation with an infinite-dimensional domain or codomain can be challenging to determine directly. However, the nullity can be related to the dimension of the domain or codomain and the nullity of the transformation on a finite-dimensional subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8974413002170438
      },
      {
        "text": "Linear Transformations on Infinite-Dimensional Vector Spaces: The rank-nullity theorem may not directly apply to linear transformations on infinite-dimensional vector spaces. In such cases, other theorems or techniques, such as the use of finite-dimensional subspaces or the study of the transformation's properties on a specific basis, may be necessary.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8827099180359763
      },
      {
        "text": "Rank-Nullity Theorem for Linear Transformations with a Non-Orthogonal Basis: The rank-nullity theorem may not directly apply to linear transformations with a non-orthogonal basis. In such cases, other theorems or techniques, such as the use of orthogonal projections or the study of the transformation's properties on a specific basis, may be necessary.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8298534391659491
      },
      {
        "text": "Rank-Nullity Theorem for Infinite-Dimensional Spaces: Although the Rank-Nullity Theorem is often discussed for finite-dimensional spaces, there are extensions to infinite-dimensional spaces that can provide valuable insights.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8398254752525715
      },
      {
        "text": "Linear Independence and Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces: The rank-nullity theorem still holds for infinite-dimensional vector spaces, but the dimensions of the kernel and range are not necessarily finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.804814267329299
      },
      {
        "text": "Linear Transformations on Vector Spaces with Non-Standard Structures: The rank-nullity theorem may need to be adapted or generalized to linear transformations on vector spaces with non-standard structures, such as non-orthogonal bases or non-standard inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8874553835191736
      },
      {
        "text": "Rank and Nullity of Linear Transformations with Infinite-Dimensional Domain or Codomain: The rank-nullity theorem may not directly apply to linear transformations with an infinite-dimensional domain or codomain. In such cases, other theorems or techniques, such as the use of finite-dimensional subspaces or the study of the transformation's properties on a specific basis, may be necessary.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9621181164747951
      },
      {
        "text": "Dimension of the Null Space: The Rank-Nullity Theorem relates the rank of a linear transformation to the dimensions of the range and null space. However, it does not provide a direct formula or theorem for the dimension of the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8342006106417305
      },
      {
        "text": "Dimension of the Range of a Linear Transformation: The Rank-Nullity Theorem states that the rank of a linear transformation is equal to the dimension of the range minus the dimension of the null space. However, it does not provide a direct formula or theorem for the dimension of the range.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8571259579655064
      },
      {
        "text": "Rank-Nullity Theorem Variations: While the rank-nullity theorem is mentioned, its variations and applications, such as the rank-nullity theorem for linear transformations or for matrices with non-square dimensions, could be explored further.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.847593294106757
      },
      {
        "text": "Rank-Nullity Theorem for Vector Spaces with a Non-Trivial Null Space: We need to explore the implications of a non-trivial null space on the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8390583384138876
      },
      {
        "text": "Extremal Linear Transformations and the Rank-Nullity Theorem: This could involve exploring the relationship between extremal linear transformations and the rank-nullity theorem, including how the extremal properties of the transformation can be used to determine its rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8060590924944802
      },
      {
        "text": "Rank-Nullity Theorem for Linear Transformations with a Non-Finite-Dimensional Domain or Codomain: The rank-nullity theorem can be generalized to linear transformations with a non-finite-dimensional domain or codomain, which is essential in understanding the behavior of linear transformations in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8754395030571893
      },
      {
        "text": "Linear Transformations with Non-Trivial Kernel or Image and the Rank-Nullity Theorem: This theorem states that for a linear transformation T: V -> W, the rank of T plus the nullity of T is equal to the dimension of V. However, when the kernel or image is non-trivial, the rank-nullity theorem may not hold.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8599531139188061
      },
      {
        "text": "Linear Transformations on Banach Spaces: The concept of analyzing the rank and nullity of linear transformations on Banach spaces, which is essential in understanding the properties of linear transformations in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8439530607767309
      },
      {
        "text": "Linear Transformations and Operator Theory: The concept of analyzing the rank and nullity of linear transformations in the context of operator theory, which is essential in understanding the properties of linear transformations in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8380247384207432
      },
      {
        "text": "Linear Transformations and Differential Geometry: The concept of analyzing the rank and nullity of linear transformations in the context of differential geometry, which is essential in understanding the properties of linear transformations in higher-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.823110577247175
      },
      {
        "text": "Rank-Nullity Theorem and Infinite-Dimensional Vector Spaces: The rank-nullity theorem provides a fundamental connection between the rank of a linear transformation, the nullity of the transformation, and the dimension of the original vector space. This theorem has important implications for linear algebra and its applications in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8709679705425444
      },
      {
        "text": "The Rank of a Linear Transformation: This concept is related to the rank-nullity theorem and states that the rank of a linear transformation $T$ is equal to the dimension of its image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8728173401310078
      },
      {
        "text": "Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces: The rank-nullity theorem for infinite-dimensional vector spaces may not be applicable, as the dimension of the null space may not be finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8789403149168851
      },
      {
        "text": "The Rank-Nullity Theorem for Infinite-Dimensional Spaces: This theorem generalizes the rank-nullity theorem to infinite-dimensional spaces and is essential in understanding the behavior of linear operators in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8328674743079569
      },
      {
        "text": "Rank and Nullity of Linear Transformations with Non-Standard Nilpotency: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard nilpotency, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8893065310478254
      },
      {
        "text": "Linear Transformations on Fractal Spaces with Non-Standard Dimensions: The rank-nullity theorem can be used to analyze the properties of linear transformations on fractal spaces with non-standard dimensions, which is essential in understanding the behavior of linear transformations in fractal spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8752170976812124
      },
      {
        "text": "Linear Transformations on Banach Spaces with Non-Standard Boundedness: The rank-nullity theorem can be used to analyze the properties of linear transformations on Banach spaces with non-standard boundedness, which is essential in understanding the behavior of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921483673549517
      },
      {
        "text": "Linear Transformations on Hilbert Spaces with Non-Standard Inner Products: The rank-nullity theorem can be used to analyze the properties of linear transformations on Hilbert spaces with non-standard inner products, which is essential in understanding the behavior of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9014277984107277
      },
      {
        "text": "Linear Transformations on Topological Spaces with Non-Standard Topology: The rank-nullity theorem can be used to analyze the properties of linear transformations on topological spaces with non-standard topology, which is essential in understanding the behavior of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8909855121586687
      },
      {
        "text": "Rank and Nullity of Linear Transformations with Non-Standard Algebraic Structures: The rank-nullity theorem can be used to analyze the properties of linear transformations with non-standard algebraic structures, which is essential in understanding the behavior of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9236637008266559
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Infinite-Dimensional Spaces: The rank-nullity theorem may need to be adapted or generalized to linear transformations on infinite-dimensional spaces, which can be used to analyze the properties of linear transformations in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9525527773486224
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Non-Orthogonal Bases: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-orthogonal bases, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9299976452627168
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Non-Standard Inner Products: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard inner products, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9229585440846304
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Non-Standard Metrics: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard metrics, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9188724716982141
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Non-Standard Algebraic Structures: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard algebraic structures, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.936191551349115
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Non-Standard Topology: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard topology, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9170572865065323
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Non-Standard Homology: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard homology, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9183965627320412
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Non-Standard Spectral Theory: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard spectral theory, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9251145891087569
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Non-Standard Measure Theory: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard measure theory, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9306146445459795
      },
      {
        "text": "Linear Transformations and the Rank-Nullity Theorem for Non-Standard Fractal Geometry: The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard fractal geometry, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9200102385403129
      },
      {
        "text": "Linear Transformations on Vector Spaces with Non-Standard Inner Products (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations with non-standard inner products, such as non-orthogonal bases or non-standard inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8818758687073407
      },
      {
        "text": "Linear Transformations on Riemannian Manifolds with Non-Standard Metrics (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on Riemannian manifolds with non-standard metrics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8619551776616518
      },
      {
        "text": "Linear Transformations on Fractal Spaces with Non-Standard Dimensions (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on fractal spaces with non-standard dimensions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9065952542320266
      },
      {
        "text": "Linear Transformations on Banach Spaces with Non-Standard Norms (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on Banach spaces with non-standard norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9007476560612275
      },
      {
        "text": "Linear Transformations on Hilbert Spaces with Non-Standard Inner Products (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on Hilbert spaces with non-standard inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8991476099501841
      },
      {
        "text": "Linear Transformations on Infinite-Dimensional Vector Spaces with Non-Standard Bases (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on infinite-dimensional vector spaces with non-standard bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9212076832188446
      },
      {
        "text": "Linear Transformations on Topological Spaces with Non-Standard Topology (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on topological spaces with non-standard topology.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8882869194435912
      },
      {
        "text": "Linear Transformations on Algebraic Topology with Non-Standard Algebraic Structures (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on algebraic topology with non-standard algebraic structures.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8688573706585329
      },
      {
        "text": "Linear Transformations on Measure Spaces with Non-Standard Measure Theory (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on measure spaces with non-standard measure theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8867465512249412
      },
      {
        "text": "Linear Transformations on Fractal Geometry with Non-Standard Fractal Properties (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on fractal geometry with non-standard fractal properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8907089042440853
      },
      {
        "text": "Linear Transformations on Group Theory with Non-Standard Group Actions (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on group theory with non-standard group actions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8586222993879631
      },
      {
        "text": "Linear Transformations on Representation Theory with Non-Standard Representation Spaces (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on representation theory with non-standard representation spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8997293117311748
      },
      {
        "text": "Linear Transformations on Operator Algebras with Non-Standard Operator Spaces (Alternative Form): The rank-nullity theorem may need to be adapted or generalized to linear transformations on operator algebras with non-standard operator spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9217810603706955
      },
      {
        "text": "Linear Transformation and Infinite Dimensionality: Exploring the properties of linear transformations on infinite-dimensional vector spaces, including the Rank-Nullity Theorem for infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8910910092836878
      },
      {
        "text": "Rank and Nullity for Linear Transformations with Degenerate Image or Kernel in Infinite-Dimensional Spaces: This is a subtle nuance that needs to be addressed, as the rank-nullity theorem may not directly apply to linear transformations on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9024525365821052
      },
      {
        "text": "Linear Transformations on Banach Spaces with Non-Standard Norms and Boundedness: This is a new area of study that requires an extension of the rank-nullity theorem to Banach spaces with non-standard norms and boundedness.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8634625399619117
      },
      {
        "text": "Rank and Nullity for Linear Transformations with Non-Standard Inner Products in Banach Spaces: This is a subtle nuance that needs to be addressed, as the rank-nullity theorem may not directly apply to linear transformations on Banach spaces with non-standard inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8885936114872957
      },
      {
        "text": "Linear Transformations on Fractal Spaces with Non-Standard Dimensions and Fractal Properties: This is a new area of study that requires an extension of the rank-nullity theorem to fractal spaces with non-standard dimensions and fractal properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.835911195458934
      },
      {
        "text": "Linear Transformations on Group Theory with Non-Standard Group Actions and Representation Spaces: This is a new area of study that requires an extension of the rank-nullity theorem to group theory with non-standard group actions and representation spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8153184310148256
      },
      {
        "text": "Linear Transformations on Representation Theory with Non-Standard Representation Spaces and Operator Algebras: This is a new area of study that requires an extension of the rank-nullity theorem to representation theory with non-standard representation spaces and operator algebras.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8396058859185047
      },
      {
        "text": "Linear Transformations on Operator Algebras with Non-Standard Operator Spaces and Measure Theory: This is a new area of study that requires an extension of the rank-nullity theorem to operator algebras with non-standard operator spaces and measure theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8373458757082772
      },
      {
        "text": "Linear Transformations on Measure Theory with Non-Standard Measure Spaces and Fractal Geometry: This is a new area of study that requires an extension of the rank-nullity theorem to measure theory with non-standard measure spaces and fractal geometry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.826516715421751
      }
    ]
  },
  {
    "representative_text": "Nullity of a Linear Transformation with a Non-Null Matrix Representation: If the matrix representation of a linear transformation has more than full rank (i.e., more than the dimension of the domain), the nullity of the transformation is equal to the nullity of the matrix representation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Nullity of a Linear Transformation with a Non-Null Matrix Representation: If the matrix representation of a linear transformation has more than full rank (i.e., more than the dimension of the domain), the nullity of the transformation is equal to the nullity of the matrix representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Relationship between Rank and Nullity of a Linear Transformation and the Determinant of its Matrix Representation: The determinant of the matrix representation of a linear transformation is equal to the product of its rank and nullity.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relationship between Rank and Nullity of a Linear Transformation and the Determinant of its Matrix Representation: The determinant of the matrix representation of a linear transformation is equal to the product of its rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations with a Trivial Image: If the image of a linear transformation is trivial (i.e., contains only the zero vector), the rank of the transformation is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations with a Trivial Image: If the image of a linear transformation is trivial (i.e., contains only the zero vector), the rank of the transformation is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Hadamard Product: A method for finding the element-wise product of two matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Hadamard Product: A method for finding the element-wise product of two matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Hadamard Product for 2x2 Matrices: A simplified version of the Hadamard product for 2x2 matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9394654286550992
      }
    ]
  },
  {
    "representative_text": "Cyclic Vectors: A vector v is called cyclic if the set {v, v^2, v^3, ...} is linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Cyclic Vectors: A vector v is called cyclic if the set {v, v^2, v^3, ...} is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cyclic Vectors: A vector v is called cyclic if the set {v, v^2, v^3, ...} is linearly independent. This concept is related to the concept of linear independence and basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8835838857105132
      }
    ]
  },
  {
    "representative_text": "Basis Vectors with Zero Components: A set of vectors {v1, v2, ..., vn} is a basis even if some of the vectors have zero components, but not all.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis Vectors with Zero Components: A set of vectors {v1, v2, ..., vn} is a basis even if some of the vectors have zero components, but not all.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Dependence of Sets of Infinite Vectors: A set of infinite vectors {v1, v2, ...} is linearly dependent if and only if the only linear combination a1v1 + a2v2 + ... = 0 is when at least one coefficient a1, a2, ... = 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Dependence of Sets of Infinite Vectors: A set of infinite vectors {v1, v2, ...} is linearly dependent if and only if the only linear combination a1v1 + a2v2 + ... = 0 is when at least one coefficient a1, a2, ... = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hausdorff Dimension: The Hausdorff dimension of a set of vectors is a measure of its size and complexity, and it can be used to study the properties of vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hausdorff Dimension: The Hausdorff dimension of a set of vectors is a measure of its size and complexity, and it can be used to study the properties of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cayley-Hamilton Theorem: A theorem that states that every square matrix satisfies its own characteristic equation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 20,
    "detailed_sources": [
      {
        "text": "Cayley-Hamilton Theorem: A theorem that states that every square matrix satisfies its own characteristic equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cayley-Hamilton Theorem: This theorem states that every square matrix satisfies its own characteristic equation. This theorem is useful in understanding the properties of eigenvalues and the behavior of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8811505013186703
      },
      {
        "text": "The Cayley-Hamilton Theorem: The Cayley-Hamilton theorem states that every square matrix satisfies its own characteristic equation. This theorem is related to the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9346248118279903
      },
      {
        "text": "The Cayley-Hamilton Theorem: A theorem that states that every square matrix satisfies its own characteristic equation, which can be useful in computing determinants and inverses.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9247986686322998
      },
      {
        "text": "Cayley-Hamilton Theorem: This theorem states that every square matrix satisfies its own characteristic equation. It can be used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9366933690224812
      },
      {
        "text": "The Cayley-Hamilton Theorem: A matrix satisfies its own characteristic polynomial, and this theorem provides a fundamental connection between linear transformations and their representations as matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8961337725565841
      },
      {
        "text": "The Cayley-Hamilton Theorem for Block Matrices: This theorem states that every block matrix satisfies its own characteristic equation, which is a generalization of the Cayley-Hamilton Theorem for square matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8413086552038302
      },
      {
        "text": "Cayley-Hamilton Theorem: This theorem states that every square matrix satisfies its own characteristic equation. This theorem can be used to prove the fundamental theorem of eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9255414225980403
      },
      {
        "text": "The Cayley-Hamilton Theorem: This theorem states that every square matrix satisfies its own characteristic equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9538760977143468
      },
      {
        "text": "The Cayley-Hamilton Theorem for Complex Matrices: This theorem is similar to the one mentioned above but for complex matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8076649328184364
      },
      {
        "text": "The Cayley-Hamilton Theorem and its Implications: The Cayley-Hamilton theorem states that every square matrix satisfies its own characteristic equation. This has implications for the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.934212598562711
      },
      {
        "text": "The Cayley-Hamilton Theorem: A theorem that describes the relationship between the characteristic polynomial of a matrix and its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9095895809479675
      },
      {
        "text": "The Cayley-Hamilton Theorem for Non-Symmetric Matrices: A theorem that describes the relationship between the characteristic polynomial of a non-symmetric matrix and its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8814463663357581
      },
      {
        "text": "The Computation of Eigenvalues using the Cayley-Hamilton Theorem: The Cayley-Hamilton theorem states that every square matrix satisfies its own characteristic equation. This theorem can be used to compute eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8908340078519947
      },
      {
        "text": "Cayley-Hamilton Theorem: This theorem states that every square matrix satisfies its own characteristic equation. This theorem can be used to find the inverse of a matrix by exploiting the relationship between the matrix and its characteristic polynomial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.862820294086734
      },
      {
        "text": "Cayley-Hamilton Theorem: Explore the Cayley-Hamilton theorem, which states that every square matrix satisfies its own characteristic equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9418360147874649
      },
      {
        "text": "Cayley-Hamilton Theorem for determinants: The Cayley-Hamilton Theorem states that every square matrix satisfies its own characteristic equation, which can be used to calculate the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8974562488135032
      },
      {
        "text": "Cayley-Hamilton Theorem and Eigenvalues: The relationship between the Cayley-Hamilton Theorem and eigenvalues, which is a fundamental concept in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8661400296409555
      },
      {
        "text": "Cayley-Hamilton Theorem: A theorem that states that every square matrix satisfies its own characteristic polynomial, which is a fundamental result in linear algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9018152322782094
      },
      {
        "text": "Cayley-Hamilton Theorem: A theorem that states that every square matrix satisfies its own characteristic polynomial.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9279291162444523
      }
    ]
  },
  {
    "representative_text": "Fundamental Theorem of Linear Algebra: A theorem that states that every linear transformation has a basis and that the basis is unique up to scalar multiplication.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Fundamental Theorem of Linear Algebra: A theorem that states that every linear transformation has a basis and that the basis is unique up to scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product Spaces and Orthogonality: In addition to the definition of orthogonality, it's worth noting that orthogonal projections can be used to diagonalize matrices and find eigenvalues and eigenvectors. This involves finding an orthogonal matrix whose columns are eigenvectors of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner Product Spaces and Orthogonality: In addition to the definition of orthogonality, it's worth noting that orthogonal projections can be used to diagonalize matrices and find eigenvalues and eigenvectors. This involves finding an orthogonal matrix whose columns are eigenvectors of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Properties: In addition to the definition of eigenvalues and eigenvectors, it's worth noting that eigenvalues can be real or complex, and eigenvectors can be real or complex. The eigenvectors corresponding to distinct eigenvalues are linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Properties: In addition to the definition of eigenvalues and eigenvectors, it's worth noting that eigenvalues can be real or complex, and eigenvectors can be real or complex. The eigenvectors corresponding to distinct eigenvalues are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue and Eigenvector Properties: Additional properties of eigenvalues and eigenvectors, such as the fact that the eigenvectors of a matrix are orthogonal to each other, and that the eigenvalues of a matrix are the roots of its characteristic equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8204293861899417
      },
      {
        "text": "Eigenvalue and Eigenvector Properties for Complex Eigenvalues: In addition to the definition of eigenvalues and eigenvectors, it's worth noting that complex eigenvalues can be expressed as λ = σ + iτ, where σ and τ are real numbers. The eigenvectors corresponding to complex eigenvalues are also complex vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8604009206312792
      },
      {
        "text": "The Eigenvalue and Eigenvector Properties for Linear Transformations: This includes the properties of eigenvalues and eigenvectors for linear transformations, such as the fact that eigenvalues can be real or complex and eigenvectors can be real or complex.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8482727285315564
      },
      {
        "text": "The Eigenvalue and Eigenvector Properties for Complex Eigenvalues: The study of eigenvalue and eigenvector properties for complex eigenvalues can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8670260236759952
      },
      {
        "text": "Eigenvector Equations with Complex Eigenvalues: The eigenvector equations can be generalized to complex eigenvalues, which requires an understanding of complex linear algebra and the properties of complex matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.801869689420633
      },
      {
        "text": "Eigenvalue and Eigenvector Properties for Complex Eigenvalues: There are many theorems and properties related to complex eigenvalues, such as the fact that complex eigenvalues can be expressed as λ = σ + iτ, where σ and τ are real numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8535276986530853
      },
      {
        "text": "Eigenvector Orthogonality with Complex Eigenvalues: Understanding the properties of eigenvector orthogonality with complex eigenvalues is essential in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8436307658445537
      },
      {
        "text": "Eigenvalue and eigenvector properties: The properties of eigenvalues and eigenvectors, such as the fact that they are scalar and vector, respectively, and that they satisfy the characteristic equation of the matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8291599130696417
      }
    ]
  },
  {
    "representative_text": "Diagonalization and Jordan Canonical Form: A matrix can be diagonalized if and only if it is diagonalizable. Diagonalization is a process of finding a basis of eigenvectors of a matrix and representing the matrix as a diagonal matrix. The Jordan canonical form is a matrix that has the same eigenvalues as the original matrix, but with Jordan blocks instead of diagonal entries.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Diagonalization and Jordan Canonical Form: A matrix can be diagonalized if and only if it is diagonalizable. Diagonalization is a process of finding a basis of eigenvectors of a matrix and representing the matrix as a diagonal matrix. The Jordan canonical form is a matrix that has the same eigenvalues as the original matrix, but with Jordan blocks instead of diagonal entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem of Linear Independence and Span: In addition to Theorem 1, it's worth noting that a set of vectors is linearly independent if and only if the only way to express the zero vector as a linear combination of these vectors is with all coefficients equal to zero. This is equivalent to saying that the vectors are not linearly dependent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Theorem of Linear Independence and Span: In addition to Theorem 1, it's worth noting that a set of vectors is linearly independent if and only if the only way to express the zero vector as a linear combination of these vectors is with all coefficients equal to zero. This is equivalent to saying that the vectors are not linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem of Linear Independence and Span for Complex Vectors: In addition to Theorem 1, it's worth noting that a set of complex vectors is linearly independent if and only if the only way to express the zero vector as a linear combination of these vectors is with all coefficients equal to zero. This is equivalent to saying that the vectors are not linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9306691259516996
      },
      {
        "text": "The Linear Independence and Span of Complex Vectors: This includes the properties of linear independence and span for complex vectors, such as the fact that a set of complex vectors is linearly independent if and only if the only way to express the zero vector as a linear combination of these vectors is with all coefficients equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8018224870255928
      }
    ]
  },
  {
    "representative_text": "Theorem of Basis and Dimension: In addition to Theorem 1, it's worth noting that a set of vectors is a basis for a vector space if and only if it is linearly independent and spans the vector space. This is equivalent to saying that the set of vectors has the maximum possible number of linearly independent vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Theorem of Basis and Dimension: In addition to Theorem 1, it's worth noting that a set of vectors is a basis for a vector space if and only if it is linearly independent and spans the vector space. This is equivalent to saying that the set of vectors has the maximum possible number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem of Basis and Dimension for Complex Vectors: In addition to Theorem 1, it's worth noting that a set of complex vectors is a basis for a complex vector space if and only if it is linearly independent and spans the vector space. This is equivalent to saying that the set of vectors has the maximum possible number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8930616091640744
      }
    ]
  },
  {
    "representative_text": "Theorem of Linear Transformation and Matrix Representation: In addition to Theorem 1, it's worth noting that a linear transformation can be represented by a matrix if and only if the transformation is a linear combination of basis vectors. This is equivalent to saying that the transformation can be represented as a linear combination of basis vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem of Linear Transformation and Matrix Representation: In addition to Theorem 1, it's worth noting that a linear transformation can be represented by a matrix if and only if the transformation is a linear combination of basis vectors. This is equivalent to saying that the transformation can be represented as a linear combination of basis vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Sturm-Liouville Theorem: A theorem that describes the properties of a matrix that can be diagonalized using orthogonal matrices, which is useful in solving systems of linear equations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sturm-Liouville Theorem: A theorem that describes the properties of a matrix that can be diagonalized using orthogonal matrices, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Orthogonal Projection: A theorem that describes the properties of the orthogonal projection of a vector onto a subspace, which is useful in solving systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Orthogonal Projection: A theorem that describes the properties of the orthogonal projection of a vector onto a subspace, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem of the Sum of Orthogonal Projections: A theorem that describes the properties of the sum of orthogonal projections of a vector onto multiple subspaces, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9187921732695709
      },
      {
        "text": "The Procrustean Theorem: A theorem that describes the properties of the orthogonal projection of a vector onto a subspace, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9295413355678097
      }
    ]
  },
  {
    "representative_text": "The Cauchy-Schwarz Inequality: An inequality that describes the relationship between the dot product and the norms of two vectors, which is useful in understanding the properties of inner products.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Cauchy-Schwarz Inequality: An inequality that describes the relationship between the dot product and the norms of two vectors, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Parallelogram Law: A law that describes the relationship between the norms of two vectors and the norm of their sum, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8211810470132674
      },
      {
        "text": "The Bessel's Inequality: An inequality that describes the relationship between the inner product and the norms of a set of vectors, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9009339770382837
      },
      {
        "text": "The Cauchy-Schwarz Inequality for Inner Product Spaces: A theorem that describes the relationship between the dot product and the norms of two vectors in an inner product space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9257848440468146
      }
    ]
  },
  {
    "representative_text": "The Plancherel's Theorem: A theorem that describes the properties of the Fourier transform on an inner product space, which is useful in understanding the properties of inner products.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "The Plancherel's Theorem: A theorem that describes the properties of the Fourier transform on an inner product space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Fourier Transform of an Inner Product Space: A theorem that describes the properties of the Fourier transform on an inner product space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9187149477560073
      },
      {
        "text": "The Fourier Series of an Inner Product Space: A theorem that describes the properties of the Fourier series on an inner product space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9146570623759827
      },
      {
        "text": "The Fourier Integral Theorem: A theorem that describes the properties of the Fourier integral on an inner product space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9477106439529439
      },
      {
        "text": "The Fourier Transform of a Vector: A theorem that describes the properties of the Fourier transform of a vector in an inner product space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.946052032967013
      },
      {
        "text": "The Fourier Series of a Vector: A theorem that describes the properties of the Fourier series of a vector in an inner product space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9379916472389469
      },
      {
        "text": "The Fourier Integral of a Vector: A theorem that describes the properties of the Fourier integral of a vector in an inner product space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.960144515340541
      }
    ]
  },
  {
    "representative_text": "Dimension of a Quotient Space: The dimension of a quotient space is the difference between the dimension of the original vector space and the dimension of the subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension of a Quotient Space: The dimension of a quotient space is the difference between the dimension of the original vector space and the dimension of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Subspace Invariant: A linear transformation that maps a subspace to itself is a subspace invariant.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Subspace Invariant: A linear transformation that maps a subspace to itself is a subspace invariant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Invariant Subspaces: A subspace W of a vector space V is an invariant subspace of a linear transformation T if T(W) ⊆ W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.872190468129816
      },
      {
        "text": "Linear Transformations and the Invariant Subspace Theorem: This theorem states that if T: V -> V is a linear transformation and W is a subspace of V, then the subspace generated by W under T is invariant under T, meaning that T(W) is contained in W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8245950983208298
      },
      {
        "text": "The Invariant Subspace Theorem: This theorem states that if a linear transformation $T$ has an invariant subspace $W$, then the image of $W$ under $T$ is also an invariant subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9029064081578968
      }
    ]
  },
  {
    "representative_text": "Diagonalization of Matrices: A matrix can be diagonalized, and the diagonalization is unique up to a change of basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Diagonalization of Matrices: A matrix can be diagonalized, and the diagonalization is unique up to a change of basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalues and Eigenvectors: A matrix has eigenvalues and eigenvectors if the matrix can be diagonalized.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8206175316860191
      },
      {
        "text": "Diagonalizable Matrices with Non-Unique Eigenvalues: A matrix can be diagonalizable even if its eigenvalues are not unique. This is because the matrix can be transformed into a block diagonal matrix, where each block corresponds to a unique eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8135941883141958
      },
      {
        "text": "The Diagonalization of a Matrix with Complex Eigenvalues: A matrix can be diagonalized even if its eigenvalues are complex. This is because the matrix can be transformed into a block diagonal matrix, where each block corresponds to a unique eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8846110985253028
      },
      {
        "text": "Diagonalization of Complex Matrices: A matrix can be diagonalized if and only if it is diagonalizable. Diagonalization is a process of finding a basis of eigenvectors of a matrix and representing the matrix as a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8694932121498544
      },
      {
        "text": "The Diagonalization of a Matrix with Complex Eigenvalues: A matrix can be diagonalized even if its eigenvalues are complex.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.862656061500584
      },
      {
        "text": "Diagonalization of a Matrix with Complex Eigenvalues: A matrix can be diagonalized even if its eigenvalues are complex, but this requires a more advanced understanding of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8849851702244528
      }
    ]
  },
  {
    "representative_text": "Least Squares Approximation: A matrix can be used to approximate a vector using the least squares method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Least Squares Approximation: A matrix can be used to approximate a vector using the least squares method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Outer Product: The outer product of two vectors is a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Outer Product: The outer product of two vectors is a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Product: The tensor product of two vector spaces is a new vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Tensor Product: The tensor product of two vector spaces is a new vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Tensor Product: The tensor product of two matrices A and B is a new matrix formed by taking the outer product of the columns of A and the rows of B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8548155565604775
      },
      {
        "text": "Tensor Products and Tensor Spaces: The tensor product of two vector spaces is a new vector space, and tensor spaces are an important concept in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8788510174050523
      },
      {
        "text": "Tensor Product: This is a fundamental concept in linear algebra that states that the tensor product of two matrices A and B is a matrix that contains the products of the elements of A and B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8870730213133271
      },
      {
        "text": "Tensor Products of Vector Spaces with Different Dimensions: The tensor product of two vector spaces with different dimensions is a new vector space, but its dimension may not be the sum of the dimensions of the two spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.831370009068764
      },
      {
        "text": "Tensor Product of Vector Spaces: The concept of the tensor product of two vector spaces, which is a way of combining the two spaces into a new vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9214283967778407
      }
    ]
  },
  {
    "representative_text": "Tensor Decomposition: A tensor can be decomposed into a sum of tensor products.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Tensor Decomposition: A tensor can be decomposed into a sum of tensor products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Tensor Decomposition and Tensor Networks: A tensor can be decomposed into a sum of tensor products, and tensor networks are a powerful tool for computing tensor decompositions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9166005232852039
      },
      {
        "text": "Tensor Decomposition and Linear Algebra: Tensor decomposition provides a fundamental connection between tensor products and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8499485362606731
      }
    ]
  },
  {
    "representative_text": "Dual Space: The set of all linear functionals from a vector space to the field of scalars, denoted as $V^*$. This concept is essential in understanding the relationship between linear transformations and linear functionals.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Dual Space: The set of all linear functionals from a vector space to the field of scalars, denoted as $V^*$. This concept is essential in understanding the relationship between linear transformations and linear functionals.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dual Space: The dual space of a vector space V, denoted as V*, is the set of all linear functionals on V. A linear functional f on V is a function that satisfies f(u + v) = f(u) + f(v) and f(cu) = cf(u) for all u, v in V and c in F.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8953346508943472
      },
      {
        "text": "The Dual Space and its Applications: The dual space is the set of all linear functionals from a vector space to the field of scalars. This concept is essential in understanding the relationship between linear transformations and linear functionals.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8860862413492471
      },
      {
        "text": "The concept of the dual space being the set of all linear functionals from a vector space to the field of scalars",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8282849886686086
      },
      {
        "text": "Dual Vector Space: The vector space of linear functionals that map vectors from the original vector space to scalars.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9124584797793724
      },
      {
        "text": "Dual Basis: A set of linear functionals that span the dual vector space, which is isomorphic to the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.863304579114424
      },
      {
        "text": "Dual Space and Linear Functionals:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8519981341416429
      },
      {
        "text": "Dual Basis and Dual Vector Space: The dual basis is a set of linear functionals that span the dual vector space, which is isomorphic to the original vector space. The dual vector space is a vector space with a scalar product that assigns a scalar value to each pair of linear functionals.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8415977922486089
      },
      {
        "text": "Dual Basis and Dual Vector Space: The dual basis is a set of linear functionals that span the dual vector space, which is isomorphic to the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8885737780194015
      }
    ]
  },
  {
    "representative_text": "Dual Linear Transformation: A linear transformation $T^: W^ \\to V^*$ that is the adjoint of a linear transformation $T: V \\to W$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Dual Linear Transformation: A linear transformation $T^: W^ \\to V^*$ that is the adjoint of a linear transformation $T: V \\to W$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Adjoint Operator: The adjoint operator of a linear transformation T, denoted as T^*, is a linear transformation from the dual space W^ to the original space V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8652234189831994
      },
      {
        "text": "The Adjoint Operator: This concept is related to the dual space and linear functionals and states that the adjoint operator of a linear transformation $T$ is a linear transformation from the dual space $W^*$ to the original space $V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8626165459081372
      },
      {
        "text": "The Linear Operator Adjoint Operator: This concept is related to the dual space and linear functionals and states that the adjoint operator of a linear transformation $T$ is a linear transformation from the dual space $W^*$ to the original space $V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9440200980892559
      },
      {
        "text": "The Linear Operator Adjoint Operator and Its Properties: This concept is related to the dual space and linear functionals and involves understanding the properties of the linear operator adjoint operator.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8492793045181919
      }
    ]
  },
  {
    "representative_text": "Orthogonal Transformation: A linear transformation that preserves the inner product, meaning that the inner product of two vectors is equal to the inner product of their images under the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Transformation: A linear transformation that preserves the inner product, meaning that the inner product of two vectors is equal to the inner product of their images under the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection: A linear transformation that projects a vector onto a subspace, resulting in a vector that is orthogonal to the orthogonal complement of the subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection: A linear transformation that projects a vector onto a subspace, resulting in a vector that is orthogonal to the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Projection: A technique that projects a vector onto a subspace, resulting in a vector that is orthogonal to the original vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8974711644194218
      },
      {
        "text": "The Orthogonal Projections: An orthogonal projection is a linear transformation that projects a vector onto a subspace, resulting in a vector that is orthogonal to the orthogonal complement of the subspace. This concept is essential in understanding the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8749649119202498
      },
      {
        "text": "The Orthogonal Projections: An orthogonal projection is a linear transformation that projects a vector onto a subspace, resulting in a vector that is orthogonal to the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9614710223922189
      },
      {
        "text": "The concept of an orthogonal projection being a linear transformation that projects a vector onto a subspace, resulting in a vector that is orthogonal to the orthogonal complement of the subspace",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8695578209626506
      },
      {
        "text": "The concept of \"orthogonal\" and \"orthogonal\" projections: Orthogonal projections are projections that preserve the length of the vector being projected.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8256918206131243
      },
      {
        "text": "Orthogonal Projections: An orthogonal projection is a linear transformation that maps a vector to its closest approximation in the subspace spanned by the orthonormal basis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9212212490418861
      }
    ]
  },
  {
    "representative_text": "Min-Max Theorem: A theorem that states that the eigenvalues of a matrix can be found by minimizing or maximizing the Rayleigh quotient, which is used to compute the eigenvalues of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Min-Max Theorem: A theorem that states that the eigenvalues of a matrix can be found by minimizing or maximizing the Rayleigh quotient, which is used to compute the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Min-Max Theorem: This theorem states that the eigenvalues of the orthogonal projection matrix onto a subspace are equal to the squared lengths of the orthogonal vectors in that subspace. This theorem provides a way to determine the eigenvalues of the projection matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8099269713809103
      },
      {
        "text": "Min-Max Theorem: This theorem states that the eigenvalues of a matrix A are the maximum of the Rayleigh quotients of A with respect to all possible vectors v in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9220599071445019
      },
      {
        "text": "Min-Max Theorem: The min-max theorem states that the eigenvalues of a matrix A are the maximum of the Rayleigh quotients of A with respect to all possible vectors v in the space. This is an important concept in linear algebra, as it can be used to estimate the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9294213124124875
      }
    ]
  },
  {
    "representative_text": "Rayleigh Quotient: A scalar value that is used to compute the eigenvalues of a matrix, and is defined as the ratio of the inner product of a vector and its image under the linear transformation to the inner product of the vector with itself.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Rayleigh Quotient: A scalar value that is used to compute the eigenvalues of a matrix, and is defined as the ratio of the inner product of a vector and its image under the linear transformation to the inner product of the vector with itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rayleigh Quotient: The Rayleigh quotient is a scalar value that can be used to estimate the eigenvalues of a matrix A. It is defined as the ratio of the inner product of a vector and the matrix to the inner product of the vector with itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9476581952199847
      },
      {
        "text": "Rayleigh Quotient: This is a formula used to estimate the eigenvalues of a matrix, which is useful in understanding the properties of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8885348081064819
      },
      {
        "text": "The Rayleigh-Ritz Method: This is a technique used to estimate the eigenvalues of a matrix by applying a Rayleigh quotient to a set of basis vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8614597962035565
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Rayleigh Quotient: The Rayleigh quotient is a method for computing the eigenvalues and eigenvectors of a matrix. It works by computing the Rayleigh quotient of the matrix and its eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8722142381807253
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Rayleigh Quotient with Shifts: The Rayleigh quotient with shifts is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by computing the Rayleigh quotient of the matrix and its eigenvectors with shifts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8210704816709664
      },
      {
        "text": "The Use of Rayleigh Quotient in Numerical Computations: The Rayleigh quotient is a formula used to estimate the eigenvalues of a matrix, and understanding its application is essential in ensuring the accuracy of numerical computations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8962770066756511
      },
      {
        "text": "The Rayleigh Quotient: The Rayleigh quotient is a scalar value that is used to estimate the eigenvalues of a matrix. It is a fundamental tool in linear algebra and is widely used in various applications of orthogonal diagonalization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.900378038038361
      },
      {
        "text": "The Rayleigh Quotient: The Rayleigh quotient is a formula used to estimate the eigenvalues of a matrix. It is defined as the ratio of the dot product of a vector and its transpose to the dot product of the vector with itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9466068103805523
      },
      {
        "text": "Computation of Eigenvalues using the Rayleigh-Ritz Method: The Rayleigh-Ritz method is an iterative method for computing eigenvalues of a matrix. It involves minimizing the Rayleigh quotient of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.862883956622036
      },
      {
        "text": "Orthogonal Projection Matrices and the Rayleigh Quotient: The Rayleigh quotient is a way to compute the eigenvalues of a matrix. It has applications in orthogonal projection matrices, particularly when dealing with self-adjoint matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8438532745131762
      }
    ]
  },
  {
    "representative_text": "Characteristic Polynomial: The characteristic polynomial of a matrix is a polynomial that is used to compute the eigenvalues of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Characteristic Polynomial: The characteristic polynomial of a matrix is a polynomial that is used to compute the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Characteristic Polynomial: The characteristic polynomial of a matrix A is defined as p_A(x) = det(xI - A), where I is the identity matrix. The roots of this polynomial are the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.87678708835778
      },
      {
        "text": "Characteristic Polynomials: The characteristic polynomial of a matrix A is the polynomial det(A - λI), where λ is an eigenvalue. This concept is closely related to the minimal polynomial and is useful in understanding the properties of eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8065968661655745
      },
      {
        "text": "The Characteristic Polynomial: The characteristic polynomial of a matrix is the polynomial of degree n that annihilates the matrix. The characteristic polynomial is related to the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8232294915303352
      },
      {
        "text": "The Characteristic and Minimal Polynomials: The characteristic polynomial and the minimal polynomial of a matrix are polynomials that are used to compute the eigenvalues of the matrix. These polynomials are essential in understanding the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8258003297860228
      },
      {
        "text": "Matrix Determinant and Characteristic Polynomials: The relationship between the determinant of a matrix and its characteristic polynomial is an important concept. The characteristic polynomial of a matrix is defined as det(A - λI), where λ is the eigenvalue and I is the identity matrix. The determinant of the characteristic polynomial can be used to determine the number of eigenvalues and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8228377273850807
      },
      {
        "text": "The Characteristic Polynomial: This concept is related to the eigenvalues and states that the characteristic polynomial of a matrix is a polynomial that is used to compute the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9047577384533546
      },
      {
        "text": "The Connection between Eigenvalues and the Characteristic Polynomial: The relationship between eigenvalues and the characteristic polynomial of a matrix is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8660018178478823
      },
      {
        "text": "The relationship between eigenvalues and the characteristic polynomial: The characteristic polynomial of a matrix is a polynomial in the variable λ (lambda) that is equal to zero when λ is an eigenvalue of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8267475156707178
      },
      {
        "text": "Linear Transformation and Characteristic Polynomial: Exploring the characteristic polynomial of a linear transformation, which provides a way to determine the eigenvalues of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8532231389710496
      }
    ]
  },
  {
    "representative_text": "Minimal Polynomial: The minimal polynomial of a matrix is the monic polynomial of smallest degree that annihilates the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Minimal Polynomial: The minimal polynomial of a matrix is the monic polynomial of smallest degree that annihilates the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Minimal Polynomials: The minimal polynomial of a matrix A is the monic polynomial of lowest degree that annihilates A. This concept is useful in understanding the properties of eigenvalues and the behavior of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8522451354376896
      },
      {
        "text": "Minimal Polynomials: These are polynomials that annihilate a matrix, meaning that when the polynomial is applied to the matrix, the result is the zero matrix. The minimal polynomial is related to the characteristic polynomial and can be used to determine the diagonalizability of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921642036198738
      },
      {
        "text": "The Minimum Polynomial: The minimum polynomial of a matrix is the monic polynomial of smallest degree that annihilates the matrix. The minimum polynomial is related to the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.83788334023596
      },
      {
        "text": "The relationship between the eigenvalues of a matrix and its minimal polynomial: The minimal polynomial of a matrix is the monic polynomial of smallest degree that annihilates the matrix. This is related to the concept of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8670641808788999
      },
      {
        "text": "Minimal Polynomial: A polynomial that annihilates a matrix, which is related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8500388131872794
      },
      {
        "text": "The Minimal Polynomial: This concept is related to the eigenvalues and states that the minimal polynomial of a matrix is the monic polynomial of smallest degree that annihilates the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9093012523386341
      },
      {
        "text": "The Computation of Eigenvalues using the Minimal Polynomial: The minimal polynomial of a matrix is a polynomial that annihilates the matrix. The minimal polynomial can be used to compute eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8813248403622712
      },
      {
        "text": "The definition of the minimal polynomial: A polynomial that annihilates a matrix, which is related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9032486374165627
      }
    ]
  },
  {
    "representative_text": "Null Space and Column Space Relationship: The relationship between the null space and column space of a matrix A, which is closely related to the rank-nullity theorem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Null Space and Column Space Relationship: The relationship between the null space and column space of a matrix A, which is closely related to the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and Span: A discussion of the relationship between linear independence and span of vectors, and how this relates to the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8006491460870047
      },
      {
        "text": "The Row Space and Column Space: These concepts involve understanding the properties of matrices and their relationship to linear independence and dependence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8401751900956547
      },
      {
        "text": "Linear Independence and Dependence: Advanced concepts related to linear independence and dependence, such as the relationship between linear independence and the rank of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8509943343428212
      },
      {
        "text": "Kernel and Image Relationship: The relationship between the kernel (null space) and image (column space) of a matrix A, which is closely related to the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8555709229862833
      },
      {
        "text": "Span of the Null Space: The span of the null space of a matrix. This concept is related to linear independence and the relationship between linear independence and the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8018436032275174
      },
      {
        "text": "The Invariant Subspace Theorem: A theorem that describes the relationship between the null space and the range of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8032256967822704
      },
      {
        "text": "The Connection between Matrix Rank and Nullity: The relationship between the rank of a matrix and its nullity is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8490020443497517
      },
      {
        "text": "Linear Transformation Matrix: The relationship between linear transformation matrices and the properties of the transformation, such as its rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8353244152768781
      }
    ]
  },
  {
    "representative_text": "Determinant Properties: Additional properties of determinants, such as the fact that the determinant of a matrix is equal to the determinant of its transpose, and that the determinant of a product of two matrices is equal to the product of their determinants.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Determinant Properties: Additional properties of determinants, such as the fact that the determinant of a matrix is equal to the determinant of its transpose, and that the determinant of a product of two matrices is equal to the product of their determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant Properties and theorems: This involves the study of properties and theorems related to the determinant of a matrix, such as the determinant of the inverse of a matrix, the determinant of the transpose of a matrix, and the determinant of a product of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8188688824577206
      },
      {
        "text": "Properties of Determinants: Various properties of determinants, such as the effect of row and column operations on the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8843790010580777
      },
      {
        "text": "Properties of Determinants for Block Matrices: Determinants of block matrices, which can be decomposed into smaller matrices. This includes properties such as the determinant of a block matrix being equal to the product of the determinants of the individual blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8356367160772254
      },
      {
        "text": "Properties of Determinants under Simultaneous Row and Column Operations: This includes the properties of the determinant under simultaneous row and column operations, which is a more general version of the properties of determinants under elementary row and column operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8552975667049281
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) Properties: Additional properties of SVD, such as the fact that SVD is a factorization of a matrix into three matrices, and that the SVD of a matrix is unique.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) Properties: Additional properties of SVD, such as the fact that SVD is a factorization of a matrix into three matrices, and that the SVD of a matrix is unique.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Properties of the SVD: The SVD has several properties, including uniqueness, orthogonality, and span properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8435042543415233
      },
      {
        "text": "Singular Value Decomposition (SVD) Properties: The SVD has several properties, including the fact that it can be used to find the inverse of a matrix, and that it is orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8891040590038165
      },
      {
        "text": "Singular Value Decomposition (SVD) Properties: In addition to the properties mentioned earlier, the SVD also has the property that it can be used to find the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9146725396138389
      },
      {
        "text": "Properties of Singular Values: The properties of singular values, including the concept of the singular values of a matrix and their applications in SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8298524582227257
      },
      {
        "text": "Singular Value Decomposition (SVD) Properties: SVD is a factorization of a matrix into three matrices: U, Σ, and V^T, such that U is unitary, Σ is diagonal, and V^T is orthogonal. SVD has several properties, such as the fact that SVD is a factorization of a matrix into three matrices, and that the SVD of a matrix is unique.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8967756066632523
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection: A method for finding the projection of a vector onto a subspace, which is closely related to the concept of orthogonal matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection: A method for finding the projection of a vector onto a subspace, which is closely related to the concept of orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Projections and Orthogonal Projections: A method for projecting a vector onto a subspace, which can be used to find the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9019713296509206
      },
      {
        "text": "Orthogonal Projections: A method for projecting a vector onto a subspace by finding the orthogonal projection of the vector onto a basis for the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9196302048737242
      },
      {
        "text": "Orthogonal Projections: A method used to find the projection of a vector onto another vector, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8779787831767141
      },
      {
        "text": "Orthogonal Projections (continued): Orthogonal projections are a method for projecting a vector onto a subspace by finding the orthogonal projection of the vector onto a basis for the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9105023515264348
      },
      {
        "text": "Orthogonal Projection: A method for projecting a vector onto a subspace, which is useful for determining linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.919359085690549
      },
      {
        "text": "Projections and Orthogonal Projections: A method for finding the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9116004511663445
      },
      {
        "text": "Orthogonal Projections and Dimension: Orthogonal projections are a method for projecting a vector onto a subspace by finding the orthogonal projection of the vector onto a basis for the subspace. This method can be used to find the dimension of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8909787982003116
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Computation: A discussion of methods for computing eigenvalues and eigenvectors, such as power iteration, QR algorithm, and eigensystem decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Computation: A discussion of methods for computing eigenvalues and eigenvectors, such as power iteration, QR algorithm, and eigensystem decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Decomposition and its Applications: This involves the decomposition of a matrix into its eigenvalues and eigenvectors, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8085065666302644
      },
      {
        "text": "Diagonalization and its Applications: This involves the diagonalization of a matrix, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8283920863816097
      },
      {
        "text": "Matrix-Vector Products and their Applications: This involves the study of matrix-vector products, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8551157202402914
      },
      {
        "text": "Computational Methods for Eigenvalues and Eigenvectors: There are several computational methods available for finding eigenvalues and eigenvectors, including power iteration, QR iteration, and the Lanczos algorithm. These methods are useful in solving large-scale linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.85338211396898
      },
      {
        "text": "Computational Methods: These are algorithms and techniques used to compute eigenvalues and eigenvectors of a matrix. Computational methods include methods such as QR algorithm, power iteration, and Jacobi iteration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8031957806224682
      },
      {
        "text": "Diagonalization and its Applications: The method of diagonalization and its applications in solving systems of linear equations and finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8384033490227396
      },
      {
        "text": "Numerical Computation of Eigenvalues and Eigenvectors: The numerical computation of eigenvalues and eigenvectors can be done using various algorithms such as the QR algorithm and the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8372103562095652
      },
      {
        "text": "Numerical Methods for Computing Eigenvalues and Eigenvectors: The discussion of numerical methods for computing eigenvalues and eigenvectors, including the power method, QR algorithm, and Jacobi method, as well as their limitations and applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8397951500553635
      },
      {
        "text": "The Diagonalization of Complex Matrices: This includes the process of finding a basis of eigenvectors of a complex matrix and representing the matrix as a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8183760628874412
      },
      {
        "text": "Numerical Methods for Eigenvalue Decomposition: This includes techniques such as the use of linear algebra to solve eigenvalue decomposition problems, and to perform numerical eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8615120135673884
      },
      {
        "text": "The Computation of Eigenvalues and Eigenvectors: This is a fundamental problem in linear algebra, and various algorithms such as the QR algorithm, the Jacobi algorithm, and the power method can be used to compute eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8829548068244222
      },
      {
        "text": "Numerical Methods for Finding Eigenvalues and Eigenvectors: This topic deals with various numerical methods used to find eigenvalues and eigenvectors of a matrix, such as the QR algorithm, the power method, and the Lanczos algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9043322012643473
      },
      {
        "text": "Eigenvalue Decomposition Techniques: Techniques such as power iteration, QR decomposition, and Jacobi iteration for finding eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8366701198049127
      }
    ]
  },
  {
    "representative_text": "Numerical Linear Algebra Algorithms: A discussion of numerical linear algebra algorithms, such as Gaussian elimination, LU decomposition, and singular value decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Numerical Linear Algebra Algorithms: A discussion of numerical linear algebra algorithms, such as Gaussian elimination, LU decomposition, and singular value decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Linear Algebra Algorithms: Numerical linear algebra algorithms are methods used to solve linear systems and find eigenvalues and eigenvectors of a matrix. Examples include the QR algorithm, the power method, and the Lanczos algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8216912385275875
      },
      {
        "text": "Numerical Linear Algebra Libraries: Numerical linear algebra libraries are software packages that provide implementations of numerical linear algebra algorithms, such as the QR algorithm and the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.821041208313888
      },
      {
        "text": "Numerical Linear Algebra Algorithms: Numerical linear algebra algorithms such as the QR algorithm, the power method, and the Lanczos algorithm can be used to solve linear systems and find eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.906446087671255
      },
      {
        "text": "Numerical Linear Algebra Libraries: Numerical linear algebra libraries such as LAPACK and BLAS provide implementations of numerical linear algebra algorithms, such as the QR algorithm and the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9057054776718649
      }
    ]
  },
  {
    "representative_text": "Linear Algebra Applications: A discussion of the applications of linear algebra in various fields, such as physics, engineering, computer science, and economics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra Applications: A discussion of the applications of linear algebra in various fields, such as physics, engineering, computer science, and economics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cokernel and Cofactor: The cokernel of a linear transformation is the set of vectors that are not mapped to the zero vector, while the cofactor is a matrix related to the cokernel.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cokernel and Cofactor: The cokernel of a linear transformation is the set of vectors that are not mapped to the zero vector, while the cofactor is a matrix related to the cokernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Adjoint and Adjoint Matrix: The adjoint of a linear transformation is a linear transformation that satisfies a specific relationship with the original transformation, and the adjoint matrix is a matrix representation of this relationship.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Adjoint and Adjoint Matrix: The adjoint of a linear transformation is a linear transformation that satisfies a specific relationship with the original transformation, and the adjoint matrix is a matrix representation of this relationship.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Adjoint and Adjoint Matrix (Relationship with Kernel and Image): This could involve exploring the relationship between the adjoint matrix and the kernel and image of a linear transformation, including how the adjoint matrix can be used to determine the properties of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8079635166579457
      },
      {
        "text": "Adjoint and Adjoint Matrix (Relationship with Kernel and Image): The relationship between the adjoint matrix and the kernel and image of a linear transformation, including how the adjoint matrix can be used to determine the properties of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9296559264027267
      }
    ]
  },
  {
    "representative_text": "Dual Basis and Dual Basis for a Linear Transformation: The dual basis of a linear transformation is a set of vectors that, when used to transform the domain, produces a vector that is orthogonal to the image.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dual Basis and Dual Basis for a Linear Transformation: The dual basis of a linear transformation is a set of vectors that, when used to transform the domain, produces a vector that is orthogonal to the image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Inner Product Spaces: Understanding linear transformations in the context of inner product spaces can provide additional insights into the behavior of the transformation and its properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Inner Product Spaces: Understanding linear transformations in the context of inner product spaces can provide additional insights into the behavior of the transformation and its properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Inner Product Spaces and Norms: The study of inner product spaces and norms can provide a deeper understanding of the properties of vector spaces and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8782141049800677
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Banach Spaces: The properties of linear transformations in Banach spaces, which are complete normed vector spaces, can provide valuable insights into the behavior of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Banach Spaces: The properties of linear transformations in Banach spaces, which are complete normed vector spaces, can provide valuable insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Topology: The topological properties of linear transformations, such as continuity and compactness, can provide additional insights into the behavior of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Topology: The topological properties of linear transformations, such as continuity and compactness, can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Non-Zero Eigenvalues: If a matrix A has non-zero eigenvalues, then det(A) ≠ 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Non-Zero Eigenvalues: If a matrix A has non-zero eigenvalues, then det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Rank-Deficient Columns or Rows: If a matrix A has rank-deficient columns or rows, then det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8063860518328607
      },
      {
        "text": "Determinant of a Matrix with Non-Full Rank: If a matrix A has non-full rank, then det(A) ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9343050569044913
      },
      {
        "text": "Determinant and Rank-Deficient Columns or Rows: The determinant of a matrix A with rank-deficient columns or rows is not equal to 0. This is a subtle nuance of determinants and rank-deficient columns or rows.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8510833318631161
      },
      {
        "text": "Determinant of a Matrix with Linearly Dependent Columns or Rows and Non-Full Rank: The determinant of a matrix with linearly dependent columns or rows and non-full rank is not equal to 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8805861801771938
      }
    ]
  },
  {
    "representative_text": "Perturbation Theory: Perturbation theory is a technique used to study the behavior of a matrix or linear transformation near a given point. It involves approximating the matrix or linear transformation by a simpler one, and then studying the effects of small perturbations on the resulting behavior.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Perturbation Theory: Perturbation theory is a technique used to study the behavior of a matrix or linear transformation near a given point. It involves approximating the matrix or linear transformation by a simpler one, and then studying the effects of small perturbations on the resulting behavior.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Perturbation Theory: This theory studies the behavior of eigenvalues and eigenvectors of a matrix when the matrix is slightly perturbed.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8024276805006261
      },
      {
        "text": "Perturbation Theory: This is a branch of linear algebra that studies how small changes in a matrix affect its eigenvalues and eigenvectors. It is an essential tool for understanding the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.913171899231146
      },
      {
        "text": "Perturbation of Eigenvalues: This is a branch of linear algebra that studies how small changes in a matrix affect its eigenvalues. It is an essential tool for understanding the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8983873711900776
      },
      {
        "text": "Perturbation Theory: A method for studying the behavior of linear transformations and their matrix representations near a given linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8622580742811234
      },
      {
        "text": "Perturbation Theory for Eigenvalues: Perturbation theory is a branch of linear algebra that studies how small changes in a matrix affect its eigenvalues. This is an important concept in linear algebra, as it can be used to understand the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9223990313261038
      },
      {
        "text": "Perturbation Theory for Eigenvalues: This is a branch of linear algebra that studies how small changes in a matrix affect its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9399001497027467
      },
      {
        "text": "Perturbation Theory: Perturbation theory is a mathematical technique used to study the behavior of a matrix or a system of linear equations when a small change is made to the matrix or the system.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9192346088972856
      },
      {
        "text": "Perturbation Theory of Eigenvalues: This theory studies the behavior of eigenvalues of a matrix when the matrix is slightly perturbed. It provides a way to approximate the eigenvalues of a perturbed matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9244721284558362
      },
      {
        "text": "Perturbation Theory: Discuss perturbation theory, which studies how small changes in a matrix affect its eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8596829721313908
      },
      {
        "text": "Eigenvalue Perturbation Theory: A method for approximating the eigenvalues of a matrix using perturbation theory.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8544425065670216
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Complex Eigenvalues: If a matrix A has complex eigenvalues, then det(A) = e^(iθ), where θ is the sum of the arguments of the eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues: If a matrix A has complex eigenvalues, then det(A) = e^(iθ), where θ is the sum of the arguments of the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant and Complex Eigenvalues: The determinant of a matrix A with complex eigenvalues is equal to e^(iθ), where θ is the sum of the arguments of the eigenvalues. This is a fundamental property of determinants and complex eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9417483535579833
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Non-Orthogonal Basis: If a matrix A represents a linear transformation with respect to a non-orthogonal basis, then det(A) ≠ det(A^T), where A^T is the transpose of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Basis: If a matrix A represents a linear transformation with respect to a non-orthogonal basis, then det(A) ≠ det(A^T), where A^T is the transpose of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Non-Linear Transformations: If a matrix A represents a non-linear transformation, then det(A) ≠ det(A^T).",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Non-Linear Transformations: If a matrix A represents a non-linear transformation, then det(A) ≠ det(A^T).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Linear Transformations: If a matrix A represents a non-orthogonal linear transformation, then det(A) ≠ det(A^T).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9296517708674128
      },
      {
        "text": "Determinant of a Matrix with Non-Symmetric Linear Transformations: The determinant of a matrix representing a non-symmetric linear transformation is not necessarily equal to the determinant of the transpose of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9232502874866328
      },
      {
        "text": "Determinant of a Matrix with Non-Symmetric Linear Transformations and Non-Constant Coefficients: The determinant of a matrix representing a non-symmetric linear transformation and non-constant coefficients is not necessarily equal to the determinant of the transpose of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9282824030960972
      },
      {
        "text": "Determinant of a Matrix with Non-Linear Transformations (e.g., polynomial transformations): The determinant of a matrix representing a non-linear transformation is not necessarily equal to the determinant of the transpose of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9295600983594761
      },
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Linear Transformations (e.g., transformations with a non-orthogonal basis): The determinant of a matrix representing a non-orthogonal linear transformation is not necessarily equal to the determinant of the transpose of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9496010337646892
      },
      {
        "text": "Determinant of a matrix with a non-linear transformation and a non-constant coefficient matrix: The determinant of a matrix representing a non-linear transformation with a non-constant coefficient matrix is not necessarily equal to the determinant of the transpose of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9475525031333039
      }
    ]
  },
  {
    "representative_text": "Nuclear Norm and Matrix Perturbation Theory: This involves the study of the nuclear norm of a matrix, which is a measure of its \"size\" or \"energy\". It also includes techniques for approximating the solution to a linear system by perturbing the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Nuclear Norm and Matrix Perturbation Theory: This involves the study of the nuclear norm of a matrix, which is a measure of its \"size\" or \"energy\". It also includes techniques for approximating the solution to a linear system by perturbing the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Norms and their Properties: This involves the study of matrix norms, which are measures of the size or energy of a matrix. It also includes properties and theorems related to matrix norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8097229396620571
      },
      {
        "text": "Matrix Norms: Matrix norms are used to measure the magnitude of a matrix, and are essential in understanding the properties of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8298925617981361
      }
    ]
  },
  {
    "representative_text": "Block Matrices and Block Triangular Matrices: This involves the study of matrices with block structure, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Block Matrices and Block Triangular Matrices: This involves the study of matrices with block structure, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Block Matrices and their Properties: This involves the study of block matrices, which are matrices that can be decomposed into smaller matrices, and the properties of these matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8668971144626685
      },
      {
        "text": "Block Matrices and their Properties with Complex Entries: This involves the study of block matrices with complex entries, which includes the study of block matrices, their properties, and theorems related to block matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8658322048038964
      },
      {
        "text": "Block Matrices and their Properties in Higher Dimensions: The study of block matrices in higher dimensions, including their properties, theorems, and applications in various fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.887952370452674
      },
      {
        "text": "Block Matrices and their Properties with Complex Entries: The study of block matrices with complex entries, including their properties, theorems, and applications in various fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9219678634726849
      },
      {
        "text": "Block Matrices and their Properties in Higher Dimensions with Complex Entries: The study of block matrices in higher dimensions with complex entries, including their properties, theorems, and applications in various fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9307558239679552
      },
      {
        "text": "Block Diagonalization of a Matrix with Complex Entries: This involves the study of block diagonalization of a matrix with complex entries, including its properties and theorems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8774192946002015
      }
    ]
  },
  {
    "representative_text": "Commutativity and Associativity of Matrix Operations: This involves the study of the properties of matrix operations, such as the commutativity and associativity of matrix addition, multiplication, and transposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Commutativity and Associativity of Matrix Operations: This involves the study of the properties of matrix operations, such as the commutativity and associativity of matrix addition, multiplication, and transposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Multiplication Properties: Understanding the properties of matrix multiplication, such as the associative property, the distributive property, and the existence of a zero matrix, is essential in matrix operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8184446983549818
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Interpolation: This involves finding the eigenvalues of a matrix that interpolate between the eigenvalues of two given matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Eigenvalue Interpolation: This involves finding the eigenvalues of a matrix that interpolate between the eigenvalues of two given matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Interpolation using Polynomial Interpolation: This involves using polynomial interpolation to find the eigenvalues of a matrix that interpolate between the eigenvalues of two given matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.939163722874228
      },
      {
        "text": "Eigenvalue Interpolation using Tensor Decomposition: This involves the study of tensor decomposition, which is a method for interpolating eigenvalues using tensor products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8200497939086655
      },
      {
        "text": "Eigenvalue Interpolation using Machine Learning Methods: This involves the study of machine learning methods for interpolating eigenvalues, such as neural networks and kernel methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8452434881223846
      },
      {
        "text": "Eigenvalue Interpolation using Tensor Decomposition and Non-Linear Interpolation: This involves the study of tensor decomposition and non-linear interpolation for eigenvalue interpolation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9172336381713639
      },
      {
        "text": "Eigenvalue Interpolation using Machine Learning Methods and Non-Linear Interpolation: This involves the study of machine learning methods for eigenvalue interpolation, including non-linear interpolation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9072007725530643
      },
      {
        "text": "Eigenvalue Interpolation using Non-Linear Interpolation Methods: The study of non-linear interpolation methods for eigenvalue interpolation, including machine learning methods and other techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8952157862248777
      },
      {
        "text": "Eigenvalue Interpolation using Tensor Decomposition and Non-Linear Interpolation: The study of tensor decomposition and non-linear interpolation for eigenvalue interpolation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8482095213065555
      },
      {
        "text": "Eigenvalue Interpolation using Polynomial Interpolation with Complex Entries: This involves the study of polynomial interpolation for eigenvalue interpolation using complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8734547682626601
      },
      {
        "text": "Eigenvalue Interpolation: Eigenvalue interpolation is a method for interpolating eigenvalues of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9281748656145672
      },
      {
        "text": "Eigenvalue Interpolation with Polynomial Interpolation: Eigenvalue interpolation with polynomial interpolation is a method for interpolating eigenvalues of a matrix using polynomial interpolation.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9230498071503875
      },
      {
        "text": "Eigenvalue Interpolation with Polynomial Interpolation and SVD: Eigenvalue interpolation is a method for interpolating eigenvalues of a matrix. Polynomial interpolation is a method for approximating a function using a polynomial. SVD (Singular Value Decomposition) is a factorization of a matrix into the product of three matrices: U, Σ, and V. Eigenvalue interpolation with polynomial interpolation and SVD is used in numerical linear algebra to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8086606587008225
      }
    ]
  },
  {
    "representative_text": "Matrix Exponentiation Methods: This involves the study of methods for exponentiating a matrix, such as the matrix exponential, the power method, and the Laguerre method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Matrix Exponentiation Methods: This involves the study of methods for exponentiating a matrix, such as the matrix exponential, the power method, and the Laguerre method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Exponents: These are a way to extend the concept of matrix multiplication to fractional exponents, which can be useful in solving certain types of linear equations and systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.808312426131762
      },
      {
        "text": "Matrix Exponentiation: Matrix exponentiation is a way to extend the concept of matrix multiplication to fractional exponents, which can be useful in solving certain types of linear equations and systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9211472357174312
      },
      {
        "text": "Matrix Exponentiation: A method for computing the power of a matrix, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.823625121541542
      },
      {
        "text": "Matrix exponentiation: A method for computing the power of a matrix, which can be used to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8685583818171516
      }
    ]
  },
  {
    "representative_text": "Matrix Polynomial Equations: This involves the study of equations involving matrices and polynomials, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Matrix Polynomial Equations: This involves the study of equations involving matrices and polynomials, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix-Linear Algebraic Equations: This involves the study of equations involving matrices and linear algebraic expressions, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8905099390542156
      },
      {
        "text": "Matrix Identities: This involves the study of identities involving matrices, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9144680508943528
      }
    ]
  },
  {
    "representative_text": "Commutativity of Matrix Multiplication: This involves the study of the commutativity of matrix multiplication, which is a fundamental property of matrix multiplication.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Commutativity of Matrix Multiplication: This involves the study of the commutativity of matrix multiplication, which is a fundamental property of matrix multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Non-commutativity of Matrix Multiplication: This involves the study of the non-commutativity of matrix multiplication, which is a fundamental property of matrix multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9515486438895431
      }
    ]
  },
  {
    "representative_text": "Matrix Inequalities: This involves the study of inequalities involving matrices, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Inequalities: This involves the study of inequalities involving matrices, which can be used to solve systems of linear equations and find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Analogues of Algebraic Results: This involves the study of algebraic results that have matrix analogues, such as the Cayley-Hamilton theorem and the matrix version of the Fundamental Theorem of Algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Matrix Analogues of Algebraic Results: This involves the study of algebraic results that have matrix analogues, such as the Cayley-Hamilton theorem and the matrix version of the Fundamental Theorem of Algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Identities and Analogues of Algebraic Results: This involves the study of matrix identities and analogues of algebraic results, such as the Cayley-Hamilton theorem and the matrix version of the Fundamental Theorem of Algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9421305528917743
      },
      {
        "text": "Matrix Identities and Analogues of Algebraic Results in Higher Dimensions: The study of matrix identities and analogues of algebraic results in higher dimensions, including the Cayley-Hamilton theorem, the Fundamental Theorem of Algebra, and the matrix version of the Fundamental Theorem of Calculus.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.832225115779655
      },
      {
        "text": "Matrix Identities and Analogues of Algebraic Results in Higher Dimensions with Complex Entries: This involves the study of matrix identities and analogues of algebraic results in higher dimensions with complex entries, including the Cayley-Hamilton theorem, the Fundamental Theorem of Algebra, and the matrix version of the Fundamental Theorem of Calculus.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9182139489647384
      },
      {
        "text": "Matrix Identities and Analogues of Algebraic Results in Higher Dimensions with Complex Entries: This involves the study of matrix identities and analogues of algebraic results in higher dimensions with complex dimensions with complex dimensions with complex entries with complex entries with complex entries with complex entries with complex entries with complex entries, which is not explicitly covering theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8333685079279216
      }
    ]
  },
  {
    "representative_text": "Matrix Analogues of Geometric Results: This involves the study of geometric results that have matrix analogues, such as the matrix version of the Fundamental Theorem of Calculus and the matrix version of the Euler's formula.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Analogues of Geometric Results: This involves the study of geometric results that have matrix analogues, such as the matrix version of the Fundamental Theorem of Calculus and the matrix version of the Euler's formula.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Schur's Theorem: A theorem that states that every square matrix can be diagonalized using an orthogonal matrix, and its eigenvalues are the diagonal elements.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Schur's Theorem: A theorem that states that every square matrix can be diagonalized using an orthogonal matrix, and its eigenvalues are the diagonal elements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Schur's Theorem: This theorem states that a square matrix A can be diagonalized by a unitary matrix U, such that U^(-1)AU is a diagonal matrix containing the eigenvalues of A. This theorem is useful in understanding the properties of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.914433878422835
      },
      {
        "text": "Schur's Decomposition Theorem: This theorem states that any square matrix can be decomposed into a unitary matrix U, a diagonal matrix D, and an upper triangular matrix T such that A = UDU^(-1). This decomposition can be used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8568259776181422
      },
      {
        "text": "Schur's Theorem: This theorem states that any square matrix can be orthogonally diagonalized, which implies that the Gram-Schmidt process can be used to find an orthogonal matrix that diagonalizes a given matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8676382265116087
      },
      {
        "text": "The Schur's Theorem (Alternative Proof): Schur's theorem states that a matrix is similar to a unitary matrix. An alternative proof of this theorem can be obtained using the spectral theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8215979730714665
      },
      {
        "text": "Schur's Theorem: A theorem in linear algebra that states that a square matrix is unitarily similar to a diagonal matrix if and only if it is normal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8958021288998341
      },
      {
        "text": "Schur's Theorem: This theorem states that for any square matrix A, there exists a unitary matrix U such that U^(-1)AU is upper triangular, where U^(-1)AU is the Schur matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9305731054280606
      }
    ]
  },
  {
    "representative_text": "Sylvester's Law of Inertia: A theorem that states that the determinant and the characteristic polynomial of a matrix are equal to the product of the determinants of the submatrices formed by removing one row and one column.",
    "is_in_domain": false,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Sylvester's Law of Inertia: A theorem that states that the determinant and the characteristic polynomial of a matrix are equal to the product of the determinants of the submatrices formed by removing one row and one column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Sylvester's Law of Inertia: This law states that the determinant of a matrix is equal to the product of the determinants of its submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9371829789080643
      },
      {
        "text": "Sylvester's Law of Inertia: This is a theorem that states that the determinant of a matrix can be used to determine the number of positive, negative, and zero eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9151023406317053
      },
      {
        "text": "Sylvester's Law of Inertia for determinants: Sylvester's Law of Inertia states that the determinant of a matrix can be used to determine the number of positive, negative, and zero eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9174389133998422
      },
      {
        "text": "Sylvester's Law of Inertia: This law states that the signs of the eigenvalues of a matrix can be determined by the signs of the determinants of the principal minors of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9209299006414762
      },
      {
        "text": "Sylvester's Law of Inertia: A theorem that relates the eigenvalues of a matrix to its determinant and the rank of its transpose.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.921251398668966
      }
    ]
  },
  {
    "representative_text": "Determinant of a Block Matrix: A method for calculating the determinant of a block matrix, which is a matrix with submatrices as blocks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Determinant of a Block Matrix: A method for calculating the determinant of a block matrix, which is a matrix with submatrices as blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with a Block Structure: This is a technique that involves calculating the determinant of a block matrix by treating it as a collection of smaller matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8983977953410374
      },
      {
        "text": "Block Matrix Determinants: This is a more general method for calculating the determinant of a block matrix, which is a matrix with submatrices as blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9106265293995617
      },
      {
        "text": "Determinant of a Matrix with a Block Structure: This is a technique that involves calculating the determinant of a block matrix by treating it as a collection of smaller matrices. This has applications in various fields, including control theory and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8658714145237277
      },
      {
        "text": "Determinants of Block Diagonal Matrices: The properties of the determinant of a block diagonal matrix, which can be decomposed into smaller matrices, and how it relates to the determinants of the individual blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8251690670962398
      },
      {
        "text": "Determinants of Block Triangular Matrices: The properties of the determinant of a block triangular matrix, which can be decomposed into smaller matrices, and how it relates to the determinants of the individual blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8511808475808897
      },
      {
        "text": "Block Matrix Determinants (Generalized): A more general method for calculating the determinant of a block matrix, which involves using the properties of the block matrix and the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8888808598179927
      },
      {
        "text": "Determinants of Block Triangular Matrices with Non-Standard Block Triangulation: The determinant of a block triangular matrix can be calculated using various block triangulation methods, such as the LQ decomposition or QR decomposition, where the blocks are not necessarily square.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.845900303786355
      },
      {
        "text": "Determinant of a Matrix with a Non-Standard Block Structure: The determinant of a matrix with a non-standard block structure, such as a block diagonal matrix with non-square blocks, can be calculated using various methods, including the Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8877995462352912
      },
      {
        "text": "Determinant of a Matrix Product using the Block Triangularization Method: The block triangularization method is a technique for computing the determinant of a matrix product, which can be useful in solving systems of linear equations and eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8880185466968352
      },
      {
        "text": "Determinant of a Matrix with a Block Structure: The determinant of a matrix with a block structure can be calculated using the formula for the determinant of a matrix with a block structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9074707719071772
      },
      {
        "text": "Block Matrix Determinants using the Schur Complement: This is a method for calculating the determinant of a block matrix using the Schur complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9149755238637869
      },
      {
        "text": "Block Triangularization: A method for decomposing a matrix into a block triangular form, which can be used to compute the determinant of a matrix product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8494431252097807
      }
    ]
  },
  {
    "representative_text": "Block Matrix Inversion: A method for inverting a block matrix, which is a matrix with submatrices as blocks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Block Matrix Inversion: A method for inverting a block matrix, which is a matrix with submatrices as blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Block Matrix Inversion: Block matrix inversion is a method for finding the inverse of a block matrix, which can be useful in certain applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8513981850404264
      },
      {
        "text": "Block Triangular Matrices: These are square matrices that can be partitioned into a block upper triangular matrix and a block lower triangular matrix. The inverse of a block triangular matrix can be found by inverting the block triangular matrix separately.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8171162681077832
      },
      {
        "text": "Block Matrix Inversion using the Schur Complement: This is a method for inverting a block matrix by using the Schur complement, which is a submatrix that is obtained by removing a row and a column from the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8855593885815691
      },
      {
        "text": "Block Matrix Inversion: Inversion of a block matrix, where the matrix is divided into sub-matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8582947253724655
      }
    ]
  },
  {
    "representative_text": "Minors and Cofactors of a Block Matrix: A method for calculating the minors and cofactors of a block matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Minors and Cofactors of a Block Matrix: A method for calculating the minors and cofactors of a block matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinants of Block Matrices: Determinants of block matrices, which can be decomposed into smaller matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8772180168724978
      },
      {
        "text": "Determinants and the Complement of a Matrix: The properties of the determinant of the complement of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8378070454773571
      },
      {
        "text": "Determinants and the Complement of a Matrix: The properties of the determinant of the complement of a matrix, including the relationship between the determinant of a matrix and the determinant of its complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8703785212333314
      },
      {
        "text": "Minors and Cofactors of a Block Matrix: A method for calculating the minors and cofactors of a block matrix, which involves using the properties of the block matrix and the minors and cofactors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8831065286644277
      },
      {
        "text": "The Properties of Determinants for Matrices with Non-Standard Block Structure: A method for computing the determinant of a matrix with a non-standard block structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8884510874068277
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Complex Numbers: A method for calculating the determinant of a matrix with complex numbers.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 15,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Complex Numbers: A method for calculating the determinant of a matrix with complex numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues: The calculation of the determinant of a matrix with complex eigenvalues, including the concept of the determinant of a matrix with complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8676042925880887
      },
      {
        "text": "Determinant of a Matrix with Non-Integer Eigenvalues: The calculation of the determinant of a matrix with non-integer eigenvalues, including the concept of the determinant of a matrix with non-integer entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8610724940748435
      },
      {
        "text": "Determinant of a Matrix with a Non-Linear Eigenvalue: This is a more general method for calculating the determinant of a matrix with complex eigenvalues, which is related to the properties of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.859410090355885
      },
      {
        "text": "Determinant of a Matrix with a Complex Eigenvalue and its Conjugate: This includes the properties of the determinant of a matrix with a complex eigenvalue and its conjugate, which is related to the properties of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8725607673803604
      },
      {
        "text": "Determinant of a Matrix with a Complex Eigenvalue: The determinant of a matrix with complex eigenvalues can be calculated using the complex conjugate of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9010092814652233
      },
      {
        "text": "Determinant of a Matrix with a Non-Diagonal Dominant Matrix and a Complex Eigenvalue: There are some properties and relationships that can be derived for determinants of matrices with non-diagonal dominant matrices and complex eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8646034521430271
      },
      {
        "text": "Determinant of a Matrix with a Large Number of Variables and a Complex Eigenvalue: There are some properties and relationships that can be derived for determinants of matrices with a large number of variables and complex eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8623417470082804
      },
      {
        "text": "Determinant of a Matrix with a Non-Orthogonal Basis and a Complex Eigenvalue: The determinant of a matrix with a non-orthogonal basis and a complex eigenvalue can be calculated using the complex conjugate of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8813674465760601
      },
      {
        "text": "Determinant of a Matrix with a Non-Linear Eigenvalue (Generalized): A more general method for calculating the determinant of a matrix with complex eigenvalues, which involves using the properties of the matrix and the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.890152612827525
      },
      {
        "text": "Determinant of a Matrix with a Complex Eigenvalue and its Conjugate: Properties of the determinant of a matrix with a complex eigenvalue and its conjugate, which involves using the properties of the matrix and the eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8786735447519779
      },
      {
        "text": "Properties of Determinants for Matrices with Non-Standard Eigenvalues: The determinant of a matrix with non-standard eigenvalues can be calculated using various methods, including the Schur decomposition and the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8400189278286974
      },
      {
        "text": "Determinant of a Matrix with a Complex Eigenvalue and a Non-Orthogonal Basis: There are some properties and relationships that can be derived for determinants of matrices with complex eigenvalues and non-orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9143425157255218
      },
      {
        "text": "Determinant of a Matrix with a Complex Eigenvalue using the Resolvent: This is a method for calculating the determinant of a matrix with a complex eigenvalue using the resolvent, which is a matrix that is obtained by dividing the identity matrix by the difference between the eigenvalue and the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8538356922559364
      },
      {
        "text": "Determinant of a Matrix with a Complex Eigenvalue using the Resolvent (Generalized): A method for calculating the determinant of a matrix with a complex eigenvalue using the resolvent, which is a matrix that is obtained by dividing the identity matrix by the difference between the eigenvalue and the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.828901613964339
      }
    ]
  },
  {
    "representative_text": "Barycentric Coordinates: Barycentric coordinates are a way to express a vector as a linear combination of a set of basis vectors. This concept is useful in understanding the properties of eigenvectors and the behavior of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Barycentric Coordinates: Barycentric coordinates are a way to express a vector as a linear combination of a set of basis vectors. This concept is useful in understanding the properties of eigenvectors and the behavior of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonality of Eigenvectors: The eigenvectors of a matrix are orthogonal to each other if the eigenvalues are distinct. This concept is useful in understanding the properties of eigenvectors and the behavior of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonality of Eigenvectors: The eigenvectors of a matrix are orthogonal to each other if the eigenvalues are distinct. This concept is useful in understanding the properties of eigenvectors and the behavior of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Stability: Numerical stability is a concern when computing eigenvalues and eigenvectors, particularly for large-scale matrices. This concept is useful in understanding the limitations of numerical methods and the importance of careful implementation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Numerical Stability: Numerical stability is a concern when computing eigenvalues and eigenvectors, particularly for large-scale matrices. This concept is useful in understanding the limitations of numerical methods and the importance of careful implementation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Stability: This is a concern in the computation of eigenvalues and eigenvectors. Numerical stability refers to the sensitivity of the computed eigenvalues and eigenvectors to small changes in the input matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8827765657710529
      },
      {
        "text": "Numerical Stability of Eigenvalue Computation: There are several techniques to improve the numerical stability of eigenvalue computation, such as using QR iteration or the Lanczos algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8049598158608764
      },
      {
        "text": "Numerical Stability of Computation: This topic deals with the numerical methods used to compute eigenvalues and eigenvectors, and the potential sources of numerical instability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9049153291389997
      },
      {
        "text": "Numerical Stability of Eigenvalue Computation: This topic deals with the techniques used to improve the numerical stability of eigenvalue computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8999374310744138
      }
    ]
  },
  {
    "representative_text": "Polar Decomposition: The polar decomposition of a matrix A is a factorization of A into the product of a positive semi-definite matrix and an orthogonal matrix. This can be used to compute the SVD of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Polar Decomposition: The polar decomposition of a matrix A is a factorization of A into the product of a positive semi-definite matrix and an orthogonal matrix. This can be used to compute the SVD of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "SVD of a Matrix with Non-Orthogonal Columns: The SVD of a matrix with non-orthogonal columns can be computed using the polar decomposition of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8093549439492406
      },
      {
        "text": "Properties of the SVD of a matrix with a non-orthogonal basis: The SVD of a matrix with a non-orthogonal basis can be computed using the polar decomposition of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9174007096018907
      },
      {
        "text": "SVD of a matrix with a non-orthogonal basis: The SVD of a matrix with a non-orthogonal basis can be computed using the polar decomposition of the matrix. This is an important extension of the standard SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9372682324687608
      }
    ]
  },
  {
    "representative_text": "Eigendecomposition: The eigendecomposition of a matrix A is a factorization of A into the product of a diagonal matrix and an orthogonal matrix. This can be used to compute the SVD of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigendecomposition: The eigendecomposition of a matrix A is a factorization of A into the product of a diagonal matrix and an orthogonal matrix. This can be used to compute the SVD of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigendecomposition of a matrix: The eigendecomposition of a matrix is a factorization of the matrix into the product of a diagonal matrix and an orthogonal matrix. This is an important concept that is related to the SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8952876230954911
      }
    ]
  },
  {
    "representative_text": "Bare Eigenvalue Theorem: This theorem states that if A is a square matrix and λ is a non-zero eigenvalue of A, then the matrix A - λI is invertible.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Bare Eigenvalue Theorem: This theorem states that if A is a square matrix and λ is a non-zero eigenvalue of A, then the matrix A - λI is invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bare Eigenvalue Theorem: This theorem states that if A is a square matrix and λ is a non-zero eigenvalue of A, then the matrix A - λI is invertible. This is an important theorem that is related to the SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8511139996816106
      }
    ]
  },
  {
    "representative_text": "Kronecker Product: The Kronecker product of two matrices A and B is a new matrix formed by multiplying each element of A by the entire matrix B.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Kronecker Product: The Kronecker product of two matrices A and B is a new matrix formed by multiplying each element of A by the entire matrix B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Kronecker Product: A mathematical operation that combines two matrices into a single matrix, used in various applications such as control theory and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8458340504965574
      },
      {
        "text": "Kronecker Product: This is a fundamental concept in linear algebra that states that the Kronecker product of two matrices A and B is a matrix that contains the products of the elements of A and B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9140556778404096
      },
      {
        "text": "Kronecker Product and Tensor Product: The Kronecker product and tensor product of two matrices A and B are new matrices formed by multiplying each element of A by the entire matrix B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9075296383032144
      },
      {
        "text": "Kronecker Product of Two Matrices: The Kronecker product of two matrices A and B is denoted by A ⊗ B and is defined as the matrix obtained by multiplying each element of A by the entire matrix B.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9067163674553812
      },
      {
        "text": "Kronecker Product: The Kronecker product is a way to combine two matrices into a larger matrix. It has applications in orthogonal projection matrices, particularly when dealing with block matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.859898025199261
      },
      {
        "text": "Kronecker Product: The Kronecker product is a mathematical operation that combines two matrices into a single matrix. It is used in linear algebra to study the properties of matrix equations and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9207298659299165
      },
      {
        "text": "Kronecker Product of Orthogonal Projection Matrices: This involves combining two orthogonal projection matrices into a larger matrix using the Kronecker product. It has applications in block matrices and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8430028658226749
      },
      {
        "text": "Kronecker product: A way of combining two matrices into a single matrix by multiplying them element-wise, which can be used to solve systems of linear equations and to simplify calculations involving matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8764574750159791
      },
      {
        "text": "Kronecker Product with Applications: A way of combining two matrices into a single matrix by multiplying them element-wise, which is used in linear algebra to solve systems of linear equations and to simplify calculations involving matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9115994907114502
      },
      {
        "text": "Kronecker product of matrices: The Kronecker product of two matrices is a way of combining the two matrices into a single matrix by multiplying them element-wise, which can be used to solve systems of linear equations and to simplify calculations involving matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9612504780829088
      }
    ]
  },
  {
    "representative_text": "Singular Value Bound: The singular value bound of a matrix A is a lower bound on the norm of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Singular Value Bound: The singular value bound of a matrix A is a lower bound on the norm of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Bound: The singular value bound of a matrix A is a lower bound on the norm of the matrix. This is an important concept that is related to the SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9057741067886687
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of SVD: The SVD is a numerically unstable algorithm, meaning that small errors in the input matrix can result in large errors in the output.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Numerical Stability of SVD: The SVD is a numerically unstable algorithm, meaning that small errors in the input matrix can result in large errors in the output.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Stability of SVD: The discussion of numerical stability of SVD, including the concept of numerical stability of SVD and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.829320458050996
      },
      {
        "text": "Numerical Stability of SVD: The SVD is a numerically unstable algorithm, meaning that small errors in the input matrix can result in large errors in the output. This is an important discussion of the numerical aspects of the SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9570136356878155
      },
      {
        "text": "Numerical Stability of Singular Value Decomposition (SVD): A discussion of the numerical stability of SVD, including its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9004208669016738
      },
      {
        "text": "Numerical Stability of SVD with Complex Entries: The SVD is a numerically unstable algorithm, meaning that small errors in the input matrix can result in large errors in the output. This is particularly true for matrices with complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9155747402616087
      }
    ]
  },
  {
    "representative_text": "Computational Complexity of SVD: The computational complexity of the SVD algorithm is O(n^3), where n is the size of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Computational Complexity of SVD: The computational complexity of the SVD algorithm is O(n^3), where n is the size of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Complexity of Singular Value Decomposition (SVD): The analysis of the computational complexity of SVD, including the time and space complexity of various algorithms for computing SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8612554894458277
      },
      {
        "text": "Computational complexity of SVD for large matrices: While the computational complexity of the SVD algorithm is O(n^3), there is a need to discuss the implications of this complexity for large matrices and the potential for more efficient algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8827690625864452
      },
      {
        "text": "Computational Complexity of SVD: The computational complexity of the SVD algorithm is O(n^3), where n is the size of the matrix. This is an important discussion of the computational complexity of the SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.955963403680876
      },
      {
        "text": "Computational Complexity of SVD Computation: This includes the analysis of the computational complexity of algorithms for computing SVD, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9171786149414549
      },
      {
        "text": "Computational Complexity of SVD for Large Matrices: The computational complexity of the SVD algorithm is O(n^3), where n is the size of the matrix. However, for large matrices, more efficient algorithms, such as the block QR algorithm, can be used to reduce the computational complexity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9369815951040124
      }
    ]
  },
  {
    "representative_text": "Applications of SVD beyond Image Compression: SVD has applications in other fields such as signal processing, data analysis, and machine learning.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Applications of SVD beyond Image Compression: SVD has applications in other fields such as signal processing, data analysis, and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Relationship between SVD and PCA: SVD and PCA are related, as the SVD of a matrix can be used to compute the eigenvectors of the matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relationship between SVD and PCA: SVD and PCA are related, as the SVD of a matrix can be used to compute the eigenvectors of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dominant Eigenvalues: These are eigenvalues that are greater in magnitude than the other eigenvalues of a matrix. They play a significant role in determining the stability and behavior of a linear system.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dominant Eigenvalues: These are eigenvalues that are greater in magnitude than the other eigenvalues of a matrix. They play a significant role in determining the stability and behavior of a linear system.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Deflation: This is the process of removing an eigenvalue from a matrix by subtracting a multiple of the identity matrix. Deflation can be used to simplify the computation of eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Deflation: This is the process of removing an eigenvalue from a matrix by subtracting a multiple of the identity matrix. Deflation can be used to simplify the computation of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Deflation: This is a technique used to find the eigenvalues and eigenvectors of a matrix by removing the largest eigenvalue from the matrix, which is useful in numerical computations of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9267372948302848
      },
      {
        "text": "The Use of Deflation in Numerical Computations: Deflation is a technique used to find the eigenvalues and eigenvectors of a matrix by removing the largest eigenvalue from the matrix, and understanding its application is essential in ensuring the accuracy of numerical computations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8841172245845283
      },
      {
        "text": "The Deflation Method: The deflation method is a technique used to find the eigenvalues and eigenvectors of a matrix by removing the largest eigenvalue from the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9288091183346369
      }
    ]
  },
  {
    "representative_text": "Block Diagonalization: This is a technique used to diagonalize a matrix by decomposing it into a block diagonal matrix. Block diagonalization is often used in conjunction with the Jordan canonical form.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 17,
    "detailed_sources": [
      {
        "text": "Block Diagonalization: This is a technique used to diagonalize a matrix by decomposing it into a block diagonal matrix. Block diagonalization is often used in conjunction with the Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Block Diagonalization: This technique is used to diagonalize a matrix by transforming it into a block diagonal matrix, which is useful in understanding the properties of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.908373601099105
      },
      {
        "text": "Block Diagonalization: This is a technique for diagonalizing a matrix by grouping its eigenvalues and eigenvectors into blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9118828442042587
      },
      {
        "text": "Block Diagonalization: This is a method for diagonalizing a matrix by transforming it into a block diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9422864856809112
      },
      {
        "text": "Block Diagonalization: Block diagonalization is a technique for diagonalizing a matrix by grouping its eigenvalues and eigenvectors into blocks. This is an important concept in linear algebra, as it can be used to diagonalize matrices with repeated eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.917174047802112
      },
      {
        "text": "The Use of Block Diagonalization in Numerical Computations: Block diagonalization is a technique used to diagonalize a matrix by transforming it into a block diagonal matrix, and understanding its application is essential in ensuring the accuracy of numerical computations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8722276897474377
      },
      {
        "text": "Block Triangularization: This is a technique used to diagonalize a matrix by grouping its eigenvalues and eigenvectors into blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9424680385942925
      },
      {
        "text": "Jordan Block Decomposition: This is a technique used to diagonalize a matrix by grouping its eigenvalues and eigenvectors into Jordan blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8935331444635899
      },
      {
        "text": "Eigenvalue Decomposition of Block Matrices: A process of transforming a block matrix into a diagonal matrix using eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.891953197793422
      },
      {
        "text": "The Diagonalization of a Matrix using the Block Diagonalization: The block diagonalization of a matrix can be used to diagonalize the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8341126701683198
      },
      {
        "text": "Block Triangularization: This technique is used to diagonalize a matrix by transforming it into a block triangular matrix, which is useful in understanding the properties of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9416997472321338
      },
      {
        "text": "Block Diagonalization for Non-Symmetric Matrices: This is an extension of the block diagonalization technique used for symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8098387839182211
      },
      {
        "text": "Block Diagonalization: This is a method for decomposing a matrix into a product of a block diagonal matrix and a unitary matrix. It can be used to compute the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9235137990084574
      },
      {
        "text": "Eigenvalue Decomposition of Block Matrices: This is a process of transforming a block matrix into a diagonal matrix using eigenvalue decomposition, which can be used to find the inverse of a block matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8844826381421066
      },
      {
        "text": "Eigenvalue Decomposition of Block Matrices with Complex Eigenvalues: This is a process of transforming a block matrix into a diagonal matrix using eigenvalue decomposition, which can be used to find the inverse of a block matrix with complex eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8563888082726498
      },
      {
        "text": "The Block Triangularization Method: The block triangularization method is a technique used to diagonalize a matrix by transforming it into a block triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9048296490866444
      },
      {
        "text": "Eigenvalue Decomposition of Block Matrices with Complex Eigenvalues (Generalized): A process of transforming a block matrix into a diagonal matrix using eigenvalue decomposition, which can be used to find the inverse of a block matrix with complex eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8184448807211304
      }
    ]
  },
  {
    "representative_text": "Jordan Chain: This is a sequence of generalized eigenvectors that are used to construct the Jordan canonical form of a matrix. The Jordan chain is a fundamental concept in the study of eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jordan Chain: This is a sequence of generalized eigenvectors that are used to construct the Jordan canonical form of a matrix. The Jordan chain is a fundamental concept in the study of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Generalized Eigenvectors: These are eigenvectors that have multiplicity greater than 1. Generalized eigenvectors play a crucial role in the construction of the Jordan canonical form.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Generalized Eigenvectors: These are eigenvectors that have multiplicity greater than 1. Generalized eigenvectors play a crucial role in the construction of the Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a generalized eigenvector: A generalized eigenvector of a matrix A is a vector v such that (A - λI)v = 0, but (A - λI)^2v ≠ 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8221309010694102
      }
    ]
  },
  {
    "representative_text": "Shifted Eigenvalues: These are eigenvalues that are shifted by a constant amount. Shifted eigenvalues can be used to diagonalize a matrix that is not diagonalizable.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Shifted Eigenvalues: These are eigenvalues that are shifted by a constant amount. Shifted eigenvalues can be used to diagonalize a matrix that is not diagonalizable.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Decomposition Theorems: These are theorems that provide a way to decompose a matrix into a product of simpler matrices, such as eigenvectors and eigenvalues. Decomposition theorems are essential in the study of eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Decomposition Theorems: These are theorems that provide a way to decompose a matrix into a product of simpler matrices, such as eigenvectors and eigenvalues. Decomposition theorems are essential in the study of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Regularization Methods: These are techniques used to improve the numerical stability of eigenvalue and eigenvector computations. Regularization methods include methods such as Titchmarsh's method and the Lanczos method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Regularization Methods: These are techniques used to improve the numerical stability of eigenvalue and eigenvector computations. Regularization methods include methods such as Titchmarsh's method and the Lanczos method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Computation using Rational Arithmetic: This is a technique used to compute eigenvalues of a matrix using rational arithmetic, rather than floating-point arithmetic. This technique is often used in applications where high precision is required.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvalue Computation using Rational Arithmetic: This is a technique used to compute eigenvalues of a matrix using rational arithmetic, rather than floating-point arithmetic. This technique is often used in applications where high precision is required.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Eigenvalues using Rational Arithmetic: Rational arithmetic is used to compute eigenvalues of a matrix using rational numbers, rather than floating-point numbers. This technique is often used in applications where high precision is required.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9277073428681785
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Computation using Geometric Multiplicity: This is a technique used to compute eigenvalues of a matrix by exploiting the geometric multiplicity of the eigenvalues. This technique is often used in applications where the geometric multiplicity is known.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Eigenvalue Computation using Geometric Multiplicity: This is a technique used to compute eigenvalues of a matrix by exploiting the geometric multiplicity of the eigenvalues. This technique is often used in applications where the geometric multiplicity is known.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Computation using Algebraic Multiplicity: This is a technique used to compute eigenvalues of a matrix by exploiting the algebraic multiplicity of the eigenvalues. This technique is often used in applications where the algebraic multiplicity is known.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9506143822965363
      },
      {
        "text": "The Computation of Eigenvalues using Geometric Multiplicity: Geometric multiplicity can be used to compute eigenvalues of a matrix by exploiting the properties of the eigenvectors. This technique is often used in applications where the geometric multiplicity is known.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9135130887398304
      },
      {
        "text": "The Computation of Eigenvalues using Algebraic Multiplicity: Algebraic multiplicity can be used to compute eigenvalues of a matrix by exploiting the properties of the characteristic equation. This technique is often used in applications where the algebraic multiplicity is known.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.941824125882391
      }
    ]
  },
  {
    "representative_text": "Semi-Simple Matrices: A matrix is semi-simple if all of its Jordan blocks are 1x1. This means that the matrix can be diagonalized, and its eigenvalues are distinct.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Semi-Simple Matrices: A matrix is semi-simple if all of its Jordan blocks are 1x1. This means that the matrix can be diagonalized, and its eigenvalues are distinct.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Spectral Radius: The spectral radius of a matrix is the maximum absolute value of its eigenvalues. The spectral radius is related to the stability of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "The Spectral Radius: The spectral radius of a matrix is the maximum absolute value of its eigenvalues. The spectral radius is related to the stability of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Eigenvalues and the Spectral Radius: The spectral radius is the maximum absolute value of the eigenvalues of a matrix. It is related to the stability of a linear system.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9420190358179878
      },
      {
        "text": "Spectral Radius: This is a concept in linear algebra that states that the spectral radius of a matrix A is the maximum of the absolute values of its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8947753775494078
      },
      {
        "text": "Spectral Radius: The spectral radius of a matrix is the maximum magnitude of its eigenvalues. It can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8631214019307615
      },
      {
        "text": "The concept of spectral radius: The maximum magnitude of the eigenvalues of a matrix, which can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.870253639124694
      },
      {
        "text": "Linear Transformations and the Spectral Radius of a Linear Transformation: The spectral radius of a linear transformation is the maximum absolute value of its eigenvalues. Understanding the properties of the spectral radius can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8471429224919305
      },
      {
        "text": "Spectral Radius of a Matrix: This is a concept that is closely related to eigenvalues and eigenvectors. It is defined as the maximum of the absolute values of the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8924973385959305
      },
      {
        "text": "Spectral Radius for Non-Symmetric Matrices: This is an extension of the spectral radius concept to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8067314929716489
      },
      {
        "text": "Spectral Radius: The spectral radius of a matrix is the maximum absolute value of its eigenvalues. It is used in signal processing and image analysis for tasks such as image filtering and signal compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732653370806186
      },
      {
        "text": "Spectral Radius: The spectral radius of a matrix is the maximum of the absolute values of its eigenvalues. It can be used to determine the convergence of a matrix to a limit as the matrix is multiplied by itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9337913999836226
      },
      {
        "text": "The Spectral Radius: The spectral radius of a matrix is the maximum of the absolute values of its eigenvalues. It is a useful concept in understanding the behavior of matrices and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9275916333418628
      }
    ]
  },
  {
    "representative_text": "The Inverse of a Matrix: The inverse of a matrix can be expressed in terms of its eigenvalues and eigenvectors. This is known as the eigenvalue decomposition of the inverse matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Inverse of a Matrix: The inverse of a matrix can be expressed in terms of its eigenvalues and eigenvectors. This is known as the eigenvalue decomposition of the inverse matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inversion using Eigendecomposition: The inverse of a matrix A can be computed using its eigendecomposition as A^(-1) = PDP^(-1), where P is a unitary matrix and D is a diagonal matrix containing the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8021905566662431
      },
      {
        "text": "Eigenvalue Decomposition: A process of transforming a matrix into a diagonal matrix using eigenvalue decomposition, which can be used to find the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8440199953244613
      },
      {
        "text": "The Inverse of a Matrix using Eigenvalues and Eigenvectors: The inverse of a matrix can be expressed in terms of its eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8996569078450379
      },
      {
        "text": "Diagonalization of a Matrix using the Inverse of a Matrix: The inverse of a matrix can be expressed in terms of its eigenvalues and eigenvectors, and this can be used to diagonalize a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.909769991748297
      }
    ]
  },
  {
    "representative_text": "Krylov Subspace: The Krylov subspace is a subspace spanned by the columns of a matrix raised to various powers. It is closely related to the QR algorithm and power method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Krylov Subspace: The Krylov subspace is a subspace spanned by the columns of a matrix raised to various powers. It is closely related to the QR algorithm and power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Krylov Subspace: A subspace spanned by the matrix-vector products of a matrix and a vector, which is closely related to the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9587549094562315
      },
      {
        "text": "The Krylov Subspace: This is a subspace used to estimate the eigenvalues of a matrix, which is useful in understanding the properties of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8710375392902459
      },
      {
        "text": "Krylov Subspace: The Krylov subspace is a subspace spanned by the columns of a matrix raised to various powers. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8912500157197051
      },
      {
        "text": "Krylov Subspace: A subspace spanned by the matrix-vector products of a matrix and a vector. Krylov subspaces are closely related to the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8897194042995262
      },
      {
        "text": "Krylov Subspace: The Krylov subspace is a subspace of a vector space that is spanned by a set of vectors that are related to a given linear transformation. It can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8496000572843305
      },
      {
        "text": "Krylov subspace: A subspace of a vector space that is spanned by a set of vectors that are related to a given linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8935531778976302
      },
      {
        "text": "Block Krylov Subspaces: A block Krylov subspace is a subspace spanned by the columns of the matrix K, where K is the matrix obtained by multiplying the original matrix A with a block of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8457192089177172
      },
      {
        "text": "Krylov Subspaces: A Krylov subspace is a subspace spanned by the columns of the matrix K, where K is the matrix obtained by multiplying the original matrix A with a vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9398383907239101
      },
      {
        "text": "Krylov Subspaces: A Krylov subspace is a subspace spanned by the columns of the matrix K, and eigenvector decomposition of the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8664911224304213
      },
      {
        "text": "Krylov Subspaces: Krylov subspaces are subspaces spanned by the columns of a matrix raised to the power of k, where k is a positive integer.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9481364440403579
      },
      {
        "text": "Krylov Subspaces with Numerical Stability: Krylov subspaces are subspaces spanned by the columns of a matrix raised to the power of k, where k is a positive integer. Numerical stability refers to the sensitivity of numerical methods to small changes in the input data. Krylov subspaces with numerical stability are used in numerical linear algebra to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8440251430357363
      }
    ]
  },
  {
    "representative_text": "Deflation: Deflation is a process used in the QR algorithm to eliminate the largest eigenvalue of a matrix A. This is done by subtracting a multiple of the largest eigenvector from the original matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Deflation: Deflation is a process used in the QR algorithm to eliminate the largest eigenvalue of a matrix A. This is done by subtracting a multiple of the largest eigenvector from the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Pivoting: Pivoting is a technique used in the QR algorithm to improve numerical stability by swapping rows or columns of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Pivoting: Pivoting is a technique used in the QR algorithm to improve numerical stability by swapping rows or columns of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Deflation and Pivoting: These are techniques used in the QR algorithm to improve numerical stability by eliminating the largest eigenvalue of a matrix A and swapping rows or columns of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8438823331957179
      },
      {
        "text": "Deflation and Pivoting: Deflation and pivoting are techniques used in the QR algorithm to improve numerical stability by eliminating the largest eigenvalue of a matrix A and swapping rows or columns of the matrix. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8794881806627323
      }
    ]
  },
  {
    "representative_text": "Eckart-Young Theorem: If A is a square matrix and v is a non-zero vector, then the vector that minimizes the Rayleigh quotient of A with respect to v is the eigenvector of A corresponding to the smallest eigenvalue.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eckart-Young Theorem: If A is a square matrix and v is a non-zero vector, then the vector that minimizes the Rayleigh quotient of A with respect to v is the eigenvector of A corresponding to the smallest eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bilinear Form: A bilinear form is a function that takes two vectors as input and returns a scalar value. Eigenvalues and eigenvectors can be used to analyze bilinear forms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "Bilinear Form: A bilinear form is a function that takes two vectors as input and returns a scalar value. Eigenvalues and eigenvectors can be used to analyze bilinear forms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bilinear Forms and Finite-Dimensional Vector Spaces: Bilinear forms are fundamental concepts in linear algebra, where a bilinear form is a function from a vector space to the real numbers that is linear in each argument separately.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8181417690478103
      },
      {
        "text": "Bilinear Forms and Infinite-Dimensional Vector Spaces: Bilinear forms are fundamental concepts in linear algebra, where a bilinear form is a function from a vector space to the real numbers that is linear in each argument separately. This concept has important implications for linear algebra and its applications in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9196747030068743
      },
      {
        "text": "Bilinear Forms: A bilinear form is a function that takes two vectors as input and returns a scalar value. Bilinear forms are used to define inner products and are essential in the study of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.922219593340336
      },
      {
        "text": "Bilinear Forms on Infinite-Dimensional Vector Spaces: Bilinear forms on infinite-dimensional vector spaces can be used to represent linear transformations and matrices, but their properties may differ from those on finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8334859567968351
      },
      {
        "text": "Bilinear Forms and Linear Algebra: Bilinear forms provide a fundamental connection between bilinear forms and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8813523016591311
      },
      {
        "text": "Bilinear Forms and Inner Product Spaces: This involves studying the relationship between bilinear forms and inner product spaces, including the definition of bilinear forms, bilinear form properties, and the characterization of bilinear form spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8053682053237323
      },
      {
        "text": "Bilinear Form and its Properties: A bilinear form is a function that takes two vectors as input and returns a scalar value. Bilinear forms have numerous applications in linear algebra, including the study of symmetric and skew-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9295345375471586
      },
      {
        "text": "Bilinear Form: A bilinear form is a quadratic form that can be expressed as a sum of products of the components of a vector. Determinants can be used to calculate bilinear forms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8373268405076113
      },
      {
        "text": "Bilinear forms: A way of representing inner products between vectors as linear combinations of dot products, which can be used to simplify calculations involving inner products.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8270802080362163
      },
      {
        "text": "Bilinear Form with Applications: A way of representing inner products between vectors as linear combinations of dot products, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8452586155077557
      },
      {
        "text": "Bilinear Forms: A bilinear form is a function that takes two vectors as input and returns a scalar value. Bilinear forms are used to define quadratic forms and to study the properties of matrices.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9408719733996962
      },
      {
        "text": "Bilinear Form Theory: Bilinear form theory is a branch of mathematics that deals with bilinear forms and their properties.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8265992990448021
      },
      {
        "text": "Bilinear Form Theory with Quadratic Forms: Bilinear form theory is a branch of mathematics that deals with bilinear forms and their properties. Quadratic forms are a type of bilinear form that is used to study the properties of matrices. Bilinear form theory with quadratic forms is used in numerical linear algebra to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8524339292862824
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Distribution: The eigenvalue distribution of a matrix A refers to the distribution of its eigenvalues. Eigenvalues and eigenvectors can be used to analyze the eigenvalue distribution of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Distribution: The eigenvalue distribution of a matrix A refers to the distribution of its eigenvalues. Eigenvalues and eigenvectors can be used to analyze the eigenvalue distribution of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bessel's Inequality: If A is a square matrix and v is a vector, then the sum of the squares of the components of Av is less than or equal to the sum of the squares of the components of v.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bessel's Inequality: If A is a square matrix and v is a vector, then the sum of the squares of the components of Av is less than or equal to the sum of the squares of the components of v.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Decomposition Theorem: This theorem states that for a vector space V and a subspace W, if U is a complement of W, then V = W ⊕ U, where ⊕ denotes the direct sum.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Decomposition Theorem: This theorem states that for a vector space V and a subspace W, if U is a complement of W, then V = W ⊕ U, where ⊕ denotes the direct sum.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement of the Orthogonal Complement: This involves finding the orthogonal complement of the orthogonal complement of a subspace. This is an extension of the orthogonal complement theorem and has applications in various areas of linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement of the Orthogonal Complement: This involves finding the orthogonal complement of the orthogonal complement of a subspace. This is an extension of the orthogonal complement theorem and has applications in various areas of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Complement of the Orthogonal Complement Theorem Variations: The study of the orthogonal complement of the orthogonal complement of a subspace involves various extensions of the orthogonal complement theorem. These variations have applications in various areas of linear algebra and other fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8700191022203103
      },
      {
        "text": "Orthogonal Complement of the Orthogonal Complement Theorem Variations: As mentioned earlier, this involves various extensions of the orthogonal complement theorem. These variations have applications in various areas of linear algebra and other fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9264470202312927
      },
      {
        "text": "Orthogonal Complement Theorem Variations: This involves various extensions of the orthogonal complement theorem and has applications in various areas of linear algebra and other fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9407397042033685
      },
      {
        "text": "Orthogonal Complement Theorem Variations: The Orthogonal Complement Theorem can be extended to other vector spaces, such as inner product spaces, and its variations can be used to solve optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8975913450886083
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and Linear Transformations: The relationship between the orthogonal complement of a subspace and linear transformations applied to that subspace is an important area of study.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 28,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and Linear Transformations: The relationship between the orthogonal complement of a subspace and linear transformations applied to that subspace is an important area of study.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Complement and Eigenvalues: The relationship between the orthogonal complement of a subspace and the eigenvalues of linear transformations applied to that subspace is an important area of study.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9204180537341218
      },
      {
        "text": "Orthogonal Complement and the Interplay between Orthogonality and Orthogonality: This involves the interplay between orthogonality and orthogonality in the context of linear algebra, which is an area of active research and has many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8336047520937742
      },
      {
        "text": "Orthogonal Complement and the study of orthogonal matrices: Orthogonal matrices are square matrices whose columns and rows are orthonormal vectors. The study of orthogonal matrices is closely related to the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.816676735403931
      },
      {
        "text": "Orthogonal Complement and the study of orthogonal polynomials: Orthogonal polynomials are polynomials that are orthogonal to each other with respect to a certain inner product. The study of orthogonal polynomials is closely related to the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8532893718386725
      },
      {
        "text": "Orthogonal Complement and the study of orthogonal tensors: Orthogonal tensors are tensors that are orthogonal to each other with respect to a certain inner product. The study of orthogonal tensors is closely related to the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8533656881816385
      },
      {
        "text": "Orthogonal Complement and the study of orthogonal manifolds: Orthogonal manifolds are manifolds that are orthogonal to each other with respect to a certain inner product. The study of orthogonal manifolds is closely related to the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.881386322987846
      },
      {
        "text": "Orthogonal Complement and the study of orthogonal vector fields: Orthogonal vector fields are vector fields that are orthogonal to each other with respect to a certain inner product. The study of orthogonal vector fields is closely related to the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8944423129811219
      },
      {
        "text": "Orthogonal Complement and the study of orthogonal differential equations: Orthogonal differential equations are differential equations that are orthogonal to each other with respect to a certain inner product. The study of orthogonal differential equations is closely related to the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8998633547294341
      },
      {
        "text": "Orthogonal Complement and Geometric Interpretations: The orthogonal complement of a subspace has various geometric interpretations, such as the orthogonal complement of a plane in 3D space. These interpretations can help visualize and understand the properties of orthogonal complements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8539115164447415
      },
      {
        "text": "Orthogonal Complement and Spectral Theory: The orthogonal complement of a subspace is closely related to spectral theory, which studies the properties of linear transformations and their eigenvalues. The study of orthogonal complements can provide insights into the spectral properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.884597468930648
      },
      {
        "text": "Orthogonal Complement and Representation Theory: The orthogonal complement of a subspace is also related to representation theory, which studies the representations of linear transformations as matrices. The study of orthogonal complements can provide insights into the representation theory of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8848734867365053
      },
      {
        "text": "Orthogonal Complement and Operator Theory: The orthogonal complement of a subspace is also related to operator theory, which studies the properties of linear operators. The study of orthogonal complements can provide insights into the operator theory of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9063789420357001
      },
      {
        "text": "The Interplay between Orthogonality and Orthogonality: This involves the interplay between orthogonality and orthogonality in the context of linear algebra, which is an area of active research and has many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8018040762636984
      },
      {
        "text": "The Study of Orthogonal Complement of a Subspace with Respect to a Non-Standard Inner Product: This involves studying the orthogonal complement of a subspace with respect to a non-standard inner product, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8034645278562114
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Systems: This involves studying the properties of orthogonal systems, which are sets of vectors that are orthogonal to each other with respect to a certain inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8589529052577677
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Matrices with Non-Standard Orthogonality: This involves studying the properties of orthogonal matrices with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.834054499780136
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Polynomials with Non-Standard Orthogonality: This involves studying the properties of orthogonal polynomials with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.83233141425393
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Functions with Non-Standard Orthogonality: This involves studying the properties of orthogonal functions with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8653695399168346
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Tensors with Non-Standard Orthogonality: This involves studying the properties of orthogonal tensors with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8654235770866391
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Manifolds with Non-Standard Orthogonality: This involves studying the properties of orthogonal manifolds with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8915927130342336
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Vector Fields with Non-Standard Orthogonality: This involves studying the properties of orthogonal vector fields with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9070606703116232
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Differential Equations with Non-Standard Orthogonality: This involves studying the properties of orthogonal differential equations with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8919765425334327
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Complement of a Subspace with Respect to a Non-Standard Metric: This involves studying the orthogonal complement of a subspace with respect to a non-standard metric, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9157997498633141
      },
      {
        "text": "Orthogonal Complement of a Subspace with Respect to a Non-Standard Connection: This involves studying the orthogonal complement of a subspace with respect to a non-standard connection, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8856281490873318
      },
      {
        "text": "Orthogonal Complement of a Subspace with Respect to a Non-Standard Metric-Tensor: This involves studying the orthogonal complement of a subspace with respect to a non-standard metric-tensor, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8951278215253056
      },
      {
        "text": "Orthogonal Complement of a Linear Transformation: The orthogonal complement of a linear transformation is the set of vectors that are orthogonal to every vector in the image of the transformation. This concept is essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8245932453171413
      },
      {
        "text": "Orthogonal Complement of a Subspace with Respect to a Non-Standard Basis: This involves studying the orthogonal complement of a subspace with respect to a non-standard basis, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9031500805066379
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and Singular Value Decomposition (SVD): The SVD of a matrix involves finding the orthogonal complement of the null space of the matrix, which is closely related to the orthogonal complement of a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and Singular Value Decomposition (SVD): The SVD of a matrix involves finding the orthogonal complement of the null space of the matrix, which is closely related to the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and the Fundamental Theorem of Linear Algebra: This theorem states that every linear transformation has an orthogonal complement. This theorem is a fundamental result in linear algebra and has far-reaching implications in various areas of mathematics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and the Fundamental Theorem of Linear Algebra: This theorem states that every linear transformation has an orthogonal complement. This theorem is a fundamental result in linear algebra and has far-reaching implications in various areas of mathematics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Null Space of a Linear Transformation: This concept is related to the rank-nullity theorem and states that the null space of a linear transformation $T$ is equal to the orthogonal complement of its image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8397742786932327
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and the study of orthogonal functions: Orthogonal functions are functions that are orthogonal to each other with respect to a certain inner product. The study of orthogonal functions is closely related to the orthogonal complement of a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and the study of orthogonal functions: Orthogonal functions are functions that are orthogonal to each other with respect to a certain inner product. The study of orthogonal functions is closely related to the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonalization Process: The process of transforming a matrix into an orthogonal matrix, typically using the Gram-Schmidt process or Householder transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonalization Process: The process of transforming a matrix into an orthogonal matrix, typically using the Gram-Schmidt process or Householder transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonalization of Matrices: This is a process for orthogonalizing a matrix, which is essential in many applications, such as linear algebra, computer graphics, and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.846241989416149
      }
    ]
  },
  {
    "representative_text": "Orthogonal Decomposition: The decomposition of a matrix into the product of an orthogonal matrix and a diagonal matrix, where the orthogonal matrix represents the projection onto the range of the matrix, and the diagonal matrix represents the scaling factor.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Orthogonal Decomposition: The decomposition of a matrix into the product of an orthogonal matrix and a diagonal matrix, where the orthogonal matrix represents the projection onto the range of the matrix, and the diagonal matrix represents the scaling factor.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Matrix Decomposition: An orthogonal matrix can be decomposed into the product of an orthogonal matrix and a diagonal matrix, where the orthogonal matrix represents the projection onto the range of the matrix, and the diagonal matrix represents the scaling factor.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.935514206974096
      },
      {
        "text": "Orthogonal Decomposition of a Matrix (Generalized): A matrix can be decomposed into the product of an orthogonal matrix and a diagonal matrix, where the orthogonal matrix represents the projection onto the range of the matrix, and the diagonal matrix represents the scaling factor, which can be generalized to non-orthogonal decompositions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9140057562467959
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Properties: The properties of orthogonal matrices, such as the fact that the transpose of an orthogonal matrix is its inverse, and that the determinant of an orthogonal matrix is either 1 or -1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Properties: The properties of orthogonal matrices, such as the fact that the transpose of an orthogonal matrix is its inverse, and that the determinant of an orthogonal matrix is either 1 or -1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Matrix Determinant Properties: The properties of the determinant of an orthogonal matrix, such as the fact that the determinant of an orthogonal matrix is either 1 or -1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9296363837085871
      }
    ]
  },
  {
    "representative_text": "Orthogonal Diagonalization Conditions: The conditions under which a matrix can be orthogonally diagonalized, such as the requirement that the matrix must have a full set of linearly independent eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Diagonalization Conditions: The conditions under which a matrix can be orthogonally diagonalized, such as the requirement that the matrix must have a full set of linearly independent eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Diagonalization Algorithms: Various algorithms for orthogonally diagonalizing a matrix, such as the Jacobi method, Givens rotation, and Householder transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Orthogonal Diagonalization Algorithms: Various algorithms for orthogonally diagonalizing a matrix, such as the Jacobi method, Givens rotation, and Householder transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Methods for Orthogonal Diagonalization: Numerical methods for orthogonal diagonalization, such as the QR algorithm, which is an iterative method for finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8562386311115744
      },
      {
        "text": "Computational Complexity of Orthogonal Diagonalization Algorithms: The computational complexity of various algorithms for orthogonal diagonalization, including the Jacobi method, Givens rotation, and Householder transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8615440587057588
      },
      {
        "text": "Numerical Methods for Finding Orthogonal Diagonalization Conditions: Numerical methods for finding the conditions under which a matrix can be orthogonally diagonalized, including the QR algorithm and the Jacobi method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8832039667845499
      },
      {
        "text": "Numerical Methods for Finding Orthogonal Diagonalization Conditions (Advanced): Numerical methods for finding the conditions under which a matrix can be orthogonally diagonalized, such as the QR algorithm and the Jacobi method, can be advanced using techniques such as iterative methods and regularization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8267886515607121
      }
    ]
  },
  {
    "representative_text": "Numerical Stability: The numerical stability of orthogonal diagonalization methods, which can be affected by round-off errors and other numerical issues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Numerical Stability: The numerical stability of orthogonal diagonalization methods, which can be affected by round-off errors and other numerical issues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Numerical Stability of Orthogonal Diagonalization Methods: The numerical stability of orthogonal diagonalization methods, which can be affected by round-off errors and other numerical issues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9422832093804747
      },
      {
        "text": "The Numerical Stability of Orthogonal Diagonalization Methods: The numerical stability of orthogonal diagonalization methods is a concern in numerical linear algebra, as small errors in the computation can lead to large errors in the result.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8880997045643677
      },
      {
        "text": "Numerical Stability of Orthogonal Diagonalization Methods (Advanced): The numerical stability of orthogonal diagonalization methods can be affected by round-off errors and other numerical issues, and can be improved using advanced techniques such as iterative methods and regularization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9008572817655085
      }
    ]
  },
  {
    "representative_text": "Computational Complexity: The computational complexity of orthogonal diagonalization methods, which can vary depending on the algorithm used and the size of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Computational Complexity: The computational complexity of orthogonal diagonalization methods, which can vary depending on the algorithm used and the size of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computational Complexity of Orthogonal Diagonalization Algorithms: The computational complexity of orthogonal diagonalization algorithms can vary depending on the algorithm used and the size of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9227455231095061
      },
      {
        "text": "Computational Complexity of Orthogonal Diagonalization Algorithms (Advanced): The computational complexity of orthogonal diagonalization algorithms can vary depending on the algorithm used and the size of the matrix, and can be improved using advanced techniques such as parallel processing and optimization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9160522654972509
      },
      {
        "text": "Computational Complexity of Finding Orthogonal Diagonalization Conditions (Advanced): The computational complexity of finding the conditions under which a matrix can be orthogonally diagonalized can vary depending on the algorithm used and the size of the matrix, and can be improved using advanced techniques such as parallel processing and optimization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8950070678290798
      }
    ]
  },
  {
    "representative_text": "Connection to Other Linear Algebra Concepts: The connection between orthogonal diagonalization and other linear algebra concepts, such as eigenvectors, eigenvalues, and linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Connection to Other Linear Algebra Concepts: The connection between orthogonal diagonalization and other linear algebra concepts, such as eigenvectors, eigenvalues, and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Connection to Other Linear Algebra Concepts: Orthogonal Diagonalization and Eigenvalue Decomposition: The connection between orthogonal diagonalization and eigenvalue decomposition, which is a technique used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9477636437533454
      },
      {
        "text": "Connection to Other Linear Algebra Concepts: Orthogonal Diagonalization and Orthogonal Complement: The connection between orthogonal diagonalization and the orthogonal complement, which is a subspace that is orthogonal to a given subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9530305066311606
      },
      {
        "text": "Orthogonal Diagonalization and Linear Transformations: Range and Null Space: The relationship between orthogonal diagonalization and linear transformations, including the fact that orthogonal diagonalization can be used to find the range and null space of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8514639323900217
      },
      {
        "text": "The Connection between Orthogonal Diagonalization and Eigenvalue Decomposition: The connection between orthogonal diagonalization and eigenvalue decomposition, which is a technique used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8115342934413037
      },
      {
        "text": "The Connection between Orthogonal Diagonalization and Eigenvalue Decomposition: The connection between orthogonal diagonalization and eigenvalue decomposition is a technique used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8518297135339359
      },
      {
        "text": "Connection between Orthogonal Diagonalization and Linear Algebra Concepts: Orthogonal diagonalization is closely related to other linear algebra concepts, such as eigenvectors, eigenvalues, and linear transformations, and provides a way to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.928509064906164
      },
      {
        "text": "Connection between Orthogonal Diagonalization and Eigenvalue Decomposition: Orthogonal diagonalization is closely related to eigenvalue decomposition, and provides a way to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9152591340356862
      },
      {
        "text": "Connection between Orthogonal Diagonalization and Linear Transformations: Orthogonal diagonalization is closely related to linear transformations, and provides a way to find the eigenvalues and eigenvectors of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9265873824516164
      }
    ]
  },
  {
    "representative_text": "Pythagorean Theorem for Orthogonal Vectors: This theorem states that for any two orthogonal vectors u and v, the norm of their sum is equal to the square root of the sum of their norms squared, i.e., ||u + v||² = ||u||² + ||v||².",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pythagorean Theorem for Orthogonal Vectors: This theorem states that for any two orthogonal vectors u and v, the norm of their sum is equal to the square root of the sum of their norms squared, i.e., ||u + v||² = ||u||² + ||v||².",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonalization of a Vector: Given an orthogonal vector u, the orthogonalization of u is the process of finding another orthogonal vector v such that u and v are orthogonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonalization of a Vector: Given an orthogonal vector u, the orthogonalization of u is the process of finding another orthogonal vector v such that u and v are orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Orthogonalization of a Matrix: Given a matrix A, the orthogonalization of A is the process of finding an orthogonal matrix U such that U^T A U is diagonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Orthogonalization of a Matrix: Given a matrix A, the orthogonalization of A is the process of finding an orthogonal matrix U such that U^T A U is diagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Orthogonal Decomposition of a Matrix: Given a matrix A, the orthogonal decomposition of A is the process of finding an orthogonal matrix U and a diagonal matrix Σ such that A = U Σ V^T, where V is an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8710010421452135
      },
      {
        "text": "Orthogonal Decomposition of a Matrix: The orthogonal decomposition of a matrix is a factorization of the matrix into the product of three matrices: an orthogonal matrix, a diagonal matrix, and a diagonal matrix. This concept is essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8143475887967542
      }
    ]
  },
  {
    "representative_text": "The Schmidt Matrix: This is a matrix that represents the coefficients of the orthogonal decomposition of a vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "The Schmidt Matrix: This is a matrix that represents the coefficients of the orthogonal decomposition of a vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Orthogonality of the Schmidt Matrix: Given a vector u, the Schmidt matrix represents the coefficients of the orthogonal decomposition of u. However, it is not explicitly stated whether the Schmidt matrix is orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.800325761355888
      },
      {
        "text": "Properties of the Schmidt Matrix: The Schmidt matrix represents the coefficients of the orthogonal decomposition of a vector. However, it is crucial to investigate whether the Schmidt matrix is orthogonal and its properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9408066271668489
      },
      {
        "text": "Orthogonality of the Schmidt Matrix: Given a vector u, the Schmidt matrix represents the coefficients of the orthogonal decomposition of u. However, it is necessary to verify whether the Schmidt matrix is orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9318480990716673
      },
      {
        "text": "Orthogonality of the Schmidt Process: Given a vector u, the Schmidt process represents the coefficients of the orthogonal decomposition of u. However, it is necessary to verify whether the Schmidt process is orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.867063241722877
      },
      {
        "text": "The Schmidt Matrix and Orthogonality of Matrices: The Schmidt matrix represents the coefficients of the orthogonal decomposition of a vector. However, it is crucial to investigate whether the Schmidt matrix is orthogonal and its properties when applied to matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9468505149156642
      },
      {
        "text": "Properties of the Schmidt Matrix for Matrices: The Schmidt matrix can be extended to matrices, and its properties can be investigated in this context.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8121869751818491
      },
      {
        "text": "Properties of the Schmidt Matrix for Complex Matrices: The Schmidt matrix can be extended to complex matrices, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.829944125355248
      },
      {
        "text": "Properties of the Schmidt Process for Matrices: The Schmidt process can be extended to orthogonalize matrices, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8686023155537826
      },
      {
        "text": "Orthogonal Decomposition of a Matrix using the Schmidt Process: The Schmidt process can be used to orthogonalize a matrix, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8775512559394689
      },
      {
        "text": "Properties of the Schmidt Matrix for Matrices using the Schmidt Process and Gram-Schmidt Process: The Schmidt matrix can be extended to matrices using the Schmidt process and the Gram-Schmidt process, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8782253019260511
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection using Singular Value Decomposition (SVD): This involves decomposing the original matrix into its SVD components and then using the singular values as the eigenvalues of the projection matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection using Singular Value Decomposition (SVD): This involves decomposing the original matrix into its SVD components and then using the singular values as the eigenvalues of the projection matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Projection using Polar Decomposition: This involves decomposing the original matrix into its polar decomposition components and then using the polar decomposition to construct the orthogonal projection matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8726961940089881
      },
      {
        "text": "Orthogonal Decomposition using Singular Value Decomposition (SVD): Decomposing a matrix into its SVD components and using the singular values as the eigenvalues of the projection matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9018930175457542
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection of a Vector onto a Subspace using a Given Basis: This involves projecting a vector onto a subspace using a given orthonormal basis, and this can be done using the Gram-Schmidt process.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 72,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection of a Vector onto a Subspace using a Given Basis: This involves projecting a vector onto a subspace using a given orthonormal basis, and this can be done using the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Projection of a Matrix onto a Subspace using a Householder Transformation: This involves using Householder transformations to construct an orthogonal projection matrix that projects any matrix onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8213125243303513
      },
      {
        "text": "Orthogonal Projection and Linear Transformations: This involves understanding how orthogonal projection matrices are used to represent linear transformations that map a vector to its closest approximation in a given subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8287763049198383
      },
      {
        "text": "The Gram-Schmidt Process and Orthogonal Projection: This involves understanding how the Gram-Schmidt process is used to construct an orthonormal basis and how it is related to orthogonal projection.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8325533213967793
      },
      {
        "text": "Householder Transformations and Orthogonal Projection: This involves understanding how Householder transformations are used to construct an orthogonal projection matrix and how they are related to orthogonal projection.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8851345637682992
      },
      {
        "text": "QR Decomposition and Orthogonal Projection: This involves understanding how the QR decomposition is used to construct an orthogonal projection matrix and how it is related to orthogonal projection.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8766583665990221
      },
      {
        "text": "Polar Decomposition and Orthogonal Projection: This involves understanding how the polar decomposition is used to construct an orthogonal projection matrix and how it is related to orthogonal projection.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8537010635618749
      },
      {
        "text": "Orthogonal Projection and Dimensionality Reduction: This involves understanding how orthogonal projection is used to reduce the dimensionality of a dataset and how it is related to techniques such as PCA and SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8176977825649637
      },
      {
        "text": "Orthogonal Projection of a Subspace onto a Linear Transformation: This involves understanding how to project a subspace onto a linear transformation while preserving the orthogonality between the projected subspace and the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8612143876775464
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Decompositions: This involves understanding how orthogonal projection matrices can be used to decompose a matrix into its orthogonal components, and vice versa.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.89189829703556
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Approximating Functions: This involves understanding how orthogonal projection matrices can be used to approximate functions in a given subspace, and how they can be used to improve the accuracy of approximations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8104798685515447
      },
      {
        "text": "The Relationship between Orthogonal Projection Matrices and Orthogonal Series Expansions: This involves understanding how orthogonal projection matrices can be used to expand functions in orthogonal series, and how they can be used to improve the accuracy of expansions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.811475520783903
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Operators: This involves understanding how orthogonal projection matrices can be used to represent orthogonal operators, and how they can be used to improve the accuracy of approximations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8640746895949545
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Image Reconstruction: This involves understanding how orthogonal projection matrices can be used to reconstruct images in a given subspace, and how they can be used to improve the accuracy of reconstructions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8552734295999255
      },
      {
        "text": "The Relationship between Orthogonal Projection Matrices and Orthogonal Filtering: This involves understanding how orthogonal projection matrices can be used to filter signals in a given subspace, and how they can be used to improve the accuracy of filtering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8464879046458977
      },
      {
        "text": "Orthogonal Projections: Orthogonal projections involve finding the projection of a vector onto a subspace using an orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8690989461656109
      },
      {
        "text": "The Orthogonal Projection of a Matrix: The projection of a matrix onto a subspace, which can be used to find the range and null space of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8553317860172927
      },
      {
        "text": "Orthogonal Projection and Inner Product Spaces: This involves understanding how orthogonal projection matrices are used in inner product spaces, and how they relate to the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8440629919740067
      },
      {
        "text": "Orthogonal Projection and Linear Transformations on Hilbert Spaces: This involves understanding how orthogonal projection matrices are used to represent linear transformations on Hilbert spaces, and how they relate to the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8502555569128832
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Operators on Hilbert Spaces: This involves understanding how orthogonal projection matrices can be used to represent orthogonal operators on Hilbert spaces, and how they relate to the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8619183329593705
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Quantum Mechanics: This involves understanding how orthogonal projection matrices are used in quantum mechanics to represent projection operators, and how they relate to the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8278280679774316
      },
      {
        "text": "The Relationship between Orthogonal Projection Matrices and Orthogonal Series Expansions in Functional Analysis: This involves understanding how orthogonal projection matrices can be used to expand functions in orthogonal series in functional analysis, and how they relate to the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.807946806934306
      },
      {
        "text": "Orthogonal Projection and Symmetric Spaces: This involves understanding how orthogonal projection matrices are used in symmetric spaces, and how they relate to the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8402261446662191
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Signal Processing: This involves understanding how orthogonal projection matrices are used in signal processing to filter signals, and how they relate to the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8636050140515599
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Decompositions in Algebra: This involves understanding how orthogonal projection matrices can be used to decompose a matrix into its orthogonal components, and how they relate to the algebraic structure of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8425959066949
      },
      {
        "text": "Orthogonal Projection and Operator Algebras: This involves understanding how orthogonal projection matrices are used in operator algebras to represent projections, and how they relate to the algebraic structure of the operators.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8294198939347263
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Approximation Theory: This involves understanding how orthogonal projection matrices are used in approximation theory to approximate functions, and how they relate to the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8774899317223931
      },
      {
        "text": "The Relationship between Orthogonal Projection Matrices and Orthogonal Series Expansions in Operator Theory: This involves understanding how orthogonal projection matrices can be used to expand operators in orthogonal series, and how they relate to the algebraic structure of the operators.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.826413448983903
      },
      {
        "text": "Projection Matrices and Span: Projection matrices are used to find the span of a subspace. Understanding the relationship between projection matrices and the span of a subspace is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8026658877776662
      },
      {
        "text": "The Orthogonal Projection of a Matrix: The orthogonal projection of a matrix is the projection of the matrix onto a subspace, which can be used to find the range and null space of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8491698638806224
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Geometric Algebra: This involves understanding how orthogonal projection matrices are used in geometric algebra to represent geometric transformations, such as rotations and projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8427420118872653
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Decompositions in Differential Geometry: This involves understanding how orthogonal projection matrices are used to decompose differential forms and tensors into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8086385897104729
      },
      {
        "text": "Orthogonal Projection and the Orthogonal Decomposition of Tensor Fields: This involves understanding how orthogonal projection matrices are used to decompose tensor fields into their orthogonal components, which is important in many areas of physics, such as general relativity and quantum field theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8269349181309498
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Series Expansions in Functional Analysis: This involves understanding how orthogonal projection matrices are used to expand functions in orthogonal series in functional analysis, and how they relate to the algebraic structure of the functions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8021679555462806
      },
      {
        "text": "Orthogonal Projection and the Orthogonal Decomposition of Hilbert Spaces: This involves understanding how orthogonal projection matrices are used to decompose Hilbert spaces into their orthogonal components, which is important in many areas of mathematics and physics, such as functional analysis and quantum mechanics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8683637161194476
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Nonlinear Programming: This involves understanding how orthogonal projection matrices are used in nonlinear programming to solve optimization problems, such as in the study of convex optimization and variational calculus.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.807954211482078
      },
      {
        "text": "Orthogonal Projections: Orthogonal projections are a type of linear transformation that can be used to diagonalize matrices and find eigenvalues and eigenvectors. There are many theorems and properties related to orthogonal projections, such as the fact that they are idempotent and self-adjoint.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8335699928790062
      },
      {
        "text": "The Orthogonal Projections: This concept is related to the SVD and states that an orthogonal projection is a linear transformation that projects a vector onto a subspace, resulting in a vector that is orthogonal to the orthogonal complement of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8465892950012126
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Decompositions in Topology: This involves understanding how orthogonal projection matrices are used to decompose topological spaces into their orthogonal components, which is important in many areas of mathematics and physics, such as topology and differential geometry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8531513116686795
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Computational Linear Algebra: This involves understanding how orthogonal projection matrices are used in computational linear algebra to solve systems of linear equations, compute eigenvalues and eigenvectors, and perform other linear algebra operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8667554016373953
      },
      {
        "text": "The Relationship between Orthogonal Projection Matrices and Orthogonal Series Expansions in Mathematical Physics: This involves understanding how orthogonal projection matrices are used to expand functions in orthogonal series in mathematical physics, such as in the study of quantum mechanics and general relativity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8224366918028646
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Decompositions in Algebraic Geometry: This involves understanding how orthogonal projection matrices are used to decompose algebraic varieties into their orthogonal components, which is important in many areas of mathematics and physics, such as algebraic geometry and differential geometry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8387042298611973
      },
      {
        "text": "Orthogonal Projection and the Orthogonal Decomposition of Hilbert Spaces with Infinite-Dimensional Orthonormal Bases: This involves understanding how orthogonal projection matrices are used to decompose Hilbert spaces with infinite-dimensional orthonormal bases into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8774490205982581
      },
      {
        "text": "Orthogonal Projection and the Orthogonal Decomposition of Operator Algebras: This involves understanding how orthogonal projection matrices are used to decompose operator algebras into their orthogonal components, which is important in many areas of mathematics and physics, such as operator algebras and quantum mechanics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8895077891797883
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Approximation Theory and Mathematical Analysis: This involves understanding how orthogonal projection matrices are used in approximation theory and mathematical analysis to approximate functions, solve equations, and perform other mathematical analysis tasks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8286779316867943
      },
      {
        "text": "Projection of a Matrix onto a Subspace using a Given Orthogonal Basis: This involves projecting a matrix onto a subspace using a given orthogonal basis and can be achieved using the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8531125583545373
      },
      {
        "text": "Properties of Orthogonal Projection Matrices in Inner Product Spaces: This includes studying the properties of orthogonal projection matrices in inner product spaces, such as their eigenvalues, eigenvectors, and the relationship with the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.858546105700134
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Decompositions in Differential Geometry: This includes studying the connection between orthogonal projection matrices and orthogonal decompositions in differential geometry, which is important in the study of curves and surfaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8354660169543136
      },
      {
        "text": "Orthogonal Projection and the Orthogonal Decomposition of Tensor Fields with Scalar Multipliers: This involves understanding how orthogonal projection matrices are used to decompose tensor fields with scalar multipliers into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8764109821279816
      },
      {
        "text": "Properties of Orthogonal Projection Matrices in Locally Compact Spaces: This includes studying the properties of orthogonal projection matrices in locally compact spaces, such as their eigenvalues, eigenvectors, and the relationship with the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8178440732880543
      },
      {
        "text": "The Relationship between Orthogonal Projection Matrices and Orthogonal Series Expansions in Topology: This involves studying the relationship between orthogonal projection matrices and orthogonal series expansions in topology, which is important in the study of topological spaces and manifolds.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8137815983638802
      },
      {
        "text": "Orthogonal Projection and the Orthogonal Decomposition of Operator Algebras with a Non-Standard Inner Product: This involves understanding how orthogonal projection matrices are used to decompose operator algebras with a non-standard inner product into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8703990723652277
      },
      {
        "text": "Orthogonal Projection and the Orthogonal Decomposition of Hilbert Spaces with a Non-Standard Inner Product: This involves understanding how orthogonal projection matrices are used to decompose Hilbert spaces with a non-standard inner product into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9067612676137202
      },
      {
        "text": "Properties of Orthogonal Projection Matrices in Banach Spaces: This includes studying the properties of orthogonal projection matrices in Banach spaces, such as their eigenvalues, eigenvectors, and the relationship with the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.826818684652932
      },
      {
        "text": "The Connection between Orthogonal Projection Matrices and Orthogonal Decompositions in Algebraic Geometry: This involves studying the connection between orthogonal projection matrices and orthogonal decompositions in algebraic geometry, which is important in the study of algebraic varieties and schemes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8100055666550579
      },
      {
        "text": "The Use of Orthogonal Projection Matrices in Nonlinear Optimization: This includes studying the use of orthogonal projection matrices in nonlinear optimization, such as in the study of convex optimization and variational calculus.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8340076051848617
      },
      {
        "text": "Properties of Orthogonal Projection Matrices in Topological Spaces: This includes studying the properties of orthogonal projection matrices in topological spaces, such as their eigenvalues, eigenvectors, and the relationship with the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8438127549799805
      },
      {
        "text": "The Orthogonal Projection of a Matrix: The orthogonal projection of a matrix is the projection of the matrix onto a subspace. It is a fundamental tool in linear algebra and is widely used in various applications of orthogonal diagonalization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8262459991840099
      },
      {
        "text": "Properties of orthogonal projection matrices in infinite-dimensional spaces: This includes studying the properties of orthogonal projection matrices in infinite-dimensional spaces, such as their eigenvalues, eigenvectors, and the relationship with the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8552966567600762
      },
      {
        "text": "The use of orthogonal projection matrices in numerical analysis: This includes studying the use of orthogonal projection matrices in numerical analysis, such as in the solution of linear systems and the approximation of functions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8535666742318666
      },
      {
        "text": "Orthogonal projection matrices and the decomposition of operator algebras with a non-standard norm: This involves understanding how orthogonal projection matrices are used to decompose operator algebras with a non-standard norm into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8432130667913464
      },
      {
        "text": "Orthogonal projection matrices and the decomposition of Hilbert spaces with a non-standard orthonormal basis: This involves understanding how orthogonal projection matrices are used to decompose Hilbert spaces with a non-standard orthonormal basis into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8806637493819579
      },
      {
        "text": "Orthogonal projection matrices and the decomposition of algebraic varieties: This involves understanding how orthogonal projection matrices are used to decompose algebraic varieties into their orthogonal components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8783882254963866
      },
      {
        "text": "Orthogonal Projection of a Vector onto a Subspace using a Non-Standard Metric: This involves projecting a vector onto a subspace using a non-standard metric, which can be achieved using the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8241860301361557
      },
      {
        "text": "Orthogonal Projection of a Matrix onto a Subspace using a Non-Standard Inner Product: This involves projecting a matrix onto a subspace using a non-standard inner product, which can be achieved using the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8614650924929024
      },
      {
        "text": "Projections onto Subspaces with Non-Standard Metrics: This involves projecting vectors onto subspaces using non-standard metrics, which can lead to more complex and nuanced applications of orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8491842164590268
      },
      {
        "text": "Projection of a Matrix onto a Subspace using a Non-Standard Orthonormal Basis: This involves projecting a matrix onto a subspace using a non-standard orthonormal basis, which can lead to more complex and nuanced applications of orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8515671493437762
      },
      {
        "text": "Orthogonal Projections in Infinite-Dimensional Spaces: Orthogonal projections involve finding the projection of a vector onto a subspace using an orthogonal basis in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8579542352702155
      },
      {
        "text": "Projections onto Subspaces with Non-Standard Metrics: Projecting vectors onto subspaces using non-standard metrics, leading to more complex and nuanced applications of orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8366187903286936
      },
      {
        "text": "Projections onto Subspaces with Non-Standard Inner Products: Projecting vectors onto subspaces using non-standard inner products, which can lead to more complex and nuanced applications of orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8640322635411313
      },
      {
        "text": "Projections onto Subspaces with Non-Standard Orthonormal Bases: Projecting vectors onto subspaces using non-standard orthonormal bases, leading to more complex and nuanced applications of orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.843469371167346
      },
      {
        "text": "Orthogonal Projection of a Matrix onto a Subspace using a Non-Standard Metric: This involves projecting a matrix onto a subspace using a non-standard metric, leading to more complex and nuanced applications of orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8490129494801157
      }
    ]
  },
  {
    "representative_text": "Theorem on the Range of an Orthogonal Projection Matrix: This theorem states that the range of an orthogonal projection matrix is equal to the subspace onto which it projects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem on the Range of an Orthogonal Projection Matrix: This theorem states that the range of an orthogonal projection matrix is equal to the subspace onto which it projects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection and Eigenvalues: This involves understanding how orthogonal projection matrices are related to the eigenvalues of the original matrix and how they can be used to diagonalize the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection and Eigenvalues: This involves understanding how orthogonal projection matrices are related to the eigenvalues of the original matrix and how they can be used to diagonalize the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Projection and the Spectral Theorem for Hermitian Matrices: This involves understanding how orthogonal projection matrices are used to diagonalize Hermitian matrices and to compute their eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8777409981710858
      },
      {
        "text": "Orthogonal Projection and the Spectral Theorem for Compact Operators: This involves understanding how orthogonal projection matrices are used to diagonalize compact operators and to compute their eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9157889083737927
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection and Singular Values: This involves understanding how orthogonal projection matrices are related to the singular values of the original matrix and how they can be used to determine the singular values.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection and Singular Values: This involves understanding how orthogonal projection matrices are related to the singular values of the original matrix and how they can be used to determine the singular values.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of singular values, including the properties of singular values and the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8156212928313904
      },
      {
        "text": "Orthogonal Projections and Singular Value Decomposition: Orthogonal projections are a method for projecting a vector onto a subspace by finding the orthogonal projection of the vector onto a basis for the subspace. This method can be used to find the singular values of a matrix using singular value decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8237162025139688
      },
      {
        "text": "Orthogonal Decomposition and Singular Value Decomposition: This involves studying the properties of orthogonal decompositions and singular value decompositions, including the definition of orthogonal decompositions, singular value decomposition properties, and the characterization of singular value decomposition spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8145501540216284
      },
      {
        "text": "Singular Value Decomposition and Matrix Factorization: This involves studying the properties of singular value decompositions and matrix factorizations, including the definition of singular value decompositions, matrix factorization properties, and the characterization of singular value decomposition spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8410332432890384
      }
    ]
  },
  {
    "representative_text": "Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Orthogonal Complement: This theorem states that the orthogonal projection matrix and its orthogonal complement are orthogonal to each other.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Orthogonal Complement: This theorem states that the orthogonal projection matrix and its orthogonal complement are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Range: This theorem states that the orthogonal projection matrix and its range are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9468230478443216
      },
      {
        "text": "The Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Null Space: This theorem states that the orthogonal projection matrix and its null space are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9677837470409906
      },
      {
        "text": "The Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Dual: This theorem states that the orthogonal projection matrix and its dual are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9507978924649696
      },
      {
        "text": "The Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Range in Banach Spaces: This theorem states that the orthogonal projection matrix and its range are orthogonal to each other in Banach spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.929978458193838
      },
      {
        "text": "The Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Orthogonal Complement in Infinite-Dimensional Spaces: This theorem states that the orthogonal projection matrix and its orthogonal complement are orthogonal to each other in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9323215904262647
      },
      {
        "text": "The Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Dual in Banach Spaces: This theorem states that the orthogonal projection matrix and its dual are orthogonal to each other in Banach spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9476489688146508
      },
      {
        "text": "The Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Null Space in Locally Compact Spaces: This theorem states that the orthogonal projection matrix and its null space are orthogonal to each other in locally compact spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9310049158237077
      },
      {
        "text": "The Theorem on the Orthogonality of an Orthogonal Projection Matrix and Its Range in Hilbert Spaces: This theorem states that the orthogonal projection matrix and its range are orthogonal to each other in Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9630221400893265
      }
    ]
  },
  {
    "representative_text": "Least Squares Method: This is an alternative method for orthogonalizing vectors, which minimizes the sum of the squares of the differences between the original vector and its orthogonal projection.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Least Squares Method: This is an alternative method for orthogonalizing vectors, which minimizes the sum of the squares of the differences between the original vector and its orthogonal projection.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Sturm-Liu Theorem: This theorem provides a condition for determining whether a set of vectors can be orthonormalized.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sturm-Liu Theorem: This theorem provides a condition for determining whether a set of vectors can be orthonormalized.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Aspects of Gram-Schmidt Process: This includes topics such as numerical stability, conditioning, and the use of iterative methods to improve the efficiency of the Gram-Schmidt process.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Computational Aspects of Gram-Schmidt Process: This includes topics such as numerical stability, conditioning, and the use of iterative methods to improve the efficiency of the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Aspects of Gram-Schmidt Process: The Gram-Schmidt process has various computational aspects, including numerical stability, conditioning, and the use of iterative methods to improve efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9428046559890269
      },
      {
        "text": "Computational Complexity of Gram-Schmidt Process: This topic examines the computational complexity of the Gram-Schmidt process, including its time and space complexity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.878500208661434
      },
      {
        "text": "Computational Efficiency: The Gram-Schmidt process has a computational complexity of O(n^3), but there are methods to improve its efficiency, such as using sparse matrices or parallel processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8984851630886375
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt Process with Partial Orthogonality: This is an extension of the Gram-Schmidt process to cases where the vectors are not fully orthogonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt Process with Partial Orthogonality: This is an extension of the Gram-Schmidt process to cases where the vectors are not fully orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt Process for Non-Standard Inner Products: This includes the extension of the Gram-Schmidt process to inner products that are not the standard inner product, such as the Euclidean norm or the Frobenius norm.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 22,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt Process for Non-Standard Inner Products: This includes the extension of the Gram-Schmidt process to inner products that are not the standard inner product, such as the Euclidean norm or the Frobenius norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gram-Schmidt Process for Inner Product Spaces: The Gram-Schmidt process can be generalized to inner product spaces to construct an orthonormal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8522899964918799
      },
      {
        "text": "Gram-Schmidt Process with Non-Standard Orthogonality Conditions: The Gram-Schmidt process can be modified to accommodate non-standard orthogonality conditions, such as using a different inner product or a different notion of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8896257314146306
      },
      {
        "text": "Gram-Schmidt Process with Partial Orthogonality Conditions: The Gram-Schmidt process can be modified to accommodate partial orthogonality conditions, where some of the vectors are not orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9040099508768022
      },
      {
        "text": "Gram-Schmidt Process with Non-Standard Norms: The Gram-Schmidt process can be modified to accommodate non-standard norms, which are norms that are not the Euclidean norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9489597128811291
      },
      {
        "text": "Orthogonalization of Tensors: The Gram-Schmidt process can be extended to higher-dimensional spaces, such as tensors, to orthogonalize them.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8763053213545939
      },
      {
        "text": "Gram-Schmidt Process for Non-Standard Inner Products: The Gram-Schmidt process can be extended to inner products that are not the standard inner product, such as the Euclidean norm or the Frobenius norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9441360778634726
      },
      {
        "text": "Gram-Schmidt Process with Non-Standard Norms: This is the modification of the Gram-Schmidt process to accommodate non-standard norms, which are norms that are not the Euclidean norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9137163247557324
      },
      {
        "text": "Gram-Schmidt Process with Non-Standard Orthogonality Conditions: This is the modification of the Gram-Schmidt process to accommodate non-standard orthogonality conditions, such as using a different inner product or a different notion of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9238760744657625
      },
      {
        "text": "Gram-Schmidt Process for Non-Standard Norms in Hilbert Spaces: This is the modification of the Gram-Schmidt process to accommodate non-standard norms in Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9151033976607226
      },
      {
        "text": "Gram-Schmidt Process for Non-Standard Inner Products: This topic explores the extension of the Gram-Schmidt process to inner products that are not the standard inner product, which can be used in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9136833519523384
      },
      {
        "text": "Gram-Schmidt Process for Non-Standard Norms: This topic explores the modification of the Gram-Schmidt process to accommodate non-standard norms, which can be used in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9226779753353682
      },
      {
        "text": "Gram-Schmidt Process for Hilbert Spaces with Non-Standard Norms: This topic explores the modification of the Gram-Schmidt process to accommodate non-standard norms in Hilbert spaces, which can be used in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9056304333363541
      },
      {
        "text": "The Gram-Schmidt Process for Non-Standard Inner Products in Banach Spaces: This involves extending the Gram-Schmidt process to inner products that are not the standard inner product, which can be used in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9108222204221861
      },
      {
        "text": "Gram-Schmidt Process with Non-Standard Orthogonality Conditions: This involves modifying the Gram-Schmidt process to accommodate non-standard orthogonality conditions, such as using a different inner product or a different notion of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9440606258825315
      },
      {
        "text": "Orthogonal Decomposition using the Gram-Schmidt Process with Non-Standard Orthonormal Bases: This involves decomposing a vector space into an orthogonal basis using a non-standard orthonormal basis and the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8189003691266056
      },
      {
        "text": "Gram-Schmidt Process for Non-standard Norms: Investigate the modification of the Gram-Schmidt process to accommodate non-standard norms, such as the infinity norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9206735123655284
      },
      {
        "text": "Gram-Schmidt Process for Non-standard Inner Products in Banach Spaces: Investigate the extension of the Gram-Schmidt process to inner products that are not the standard inner product, such as the Frobenius norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9044245148823101
      },
      {
        "text": "Gram-Schmidt Process for Non-standard Norms in Hilbert Spaces: Investigate the modification of the Gram-Schmidt process to accommodate non-standard norms in Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9162561512904308
      },
      {
        "text": "Gram-Schmidt Process with Non-Orthogonal Projections: Extend the Gram-Schmidt process to cases where the projections are not orthogonal, which can occur in certain applications such as image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8512797060365362
      },
      {
        "text": "Gram-Schmidt Process for Non-Standard Norms: Modify the Gram-Schmidt process to accommodate non-standard norms, which can be useful in applications where the norm is not the Euclidean norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9378749294188182
      },
      {
        "text": "Gram-Schmidt Process with Non-Standard Orthogonality Conditions: Modify the Gram-Schmidt process to accommodate non-standard orthogonality conditions, such as using a different inner product or a different notion of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9017609542823674
      }
    ]
  },
  {
    "representative_text": "The Role of Gram-Schmidt Process in Quantum Information Processing: This includes the application of the Gram-Schmidt process in quantum information processing, such as quantum error correction and quantum computing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Role of Gram-Schmidt Process in Quantum Information Processing: This includes the application of the Gram-Schmidt process in quantum information processing, such as quantum error correction and quantum computing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Connection between Gram-Schmidt Process and Other Linear Algebra Techniques: This includes the relationship between the Gram-Schmidt process and other linear algebra techniques, such as eigenvalue decomposition and singular value decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "The Connection between Gram-Schmidt Process and Other Linear Algebra Techniques: This includes the relationship between the Gram-Schmidt process and other linear algebra techniques, such as eigenvalue decomposition and singular value decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Eigenvectors and the Gram-Schmidt Process: The Gram-Schmidt process can be used to construct an orthogonal basis of eigenvectors. This can be used to analyze the behavior of linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8333318391454688
      },
      {
        "text": "The Use of the Gram-Schmidt Process in Eigenvector Decomposition: The Gram-Schmidt process is a technique used to orthogonalize a set of vectors, and understanding its application in eigenvector decomposition is essential in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8799178932673996
      },
      {
        "text": "The Connection between the Gram-Schmidt Process and the Singular Value Decomposition: The Gram-Schmidt process is closely related to the singular value decomposition (SVD) of a matrix, and can be used to compute the SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8324112304171082
      },
      {
        "text": "Connection to Eigenvalue Decomposition: The Gram-Schmidt process is closely related to eigenvalue decomposition, as it can be used to compute the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9262453368295684
      },
      {
        "text": "Connection to Singular Value Decomposition: The Gram-Schmidt process is also closely related to singular value decomposition (SVD), as it can be used to compute the singular values and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9455971526606533
      }
    ]
  },
  {
    "representative_text": "Jacobi's Formula: This formula is used to calculate the determinant of a matrix using the Jacobi method, which is a way of approximating the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jacobi's Formula: This formula is used to calculate the determinant of a matrix using the Jacobi method, which is a way of approximating the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Schur's Theorem: This theorem states that any square matrix can be transformed into an upper triangular matrix using a similarity transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Schur's Theorem: This theorem states that any square matrix can be transformed into an upper triangular matrix using a similarity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Schur's Triangularization Theorem: A theorem that states that any square matrix can be triangularized, which can be useful in computing determinants and inverses.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.863066831194743
      },
      {
        "text": "The Schur's Theorem: This theorem states that a matrix A can be transformed into a block diagonal matrix using a similarity transformation, where each block corresponds to a Jordan block.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8872951665054889
      },
      {
        "text": "The Triangularization Theorem: This theorem states that a matrix A can be transformed into an upper or lower triangular matrix using a similarity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9197638336924194
      },
      {
        "text": "Gelfand's Theorem: Gelfand's theorem states that any matrix can be transformed into a triangular matrix using a similarity transformation, and there are some properties and relationships that can be derived for triangular matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.85162907855812
      },
      {
        "text": "Schur's Triangularization: This is a theorem that states that any square matrix can be transformed into a triangular matrix using a similarity transformation. The determinant of the original matrix is equal to the determinant of the triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9252251047718286
      },
      {
        "text": "The Schur's Theorem: A theorem that describes the relationship between the eigenvalues of a matrix and its Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8428742342492308
      },
      {
        "text": "Schur's Triangularization Theorem: This theorem states that every square matrix A can be transformed into an upper triangular matrix T using a similarity transformation, i.e., T = P^(-1)AP, where P is an invertible matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9056652482819089
      },
      {
        "text": "Kron's Theorem: This theorem states that for any square matrix A, there exists an orthogonal matrix P such that P^TAP is upper triangular.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8619480823925336
      },
      {
        "text": "Block Triangularization Theorem: A theorem that states that any matrix can be block triangularized, which can be useful in computing determinants and inverses.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8164245973002171
      },
      {
        "text": "Schur's Triangularization: A method for diagonalizing a matrix by applying a similarity transformation to transform the matrix into a triangular form.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.818548386958915
      }
    ]
  },
  {
    "representative_text": "Gelfand's Formula: This formula is used to calculate the determinant of a matrix using the Gelfand method, which involves using the eigenvalues and eigenvectors of the matrix to calculate the determinant.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Gelfand's Formula: This formula is used to calculate the determinant of a matrix using the Gelfand method, which involves using the eigenvalues and eigenvectors of the matrix to calculate the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Advanced Determinant Properties, such as the Cauchy-Binet Formula: This formula is a way to compute the determinant of a matrix using the determinants of submatrices, which can be useful in certain applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8059081631614846
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD): An extension of the concept of determinant to non-square matrices, which can be used to determine the existence and uniqueness of the inverse of a matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD): An extension of the concept of determinant to non-square matrices, which can be used to determine the existence and uniqueness of the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Pseudoinverse: A generalization of the inverse matrix concept to non-square matrices, which can be used to determine the existence and uniqueness of the solution to a system of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 16,
    "detailed_sources": [
      {
        "text": "Pseudoinverse: A generalization of the inverse matrix concept to non-square matrices, which can be used to determine the existence and uniqueness of the solution to a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Moore-Penrose Inverse: This is a generalization of the inverse of a matrix that can be used for any matrix, not just invertible ones.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8169063300658246
      },
      {
        "text": "Pseudoinverse: A generalization of the inverse matrix to non-square matrices, which can be used to solve systems of linear equations with more variables than unknowns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9352745514260092
      },
      {
        "text": "Pseudoinverse: Pseudoinverse is a generalization of the inverse of a matrix that can be used to describe the properties of a matrix that are not invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8651037432749913
      },
      {
        "text": "The Moore-Penrose Inverse: The Moore-Penrose inverse of a matrix is a generalization of the inverse matrix that is defined for all square matrices, not just invertible ones. It is used to solve systems of linear equations and to find the least squares solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8949522608133096
      },
      {
        "text": "Moore-Penrose Inverse: The Moore-Penrose inverse is a generalization of the inverse of a matrix that can be used for any matrix, not just invertible ones. It is named after the mathematicians Roger Moore and Roger Penrose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8728847457870046
      },
      {
        "text": "The concept of a pseudoinverse and its relation to the orthogonal decomposition: The pseudoinverse of a matrix is a generalization of the inverse matrix that can be used in situations where the inverse does not exist.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8592541840982908
      },
      {
        "text": "The concept of a generalized inverse and its relation to the orthogonal decomposition: A generalized inverse of a matrix is a matrix that satisfies a certain condition when multiplied by the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8308167119229315
      },
      {
        "text": "Pseudoinverse and Moore-Penrose Inverse: Discuss the concept of the pseudoinverse and the Moore-Penrose inverse, which can be used to extend the domain of matrix inverses to non-invertible matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8589775853618493
      },
      {
        "text": "Matrix Inversion using the Moore-Penrose Inverse: The Moore-Penrose inverse is a generalization of the inverse matrix that can be used for matrices that are not invertible in the classical sense.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9174663617001372
      },
      {
        "text": "The Moore-Penrose Inverse of a Matrix: The Moore-Penrose inverse of a matrix is a generalization of the inverse matrix that can be used in situations where the inverse does not exist. This concept is essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8631161191683203
      },
      {
        "text": "Moore-Penrose Inverse of a Matrix: The Moore-Penrose inverse of a matrix A is a matrix that satisfies the following properties: (i) A A^-1 A = A, (ii) A^-1 A A^-1 = A^-1, (iii) (A A^-1)^T = A A^-1, and (iv) (A^-1 A)^T = A^-1 A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8244534997164463
      },
      {
        "text": "Moore-Penrose Inverse of Complex Matrices: The generalization of the inverse of a matrix to complex matrices, which can be used for any matrix, not just invertible ones.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8978002092464223
      },
      {
        "text": "Pseudoinverse and Moore-Penrose Inverse: Explore pseudoinverse and Moore-Penrose inverse, which are generalizations of the inverse of a matrix that can be used to describe the properties of a matrix that are not invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9234997952501378
      },
      {
        "text": "Pseudoinverse: A mathematical concept that is used in linear algebra to solve systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8001898503623657
      },
      {
        "text": "Moore-Penrose Inverse: A mathematical concept that is used in linear algebra to solve systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8477249640743809
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Complex Elements: The determinant of a matrix with complex elements can be calculated using the same formulas as for real matrices, but with additional considerations for the complex arithmetic.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Complex Elements: The determinant of a matrix with complex elements can be calculated using the same formulas as for real matrices, but with additional considerations for the complex arithmetic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Complex Entries: The determinant of a matrix with complex entries can be computed using the same formulas as for real matrices, but the results may be complex numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9338949164319188
      },
      {
        "text": "Determinant of a Matrix with Non-Integer Coefficients: The determinant of a matrix with non-integer coefficients can be computed using the same formulas as for real matrices, but the results may be non-integer numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8295525680975655
      },
      {
        "text": "Determinant of a Matrix with Complex Entries: The determinant of a matrix with complex entries can be calculated using the same methods as the determinant of a matrix with real entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9345371370195146
      },
      {
        "text": "Determinant of a Matrix with Complex Entries: The determinant of a matrix with complex entries is defined using the complex conjugate of the matrix, and there are some properties and relationships that can be derived for complex determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8707967628387139
      },
      {
        "text": "Determinant of a Matrix with Complex Coefficients: The determinant of a matrix with complex coefficients can be computed using the same formulas as for real matrices, but the results may be complex numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9447152375893183
      },
      {
        "text": "Determinant of a Matrix with Non-Uniform Scaling: The determinant of a matrix with non-uniform scaling can be computed using the same formulas as for real matrices, but the results may be non-integer numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8213326165160472
      },
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues: The determinant of a matrix with complex eigenvalues can be calculated using the same formulas as for real matrices, but with additional considerations for the complex arithmetic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9190901007259604
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Singularities: The determinant of a matrix with singularities (e.g., holes or poles) can be calculated using advanced techniques, such as the use of Laurent series or the Cauchy integral formula.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Singularities: The determinant of a matrix with singularities (e.g., holes or poles) can be calculated using advanced techniques, such as the use of Laurent series or the Cauchy integral formula.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with a Non-Integrable Singularity: There are some properties and relationships that can be derived for determinants of matrices with non-integrable singularities, such as the determinant of a matrix with a non-integrable singularity being infinite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8091112800136937
      },
      {
        "text": "Determinant of a Matrix with a Non-Integrable Singularity and a Complex Eigenvalue: There are some properties and relationships that can be derived for determinants of matrices with non-integrable singularities and complex eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8858867639649353
      },
      {
        "text": "Determinant of a Matrix with a Non-Integrable Singularity: There are some properties and relationships that can be derived for determinants of matrices with non-integrable singularities.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9442840134224189
      },
      {
        "text": "Determinant of a Matrix with a Non-Integrable Singularity and a Non-Diagonal Dominant Matrix: There are some properties and relationships that can be derived for determinants of matrices with non-integrable singularities and non-diagonal dominant matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9451049815295118
      },
      {
        "text": "Determinant of a Matrix with a Non-Integrable Singularity and a Nilpotent factors: This is a specific case of a determinant calculation that involves a non-integrable singularity and nilpotent factors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8722159580957447
      }
    ]
  },
  {
    "representative_text": "Numerical Stability: The determinant of a matrix can be sensitive to numerical errors, and techniques such as LU decomposition or SVD can be used to improve the stability of numerical methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Stability: The determinant of a matrix can be sensitive to numerical errors, and techniques such as LU decomposition or SVD can be used to improve the stability of numerical methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Linear Dependencies: The determinant of a matrix with linear dependencies can be calculated using advanced techniques, such as the use of the Gram-Schmidt process or the Cauchy-Schwarz inequality.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Linear Dependencies: The determinant of a matrix with linear dependencies can be calculated using advanced techniques, such as the use of the Gram-Schmidt process or the Cauchy-Schwarz inequality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Complexity: The computational complexity of calculating the determinant of a matrix can be high, especially for large matrices, and techniques such as sparse matrix algorithms or parallel computing can be used to improve performance.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Complexity: The computational complexity of calculating the determinant of a matrix can be high, especially for large matrices, and techniques such as sparse matrix algorithms or parallel computing can be used to improve performance.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rank and Nullity Theorem: This theorem states that for a matrix A with an inverse A^-1, the rank of A is equal to the rank of A^-1, and the nullity of A is equal to the nullity of A^-1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Rank and Nullity Theorem: This theorem states that for a matrix A with an inverse A^-1, the rank of A is equal to the rank of A^-1, and the nullity of A is equal to the nullity of A^-1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Pseudoinverse: A matrix A is said to have a pseudoinverse if it is not invertible but is close to being invertible. The pseudoinverse can be used to solve systems of linear equations when the matrix is not invertible.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Pseudoinverse: A matrix A is said to have a pseudoinverse if it is not invertible but is close to being invertible. The pseudoinverse can be used to solve systems of linear equations when the matrix is not invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Invertible and Non-Invertible Matrices: A discussion of the differences between invertible and non-invertible matrices, including the concept of a pseudoinverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8126622211349572
      },
      {
        "text": "Pseudoinverse Properties: The pseudoinverse of a matrix has several properties, including the fact that it can be used to solve systems of linear equations when the matrix is not invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8342356314178501
      }
    ]
  },
  {
    "representative_text": "Regularization: This is a technique used to regularize a matrix by adding a small multiple of the identity matrix to the original matrix. Regularization can be used to improve the numerical stability of matrix inversion algorithms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Regularization: This is a technique used to regularize a matrix by adding a small multiple of the identity matrix to the original matrix. Regularization can be used to improve the numerical stability of matrix inversion algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Regularization: A technique used to regularize matrices to improve numerical stability, such as adding a small value to the diagonal elements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9123119810473121
      },
      {
        "text": "Regularization Techniques: Regularization techniques such as Tikhonov regularization and ridge regression can be used to improve the numerical stability of matrix inversion algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8023051494480005
      },
      {
        "text": "Regularization Techniques: In addition to Tikhonov regularization and ridge regression, other regularization techniques such as the L-curve method and the Bayesian method can be used to improve the numerical stability of matrix inversion algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8267041560043537
      },
      {
        "text": "Regularization Techniques for Inverse Matrix Computation: Regularization techniques such as Tikhonov regularization, ridge regression, and the L-curve method can be used to improve the numerical stability of matrix inversion algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.923720253598887
      },
      {
        "text": "Regularization Techniques for Ill-Conditioned Matrices: Explore regularization techniques, such as Tikhonov regularization, ridge regression, and L2 regularization, to improve the stability of matrix inversions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8201618809739146
      },
      {
        "text": "Regularization Techniques: Regularization techniques are used to improve the stability of the SVD algorithm by adding a small penalty term to the objective function.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8073309086734348
      },
      {
        "text": "Regularization Techniques for Non-Linear Systems: Regularization techniques are methods used to improve the numerical stability of matrix inversion algorithms for non-linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8661221535199526
      },
      {
        "text": "Regularization Techniques: Regularization techniques such as Tikhonov regularization, ridge regression, and the L-curve method can be used to improve the numerical stability of matrix inversion algorithms for non-linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.887454086278666
      },
      {
        "text": "Regularization Techniques for Non-Symmetric Matrices in High-Dimensional Spaces: Regularization techniques such as Tikhonov regularization, ridge regression, and the L-curve method can be used to improve the numerical stability of matrix inversion algorithms for non-symmetric matrices in high-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8722918821873593
      },
      {
        "text": "Regularization Techniques: These are methods that are used to improve the numerical stability of a matrix inversion method. Regularization techniques include the use of pseudoinverse matrices and the addition of a small constant to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.93354570207597
      },
      {
        "text": "Regularization Techniques for SVD Computation: Regularization techniques can be used to improve the stability of the SVD algorithm by adding a small penalty term to the objective function.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8044785284379763
      },
      {
        "text": "Regularization Techniques: Discuss regularization techniques, which are methods that are used to improve the numerical stability of a matrix inversion method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.933704779044214
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Interlacing: This theorem states that the eigenvalues of a matrix A and the eigenvalues of the transpose of A are interlaced, i.e., if λ1 < λ2 < ... < λn are the eigenvalues of A, then λ1 < λ1' < λ2' < ... < λn' are the eigenvalues of A^T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Interlacing: This theorem states that the eigenvalues of a matrix A and the eigenvalues of the transpose of A are interlaced, i.e., if λ1 < λ2 < ... < λn are the eigenvalues of A, then λ1 < λ1' < λ2' < ... < λn' are the eigenvalues of A^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inversion of a Matrix with Complex Entries: The inversion of a matrix with complex entries can be computed using the same algorithms as for real matrices, but the results may be complex numbers.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Inversion of a Matrix with Complex Entries: The inversion of a matrix with complex entries can be computed using the same algorithms as for real matrices, but the results may be complex numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Inverse of a Matrix with Complex Coefficients: The inversion of a matrix with complex coefficients can be computed using the same algorithms as for real matrices, but the results may be complex numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9270611831484286
      },
      {
        "text": "Inverse of a Matrix with Non-Uniform Scaling: The inversion of a matrix with non-uniform scaling can be computed using the same algorithms as for real matrices, but the results may be non-integer numbers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8378166742566805
      }
    ]
  },
  {
    "representative_text": "Numerical Computation of Determinants and Inverses: This topic deals with the numerical computation of determinants and inverses using algorithms such as LU decomposition and Cholesky decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Numerical Computation of Determinants and Inverses: This topic deals with the numerical computation of determinants and inverses using algorithms such as LU decomposition and Cholesky decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Computation of Determinants and Inverses: In addition to the LU decomposition and Cholesky decomposition, other numerical methods such as the Jacobi method and the Gauss-Seidel method can be used to compute determinants and inverses.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8189124570282086
      },
      {
        "text": "The Computation of Determinants: This is a fundamental problem in linear algebra, and various algorithms such as the LU decomposition, the Cholesky decomposition, and the Sylvester's theorem can be used to compute determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8415066873503192
      },
      {
        "text": "Computational Complexity of Determinant and Inverse Computation: This includes the analysis of the computational complexity of algorithms for computing determinants and inverses, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8350815965188927
      },
      {
        "text": "Numerical Stability of Determinant and Inverse Computation: This includes the discussion of the numerical stability of algorithms for computing determinants and inverses, including the concept of numerical stability and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8997503257947965
      },
      {
        "text": "Computational Complexity of Determinant and Inverse Computation: The computational complexity of computing determinants and inverses of matrices can be analyzed using various methods, such as the Schur decomposition or the power of A",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8414980842809476
      },
      {
        "text": "Computational Complexity of Determinant and Inverse Computation: The computational complexity of computing determinants and inverses of matrices can be analyzed using various methods, such as the Schur decomposition or the power of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8912765850817406
      }
    ]
  },
  {
    "representative_text": "Optimization of Matrix Inversion Algorithms: This topic deals with the optimization of matrix inversion algorithms to minimize the number of operations required to compute the inverse of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Optimization of Matrix Inversion Algorithms: This topic deals with the optimization of matrix inversion algorithms to minimize the number of operations required to compute the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Optimization of Matrix Inversion Algorithms: The optimization of matrix inversion algorithms can be done using various techniques such as the minimization of the number of operations required to compute the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9084769173556583
      }
    ]
  },
  {
    "representative_text": "Application of Determinants and Inverses in Signal Processing: This topic deals with the application of determinants and inverses in signal processing, such as in filtering and image processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Application of Determinants and Inverses in Signal Processing: This topic deals with the application of determinants and inverses in signal processing, such as in filtering and image processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Application of Determinants and Inverses in Signal Processing: In addition to filtering and image processing, other applications of determinants and inverses include audio processing, medical imaging, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921539608400181
      },
      {
        "text": "The application of determinants in signal processing: Determinants are used in signal processing to analyze and design filters.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8968878118138488
      },
      {
        "text": "The application of determinants in computer graphics: Determinants are used in computer graphics to perform transformations and projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8438895439742048
      }
    ]
  },
  {
    "representative_text": "Minors and the Inversion of a Matrix: The minors of a matrix and how they relate to the inversion of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minors and the Inversion of a Matrix: The minors of a matrix and how they relate to the inversion of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Properties of Determinants for Non-Square Matrices: Determinants of non-square matrices, such as matrices with fewer rows than columns or vice versa.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Properties of Determinants for Non-Square Matrices: Determinants of non-square matrices, such as matrices with fewer rows than columns or vice versa.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Rank-Nullity Theorem: A theorem that states that the rank and nullity of a matrix are related, which can be useful in computing determinants and inverses.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Rank-Nullity Theorem: A theorem that states that the rank and nullity of a matrix are related, which can be useful in computing determinants and inverses.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rank and Nullity Theorems: The rank and nullity theorems are theorems that relate the rank and nullity of a matrix to its determinant and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8330500481265057
      },
      {
        "text": "Determinants of Matrices with a Given Rank and Nullity: The properties of the determinant of a matrix with a given rank and nullity, including the relationship between the rank and nullity of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.82673513882031
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Orthogonality: A set of vectors is linearly independent if and only if the vectors are orthogonal to each other with respect to a given inner product.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Independence and Orthogonality: A set of vectors is linearly independent if and only if the vectors are orthogonal to each other with respect to a given inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonality and Linear Independence: A set of vectors $\\{v1, v2, \\ldots, vn\\}$ is linearly independent if and only if the vectors are orthogonal to each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9554099304283017
      },
      {
        "text": "The Relationship between Linear Independence and the Orthogonal Complement: If a vector is orthogonal to every vector in a set, then the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9197272242038894
      },
      {
        "text": "Linear Independence Implies Orthogonal Complement: A set of vectors is linearly independent if and only if the orthogonal complement of the span of the set is equal to the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9236418732707271
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Bilinear Form: A set of vectors is linearly independent if and only if the corresponding bilinear form is non-zero.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and Bilinear Form: A set of vectors is linearly independent if and only if the corresponding bilinear form is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Linear Transformations: A set of vectors is linearly independent if and only if the corresponding linear transformation is one-to-one.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Linear Independence and Linear Transformations: A set of vectors is linearly independent if and only if the corresponding linear transformation is one-to-one.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Linear Independence: A linear transformation $T: V \\to W$ is injective if and only if the null space of $T$ is trivial, i.e., contains only the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8294543709138427
      },
      {
        "text": "Linear Transformations and Linear Independence: The relationship between linear independence and linear transformations, including the fact that a linear transformation is one-to-one if and only if its kernel is trivial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8773615207963521
      },
      {
        "text": "Linear Transformations and Linear Dependence: The relationship between linear dependence and linear transformations, including the fact that a linear transformation is not one-to-one if and only if its kernel is non-trivial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8743685126192697
      },
      {
        "text": "The Linear Independence of the Image of a Linear Transformation: Linear independence is preserved under linear transformations. Specifically, if a set of vectors is linearly independent, then the image of the set under a linear transformation is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8526596532429433
      },
      {
        "text": "The relationship between linear independence and homomorphisms: A homomorphism between vector spaces preserves linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8231318394216076
      },
      {
        "text": "Existence of a Linear Transformation with a Trivial Null Space: Exploring the conditions under which a linear transformation has a trivial null space, including the role of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8167581005355882
      },
      {
        "text": "Linear Transformation and Linear Independence: A crucial concept in understanding the behavior of linear transformations, including the fact that a linear transformation is one-to-one if and only if its kernel is trivial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8749193894524484
      },
      {
        "text": "Linear Transformation and Linear Dependence: Discussing the relationship between linear dependence and linear transformations, including the fact that a linear transformation is not one-to-one if and only if its kernel is non-trivial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.850521616348449
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Span under Scalar Multiplication: If a set of vectors is linearly independent, then the set is closed under scalar multiplication.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence and Span under Scalar Multiplication: If a set of vectors is linearly independent, then the set is closed under scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of a Set of Vectors with Scalar Multiples: The concept of linear independence can be extended to include scalar multiples of the vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8509381960017851
      },
      {
        "text": "The Role of Scalar Multiplication in Understanding Linear Independence: Scalar multiplication plays a crucial role in understanding linear independence, as a set of vectors is linearly independent if and only if the set is closed under scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572747509669301
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Span under Linear Combination: If a set of vectors is linearly independent, then the set is closed under linear combinations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and Span under Linear Combination: If a set of vectors is linearly independent, then the set is closed under linear combinations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Basis for Infinite-Dimensional Vector Spaces: A set of vectors is linearly independent if and only if the set is a basis for an infinite-dimensional vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Linear Independence and Basis for Infinite-Dimensional Vector Spaces: A set of vectors is linearly independent if and only if the set is a basis for an infinite-dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis Theorem for Infinite-Dimensional Spaces: This theorem states that a set of vectors is a basis for an infinite-dimensional vector space if and only if it is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8868462195767364
      },
      {
        "text": "The Theorem of Linear Independence of Spanning Sets in Infinite Dimensional Spaces**: This theorem states that if a set of vectors spans an infinite dimensional vector space, then the set is linearly independent if and only if it is a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9131010491167236
      },
      {
        "text": "Theorem of Linear Independence in Infinite-Dimensional in Infinite-Dimensional Vector spaces: This theorem: Thisorem: This theorem in Infinite-Dimensional Vector spaces in a basis: This theorem: This theorem**: This theorem in a set of Linear Independence of a set of Linear Independence of Linear Independence in a set of Linear Independence of Linear Independence of a generalizes Theorem of Linear Independence of Linear Independence of Linear Independence of Linear Independence of Linear Independence of a set of Linear Independence of Linear Independence of Linear Independence of Linear Independence of a basis: This theorem generalizes theoremspecial Linear Independence of Linear Independence of a set of Linear Independence in a set of Linear Independence of a set of Linear Independence of Linear Independence of Linear Independence of Linear Independence of Linear Independence of Linear Independence of Linear Independence of Linear Independence of Linear Independence of a set of a set of a set of Linear Independence of a set of Linear Independence of Linear Independence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.865380939658341
      },
      {
        "text": "The Fundamental Theorem of Linear Independence: This theorem states that there are some of a basis Implies Linear Independence Implies Basis Implies basis Implies basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Theorem for Infinite-Dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8862310654929191
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Span and Span Implies Linear Independence are not true in general: While these statements are true for infinite-dimensional vector spaces, they are not necessarily true for finite-dimensional vector spaces. For example, a set of vectors can be linearly independent but not span the entire space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Span and Span Implies Linear Independence are not true in general: While these statements are true for infinite-dimensional vector spaces, they are not necessarily true for finite-dimensional vector spaces. For example, a set of vectors can be linearly independent but not span the entire space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence Implies Span and Span Implies Linear Independence are not true in general: While these statements are true for infinite-dimensional vector spaces, they are not necessarily true for finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9562067636057785
      },
      {
        "text": "**Linear Independence Implies Span with Respect to a Given Basis in Finite-Dimensional Vector spaces are not necessarily true for Linear Independence:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.913273674034764
      }
    ]
  },
  {
    "representative_text": "The concept of a \"non-trivial\" basis: A basis of a vector space that is not the trivial basis (containing only the zero vector) is called a \"non-trivial\" basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The concept of a \"non-trivial\" basis: A basis of a vector space that is not the trivial basis (containing only the zero vector) is called a \"non-trivial\" basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of \"Non-trivial\" basis: A basis of a vector space that is not the trivial basis (containing only the zero vector).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9168028637751169
      }
    ]
  },
  {
    "representative_text": "The concept of a \"minimal\" basis: A basis of a vector space that has the minimum number of vectors necessary to span the space is called a \"minimal\" basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The concept of a \"minimal\" basis: A basis of a vector space that has the minimum number of vectors necessary to span the space is called a \"minimal\" basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a minimal basis****: A minimal basis for a vector space is a basis that has the smallest possible number of elements. This concept is related to the concept of a free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8894214834487877
      },
      {
        "text": "The concept of \"Minimal\" basis: A basis of a vector space that has the minimum number of vectors necessary to span the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9296090741846874
      },
      {
        "text": "The concept of \"Minimal\" basis: A basis of a vector space that has the minimum number of vectors necessary to span the space is essential to understand in the context of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8553773855197033
      },
      {
        "text": "Minimal basis: A minimal basis for a vector space is a basis that has the basis**: Theorem: Theorem: Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8833969204953249
      }
    ]
  },
  {
    "representative_text": "The concept of \"linearly independent\" vs. \"linearly independent with respect to a given subspace\": A set of vectors can be linearly independent with respect to a given subspace, but not linearly independent with respect to the entire vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "The concept of \"linearly independent\" vs. \"linearly independent with respect to a given subspace\": A set of vectors can be linearly independent with respect to a given subspace, but not linearly independent with respect to the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of \"dependent\" vs. \"linearly dependent\": A set of vectors can be dependent in the sense that one vector can be expressed as a linear combination of the others, but not necessarily linearly dependent in the classical sense.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8193122401756673
      },
      {
        "text": "The concept of \"dependent\" vs. \"linearly dependent with respect to a given subspace\": A set of vectors can be dependent with respect to a given subspace, but not necessarily linearly dependent with respect to the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9461736023879068
      },
      {
        "text": "The concept of \"Linear Independence with Respect to a Given Subspace\": A set of vectors can be linearly independent with respect to a given subspace, but not linearly independent with respect to the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9313887883240065
      },
      {
        "text": "The concept of \"Linear Independence with Respect to a Given Basis\": A set of vectors can be linearly independent with respect to a given basis, but not linearly independent with respect to the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.918264106014252
      },
      {
        "text": "The Concept of a Linearly Dependent Set of Vectors in a Vector Space: A set of vectors can be linearly dependent in one vector space but not in another.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8971269819285056
      },
      {
        "text": "The concept of \"Basis of a subspace\": A set of vectors that spans a subspace and is linearly independent with respect to the subspace is essential to understand in the context of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8407832211580792
      },
      {
        "text": "Dependent vs. Linearly Dependent with Respect to a Given Basis: A set of vectors can be dependent with respect to a given basis, but not linearly dependent in the classical sense.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8779668248727313
      },
      {
        "text": "Dependent Vectors with Respect to a Given Basis: A set of vectors can be dependent with respect to a given a crucial in",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8308059620540066
      }
    ]
  },
  {
    "representative_text": "The concept of a \"linear combination of subspaces\": The sum of two or more subspaces is a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The concept of a \"linear combination of subspaces\": The sum of two or more subspaces is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"span of subspaces\": The span of two or more subspaces is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9143221556700964
      },
      {
        "text": "Span of Subspaces: The span of two or more subspaces is a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9115862599680283
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Orthogonality: The concept of linear independence can be extended to orthogonal vectors, where the dot product of two orthogonal vectors is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 39,
    "detailed_sources": [
      {
        "text": "Linear Independence and Orthogonality: The concept of linear independence can be extended to orthogonal vectors, where the dot product of two orthogonal vectors is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Linear Independence and Orthogonality: Linear independence and orthogonality are related, as two vectors are orthogonal if and only if one of them is a scalar multiple of the other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8720841653584612
      },
      {
        "text": "Linear Independence and Orthogonality: The concept of linear independence can be related to orthogonality, particularly in the context of inner product spaces. Understanding the relationship between linear independence and orthogonality can provide insights into the structure of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8675532723102908
      },
      {
        "text": "The Connection between Linear Independence and Orthogonality: Orthogonality is an important concept in linear algebra, and it is closely related to linear independence. A set of vectors is said to be orthogonal if the dot product of any two distinct vectors in the set is zero. This concept is essential in understanding linear independence in the context of inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9141983196347214
      },
      {
        "text": "Linear Independence and Orthogonality: The concept of orthogonal vectors and its relationship with linear independence. Specifically, two vectors are orthogonal if their dot product is zero, and this property can be used to determine linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.929631561077836
      },
      {
        "text": "Linear Independence and Orthogonalization: Linear independence can be extended to orthogonal vectors. Specifically, two vectors are orthogonal if and only if one of them is a scalar multiple of the other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.899147174262285
      },
      {
        "text": "Linear Independence and Orthogonality: A set of vectors is linearly independent if and only if the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has only the trivial solution, which is related to the concept of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8934607874977419
      },
      {
        "text": "The Interplay between Linear Independence and Orthogonality: Understanding the relationship between linear independence and orthogonality, such as in the context of orthogonal projections or the Gram-Schmidt process, could be beneficial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8870822923073891
      },
      {
        "text": "The Relationship between Linear Independence and Orthogonality in Inner Product Spaces: Understanding the relationship between linear independence and orthogonality can provide insights into the structure of inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8770686010147495
      },
      {
        "text": "The Concept of Linear Independence in the Context of Inner Product Spaces: This concept involves understanding the properties of linear independence in inner product spaces, such as Euclidean spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.828598716954855
      },
      {
        "text": "The Role of Orthogonality in Linear Independence: Orthogonality plays a crucial role in understanding linear independence, especially in the context of inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9137986520632071
      },
      {
        "text": "Relating Linear Independence to Orthogonality: The concept of linear independence can be related to orthogonality in vector spaces, particularly in the context of orthogonal projections and orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9500077799246917
      },
      {
        "text": "Linear Independence and Orthogonal Complement: The orthogonal complement of a subspace is the set of vectors that are orthogonal to every vector in the subspace. The relationship between linear independence and orthogonal complement is crucial in understanding vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8868666136399159
      },
      {
        "text": "Interpretation of Linear Independence in Inner Product Spaces: Understanding the concept of linear independence in the context of inner product spaces, which involves the use of inner product to define a notion of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8936678097276083
      },
      {
        "text": "Relationship between Linear Independence and Orthogonality in Inner Product Spaces: Investigating the relationship between linear independence and orthogonality, particularly in the context of inner product spaces, to gain insights into the structure of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.884492889730778
      },
      {
        "text": "Linear Independence and Orthogonality in Hilbert Spaces: Extending the concept of linear independence to Hilbert spaces, which involves the use of inner product to define a notion of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8774287422535325
      },
      {
        "text": "The Role of Orthogonality in Linear Independence: Exploring the role of orthogonality in linear independence, particularly in the context of inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9447426265566916
      },
      {
        "text": "The Impact of Non-Standard Inner Products: Investigating the impact of non-standard inner products on linear independence, which involves the use of alternative notions of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8092459022111596
      },
      {
        "text": "The role of inner product spaces in linear independence: Inner product spaces provide a way to define linear independence using inner products. A set of vectors is said to be linearly independent if the inner product of any two distinct vectors in the set is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.881792048951618
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space using the Inner Product: The relationship between linear independence and the null space can be understood using the inner product, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8924087214446363
      },
      {
        "text": "Linear Independence and Orthogonal Projections: The concept of orthogonal projections and its relationship with linear independence, particularly in the context of orthogonal complements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8574549293373394
      },
      {
        "text": "The Relationship Between Linear Independence and Orthogonality in Infinite-Dimensional Vector Spaces: The relationship between linear independence and orthogonality in infinite-dimensional vector spaces is essential to understand in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.913480724917332
      },
      {
        "text": "Orthogonality and Linear Independence for Infinite-Dimensional Vector Spaces: We need to consider the implications of orthogonality on linear independence for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8568753177710489
      },
      {
        "text": "Linear Independence and Vector Projections: The concept of vector projections and its relationship with linear independence, particularly in the context of orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8859633077012489
      },
      {
        "text": "The concept of vector orthogonality: A concept used to describe the relationship between two vectors, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8518119470734005
      },
      {
        "text": "Linear Independence and Orthogonality in Inner Product Spaces: While the relationship between linear independence and orthogonality is mentioned, a more in-depth exploration of this concept is needed.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8942917203755225
      },
      {
        "text": "Orthogonality and Linear Independence for Infinite-Dimensional Vector Spaces: The implications of orthogonality on linear independence in infinite-dimensional vector spaces are not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8536249578068471
      },
      {
        "text": "Impact of Non-Standard Inner Products on Linear Independence in Infinite-Dimensional Spaces: Investigating the impact of non-standard inner products on linear independence in infinite-dimensional spaces, which involves the use of alternative notions of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.818310817459933
      },
      {
        "text": "Impact of Non-Standard Vector Spaces on Linear Independence: Exploring the impact of non-standard vector spaces on linear independence, which involves the use of alternative notions of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8834440586057799
      },
      {
        "text": "Linear Independence and the Dimension of a Hilbert Space with Non-Standard Inner Products: Exploring the concept of linear independence in Hilbert spaces with non-standard inner products, which involves the use of alternative notions of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8668087778274788
      },
      {
        "text": "Impact of Non-Standard Hilbert Spaces on Linear Independence: Investigating the impact of non-standard Hilbert spaces on linear independence, which involves the use of alternative notions of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8515761517682986
      },
      {
        "text": "Linear Independence and Orthogonality: A set of vectors is said to be orthogonal if the dot product of any two distinct vectors in the set is zero. This concept is essential in understanding linear independence in the context of inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8990156179124513
      },
      {
        "text": "The Relationship Between Linear Independence and Orthogonality in Infinite-Dimensional Spaces: The relationship between linear independence and orthogonality in infinite-dimensional spaces is more complex and nuanced than in finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8927278622692416
      },
      {
        "text": "Linear Independence and Orthogonal Vectors: While this point is already mentioned, it's worth noting that linear independence can be extended to orthogonal vectors, where the dot product of two orthogonal vectors is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8801326282206794
      },
      {
        "text": "Impact of Non-Standard Banach Spaces on Linear Independence: Investigating the impact of non-standard Banach spaces on linear independence, which involves the use of alternative notions of orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8377898211588135
      },
      {
        "text": "Linear independence in Hilbert spaces with non-standard inner products: Investigate the impact of non-standard inner products on linear independence in infinite-dimensional Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8513447923909487
      },
      {
        "text": "Orthogonality and Linear Independence for Infinite-Dimensional Vector Spaces: Orthogonality is a property of vectors that can be used to determine linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921557845118046
      },
      {
        "text": "Relationship between linear independence and the dimension of a vector space using the concept of \"orthogonal\": The concept of orthogonal projections can be used to understand the relationship between linear independence and the dimension of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8944453611139251
      },
      {
        "text": "Orthogonality and Linear Independence: The orthogonality of functions can be used to determine linear independence. Two functions are orthogonal if and only if their inner product is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8698950201453919
      }
    ]
  },
  {
    "representative_text": "Span of the Conjugate: The span of a set of vectors is equal to the span of its conjugate, which is the set of all linear combinations of the conjugates of the original vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Span of the Conjugate: The span of a set of vectors is equal to the span of its conjugate, which is the set of all linear combinations of the conjugates of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Independence of the Conjugate of a Set of Vectors: This point is not explicitly mentioned but is related to the concept of the span of the conjugate.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8531669559046817
      }
    ]
  },
  {
    "representative_text": "Linear Independence of a Subspace: A subspace is said to be linearly independent if every vector in the subspace can be expressed as a linear combination of the other vectors in the subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence of a Subspace: A subspace is said to be linearly independent if every vector in the subspace can be expressed as a linear combination of the other vectors in the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Transformations: Linear independence is preserved under linear transformations, meaning that if a set of vectors is linearly independent, then the image of the set under a linear transformation is also linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 20,
    "detailed_sources": [
      {
        "text": "Linear Independence and Transformations: Linear independence is preserved under linear transformations, meaning that if a set of vectors is linearly independent, then the image of the set under a linear transformation is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Concept of Linear Independence in Linear Transformations: Linear independence is also an important concept in the context of linear transformations. A linear transformation is said to be one-to-one if it preserves linear independence. This means that if a set of vectors is linearly independent, then the corresponding linear transformation maps the set to a set of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8664097157282895
      },
      {
        "text": "Linear Independence and Homomorphisms: The concept of homomorphisms between vector spaces and its relationship with linear independence. Specifically, a homomorphism between vector spaces preserves linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8270100455526733
      },
      {
        "text": "Relationship between Linear Independence and Linear Transformation: A linear transformation is a function between vector spaces that preserves the operations of vector addition and scalar multiplication, which is related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8663553450927762
      },
      {
        "text": "Linear Transformations and Linear Independence: We need to consider how linear transformations affect linear independence, particularly in the context of injectivity and surjectivity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8702253466181502
      },
      {
        "text": "Relationship between Linear Independence and Linear Transformations: The concept of linear independence can be related to linear transformations, particularly in the context of the rank-nullity theorem and the dimension theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9106261347709756
      },
      {
        "text": "The Role of Linear Independence in the Study of Linear Transformations: Investigating the role of linear independence in the study of linear transformations, particularly in the context of infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.837270296731343
      },
      {
        "text": "Linear Transformations and Linear Independence for Infinite-Dimensional Vector Spaces: We need to consider how linear transformations affect linear independence, particularly in the context of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9105603960841401
      },
      {
        "text": "Span of a Linear Transformation and Linear Independence: We need to consider the relationship between the span of a linear transformation and linear independence, particularly in the context of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8969846658949019
      },
      {
        "text": "Linear Transformations and Linear Independence for Finite-Dimensional Vector Spaces: We need to consider how linear transformations affect linear independence, particularly in the context of finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9057289459379725
      },
      {
        "text": "The Linear Independence and Span of Complex Vectors: The study of linear independence and span of complex vectors can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8425989618203664
      },
      {
        "text": "Linear Independence and Homomorphisms with Respect to a Given Basis: A homomorphism between vector spaces preserves linear independence with respect to a given basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8623782116226301
      },
      {
        "text": "Linear Transformations and Linear Independence for Infinite-Dimensional Vector Spaces: The impact of linear transformations on linear independence in infinite-dimensional vector spaces is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8652413780267858
      },
      {
        "text": "Linear Transformations and Linear Independence for Finite-Dimensional Vector Spaces: The impact of linear transformations on linear independence in finite-dimensional vector spaces is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8663317151238079
      },
      {
        "text": "The Linear Independence of the Image of a Linear Transformation under a Change of Basis with a Non-Empty Kernel: This point is not explicitly mentioned but is related to the concept of linear independence under a change of basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.842401547746624
      },
      {
        "text": "Linear Transformations and Linear Independence for Infinite-Dimensional Vector Spaces: Analyze the impact of linear transformations on linear independence in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8903066073300905
      },
      {
        "text": "Linear Transformations and Linear Independence for Finite-Dimensional Vector Spaces: Analyze the impact of linear transformations on linear independence in finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9124275546517688
      },
      {
        "text": "Linear Transformations and Linear Independence: The study of linear transformations and their relationship to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9044594060310317
      },
      {
        "text": "Linear Independence and Linear Transformations with Non-Scalar Coefficients: This topic explores how linear independence is preserved under linear transformations with non-scalar coefficients.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8895603781914219
      },
      {
        "text": "Linear Independence and Linear Transformations with Non-Linear Transformations: This topic explores how linear independence is preserved under linear transformations with non-linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8868450816886548
      }
    ]
  },
  {
    "representative_text": "Dimension of the intersection of spans: The dimension of the intersection of two or more spans is less than or equal to the minimum of the dimensions of the individual spans.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension of the intersection of spans: The dimension of the intersection of two or more spans is less than or equal to the minimum of the dimensions of the individual spans.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis of the intersection of spans: The intersection of two or more spans contains a basis for the intersection of the spans if and only if the basis spans the entire intersection.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis of the intersection of spans: The intersection of two or more spans contains a basis for the intersection of the spans if and only if the basis spans the entire intersection.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear dependence in the intersection of spans: A set of vectors is linearly dependent in the intersection of spans if and only if the set is linearly dependent in at least one of the individual spans.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear dependence in the intersection of spans: A set of vectors is linearly dependent in the intersection of spans if and only if the set is linearly dependent in at least one of the individual spans.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Schmidt decomposition: A subspace can be decomposed into a direct sum of a basis and a complementary basis, where the basis is a basis for the original subspace and the complementary basis is a basis for the orthogonal complement of the original subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Schmidt decomposition: A subspace can be decomposed into a direct sum of a basis and a complementary basis, where the basis is a basis for the original subspace and the complementary basis is a basis for the orthogonal complement of the original subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Reduction of a linear transformation: A linear transformation can be reduced to a simpler form by removing linearly dependent columns or rows.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Reduction of a linear transformation: A linear transformation can be reduced to a simpler form by removing linearly dependent columns or rows.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Least squares problem: The least squares problem is a problem of finding the best-fitting linear combination of a set of vectors to a given set of data points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Least squares problem: The least squares problem is a problem of finding the best-fitting linear combination of a set of vectors to a given set of data points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hamilton's Basis Theorem: A set of linearly independent vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}n\\}$ is a basis for a vector space if and only if the set is linearly independent and there exists a basis $\\{\\mathbf{e}1, \\mathbf{e}2, ..., \\mathbf{e}n\\}$ such that each $\\mathbf{v}i$ can be expressed as a linear combination of the basis vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hamilton's Basis Theorem: A set of linearly independent vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}n\\}$ is a basis for a vector space if and only if the set is linearly independent and there exists a basis $\\{\\mathbf{e}1, \\mathbf{e}2, ..., \\mathbf{e}n\\}$ such that each $\\mathbf{v}i$ can be expressed as a linear combination of the basis vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span and Linear Combinations: A linear combination of vectors $\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n$ can be expressed as a sum of linear combinations of the vectors. This concept is related to the concept of span and basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Span and Linear Combinations: A linear combination of vectors $\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n$ can be expressed as a sum of linear combinations of the vectors. This concept is related to the concept of span and basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Relationship between the Span of a Set of Vectors and the Span of its Linear Combinations: The concept of linear combinations of vectors is closely related to the concept of span. However, it is not explicitly stated how the span of a set of vectors relates to the span of its linear combinations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8567389526321334
      },
      {
        "text": "Dimension of the Span of a Linear Combination of Vectors: The concept of linear combinations of vectors is closely related to the concept of span. However, it is not explicitly stated how the dimension of the span of a linear combination of vectors relates to the dimension of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9317674545189176
      },
      {
        "text": "Dimension of the Span of a Linear Combination of Vectors: We need to explore the relationship between the dimension of the span of a linear combination of vectors and the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8428413343403225
      },
      {
        "text": "Relationship between the Span of a Set of Vectors and the Span of its Linear Combinations: This relationship is not explicitly stated in the existing points. It would be helpful to explore how the span of a set of vectors relates to the span of its linear combinations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8926585665041533
      },
      {
        "text": "Dimension of the Span of a Linear Combination of Vectors: While we have the relationship between the dimension of the span of a linear combination of vectors and the original vectors, we need to explore this relationship in more detail.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9178179941761697
      },
      {
        "text": "Span of a Linear Combination of Vectors and Linear Independence: The relationship between the span of a linear combination of vectors and linear independence is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8792792455833958
      },
      {
        "text": "Span of a Linear Combination of Vectors and Linear Independence for Infinite-Dimensional Vector Spaces: The relationship between the span of a linear combination of vectors and linear independence is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8974324693498263
      }
    ]
  },
  {
    "representative_text": "Null Space and Span: The null space of a linear transformation $T$ is the set of vectors $\\mathbf{x}$ such that $T(\\mathbf{x}) = \\mathbf{0}$. The span of the null space of $T$ is related to the concept of span and basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 26,
    "detailed_sources": [
      {
        "text": "Null Space and Span: The null space of a linear transformation $T$ is the set of vectors $\\mathbf{x}$ such that $T(\\mathbf{x}) = \\mathbf{0}$. The span of the null space of $T$ is related to the concept of span and basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Relationship between Linear Independence and the Null Space: This concept involves the null space of a linear transformation, which is the set of all vectors that are mapped to the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8195168889499115
      },
      {
        "text": "Linear Independence and Null Space: Understanding the relationship between linear independence and the null space of a matrix can provide insights into the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8552500323217574
      },
      {
        "text": "The Null Space: The null space of a matrix is the set of all vectors that, when multiplied by the matrix, result in the zero vector. This concept is related to linear dependence and independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8674635629984753
      },
      {
        "text": "The Null Space and Its Applications: The null space of a linear transformation is the set of vectors that are mapped to the zero vector. This concept has many applications in linear algebra, including the fact that the null space of a linear transformation is equal to the orthogonal complement of the image of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8736078267949954
      },
      {
        "text": "The Role of the Null Space in Linear Independence: The null space of a linear transformation plays a crucial role in understanding linear independence. The kernel of a linear transformation is the set of vectors that are mapped to the zero vector. If the kernel is trivial (i.e., contains only the zero vector), then the linear transformation is one-to-one, and the corresponding vectors are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8690493659631237
      },
      {
        "text": "Null Space and Linear Independence: The null space of a matrix is related to linear independence. Specifically, if a matrix has a non-zero determinant, then its row space and column space are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8637895156825277
      },
      {
        "text": "Relationship between the Null Space and Span of a Linear Transformation: The Null Space and Span theorem states that the null space of a linear transformation is a subspace of the vector space. However, it does not explicitly address the relationship between the null space and span of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8337484084003572
      },
      {
        "text": "Null Space and Span: The null space of a matrix is related to the concept of span, where the null space is the set of vectors that are mapped to the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9043407217476971
      },
      {
        "text": "Null Space and Span Relationship: The null space and span are closely related, but we need to explore the specifics of how they interact, such as the dimension of the null space and how it relates to the span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8506174403591149
      },
      {
        "text": "Dimension of the Null Space and Span for Infinite-Dimensional Vector Spaces: We need to explore the relationship between the dimension of the null space and span for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8046322259488138
      },
      {
        "text": "Relationship between Span and the Null Space: The null space of a matrix is related to the span of a set of vectors. Understanding the relationship between the null space and the span is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9233634179690401
      },
      {
        "text": "The Null Space Theorem: This theorem states that the null space of a matrix is a vector space. This theorem has implications for the linear independence of vectors in the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8291981355982467
      },
      {
        "text": "Null Space and Span Relationship for Infinite-Dimensional Vector Spaces: We need to explore the specifics of how the null space and span interact, particularly in the context of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8416208748912666
      },
      {
        "text": "Null Space Theorem: This theorem states that the null space of a linear transformation is a subspace of the vector space. It is closely related to the concept of span and basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9041990724367595
      },
      {
        "text": "Null Space Theorem: This theorem states that the null space of a linear transformation is a subspace of the vector space. It would be helpful to explore the relationship between the null space and span of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9103632905569465
      },
      {
        "text": "The relationship between linear independence and the Null Space Theorem: The Null Space Theorem states that the null space of a matrix is a vector space. This theorem can be used to show that the null space of a matrix is related to the linear independence of vectors in the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8988067079664366
      },
      {
        "text": "Null Space Theorem and its Implications: Delving deeper into the Null Space Theorem, including its implications for understanding the relationship between the null space of a linear transformation and the span of the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9053111764595141
      },
      {
        "text": "Relationship between the Null Space and Span of a Linear Transformation: Exploring the relationship between the null space and span of a linear transformation, including the existence of a null space basis and the dimension of the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8846152492812654
      },
      {
        "text": "Null Space and Span Relationship for Infinite-Dimensional Vector Spaces: The interaction between the null space and span in infinite-dimensional vector spaces is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8277047454883155
      },
      {
        "text": "Dimension of the Null Space and Span for Infinite-Dimensional Vector Spaces: The relationship between the dimension of the null space and span in infinite-dimensional vector spaces is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8335883488786033
      },
      {
        "text": "Null Space Theorem and its Implications for Understanding the Dimension of the Null Space: Delve deeper into the Null Space Theorem and its implications for understanding the dimension of the null space, including the relationship between the null space and the span of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8854243449950516
      },
      {
        "text": "Null Space Theorem and its Implications for Vector Spaces with Non-Trivial Null Spaces: Investigate the implications of the Null Space Theorem for vector spaces with non-trivial null spaces, including the relationship between the null space and the span of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8770132204168426
      },
      {
        "text": "Span of the Null Space: The span of the null space of a matrix is a proper subspace of the codomain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8030607008927613
      },
      {
        "text": "Existence of a Basis for a Vector Space with a Non-Trivial Null Space: This concept is crucial in understanding the existence of a basis for a vector space with a non-trivial null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8168394576558367
      },
      {
        "text": "Null Space Theorem and its Implications for Vector Spaces with Non-Trivial Null Spaces: This theorem has implications for understanding the relationship between the null space and the span of a linear transformation in vector spaces with non-trivial null spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9033092819147184
      }
    ]
  },
  {
    "representative_text": "Linear Independence of Null Space: If a set of vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n\\}$ is linearly independent, then the null space of the linear transformation $T$ with respect to the set of vectors is also linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence of Null Space: If a set of vectors $\\{\\mathbf{v}1, \\mathbf{v}2, ..., \\mathbf{v}_n\\}$ is linearly independent, then the null space of the linear transformation $T$ with respect to the set of vectors is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lagrange's Theorem: This theorem states that if a set of vectors is linearly dependent, then it can be expressed as a linear combination of a smaller set of linearly independent vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lagrange's Theorem: This theorem states that if a set of vectors is linearly dependent, then it can be expressed as a linear combination of a smaller set of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Complement Theorem: This theorem states that if a vector space has a basis, then the complement of the basis is also a basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Complement Theorem: This theorem states that if a vector space has a basis, then the complement of the basis is also a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Duality Theorem: This theorem states that if a set of vectors is linearly independent, then its dual space (the space of linear functionals on the original space) is also linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Duality Theorem: This theorem states that if a set of vectors is linearly independent, then its dual space (the space of linear functionals on the original space) is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Row Operations: This refers to the set of operations that can be performed on a matrix to transform it into row echelon form, including adding multiples of one row to another row, multiplying a row by a scalar, and interchanging two rows.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Row Operations: This refers to the set of operations that can be performed on a matrix to transform it into row echelon form, including adding multiples of one row to another row, multiplying a row by a scalar, and interchanging two rows.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Row Operations: Row operations are a set of elementary row operations that can be performed on the augmented matrix to transform it into upper triangular form.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8531152879600551
      },
      {
        "text": "Elementary Row and Column Operations: Elementary row and column operations are a set of row and column operations that can be performed on the augmented matrix to transform it into upper triangular form.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9194692555146271
      }
    ]
  },
  {
    "representative_text": "Pivot and Non-Pivot Columns: This refers to the columns of a matrix that have pivots (leading 1s) and those that do not.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pivot and Non-Pivot Columns: This refers to the columns of a matrix that have pivots (leading 1s) and those that do not.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Dependence in Infinite Dimensional Spaces: While linear independence and dependence are well-defined in finite-dimensional spaces, they are more nuanced in infinite-dimensional spaces, where we need to consider the concept of convergence.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 56,
    "detailed_sources": [
      {
        "text": "Linear Independence and Dependence in Infinite Dimensional Spaces: While linear independence and dependence are well-defined in finite-dimensional spaces, they are more nuanced in infinite-dimensional spaces, where we need to consider the concept of convergence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and Basis in Infinite-Dimensional Vector Spaces: In infinite-dimensional vector spaces, the concept of linear independence and basis is more complex. A set of vectors can be linearly independent but not span the entire space, or vice versa.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8721827665100883
      },
      {
        "text": "Linear Independence of Infinite Sets: While finite linear independence is well-understood, the concept of linear independence for infinite sets is more nuanced. This includes understanding the concept of convergence of series and its implications on linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8923602652193894
      },
      {
        "text": "The Fundamental Theorem of Linear Independence for Infinite-Dimensional Vector Spaces: While the Fundamental Theorem of Linear Independence is well-established for finite-dimensional spaces, its extension to infinite-dimensional spaces requires careful consideration of the properties of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8500231068114942
      },
      {
        "text": "Linear Independence and Bases in Infinite-Dimensional Vector Spaces: In infinite-dimensional spaces, the concept of a basis is more nuanced. Understanding the relationship between linear independence and the existence of a basis in infinite-dimensional spaces is crucial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9125505476836385
      },
      {
        "text": "The Concept of Linear Independence in Infinite-Dimensional Spaces: In infinite-dimensional spaces, linear independence can be more complex to determine. This concept involves understanding the properties of infinite sets of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9369397702513917
      },
      {
        "text": "The Concept of Linear Dependence in Infinite-Dimensional Spaces: Similar to linear independence, linear dependence in infinite-dimensional spaces involves determining whether a set of vectors can be expressed as a linear combination of other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9034783144253014
      },
      {
        "text": "Linear Independence and the Concept of Linear Independence in Infinite-Dimensional Spaces: In infinite-dimensional spaces, linear independence is a more complex concept. A set of vectors can be linearly independent in an infinite-dimensional space even if they are not linearly independent in a finite-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9558510201644674
      },
      {
        "text": "Linear Independence in Infinite-Dimensional Spaces: In infinite-dimensional spaces, we need to consider the concept of convergence and how it affects linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9484704652081521
      },
      {
        "text": "Linear Independence of Spanning Sets in Infinite Dimensional Spaces: While the concept is mentioned, exploring its implications in infinite dimensional spaces, such as Banach spaces, could provide additional insights.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8438231133828707
      },
      {
        "text": "The Interplay between Linear Independence and Bases in Vector Spaces: Understanding the relationship between linear independence and bases in different types of vector spaces, such as finite-dimensional spaces, infinite-dimensional spaces, or spaces with different topologies, could provide additional insights.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.869059556266701
      },
      {
        "text": "The Connection between Linear Independence and Dimension in Infinite Dimensional Spaces: Exploring the relationship between linear independence and dimension in infinite dimensional spaces, such as Banach spaces or Hilbert spaces, could provide additional insights.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8809462552959828
      },
      {
        "text": "The Use of Topology in Linear Independence and Span: Topological concepts, such as the Hausdorff dimension or the concept of convergence in a topological vector space, could be applied to the context of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8317980365570405
      },
      {
        "text": "Linear Independence of Infinite Subspaces: This concept involves understanding the linear independence of subspaces in infinite-dimensional vector spaces, which is closely related to the concept of linear independence of the original set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8897953673051767
      },
      {
        "text": "Linear Independence in Infinite-Dimensional Vector Spaces: The concept of linear independence can be extended to infinite-dimensional vector spaces, where the dimension of the space may not be finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8901842665735322
      },
      {
        "text": "Linear Independence in Vector Spaces with Non-Standard Topologies: The concept of linear independence can be extended to vector spaces with non-standard topologies, such as Fréchet spaces or Banach spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8787562975966223
      },
      {
        "text": "Linear Independence and Span in Infinite-Dimensional Spaces: In infinite-dimensional spaces, linear independence and span are defined in terms of convergence, and we need to consider the concept of a generalized basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9135966032638079
      },
      {
        "text": "Linear Independence and Linear Dependence in Infinite-Dimensional Spaces: This concept involves the relationship between linear independence and linear dependence in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8998465816325925
      },
      {
        "text": "The Interplay between Linear Independence and the Null Space in Infinite Dimensional Spaces: This concept explores the relationship between linear independence and the null space in infinite dimensional spaces, such as Banach spaces or Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8484435067170812
      },
      {
        "text": "The Role of Linear Independence in the Study of Infinite-Dimensional Vector Spaces: Investigating the role of linear independence in the study of infinite-dimensional vector spaces, particularly in the context of Hilbert spaces and inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8073953850053495
      },
      {
        "text": "The Impact of Linear Independence on the Properties of a Vector Space: Exploring the impact of linear independence on the properties of a vector space, particularly in the context of infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8911492431368014
      },
      {
        "text": "Basis and Linear Independence for Infinite-Dimensional Vector Spaces: We need to consider the implications of having a basis for an infinite-dimensional vector space on linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9079481377166168
      },
      {
        "text": "The concept of linear independence in Banach spaces: Banach spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily finite-dimensional. A set of vectors in a Banach space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8727878801400014
      },
      {
        "text": "Linear Independence in Fréchet Spaces: The concept of linear independence can be extended to Fréchet spaces, which are a type of topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8653869626755732
      },
      {
        "text": "Linear Independence in Banach Spaces: The concept of linear independence can be extended to Banach spaces, which are a type of complete normed vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8771942645654325
      },
      {
        "text": "Linear Independence and the Dimension of a Vector Space is Infinite: The concept of linear independence can be related to the dimension of an infinite-dimensional vector space, particularly in the context of linear independence and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9435756245861553
      },
      {
        "text": "Linear Independence of Spanning Sets in Infinite Dimensional Spaces: Exploring the implications of linear independence in infinite dimensional spaces, such as Banach spaces or Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9321856008134755
      },
      {
        "text": "Linear Independence and the Dimension of a Hilbert Space: Exploring the impact of linear independence on the dimension of a Hilbert space, which involves the relationship between the dimension of a Hilbert space and the number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8616164256874361
      },
      {
        "text": "Linear Independence of a Set with a Linear Combination of Vectors for Infinite-Dimensional Vector Spaces: We need to explore the implications of having a linear combination of vectors on linear independence for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8737041808847643
      },
      {
        "text": "Linear Independence and Span in Non-Standard Bases: In non-standard bases, linear independence and span are defined in terms of convergence, and we need to consider the concept of a generalized basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8426531262721941
      },
      {
        "text": "Linear Independence and the Dimension of a Vector Space in Infinite-Dimensional Spaces: This involves exploring the impact of linear independence on the dimension of a vector space in infinite-dimensional spaces, which involves the relationship between the dimension of a vector space and the number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9023188518024294
      },
      {
        "text": "Linear Independence and Orthogonality in Infinite-Dimensional Spaces: This involves exploring the relationship between linear independence and orthogonality in infinite-dimensional spaces, particularly in the context of Hilbert spaces and inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8815060429005317
      },
      {
        "text": "The Relationship between Linear Independence and the Rank of a Matrix in Infinite-Dimensional Spaces: This involves examining the relationship between linear independence and the rank of a matrix in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8594932205063095
      },
      {
        "text": "The Concept of Linear Independence in the Context of Partial Differential Equations: This concept involves understanding the properties of linear independence in the context of partial differential equations, which is a more advanced application of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8015120209422191
      },
      {
        "text": "The Concept of Linear Independence in the Context of Harmonic Analysis: This concept involves understanding the properties of linear independence in the context of harmonic analysis, which is a more advanced application of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8082449011396382
      },
      {
        "text": "Linear Independence and Span in Non-Standard Vector Spaces with Dependent Vectors: In non-standard vector spaces with dependent vectors, linear independence and span are defined in terms of convergence, and we need to consider the concept of a generalized basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8735395450999492
      },
      {
        "text": "Linear Independence and Basis in Non-Standard Vector Spaces: This concept explores the relationship between linear independence and basis in non-standard vector spaces, where the usual properties of linear independence and basis may not hold.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8850184989002368
      },
      {
        "text": "Linear Independence and the Dimension of a Banach Space: Banach spaces are a type of infinite-dimensional vector space, and the concept of linear independence in this context is worth exploring.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8970657370984504
      },
      {
        "text": "Linear Independence and the Existence of a Basis in Fréchet Spaces: Fréchet spaces are a type of topological vector space, and the concept of linear independence in this context is worth exploring.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8355125843381881
      },
      {
        "text": "Linear Independence and the Dimension of a Locally Convex Space: Locally convex spaces are a type of topological vector space, and the concept of linear independence in this context is worth exploring.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8338687485742883
      },
      {
        "text": "Basis and Linear Independence for Infinite-Dimensional Vector Spaces: The implications of having a basis for an infinite-dimensional vector space on linear independence are not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.862481353166074
      },
      {
        "text": "The relationship between linear independence and the dimension of a vector space in the context of infinite dimensional spaces: While the concept of dimension is well-established in finite dimensional spaces, its relationship with linear independence in infinite dimensional spaces, such as Banach spaces or Hilbert spaces, is more nuanced.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9023768173849688
      },
      {
        "text": "Linear Independence and the Dimension of a Fréchet Space: Exploring the concept of linear independence in Fréchet spaces, which are a type of topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.898804260892015
      },
      {
        "text": "Linear Independence and the Existence of a Basis in Locally Convex Spaces: Exploring the concept of linear independence in locally convex spaces, which are a type of topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8960337108969068
      },
      {
        "text": "The Relationship Between Linear Independence and the Dimension of a Vector Space in Infinite-Dimensional Spaces: The relationship between linear independence and the dimension of a vector space in infinite-dimensional spaces is more complex and nuanced than in finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9147900914718632
      },
      {
        "text": "Linear Independence and the Extension Theorem: The relationship between linear independence and the extension theorem is a crucial concept to understand in the context of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9002149373134732
      },
      {
        "text": "Infinite-Dimensional Vector Spaces: The concept of linear independence in infinite-dimensional vector spaces, including the relationship between linear independence and the dimension of the space, and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9256952842098279
      },
      {
        "text": "Interplay between Linear Independence and Orthogonality in Infinite Dimensional Spaces: This concept explores the relationship between linear independence and orthogonality in infinite dimensional spaces, such as Banach spaces or Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8943332233252193
      },
      {
        "text": "Basis and Linear Independence for Infinite-Dimensional Vector Spaces: Delve into the implications of having a basis for an infinite-dimensional vector space on linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9009461446475635
      },
      {
        "text": "Linear Independence in Infinite-Dimensional Spaces with Non-Standard Metric: In infinite-dimensional spaces, we need to consider the concept of convergence and how it affects linear independence when the vector space is equipped with a non-standard metric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8931498470733501
      },
      {
        "text": "Non-Standard Basis and Linear Independence in Infinite-Dimensional Spaces: In infinite-dimensional spaces, we need to consider the concept of convergence and how it affects linear independence when the vector space is equipped with a non-standard basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9011428146678446
      },
      {
        "text": "Linear Independence of Spanning Sets in Infinite Dimensional Spaces: While the concept of linear independence is well-established in finite dimensional spaces, its implications in infinite dimensional spaces, such as Banach spaces or Hilbert spaces, are more nuanced and require further exploration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9045601137703632
      },
      {
        "text": "Non-standard bases and linear independence: Investigate the concept of non-standard bases in infinite-dimensional vector spaces and their relationship to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.887016620861873
      },
      {
        "text": "Basis and Linear Independence for Infinite-Dimensional Vector Spaces: The basis of an infinite-dimensional vector space can affect linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9105514101138237
      },
      {
        "text": "Linear Independence and the Complement of a Subspace in Infinite-Dimensional Vector Spaces: The relationship between linear independence and the complement of a subspace in infinite-dimensional vector spaces is more complex and nuanced than in finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9078772997092632
      },
      {
        "text": "Linear Dependence in Infinite-Dimensional Spaces with Differentiable Structures: The concept of linear dependence in infinite-dimensional spaces with differentiable structures, such as Riemannian manifolds, is more advanced and requires a deeper understanding of differential geometry and analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8457753042299939
      }
    ]
  },
  {
    "representative_text": "The concept of a \"generalized basis\": A set of vectors that is not necessarily linearly independent or spanning, but still satisfies certain properties, such as being a \"spanning set\" or a \"basis for a subspace\".",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The concept of a \"generalized basis\": A set of vectors that is not necessarily linearly independent or spanning, but still satisfies certain properties, such as being a \"spanning set\" or a \"basis for a subspace\".",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"partial basis\": A set of vectors that is not a basis for the entire vector space, but still satisfies certain properties, such as being a \"spanning set\" or a \"basis for a subspace\".",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8499435335367033
      },
      {
        "text": "Generalized Basis: A set of vectors that is not necessarily linearly independent or spanning, but still satisfies certain properties, such as being a \"spanning set\" or a \"basis for a subspace\".",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9055295181251143
      }
    ]
  },
  {
    "representative_text": "The relationship between basis and dimension in non-standard bases: In some cases, a basis for a vector space may not be the standard basis, and we need to consider how the dimension of the vector space is related to the basis in such cases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "The relationship between basis and dimension in non-standard bases: In some cases, a basis for a vector space may not be the standard basis, and we need to consider how the dimension of the vector space is related to the basis in such cases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The relationship between basis and dimension in vector spaces with non-standard bases: In some cases, the dimension of a vector space may be related to the basis in a non-standard way, such as being related to the number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9054510705079698
      },
      {
        "text": "Existence of a Basis in a Vector Space with a Non-Standard Basis: The existence of a basis in a vector space can be used to determine the dimension of the space, even if the basis is not standard.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.854123935536844
      },
      {
        "text": "The Dimension of a Vector Space with a Non-Standard Basis: The dimension of a vector space can be defined differently depending on the basis used.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9230951298738262
      },
      {
        "text": "The Relationship between Basis and Dimension for Vector Spaces with a Non-Standard Scalar Multiplication: The relationship between basis and dimension can be affected by the non-standard scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8526711930329015
      },
      {
        "text": "The Relationship between Basis and Dimension for Vector Spaces with a Non-Standard Addition Operation: The relationship between basis and dimension can be affected by the non-standard addition operation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9014891763547696
      },
      {
        "text": "Basis and Dimension in Vector Spaces with Non-Standard Metric: In some cases, the dimension of a vector space may be related to the basis in a non-standard way, such as being related to the number of linearly independent vectors, when the vector space is equipped with a non-standard metric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8887856610581382
      },
      {
        "text": "Relationship Between Basis and Dimension in Vector Spaces with Non-Standard Bases (with dependent vectors): In some cases, the dimension of a vector space may be related to the basis in a non-standard way, such as being related to the number of linearly independent vectors, when the vector space has dependent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9309718221213099
      },
      {
        "text": "Relationship Between Basis and Dimension in Vector Spaces with Generalized Dependent Vectors: In some cases, the dimension of a vector space may be related to the basis in a non-standard way, such as being related to the number of linearly independent vectors, when the vector space has generalized dependent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.922226862067425
      },
      {
        "text": "Relationship between Basis and Dimension for Vector Spaces with a Non-Standard Scalar Multiplication and Addition Operation: The relationship between basis and dimension can be affected by both non-standard scalar multiplication and addition operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8841469877165692
      }
    ]
  },
  {
    "representative_text": "The concept of a \"spanning set for a subspace\": A spanning set for a subspace is a set of vectors that spans the subspace, but may not be linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The concept of a \"spanning set for a subspace\": A spanning set for a subspace is a set of vectors that spans the subspace, but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"spanning set for a subspace with dependent vectors\": A spanning set for a subspace with dependent vectors is a set of vectors that spans the subspace, but is linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9211734773560509
      },
      {
        "text": "Spanning Set for a Subspace with Generalized Dependent Vectors: A spanning set for a subspace with generalized dependent vectors is a set of vectors that spans the subspace, but is linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9318607396980569
      },
      {
        "text": "Spanning Set for a Subspace with Generalized Dependent Vectors (Existence): A spanning set for a subspace with generalized dependent vectors exists if and only if the subspace is spanned by the set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8918914934328744
      },
      {
        "text": "Spanning Set Theorem (with Dependent Vectors and Non-Standard Bases): A spanning set for a subspace with dependent vectors and a non-standard basis is a set of vectors that spans the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9178212525975922
      }
    ]
  },
  {
    "representative_text": "The relationship between linear independence and span in finite-dimensional spaces: While linear independence and span are equivalent in finite-dimensional spaces, they are not equivalent in infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 33,
    "detailed_sources": [
      {
        "text": "The relationship between linear independence and span in finite-dimensional spaces: While linear independence and span are equivalent in finite-dimensional spaces, they are not equivalent in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of 'Linearly Independent' and 'Span' in relation to the vector space's dimension: Understanding the relationship between the dimension of a vector space, linear independence, and span can provide insights into the properties of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8228314966409206
      },
      {
        "text": "The concept of 'Linearly Independent' and 'Span' in relation to the vector space's basis: Understanding the relationship between the existence of a basis, linear independence, and span can provide insights into the structure of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8951926502350274
      },
      {
        "text": "Basis and Span Relationship: A deeper understanding of the relationship between a basis and its span, including how the dimension of the space relates to the span of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8371583340885267
      },
      {
        "text": "Dimension of a Vector Space and its Span: The relationship between the dimension of a vector space and the span of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8547380538345402
      },
      {
        "text": "Linear Independence and Bases in Finite-Dimensional Vector Spaces: Linear independence and bases are fundamental concepts in finite-dimensional vector spaces, and there are many results that relate these concepts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8410167009945319
      },
      {
        "text": "Relationship between Linear Independence and Span in a Vector Space: The relationship between linear independence and span in a vector space can be used to determine the dimension of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8798514346231141
      },
      {
        "text": "Relationship between Linear Independence and Span in a Vector Space with a Non-Standard Basis: The relationship between linear independence and span in a vector space can be used to determine the dimension of the space, even if the basis is not standard.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9111834364814659
      },
      {
        "text": "Linear Independence Implies Span: The Dimensionality Consequence: This concept involves the consequence of linear independence on the dimensionality of the span. It states that if a set of vectors is linearly independent, then the dimension of the span is equal to the number of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8040538247471312
      },
      {
        "text": "Relationship between Span and Dimension: The dimension of a vector space is equal to the number of linearly independent vectors in the space, which is related to the concept of span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8992050949875434
      },
      {
        "text": "The concept of 'Linear Independence' and 'Span' in relation to the concept of 'Dimension' and 'Rank': Understanding the relationship between linear independence, span, dimension, and rank can provide insights into the properties of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.889074091961304
      },
      {
        "text": "Span Implies Linear Independence for Vector Spaces with a Basis: We need to consider how the span of a basis for a vector space relates to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8724216347472074
      },
      {
        "text": "Span Implies Linear Independence for Vector Spaces with a Non-Trivial Span: We need to explore the implications of having a non-trivial span on linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.86443351822693
      },
      {
        "text": "Basis and Span Relationship for Infinite-Dimensional Vector Spaces: The relationship between basis and span can be more complex in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.863442164072667
      },
      {
        "text": "Basis and Span: The basis of a vector space and the span of a set of vectors are related to the linear independence of vectors and the dimension of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9174882470997985
      },
      {
        "text": "The Relationship between Linear Independence and the Dimension of a Vector Space: The dimension of a vector space is closely related to linear independence, as a set of vectors is linearly independent if and only if the vectors span a vector space with the maximum possible dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9016662135735614
      },
      {
        "text": "The Impact of Linear Independence on the Dimension of a Vector Space: Exploring the impact of linear independence on the dimension of a vector space, which involves the relationship between the dimension of a vector space and the number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8998402632447282
      },
      {
        "text": "The Relationship between Linear Independence and the Span of a Vector Space: Investigating the relationship between linear independence and the span of a vector space, which involves the use of linear independence to define a notion of dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9043100079317472
      },
      {
        "text": "The relationship between linear independence and the dimension of a subspace: This relationship is mentioned earlier, but it's crucial to understand how the dimension of a subspace is related to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8832755483060117
      },
      {
        "text": "Linear Independence and the Dimension of a Vector Space: The concept of linear independence can be related to the dimension of a vector space, particularly in the context of the dimension theorem and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9197614876087896
      },
      {
        "text": "The Role of Linear Independence in Understanding the Structure of Vector Spaces: Linear independence plays a crucial role in understanding the structure of vector spaces, including the existence of bases, the dimension of the space, and the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8434760757853721
      },
      {
        "text": "Linear Independence and the Dimension of a Subspace with Respect to a Given Basis: Understanding the relationship between linear independence and the dimension of a subspace with respect to a given basis is crucial in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9054226123603683
      },
      {
        "text": "Linear Independence Implies Span and Span Implies Linear Independence in Finite-Dimensional Vector Spaces: Understanding the nuances of linear independence in finite-dimensional vector spaces is crucial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9013349747755499
      },
      {
        "text": "Interpretation of Linear Independence in the Context of Span: Understanding the concept of linear independence in the context of span, including the implications for the span of a set of vectors and the existence of a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8825839369190676
      },
      {
        "text": "Characterization of Span using Linear Independence: Investigate the relationship between the span of a set of vectors and the linear independence of the vectors, including the existence of a basis for the span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8852986126581457
      },
      {
        "text": "Characterization of Linear Independence using Span: Investigate the relationship between linear independence and the span of a set of vectors, including the existence of a basis for the span and the implications for understanding the dimension of the span and the existence of a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8830160643384837
      },
      {
        "text": "Relationship between Span and Basis: Delve deeper into the relationship between the span of a set of vectors and the basis of the vector space, including the implications for understanding the dimension of the span and the existence of a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8458868299402091
      },
      {
        "text": "Characterization of Linear Independence using Span: While the concept of linear independence is mentioned, it's essential to delve deeper into the relationship between linear independence and the span of a set of vectors, including the existence of a basis for the span and the implications for understanding the dimension of the span and the existence of a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8685447976371734
      },
      {
        "text": "Linear Independence Implies Span for Finite-Dimensional Spaces: This concept involves the relationship between linear independence and span in finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9147031662199399
      },
      {
        "text": "Span Implies Linear Independence Implies Basis for Finite-Dimensional Spaces: This concept involves the relationship between span, linear independence, and basis in finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9082522918811113
      },
      {
        "text": "The Relationship Between Linear Independence and the Dimension of a Vector Space in Finite-Dimensional Spaces: The relationship between linear independence and the dimension of a vector space in finite-dimensional spaces is more straightforward than in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.862360033668633
      },
      {
        "text": "Characterization of Linear Independence using Span: This concept is crucial in understanding the relationship between linear independence and the span of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8897930173914828
      },
      {
        "text": "Dimension of the Span of a Linear Combination of Vectors using Linear Independence: This concept is crucial in understanding the relationship between the dimension of the span and the span and the dimension of the dimension and the dimension of the dimension of the dimension of theore rigorous proof of theore to understand the dimension of a basis theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.875182829295432
      }
    ]
  },
  {
    "representative_text": "The concept of a \"basis for a subspace with a non-standard basis\": A basis for a subspace with a non-standard basis is a set of vectors that spans the subspace and is linearly independent, but may not be the standard basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "The concept of a \"basis for a subspace with a non-standard basis\": A basis for a subspace with a non-standard basis is a set of vectors that spans the subspace and is linearly independent, but may not be the standard basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"spanning set for a subspace with a non-standard basis\": A spanning set for a subspace with a non-standard basis is a set of vectors that spans the subspace, but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.907922652466279
      },
      {
        "text": "Basis for a Subspace with Non-Standard Basis (Converse): A basis for a subspace with a non-standard basis is a set of vectors that spans the subspace and is linearly independent, if and only if the set is a spanning set for the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8249679635586218
      },
      {
        "text": "Spanning Set for a Subspace with Non-Standard Basis (Converse): A spanning set for a subspace with a non-standard basis is a set of vectors that spans the subspace, if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8896588842643971
      },
      {
        "text": "Basis for a Subspace with Non-Standard Basis (Existence): A basis for a subspace with a non-standard basis exists if and only if the subspace is closed under the given basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8343340103710897
      },
      {
        "text": "Spanning Set for a Subspace with Non-Standard Basis (Existence): A spanning set for a subspace with a non-standard basis exists if and only if the subspace is spanned by the set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9131606219951445
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Basis: A basis for a vector space with a non-standard basis is a set of vectors that spans the subspace and is linearly independent, but may not be the standard basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8981064364959084
      },
      {
        "text": "Non-Standard Basis and Linear Independence: A set of vectors in a non-standard basis is linearly independent if and only if the set spans the vector space with a non-standard basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8513861837320302
      },
      {
        "text": "Non-Standard Basis and Spanning Property: A set of vectors in a non-standard basis spans the vector space if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9032953872212461
      }
    ]
  },
  {
    "representative_text": "The concept of a \"partial basis for a subspace\": A partial basis for a subspace is a set of vectors that spans the subspace, but may not be linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The concept of a \"partial basis for a subspace\": A partial basis for a subspace is a set of vectors that spans the subspace, but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"generalized partial basis\": A generalized partial basis is a set of vectors that spans the subspace and may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9220421056408707
      },
      {
        "text": "The concept of a \"partial basis for a subspace with non-standard basis\": A partial basis for a subspace with non-standard basis is a set of vectors that spans the subspace, but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9462111462170484
      }
    ]
  },
  {
    "representative_text": "The concept of a \"basis for a subspace with dependent vectors\": A basis for a subspace with dependent vectors is a set of vectors that spans the subspace and is linearly dependent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "The concept of a \"basis for a subspace with dependent vectors\": A basis for a subspace with dependent vectors is a set of vectors that spans the subspace and is linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"partial basis for a subspace with dependent vectors\": A partial basis for a subspace with dependent vectors is a set of vectors that spans the subspace, but is linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9296292502801664
      },
      {
        "text": "The concept of a \"generalized dependent basis\": A generalized dependent basis is a set of vectors that is linearly dependent, but still satisfies certain properties, such as being a \"spanning set\" or a \"basis for a subspace\".",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8711636781314787
      },
      {
        "text": "The concept of a \"basis for a subspace with generalized dependent vectors\": A basis for a subspace with generalized dependent vectors is a set of vectors that spans the subspace and is linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9618215059022127
      },
      {
        "text": "Basis Theorem (with dependent vectors): A basis for a subspace with dependent vectors is a set of vectors that spans the subspace and is linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9227321455408961
      },
      {
        "text": "Basis for a Subspace with Generalized Dependent Vectors (Existence): A basis for a subspace with generalized dependent vectors exists if and only if the subspace is spanned by the set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8613093905167232
      },
      {
        "text": "Generalized Dependent Basis with Non-Standard Metric: A generalized dependent basis with non-standard metric is a set of vectors that is linearly dependent, but still satisfies certain properties, such as being a \"spanning set\" or a \"basis for a subspace\", when the vector space is equipped with a non-standard metric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8608542589175076
      },
      {
        "text": "Existence of a Generalized Dependent Basis: A generalized dependent basis is a set of vectors that is linearly dependent, but still satisfies certain properties, such as being a \"spanning set\" or a \"basis for a subspace\".",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9047359936416008
      },
      {
        "text": "Generalized Dependent Basis Theorem (with non-standard metric): A generalized dependent basis with non-standard metric is a set of vectors that is linearly dependent, but still satisfies certain properties, but still satisfies certain properties, but still satisfies certain properties, but still satisfies certain properties, but still satisfies certain properties, but still satisfies certain properties,",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8508099735888862
      }
    ]
  },
  {
    "representative_text": "Spanning Sets with Infinite Vectors: A set of vectors can span a vector space even if it contains an infinite number of vectors, as long as the vectors are linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Spanning Sets with Infinite Vectors: A set of vectors can span a vector space even if it contains an infinite number of vectors, as long as the vectors are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of a Vector Space is Infinite: The dimension of an infinite-dimensional vector space can be used to determine the number of linearly independent vectors required to span the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8272155139917963
      }
    ]
  },
  {
    "representative_text": "Gloss on Basis Theorem: The Basis Theorem states that every vector space has a basis, and every basis is a linearly independent set of vectors that spans the space. It's essential to note that the basis can be finite or infinite, and the theorem holds true for both cases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gloss on Basis Theorem: The Basis Theorem states that every vector space has a basis, and every basis is a linearly independent set of vectors that spans the space. It's essential to note that the basis can be finite or infinite, and the theorem holds true for both cases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis of a Vector Space with Infinite Dimension: For infinite-dimensional vector spaces, the concept of a basis is more nuanced. In such cases, a basis can be a countably infinite set of linearly independent vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Basis of a Vector Space with Infinite Dimension: For infinite-dimensional vector spaces, the concept of a basis is more nuanced. In such cases, a basis can be a countably infinite set of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis and Basis Extension in Infinite-Dimensional Spaces: In infinite-dimensional spaces, we can extend a basis to a larger basis by adding more vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.829320512087662
      },
      {
        "text": "Basis and Basis Reduction in Infinite-Dimensional Spaces: In infinite-dimensional spaces, we can reduce a basis to a smaller basis by removing vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8485724062918778
      },
      {
        "text": "Existence of Infinite Bases: Not every vector space has a finite basis. Some vector spaces may have infinite bases, which can be used to represent linear transformations and matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8333412155737692
      },
      {
        "text": "The concept of a minimal basis in infinite dimensional spaces**: A minimal basis for an infinite dimensional vector space is a basis that has the smallest possible number of elements. This concept is related to the concept of a free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.800220630039576
      },
      {
        "text": "The concept of a minimal basis in infinite dimensional spaces: A minimal basis for an infinite dimensional vector space is a basis that has the smallest possible number of elements. This concept is related to the concept of a free basis, but its implications in infinite dimensional spaces are not fully explored.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8285614791772404
      }
    ]
  },
  {
    "representative_text": "Relationship between Linear Independence and Spanning in Higher-Dimensional Spaces: In higher-dimensional spaces, the relationship between linear independence and spanning becomes more complex. A set of vectors can be linearly independent but not span the entire space, or vice versa.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Relationship between Linear Independence and Spanning in Higher-Dimensional Spaces: In higher-dimensional spaces, the relationship between linear independence and spanning becomes more complex. A set of vectors can be linearly independent but not span the entire space, or vice versa.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Relationship between Linear Independence and Spanning in Fractals: In fractal vector spaces, the relationship between linear independence and spanning becomes more complex due to the self-similar nature of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8196041357861671
      },
      {
        "text": "Relationship between Linear Independence and Spanning in Non-Standard Vector Spaces: The relationship between linear independence and spanning becomes more complex in non-standard vector spaces, where the usual properties of linear independence and spanning may not hold.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9234066237792112
      },
      {
        "text": "Relationship between Linear Independence and Spanning in Fractals: In fractal vector spaces, the relationship between linear independence and spanning becomes more complex due to the self-similar nature of the space. This includes the study of fractal bases and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8932559929997689
      },
      {
        "text": "Relationship between Linear Independence and Spanning in Higher-Dimensional Spaces - Special Cases: This includes the study of special cases where the relationship between linear independence and spanning becomes more complex, such as in spaces with non-standard dimensions or non-standard linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8504248308378264
      },
      {
        "text": "Relationship between Linear Independence and Spanning in Non-Standard Vector Spaces - Advanced Theorems: This includes the study of advanced theorems that relate linear independence and spanning in non-standard vector spaces, such as the study of linear independence and spanning in spaces with non-standard dimensions or non-standard linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8620298357060727
      },
      {
        "text": "Relationship between Linear Independence and Spanning in Non-Standard Vector Spaces - Topological Implications: This concept examines the implications of linear independence and spanning in non-standard vector spaces on their topological properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8537787570689483
      },
      {
        "text": "Relationship between Linear Independence and Spanning in Non-Standard Vector Spaces - Analytic Implications: This concept examines the implications of linear independence and spanning in non-standard vector spaces on their analytic properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8428376095916754
      },
      {
        "text": "Relationship between Linear Independence and Spanning in Non-Standard Vector Spaces - Category-Theoretic Implications: This concept explores the implications of linear independence and spanning in non-standard vector spaces on their category-theoretic properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8541397007500255
      }
    ]
  },
  {
    "representative_text": "Existence of a Basis in Non-Standard Vector Spaces: Some non-standard vector spaces, such as finite-dimensional vector spaces with a non-standard basis, may not have a basis in the classical sense. In such cases, alternative definitions and theorems must be used.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Existence of a Basis in Non-Standard Vector Spaces: Some non-standard vector spaces, such as finite-dimensional vector spaces with a non-standard basis, may not have a basis in the classical sense. In such cases, alternative definitions and theorems must be used.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of a Basis in Infinite-Dimensional Vector Spaces with Non-Standard Dimensions: In infinite-dimensional vector spaces with non-standard dimensions, the concept of a basis becomes more complex due to the non-standard nature of the dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8530181836941264
      },
      {
        "text": "Existence of a Basis in Vector Spaces with Non-Standard Linear Transformations: In vector spaces with non-standard linear transformations, the concept of a basis becomes more complex due to the non-standard nature of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8905544709756585
      },
      {
        "text": "Basis for a Vector Space with a Non-Standard Scalar Multiplication: In vector spaces with non-standard scalar multiplication, the concept of basis and dimension can be more complex.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8905875721571407
      },
      {
        "text": "Existence of a Basis in Infinite-Dimensional Vector Spaces with Non-Standard Dimensions: In infinite-dimensional vector spaces with non-standard dimensions, the concept of a basis becomes more complex due to the non-standard nature of the dimension. This includes the study of bases in non-standard vector spaces with non-standard dimensions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.920422014274747
      },
      {
        "text": "Existence of a Basis in Non-Standard Vector Spaces with Non-Standard Dimensions: This concept explores the existence of a basis in vector spaces with non-standard dimensions, such as infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8967845412867553
      },
      {
        "text": "Linear Independence of Infinite-Dimensional Vector Spaces with Non-Standard Bases: While the concept of a basis is well-established in finite-dimensional spaces, its extension to infinite-dimensional spaces requires careful consideration of non-standard bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9013100406366706
      },
      {
        "text": "Constructive methods for finding a basis in non-standard dimension spaces: The concept of basis is essential in understanding vector spaces. However, the existing points do not provide constructive methods for finding a basis in non-standard dimension spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8229732924440892
      },
      {
        "text": "Constructive Methods for Finding a Basis in Non-Standard Dimension Spaces: The concept of basis is essential in understanding vector spaces, and constructive methods for finding a basis in non-standard dimension spaces are crucial for understanding the properties of bases in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8646959733092655
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Linear Dependence: This is a complementary concept to linear independence, where a set of vectors is said to be linearly dependent if at least one vector can be expressed as a linear combination of the other vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Linear Independence and Linear Dependence: This is a complementary concept to linear independence, where a set of vectors is said to be linearly dependent if at least one vector can be expressed as a linear combination of the other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Concept of Linear Dependence: Linear dependence is often understood as a weaker concept than linear independence. A set of vectors is said to be linearly dependent if at least one vector in the set can be expressed as a linear combination of the other vectors in the set. This concept is closely related to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.843570008147474
      },
      {
        "text": "The concept of a \"Linearly Dependent System\": A linearly dependent system is a set of vectors that is linearly dependent, meaning that at least one vector can be expressed as a linear combination of the other vectors. It is a complementary concept to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8737212301051621
      },
      {
        "text": "Linear Dependence: The opposite of linear independence, where a set of vectors can be expressed as a linear combination of the other vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8850218690576208
      },
      {
        "text": "The concept of linearly dependent vectors: The opposite of linear independence, where a set of vectors can be expressed as a linear combination of the other vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9351614891688116
      },
      {
        "text": "The concept of a \"Linearly Dependent Set of Vectors\": A set of vectors is said to be linearly dependent if at least one vector can be expressed as a linear combination of the other vectors. This concept is complementary to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9378522550698949
      },
      {
        "text": "The concept of a \"Linearly Dependent System of Bases\": A set of bases is said to be linearly dependent if at least one basis can be expressed as a linear combination of the other bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8589136674612429
      },
      {
        "text": "The concept of a \"Linearly Dependent Basis\": A basis is said to be linearly dependent if at least one basis can be expressed as a linear combination of the other bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8673411816558259
      },
      {
        "text": "The concept of a \"Spanning Set of Linearly Dependent Vectors\": A set of linearly dependent vectors that spans a vector space is a set of vectors that can be expressed as a linear combination of the other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8814864962242118
      }
    ]
  },
  {
    "representative_text": "Basis Extension Theorem for Infinite-Dimensional Spaces: This theorem states that if a set of vectors is a basis for an infinite-dimensional vector space, then any set of vectors that spans the same space can be extended to a basis by adding vectors from the original basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Basis Extension Theorem for Infinite-Dimensional Spaces: This theorem states that if a set of vectors is a basis for an infinite-dimensional vector space, then any set of vectors that spans the same space can be extended to a basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span and Basis: The Extension Theorem for Infinite-Dimensional Spaces: This theorem states that if a set of vectors spans an infinite-dimensional vector space, then it can be extended to a basis by adding vectors from a linearly independent set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9397327799527514
      },
      {
        "text": "Basis Extension Theorem for Finite-Dimensional Spaces: This theorem states that if a set of vectors is a basis for a finite-dimensional vector space, then any set of vectors that spans the same space can be extended to a basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8909663384930773
      },
      {
        "text": "Basis Extension Theorem for Infinite-Dimensional Vector Spaces: This theorem states that if a set of vectors is a basis for an infinite-dimensional vector space, and we add a new vector to the set, then the new vector is in the span of the original set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9546853481036806
      },
      {
        "text": "Basis Extension Theorem for Finite-Dimensional Vector Spaces: This theorem states that if a set of vectors is a basis for a finite-dimensional vector space, and we add a new vector to the set, then the new vector is not in the span of the original set unless it is a linear combination of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9134002776278953
      },
      {
        "text": "Basis Extension Theorem for Infinite-Dimensional Spaces with Non-Standard Scalar Multiplication: This theorem states that if a set of vectors is a basis for an infinite-dimensional vector space with non-standard scalar multiplication, then any set of vectors that spans the same space can be extended to a basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8887083566866227
      },
      {
        "text": "Basis Extension Theorem for Infinite-Dimensional Vector Spaces: This theorem states that if a set of vectors is linearly independent, then any linear combination of these vectors can be extended to a linear combination of an infinite set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9285968315278734
      },
      {
        "text": "Basis Extension Theorem for Non-Standard Vector Spaces: This theorem states that if a set of vectors is a basis for a non-standard vector space, and we add a new vector to the set, then the new vector is in the span of the original set unless it is a linear combination of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9061133235615454
      },
      {
        "text": "Basis Extension Theorem for Infinite-Dimensional Vector Spaces with Non-Standard Dimensions: This theorem states that if a set of vectors is a basis for an infinite-dimensional vector space with non-standard dimensions, and we add a new vector to the set, then the new vector is in the span of the original set unless it is a linear combination of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9491090994999258
      },
      {
        "text": "Non-standard basis extension theorems for infinite-dimensional vector spaces: The existing Basis Extension Theorem states that adding a new vector to a basis results in a new basis unless the new vector is a basis Theorem for non-standard basis in non-standard basis in non-standard basis is a basis in non-standard basis of basis of basis in non-standard basis of basis of Basis of a.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8669262776847753
      }
    ]
  },
  {
    "representative_text": "Computing the Dimension of a Vector Space using Linear Independence: This involves finding the maximum number of linearly independent vectors in a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Computing the Dimension of a Vector Space using Linear Independence: This involves finding the maximum number of linearly independent vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computing the Dimension of a Vector Space using the Null Space: This involves finding the dimension of the null space of a linear transformation, which is related to the dimension of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8292446638822923
      },
      {
        "text": "Computing the Dimension of a Vector Space using the Linear Independence of a Basis with Respect to a Non-Standard Scalar Multiplication: This involves finding the maximum number of linearly independent vectors in a vector space with respect to a non-standard scalar multiplication to determine its dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.896977132645877
      },
      {
        "text": "Computing the Dimension of a Vector Space using the Null Space with Respect to a Non-Standard Scalar Multiplication: This involves finding the dimension of the null space of a linear transformation with respect to a non-standard scalar multiplication, which is related to the dimension of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8995613993681769
      },
      {
        "text": "Computing the Dimension of a Vector Space using the Null Space for Infinite-Dimensional Spaces: This involves finding the dimension of the null space of a linear transformation in an infinite-dimensional vector space, which is related to the dimension of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9203093360606455
      },
      {
        "text": "Computing the Dimension of a Vector Space using the Null Space with Respect to a Non-Standard Scalar Multiplication for Infinite-Dimensional Spaces: This involves finding the dimension of the null space of a linear transformation with respect to a non-standard scalar multiplication in an infinite-dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9210457480768306
      },
      {
        "text": "Computing the Dimension of a Vector Space using the Linear Independence of a Basis with Respect to a Non-Standard Vector Addition for Infinite-Dimensional Spaces: This involves finding the maximum number of linearly independent vectors in a vector space with respect to a non-standard vector addition to determine its dimension in an infinite-dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9003363789050123
      },
      {
        "text": "Computing the Dimension of a Vector Space using the Rank-Nullity Theorem: This involves finding the rank and nullity of a linear transformation and using the rank-nullity theorem to determine the dimension of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.895366558738959
      }
    ]
  },
  {
    "representative_text": "The concept of a \"Free Basis\": A free basis is a basis for a vector space that is not a basis for any proper subspace of the vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a \"Free Basis\": A free basis is a basis for a vector space that is not a basis for any proper subspace of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a \"Linearly Independent Set\": A linearly independent set is a set of vectors that is linearly independent, but not necessarily spanning a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "The concept of a \"Linearly Independent Set\": A linearly independent set is a set of vectors that is linearly independent, but not necessarily spanning a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"Linearly Independent System\": A linearly independent system is a set of vectors that is linearly independent, but not necessarily spanning a vector space. It is a fundamental concept in understanding the properties of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8761274367501044
      },
      {
        "text": "The concept of a \"Linearly Independent Basis\": A linearly independent basis is a basis for a vector space that is linearly independent, but not necessarily spanning the entire vector space. It is a fundamental concept in understanding the properties of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.896147001846222
      },
      {
        "text": "The concept of a \"Linearly Independent System of Bases\": A linearly independent system of bases is a set of bases for a vector space that is linearly independent, but not necessarily spanning the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9289639336766469
      },
      {
        "text": "The concept of a \"Linearly Independent Subset of a Vector Space\": A linearly independent subset of a vector space is a subset of vectors that is linearly independent, but not necessarily spanning the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8929524046119852
      },
      {
        "text": "The concept of a \"Linearly Independent Basis with Respect to a Non-Standard Scalar Multiplication\": A linearly independent basis with respect to a non-standard scalar multiplication is a basis for a vector space with non-standard scalar multiplication that is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8390765800859274
      },
      {
        "text": "The concept of a \"Linearly Independent System of Bases with Respect to a Non-Standard Scalar Multiplication\": A linearly independent system of bases with respect to a non-standard scalar multiplication is a set of bases for a vector space with non-standard scalar multiplication that is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8978253521787432
      },
      {
        "text": "The concept of a \"Linearly Independent Subset of a Vector Space with Respect to a Non-Standard Scalar Multiplication\": A linearly independent subset of a vector space with respect to a non-standard scalar multiplication is a subset of vectors that is linearly independent, but not necessarily spanning the entire vector space with respect to the non-standard scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9066404095666243
      },
      {
        "text": "The concept of a \"Linearly Independent System of Bases with Respect to a Non-Standard Vector Addition\": A linearly independent system of bases with respect to a non-standard vector addition is a set of bases for a vector space with non-standard vector addition that is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9150941229250299
      },
      {
        "text": "The concept of a \"Linearly Independent System of Bases Extension with Respect to a Non-Standard Scalar Multiplication\": A linearly independent system of bases extension with respect to a non-standard scalar multiplication is a set of linearly independent systems of bases for a vector space with non-standard scalar multiplication that can be extended to a larger set of linearly independent systems of bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.903400420563515
      },
      {
        "text": "The concept of a \"Linearly Independent System of Bases Extension with Respect to a Non-Standard Vector Addition\": A linearly independent system of bases extension with respect to a non-standard vector addition is a set of linearly independent systems of bases for a vector space with non-standard vector addition that can be extended to a larger set of linearly independent systems of bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.903986924716043
      },
      {
        "text": "The concept of a \"Linearly Independent Subset of a Vector Space with Respect to a Non-Standard Vector Addition\": A linearly independent subset of a vector space with respect to a non-standard vector addition is a subset of vectors that is linearly independent, but not necessarily spanning the entire vector space with respect to the non-standard vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9147322822437183
      },
      {
        "text": "The concept of a \"Linearly Independent Subset of a Vector Space with Respect to a Non-Standard Vector Addition\": A subset of vectors that is linearly independent, but not necessarily spanning the entire vector space with respect to the non-standard vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.891581054481638
      },
      {
        "text": "The concept of a \"Linearly Independent Subset of a Vector Space with Respect to a Non-Standard Scalar Multiplication\": A subset of vectors that is linearly independent, but not necessarily spanning the entire vector space with respect to the non-standard scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9188150877291146
      }
    ]
  },
  {
    "representative_text": "Advanced Theorems related to the \"Gambler's Ruin Problem\": This problem involves finding the maximum number of linearly independent vectors in a vector space, given certain constraints.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Advanced Theorems related to the \"Gambler's Ruin Problem\": This problem involves finding the maximum number of linearly independent vectors in a vector space, given certain constraints.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Advanced Theorems related to the \"Gambler's Ruin Problem\" and its Applications in Probability Theory and Statistics: This problem involves finding the maximum number of linearly independent vectors in a vector space, given certain constraints. It has applications in probability theory and statistics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9155660040648039
      },
      {
        "text": "Advanced Theorems related to the \"Gambler's Ruin Problem\" and its Applications in Combinatorics: This problem involves finding the maximum number of linearly independent vectors in a vector space, given certain constraints. It has applications in combinatorics and other fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9547599541861012
      },
      {
        "text": "Advanced Theorems related to the \"Gambler's Ruin Problem\" and its Applications in Combinatorics for Infinite-Dimensional Spaces: This problem involves finding the maximum number of linearly independent vectors in an infinite-dimensional vector space, given certain constraints. It has applications in combinatorics and other fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9271365927087614
      }
    ]
  },
  {
    "representative_text": "Dimension of a Free Basis: The dimension of a free basis of a vector space is equal to the number of vectors in the free basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Dimension of a Free Basis: The dimension of a free basis of a vector space is equal to the number of vectors in the free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of a Free Basis: The dimension of a free basis of a vector space is equal to the number of vectors in the basis, which may not be equal to the dimension of the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.898867648734688
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Spanning Theorem for Infinite-Dimensional Vector Spaces: If V is an infinite-dimensional vector space and S is a set of linearly independent vectors, then S spans V.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Spanning Theorem for Infinite-Dimensional Vector Spaces: If V is an infinite-dimensional vector space and S is a set of linearly independent vectors, then S spans V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Spanning Set Implies Linear Independence for Infinite-Dimensional Vector Spaces: If V is an infinite-dimensional vector space and S is a set of vectors that span V, then S is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.944728016125609
      },
      {
        "text": "Free Basis Implies Linear Independence Implies Spanning Set Implies Linear Independence Theorem for Infinite-Dimensional Vector Spaces: If V is an infinite-dimensional vector space and S is a set of linearly independent vectors, then S spans V if and only if S is a free basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9173431752238826
      },
      {
        "text": "Spanning Set Implies Linear Independence Theorem for Infinite-Dimensional Vector Spaces: If V is an infinite-dimensional vector space and S is a set of vectors that span V, then S is linearly independent if and only if S is a spanning set for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9697215217645706
      },
      {
        "text": "If V is an infinite-dimensional vector space and S is a set of linearly independent vectors, then S spans V if and only if S is a spanning set for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9292652922070512
      },
      {
        "text": "If V is an infinite-dimensional vector space and S is a set of vectors that span V, then S is linearly independent if and only if S is a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9385519329146226
      },
      {
        "text": "Linear Independence Implies Spanning Theorem for Infinite-Dimensional Vector Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8765307518528523
      },
      {
        "text": "Basis Implies Spanning Set Implies Linear Independence Implies Basis Implies Spanning Set Implies Basis Implies Linear Independence Theorem for Infinite-Dimensional Vector Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9023641224768999
      },
      {
        "text": "Linear Independence Implies Span in Infinite-Dimensional Vector Spaces: This theorem states that if a set of vectors is linearly independent in an infinite-dimensional vector space, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9123523235295764
      },
      {
        "text": "Basis Implies Spanning Set Implies Linear Independence Implies Spanning Set Implies Basis Implies Linear Independence Implies Spanning Set Implies Basis Implies Linear Independence Theorem for Infinite-Dimensional Vector Spaces: This theorem states that if V is a finite-dimensional vector space and S is a set of vectors that span V, then S is linearly independent if and only if S is a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9453068797186504
      },
      {
        "text": "Linear Independence Implies Span for Infinite-Dimensional Vector Spaces with Respect to be a Linear Independence: A theorem for Infinite-Dimensional Vector spaces: This theorem: A Linear Independence: This theorem: This theorem: This theorem for Infinite-Dimensional Span Implies Linear Independence: This theorem**: Theorem for Infinite-Dimensional vector",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9299522980404553
      },
      {
        "text": "Linear Independence Implies Span with Respect to a Given Basis in Infinite-Dimensional Vector Spaces: A set of vectors is linearly independent if and only if its span with respect to a given basis in infinite-dimensional vector spaces is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9481461474906783
      }
    ]
  },
  {
    "representative_text": "Span of a Free Basis: The span of a free basis of a vector space may not be equal to the entire vector space, but it is equal to the span of the entire vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Span of a Free Basis: The span of a free basis of a vector space may not be equal to the entire vector space, but it is equal to the span of the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of a Free Basis: The span of a free basis is a subspace of the original vector space, but it may not be the entire space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8737262791149536
      }
    ]
  },
  {
    "representative_text": "Dimension of the Null Space: The dimension of the null space of a free basis of a vector space is equal to the dimension of the entire vector space minus the dimension of the free basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension of the Null Space: The dimension of the null space of a free basis of a vector space is equal to the dimension of the entire vector space minus the dimension of the free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Relationship between Free Basis and Spanning Set: A free basis of a vector space is a subset of a spanning set of the same vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relationship between Free Basis and Spanning Set: A free basis of a vector space is a subset of a spanning set of the same vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dimension of a Spanning Set: The dimension of a spanning set of a vector space is equal to the number of vectors in the set, which may not be equal to the dimension of the entire vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Dimension of a Spanning Set: The dimension of a spanning set of a vector space is equal to the number of vectors in the set, which may not be equal to the dimension of the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of a Spanning Set: The dimension of a spanning set for a vector space is equal to the number of vectors in the spanning set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9418784667927911
      },
      {
        "text": "The dimension of a spanning set for a vector space is equal to the number of vectors in the spanning set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9364950896775592
      },
      {
        "text": "Dimension of a Spanning Set:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8736180267415187
      },
      {
        "text": "Dimension of a Spanning Set: The dimension of a spanning set is equal to the number of linearly independent vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.941590318829554
      },
      {
        "text": "The Dimension of a Spanning Set: This theorem states that the dimension of a spanning set for a vector space is equal to the number of vectors in the spanning set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9130044452254313
      }
    ]
  },
  {
    "representative_text": "Linear Independence of a Matrix: A matrix is linearly independent if and only if its null space has dimension zero, which is related to the concept of span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 15,
    "detailed_sources": [
      {
        "text": "Linear Independence of a Matrix: A matrix is linearly independent if and only if its null space has dimension zero, which is related to the concept of span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and Null Space: The relationship between linear independence and the null space of a matrix. Specifically, a set of vectors is linearly independent if and only if the null space of the corresponding matrix is trivial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.86544562502895
      },
      {
        "text": "Linear Independence and Projective Spaces: The concept of projective spaces and its relationship with linear independence. Specifically, a set of vectors is linearly independent if and only if the corresponding points in a projective space are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8002402249340427
      },
      {
        "text": "Linear Independence and the Intersection of Subspaces: The concept of the intersection of subspaces and its relationship with linear independence. Specifically, a set of vectors is linearly independent if and only if the intersection of its span and a subspace is the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.84405605445037
      },
      {
        "text": "The Relationship Between Linear Independence and the Null Space: The null space of a matrix is closely related to linear independence. If the columns of a matrix are linearly independent, the null space of the matrix is trivial (contains only the zero vector).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8972724935057106
      },
      {
        "text": "The Connection between Linear Independence and the Null Space of a Linear Transformation: The null space of a linear transformation is closely related to linear independence, as a set of vectors is linearly independent if and only if the null space of the corresponding matrix has a trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9218134540252438
      },
      {
        "text": "Linear Independence and Dimension of the Null Space: The dimension of the null space of a matrix is related to linear independence. Specifically, the dimension of the null space is equal to the number of free variables in the system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8877508104638019
      },
      {
        "text": "Linear Independence of a Matrix: A matrix is linearly independent if and only if its null space has dimension zero. Understanding the relationship between linear independence and the null space is crucial in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9183886899362618
      },
      {
        "text": "The Null Space and Linear Dependence: The null space of a matrix is closely related to linear dependence. If the columns of a matrix are linearly dependent, the null space of the matrix is non-trivial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8794531888004834
      },
      {
        "text": "The Column Space and Linear Independence: The column space of a matrix is closely related to linear independence. If the columns of a matrix are linearly independent, the column space of the matrix is the entire codomain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8611799426197702
      },
      {
        "text": "The relationship between linear independence and the null space of a matrix: This relationship is mentioned earlier, but it's essential to understand how the null space of a matrix is related to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9121824443440554
      },
      {
        "text": "Relationship between Linear Independence and the Kernel of a Matrix: If a matrix has a non-zero kernel, then its row space and column space are linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8619811473201746
      },
      {
        "text": "Linear Independence and the Null Space: Exploring the relationship between linear independence and the null space of a linear transformation, including the concept of a null space basis and the dimension of the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9093855758406258
      },
      {
        "text": "The Relationship Between Linear Independence and the Existence of a Non-Trivial Null Space: The existence of a non-trivial null space is closely related to linear independence. If the columns of a matrix are linearly independent, the null space of the matrix is trivial (contains only the zero vector). Conversely, if the columns of a matrix are linearly dependent, the null space of the matrix is non-trivial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9005400667298156
      },
      {
        "text": "The Relationship Between Linear Independence and the Existence of a Non-Trivial Column Space: The existence of a non-trivial column space is closely related to linear independence. If the columns of a matrix are linearly independent, the column space of the matrix is the entire codomain. Conversely, if the columns of a matrix are linearly dependent, the column space of the matrix is a proper subspace of the codomain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8441087993227239
      }
    ]
  },
  {
    "representative_text": "Relationship between Linear Independence and Span: A set of vectors is linearly independent if and only if the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has only the trivial solution, which is related to the concept of span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Relationship between Linear Independence and Span: A set of vectors is linearly independent if and only if the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has only the trivial solution, which is related to the concept of span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and Null Space: A set of vectors is linearly independent if and only if the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has only the trivial solution, which is related to the concept of null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9030698715230444
      },
      {
        "text": "Linear Independence of a Set of Functions: The concept of linear independence can be extended to sets of functions, where the functions are linearly independent if and only if the equation $a1f1(x) + a2f2(x) + \\cdots + anfn(x) = 0$ has only the trivial solution for all x in the domain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8077446906656254
      },
      {
        "text": "Linear Independence of a Set of Matrices: The concept of linear independence can be extended to sets of matrices, where the matrices are linearly independent if and only if the equation $a1M1 + a2M2 + \\cdots + anMn = 0$ has only the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8767990880749117
      }
    ]
  },
  {
    "representative_text": "Lagrange's Basis Theorem: This theorem states that if a vector space V has a basis with n elements, then the dimension of V is equal to n. This theorem can be used to show that the dimension of a vector space is equal to the number of vectors in a basis.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Lagrange's Basis Theorem: This theorem states that if a vector space V has a basis with n elements, then the dimension of V is equal to n. This theorem can be used to show that the dimension of a vector space is equal to the number of vectors in a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Lagrange's Basis Theorem Variations for Non-Square Matrices and Infinite Dimensional Spaces**: Lagrange's basis theorem states that every vector space has a basis, which is a linearly independent set that spans the entire space. This theorem can be used to show that the dimension of a vector space is equal to the number of vectors in a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.911342628116939
      },
      {
        "text": "The application of Lagrange's basis theorem variations in infinite dimensional spaces: Lagrange's basis theorem states that every vector space has a basis, which is a linearly independent set that spans the entire space. This theorem can be used to show that the dimension of a vector space is equal to the number of vectors in a basis for the space. Variations of this theorem can be applied to infinite dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.917292830323324
      }
    ]
  },
  {
    "representative_text": "The concept of linear independence modulo r****: This concept is related to the concept of linear independence. It deals with the linear independence of a set of vectors modulo a scalar r.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of linear independence modulo r****: This concept is related to the concept of linear independence. It deals with the linear independence of a set of vectors modulo a scalar r.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Hausdorff dimension** : This concept is related to the concept of linear independence. It deals with the dimension of a set in terms of the Hausdorff measure.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Hausdorff dimension** : This concept is related to the concept of linear independence. It deals with the dimension of a set in terms of the Hausdorff measure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Hausdorff Dimension: The Hausdorff dimension is a measure of the complexity of a set in a vector space. It can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8987057790447134
      },
      {
        "text": "The Interplay between Linear Independence and the Hausdorff Dimension in Infinite Dimensional Spaces**: The Hausdorff dimension is a measure of the size of a set. This concept has implications for the linear independence of vectors in an infinite dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8935660368283453
      },
      {
        "text": "The Relationship between Linear Independence and the Hausdorff Dimension in Infinite Dimensional Spaces: The Hausdorff dimension is a measure of the size of a set. This concept has implications for the concept has implications for the concept in infinite dimensional spaces. This theorem states that this concept has implications for linear independence in infinite-dimensional spaces: Theore the study of linear independence of linear independence of the Hausdifferential geometry: Theorem: Theorem for Linear Independence of Linear Independence of a basis can be expressed in infinite dimensional spaces**: Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8244736975426379
      }
    ]
  },
  {
    "representative_text": "The Lipschitz constant** : This concept is related to the concept of linear independence. It deals with the relationship between the Lipschitz norm and the linear independence of a set of vectors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Lipschitz constant** : This concept is related to the concept of linear independence. It deals with the relationship between the Lipschitz norm and the linear independence of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Lipschitz Constant and Linear Independence in Banach Spaces**: The Lipschitz constant is a measure of the smoothness of a function. This concept has implications for the linear independence of vectors in a Banach space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8844918047727248
      }
    ]
  },
  {
    "representative_text": "Cyclic Vectors and Cyclic Subspaces: A vector is cyclic if it can be expressed as a linear combination of other vectors in the space. A subspace is cyclic if it contains a cyclic vector. Understanding cyclic vectors and subspaces can help in understanding the structure of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Cyclic Vectors and Cyclic Subspaces: A vector is cyclic if it can be expressed as a linear combination of other vectors in the space. A subspace is cyclic if it contains a cyclic vector. Understanding cyclic vectors and subspaces can help in understanding the structure of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of 'Cyclic Vectors' and 'Cyclic Subspaces' in Infinite-Dimensional Vector Spaces: Understanding cyclic vectors and subspaces can help in understanding the structure of linear transformations in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8177316305603397
      },
      {
        "text": "Cyclic Subspaces and Linear Transformations: This could involve discussing the properties of cyclic subspaces generated by a single vector under the action of a linear transformation, including their dimension and relationship with the kernel and image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8248340307771185
      },
      {
        "text": "Cyclic Vectors and Span: A cyclic vector is a vector that can be expressed as a linear combination of other vectors in a set. Understanding the relationship between cyclic vectors and the span of a set is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8188933548152998
      }
    ]
  },
  {
    "representative_text": "Linear Independence of Subspaces: A subspace is linearly independent if none of its vectors can be expressed as a linear combination of other vectors in the subspace. This is closely related to the concept of linear independence of the original set of vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence of Subspaces: A subspace is linearly independent if none of its vectors can be expressed as a linear combination of other vectors in the subspace. This is closely related to the concept of linear independence of the original set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Subspace Span Theorem: Given a subspace $W$ and a set of vectors $\\{v1, v2, \\ldots, vn\\}$, the span of this set is equal to $W$ if and only if the set is linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Subspace Span Theorem: Given a subspace $W$ and a set of vectors $\\{v1, v2, \\ldots, vn\\}$, the span of this set is equal to $W$ if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Values and Singular Value Decomposition (SVD): Singular values of a matrix $A$ are non-negative real numbers that can be used to determine the dimension of the null space of $A$.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Singular Values and Singular Value Decomposition (SVD): Singular values of a matrix $A$ are non-negative real numbers that can be used to determine the dimension of the null space of $A$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Values and Linear Independence for Infinite-Dimensional Vector Spaces: Singular values are non-negative real numbers that can be used to determine the dimension of the null space of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.885393867642247
      }
    ]
  },
  {
    "representative_text": "Linear Dependence of a Vector Space: This concept involves determining whether a vector space is linearly dependent. If a vector space is linearly dependent, it implies that at least one vector in the space can be expressed as a linear combination of other vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Dependence of a Vector Space: This concept involves determining whether a vector space is linearly dependent. If a vector space is linearly dependent, it implies that at least one vector in the space can be expressed as a linear combination of other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Range: The range of a matrix is the set of all vectors that can be obtained by multiplying the matrix by any vector. This concept is related to linear dependence and independence.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Range: The range of a matrix is the set of all vectors that can be obtained by multiplying the matrix by any vector. This concept is related to linear dependence and independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Bézout's Identity: This identity states that for any two polynomials f(x) and g(x) with integer coefficients, there exist polynomials p(x) and q(x) with integer coefficients such that f(x) = p(x)g(x).",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Bézout's Identity: This identity states that for any two polynomials f(x) and g(x) with integer coefficients, there exist polynomials p(x) and q(x) with integer coefficients such that f(x) = p(x)g(x).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Linear Algebra: This theorem states that every linear transformation can be represented as a matrix multiplication.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Linear Algebra: This theorem states that every linear transformation can be represented as a matrix multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Fundamental Theorem of Linear Transformations: This theorem states that every linear transformation between two vector spaces can be represented by a matrix, and every matrix can be represented by a linear transformation. This theorem is a fundamental result in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9198108718325781
      }
    ]
  },
  {
    "representative_text": "The Concept of Linear Independence of a Set of Matrices: This concept involves determining whether a set of matrices is linearly independent. If a set of matrices is linearly independent, it implies that no matrix in the set can be expressed as a linear combination of other matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Concept of Linear Independence of a Set of Matrices: This concept involves determining whether a set of matrices is linearly independent. If a set of matrices is linearly independent, it implies that no matrix in the set can be expressed as a linear combination of other matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Concept of Linear Dependence of a Set of Matrices: This concept involves determining whether a set of matrices is linearly dependent. If a set of matrices is linearly dependent, it implies that at least one matrix in the set can be expressed as a linear combination of other matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9533426021265491
      },
      {
        "text": "The Linear Dependence of the Columns of a Matrix: This concept involves determining whether the columns of a matrix are linearly dependent. If the columns are linearly dependent, the matrix is not invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8312084039217501
      },
      {
        "text": "Linear Independence of Matrices: Linear independence of matrices is a concept that describes the properties of a set of matrices in terms of their linear independence. It is related to the concept of basis and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8508849518401952
      }
    ]
  },
  {
    "representative_text": "Stable Matrices and Eigenvalues: In the context of computer graphics and game development, understanding stable matrices and eigenvalues is crucial for simulating realistic physics and collisions.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Stable Matrices and Eigenvalues: In the context of computer graphics and game development, understanding stable matrices and eigenvalues is crucial for simulating realistic physics and collisions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Normal Vectors and Normals: In computer graphics, normal vectors and normals are used to represent the direction of a surface or a plane, which is essential for tasks such as lighting and shading.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Normal Vectors and Normals: In computer graphics, normal vectors and normals are used to represent the direction of a surface or a plane, which is essential for tasks such as lighting and shading.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Barycentric Coordinates: Barycentric coordinates are a way of representing a point in 3D space in terms of the weights of its vertices, which is useful in computer graphics for tasks such as texture mapping and mesh deformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Barycentric Coordinates: Barycentric coordinates are a way of representing a point in 3D space in terms of the weights of its vertices, which is useful in computer graphics for tasks such as texture mapping and mesh deformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Projection Matrices: Projection matrices are used in computer graphics to project 3D points onto 2D planes or screens, which is essential for tasks such as rendering and animation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Projection Matrices: Projection matrices are used in computer graphics to project 3D points onto 2D planes or screens, which is essential for tasks such as rendering and animation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Projections and Transformations: Projections and transformations are used in computer graphics to project 3D models onto 2D planes or screens. Linear algebra is used to implement these transformations efficiently.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8239834850851642
      }
    ]
  },
  {
    "representative_text": "Linear Regression and Curve Fitting: In computer graphics and game development, linear regression and curve fitting are used to model the motion of objects and simulate realistic physics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Regression and Curve Fitting: In computer graphics and game development, linear regression and curve fitting are used to model the motion of objects and simulate realistic physics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Optimization Techniques: Optimization techniques such as gradient descent and least squares are used in computer graphics and game development to optimize tasks such as lighting, shading, and animation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Optimization Techniques: Optimization techniques such as gradient descent and least squares are used in computer graphics and game development to optimize tasks such as lighting, shading, and animation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Optimization Techniques for Graphics Rendering: Linear algebra is used in optimization techniques such as gradient descent and least squares to optimize tasks such as lighting, shading, and animation in computer graphics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8677076640025436
      },
      {
        "text": "Optimization Techniques for Graphics Rendering: Advanced optimization techniques, such as the use of linear algebra to optimize tasks such as lighting, shading, and animation, are essential for real-time graphics rendering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.882738584877239
      },
      {
        "text": "Optimization Techniques for Machine Learning: Optimization techniques for machine learning are used in computer graphics and game development to optimize tasks such as lighting, shading, and animation. Linear algebra is used to optimize tasks such as linear regression and curve fitting.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9064498400327521
      },
      {
        "text": "Optimization Techniques for Linear Programming: Linear programming is a type of optimization problem that involves finding the maximum or minimum of a linear function. This is useful in computer graphics for tasks such as lighting, shading, and animation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8692562621016454
      },
      {
        "text": "Optimization Techniques for Computer Vision: Optimization techniques, such as the method of gradient descent, are used to solve problems in computer vision in computer graphics and game development. These techniques involve finding the minimum or maximum of a function subject to certain constraints.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8630873891081656
      }
    ]
  },
  {
    "representative_text": "Physics-Based Modeling: Physics-based modeling is a technique used in computer graphics and game development to simulate realistic physics and collisions, which is achieved by using linear algebra to model the motion of objects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Physics-Based Modeling: Physics-based modeling is a technique used in computer graphics and game development to simulate realistic physics and collisions, which is achieved by using linear algebra to model the motion of objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Collision Response and Physics Engines: Collision response and physics engines are used in computer graphics and game development to simulate realistic collisions and physics, which is achieved by using linear algebra to model the motion of objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.829112742259972
      },
      {
        "text": "Physics-Based Modeling and Simulation: Physics-based modeling uses linear algebra to simulate the motion of objects in 3D space, taking into account factors such as gravity, friction, and elasticity. This technique is crucial in computer graphics for tasks such as animation and simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8807526126607753
      },
      {
        "text": "Linear Algebraic Techniques for Physics-Based Modeling: Linear algebraic techniques, such as the use of eigenvectors and singular value decomposition (SVD), are used in physics-based modeling to simulate realistic physics and collisions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8431696364505329
      },
      {
        "text": "Optimization of Physics-Based Simulations: Linear algebra is used to optimize physics-based simulations, such as fluid dynamics or rigid body dynamics, to reduce computational costs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.817506505571669
      }
    ]
  },
  {
    "representative_text": "Quaternions and Rotation: Quaternions are used in computer graphics and game development to represent 3D rotations, which is essential for tasks such as animation and simulation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quaternions and Rotation: Quaternions are used in computer graphics and game development to represent 3D rotations, which is essential for tasks such as animation and simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Light Transport and Global Illumination: Light transport and global illumination are used in computer graphics and game development to simulate realistic lighting and shading, which is achieved by using linear algebra to model the way light interacts with objects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Light Transport and Global Illumination: Light transport and global illumination are used in computer graphics and game development to simulate realistic lighting and shading, which is achieved by using linear algebra to model the way light interacts with objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Lighting Models and Radiosity: Linear algebra is used in lighting models to calculate the way light interacts with objects in 3D space. Radiosity is a technique used to simulate the way light scatters and absorbs in complex scenes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8421421224744027
      },
      {
        "text": "Light Transport and Global Illumination: Linear algebra is used in light transport and global illumination to simulate the way light interacts with objects in 3D space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9537047823169851
      }
    ]
  },
  {
    "representative_text": "Data Structures and Algorithms: Data structures such as graphs and trees are used in computer graphics and game development to efficiently store and manipulate large amounts of data, which is achieved by using linear algebra to optimize tasks such as collision detection and physics simulations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Data Structures and Algorithms: Data structures such as graphs and trees are used in computer graphics and game development to efficiently store and manipulate large amounts of data, which is achieved by using linear algebra to optimize tasks such as collision detection and physics simulations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Machine Learning and Neural Networks: Machine learning and neural networks are used in computer graphics and game development to simulate realistic physics and collisions, which is achieved by using linear algebra to optimize tasks such as lighting and shading.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8349799877236266
      },
      {
        "text": "Machine Learning and Deep Learning for Graphics Rendering: Machine learning and deep learning are used in computer graphics to simulate realistic physics and collisions, taking into account complex patterns and relationships in the data. Linear algebra is used to optimize tasks such as lighting and shading in these models.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8779960277763947
      },
      {
        "text": "Graph Theory and Network Analysis: Graph theory and network analysis are used in computer graphics and game development to model complex systems and networks. Linear algebra is used to perform tasks such as graph traversal, shortest path finding, and network optimization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8586201578722716
      },
      {
        "text": "Computational Biology: Computational biology is a field that uses linear algebra to analyze and model biological systems. In computer graphics and game development, linear algebra is used to perform tasks such as protein structure prediction and molecular dynamics simulations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8111123726647497
      },
      {
        "text": "Machine Learning and Deep Learning: Machine learning and deep learning are used in computer graphics and game development to simulate realistic physics and collisions. Linear algebra is used to optimize tasks such as lighting, shading, and animation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9500481292517842
      },
      {
        "text": "Machine Learning and Deep Learning for Robotics: Machine learning and deep learning are used in computer graphics and game development to simulate realistic physics and collisions in robotics. Linear algebra is used to optimize tasks such as object recognition and image registration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8995402363613805
      },
      {
        "text": "Machine Learning and Deep Learning for Computer Vision: Machine learning and deep learning are used in computer graphics and game development to simulate realistic physics and collisions. Linear algebra is used to optimize tasks such as object recognition and image registration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9450096372832728
      },
      {
        "text": "Machine Learning and Deep Learning for Physics-Based Modeling: Machine learning and deep learning are used in computer graphics and game development to simulate realistic physics and collisions. Linear algebra is used to optimize tasks such as physics-based modeling and simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9392606696908378
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) of Non-Orthogonal Matrices: An extension of SVD to non-orthogonal matrices, which is useful for image and signal processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) of Non-Orthogonal Matrices: An extension of SVD to non-orthogonal matrices, which is useful for image and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Decomposition (SVD) of Non-Orthogonal Matrices: An extension of SVD to non-orthogonal matrices, which is useful for image and signal processing. SVD can be used to decompose non-orthogonal matrices into their singular values and singular vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9371723473838809
      },
      {
        "text": "Singular Value Decomposition (SVD) of Non-Orthogonal Matrices: This technique is used to decompose non-orthogonal matrices into their singular values and singular vectors, which is widely used in machine learning and data science for tasks such as image and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9117104434691421
      },
      {
        "text": "The Singular Value Decomposition (SVD) of a Matrix with Non-Orthogonal Vectors: This is an extension of SVD to matrices with non-orthogonal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8943239186079042
      },
      {
        "text": "Singular Value Decomposition (SVD) of Non-Orthogonal Matrices: Application to Image Processing: Techniques that use SVD to decompose non-orthogonal matrices and apply them to image processing tasks such as image denoising and deblurring.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9157312123792296
      },
      {
        "text": "Singular Value Decomposition (SVD) of Non-Orthogonal Matrices: Application to Natural Language Processing: Techniques that use SVD to decompose non-orthogonal matrices and apply them to natural language processing tasks such as text classification and sentiment analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8788626086299343
      }
    ]
  },
  {
    "representative_text": "Kernel Methods: A class of machine learning algorithms that use kernel functions to map data into a higher-dimensional space, where linear algebra techniques can be applied.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Kernel Methods: A class of machine learning algorithms that use kernel functions to map data into a higher-dimensional space, where linear algebra techniques can be applied.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Kernel Methods: Using linear algebra techniques to construct kernel functions that can be used for classification and regression tasks in machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8851123939515186
      }
    ]
  },
  {
    "representative_text": "Linear Regression with RBF Kernels: A type of kernel method that uses radial basis function (RBF) kernels to perform linear regression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Regression with RBF Kernels: A type of kernel method that uses radial basis function (RBF) kernels to perform linear regression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Kernel Methods: Linear Regression with RBF Kernels: A type of kernel method that uses radial basis function (RBF) kernels to perform linear regression. Kernel methods can be used to perform non-linear classification and regression tasks by mapping the data into a higher-dimensional space where linear algebra techniques can be applied.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9105410403847518
      }
    ]
  },
  {
    "representative_text": "Tangent Space: A technique used in machine learning to represent a dataset as a tangent space, which is a linear subspace of the original space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tangent Space: A technique used in machine learning to represent a dataset as a tangent space, which is a linear subspace of the original space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Stochastic Gradient Descent (SGD): An optimization algorithm used in machine learning to minimize the loss function, which involves linear algebra operations such as matrix multiplication and vector addition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Stochastic Gradient Descent (SGD): An optimization algorithm used in machine learning to minimize the loss function, which involves linear algebra operations such as matrix multiplication and vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Stochastic Gradient Descent (SGD) Optimization Algorithm: An optimization algorithm used in machine learning to minimize the loss function, which involves linear algebra operations such as matrix multiplication and vector addition. SGD can be used to optimize the parameters of neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9563244698627456
      }
    ]
  },
  {
    "representative_text": "Batch Normalization: A technique used in deep learning to normalize the inputs to each layer, which involves linear algebra operations such as matrix multiplication and vector addition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Batch Normalization: A technique used in deep learning to normalize the inputs to each layer, which involves linear algebra operations such as matrix multiplication and vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Batch Normalization Technique: A technique used in deep learning to normalize the inputs to each layer, which involves linear algebra operations such as matrix multiplication and vector addition. Batch normalization can be used to improve the stability and performance of neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9569482964089517
      },
      {
        "text": "Batch Normalization Techniques: Methods for normalizing the inputs to each layer in a neural network to improve stability and performance.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8124055721425751
      },
      {
        "text": "Linear Algebra for Deep Learning: Batch Normalization Techniques: Methods for normalizing the inputs to each layer in a neural network to improve stability and performance.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8811754993527252
      }
    ]
  },
  {
    "representative_text": "Activation Functions: Non-linear functions used in neural networks to introduce non-linearity into the model, which can be represented using linear algebra techniques such as matrix multiplication and vector addition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Activation Functions: Non-linear functions used in neural networks to introduce non-linearity into the model, which can be represented using linear algebra techniques such as matrix multiplication and vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Activation Functions: Non-linear functions used in neural networks to introduce non-linearity into the model, which can be represented using linear algebra techniques such as matrix multiplication and vector addition. Activation functions can be used to introduce non-linearity into the model and improve its performance.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9359562663983805
      }
    ]
  },
  {
    "representative_text": "Convolutional Neural Networks (CNNs) with ReLU Activation: A type of CNN that uses the rectified linear unit (ReLU) activation function to introduce non-linearity into the model.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Convolutional Neural Networks (CNNs) with ReLU Activation: A type of CNN that uses the rectified linear unit (ReLU) activation function to introduce non-linearity into the model.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Convolutional Neural Networks (CNNs) with ReLU Activation: A type of CNN that uses the rectified linear unit (ReLU) activation function to introduce non-linearity into the model. CNNs can be used to perform image classification and object detection tasks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9559291853820651
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Time Series Analysis: Techniques such as spectral analysis and wavelet analysis, which involve linear algebra operations such as matrix multiplication and vector addition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Time Series Analysis: Techniques such as spectral analysis and wavelet analysis, which involve linear algebra operations such as matrix multiplication and vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Time Series Analysis: Spectral Analysis and Wavelet Analysis: Techniques such as spectral analysis and wavelet analysis, which involve linear algebra operations such as matrix multiplication and vector addition, are used in time series analysis to decompose time series data into its component frequencies or time-scale components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9506337985517188
      },
      {
        "text": "Linear Algebra for Time Series Analysis: This area of research involves using linear algebra techniques such as spectral analysis and wavelet analysis to analyze and model time series data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8821456695895434
      },
      {
        "text": "Linear Algebra for Time Series Analysis: Spectral Analysis of Non-Stationary Time Series: This area of research involves using linear algebra techniques such as eigendecomposition and singular value decomposition to analyze and model non-stationary time series data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8539444321812366
      },
      {
        "text": "Linear Algebra for Data Analysis with Non-Stationary Signals: Linear algebra is used in data analysis tasks with non-stationary signals such as time-series analysis, wavelet analysis, and spectral analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8556102647180778
      },
      {
        "text": "Linear Algebra for Time Series Analysis: Spectral Power Density Estimation: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to estimate the spectral power density of time series data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8638567356256812
      }
    ]
  },
  {
    "representative_text": "Manifold Learning: A technique used in machine learning to represent high-dimensional data as a lower-dimensional manifold, which involves linear algebra operations such as matrix multiplication and vector addition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Manifold Learning: A technique used in machine learning to represent high-dimensional data as a lower-dimensional manifold, which involves linear algebra operations such as matrix multiplication and vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Geometric Methods for Image Processing: Techniques such as image filtering and image segmentation, which involve linear algebra operations such as matrix multiplication and vector addition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 21,
    "detailed_sources": [
      {
        "text": "Geometric Methods for Image Processing: Techniques such as image filtering and image segmentation, which involve linear algebra operations such as matrix multiplication and vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Image Segmentation: Using linear algebra techniques to segment images into regions with similar characteristics, which is a crucial step in image analysis and computer vision.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8043913143900372
      },
      {
        "text": "Geometric Methods for Image Processing: Image Filtering and Image Segmentation: Techniques such as image filtering and image segmentation, which involve linear algebra operations such as matrix multiplication and vector addition, are used in image processing to analyze and manipulate images.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8997113516882705
      },
      {
        "text": "Linear Algebra for Computer Vision: Applying linear algebra techniques to computer vision tasks, such as image segmentation, object recognition, and tracking, is essential in modern computer vision systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8223935266267117
      },
      {
        "text": "Linear Algebraic Techniques for Computer Vision: This includes techniques such as the use of eigenvectors to perform object recognition, and the use of singular value decomposition (SVD) to perform image compression and denoising.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8453859207458395
      },
      {
        "text": "Linear Algebraic Techniques for Image Processing: This includes techniques such as the use of linear algebra to perform image filtering, and to perform image registration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9052689983811355
      },
      {
        "text": "Linear Algebra for Computer Vision: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize computer vision algorithms for tasks such as image recognition and object detection.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8502552956225269
      },
      {
        "text": "Geometric Methods for Image Analysis: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize image analysis algorithms for tasks such as image recognition and object detection.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9081147120557072
      },
      {
        "text": "Linear Algebra for Computer Vision with Non-Linear Transformations: Applying linear algebra techniques to computer vision tasks with non-linear transformations, such as the Radon transform and the Fourier transform, is essential in modern computer vision systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8223054368785567
      },
      {
        "text": "Computer Vision and Image Processing: Linear algebraic techniques, such as the use of eigenvectors and singular value decomposition (SVD), are used in computer vision and image processing to perform tasks such as object recognition and image registration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9090976002319311
      },
      {
        "text": "Linear Algebra for Computer Vision: Affine Transformations and Homography Estimation: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize computer vision algorithms for tasks such as affine transformations and homography estimation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8656785424875463
      },
      {
        "text": "Linear Algebra for Image Processing: Image Denoising and Deblurring: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize image processing algorithms for tasks such as image denoising and deblurring.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8961128367977482
      },
      {
        "text": "Linear Algebra for Computer Vision: Linear algebra is used in computer vision tasks such as image segmentation, object recognition, and tracking.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8997478981897962
      },
      {
        "text": "Linear Algebra for Image Analysis: Linear algebra is used in image analysis tasks such as image filtering, image segmentation, and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.90489360099387
      },
      {
        "text": "Linear Algebra for Computer Vision with Non-Stationary Signals: Linear algebra is used in computer vision tasks with non-stationary signals such as image segmentation, object recognition, and tracking.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8835740221372682
      },
      {
        "text": "Linear Algebra for Computer Vision: Image Segmentation: Techniques for segmenting images, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9135053743206076
      },
      {
        "text": "Image Processing with Linear Algebra: A class of algorithms for processing and analyzing images using linear algebra and image processing techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8702155659377794
      },
      {
        "text": "Linear Algebra for Computer Vision: Image Denoising: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to denoise images.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8887675254075428
      },
      {
        "text": "Linear Algebra for Computer Vision: Image Segmentation with Eigenvalue Decomposition and Singular Value Decomposition: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to segment images.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8843221411830351
      },
      {
        "text": "Computer Vision: Linear algebra is used in computer vision for tasks such as image processing and object recognition.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8855825265607049
      },
      {
        "text": "Computer Vision with Image Processing: Computer vision is a field of study that deals with the use of algorithms to analyze and interpret visual data. Image processing is a technique for processing and analyzing images. Linear algebra is used in computer vision for tasks such as image filtering, segmentation, and object recognition.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8882524058658315
      }
    ]
  },
  {
    "representative_text": "Fast Fourier Transform (FFT): An efficient algorithm for computing the discrete Fourier transform, which is widely used in signal processing and image analysis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Fast Fourier Transform (FFT): An efficient algorithm for computing the discrete Fourier transform, which is widely used in signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Wavelet Analysis: A mathematical technique for analyzing functions or signals using a set of basis functions called wavelets, which are similar to Fourier transforms but can capture more local features.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Wavelet Analysis: A mathematical technique for analyzing functions or signals using a set of basis functions called wavelets, which are similar to Fourier transforms but can capture more local features.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Decomposition: A factorization technique that decomposes a matrix into three matrices: U, Σ, and V, which are similar to SVD but with a different decomposition method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Decomposition: A factorization technique that decomposes a matrix into three matrices: U, Σ, and V, which are similar to SVD but with a different decomposition method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) Variants: SVD can be used to solve systems of linear equations, and variants such as QR decomposition and eigendecomposition can be used to solve certain types of problems.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) Variants: SVD can be used to solve systems of linear equations, and variants such as QR decomposition and eigendecomposition can be used to solve certain types of problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "SVD and Eigenvalue Decomposition: SVD and eigenvalue decomposition are related concepts that describe the properties of a matrix in terms of its singular values and eigenvalues. They are useful in solving certain types of linear equations and systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8567328627425945
      }
    ]
  },
  {
    "representative_text": "Fourier Transform Properties: Understanding properties such as the periodicity, linearity, and shift-invariance of the Fourier transform, which are essential for signal processing and image analysis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Fourier Transform Properties: Understanding properties such as the periodicity, linearity, and shift-invariance of the Fourier transform, which are essential for signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Fourier Transform Properties and Applications: In-depth understanding of Fourier transform properties, such as periodicity, linearity, and shift-invariance, and their applications in signal processing and image analysis, including tasks like filtering, convolution, and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9025563137326273
      },
      {
        "text": "Fourier Transform and Its Applications in Signal Processing and Image Analysis: In-depth understanding of the Fourier transform and its applications in signal processing and image analysis, including tasks like filtering, convolution, and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8604042280560064
      },
      {
        "text": "Fourier Transform Properties: The Fourier transform has several properties, such as linearity, shift-invariance, and periodicity, which are used in signal processing and image analysis for tasks such as filtering and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8974298046643476
      }
    ]
  },
  {
    "representative_text": "Filter Design: Designing filters that meet specific requirements, such as the Butterworth filter or the Chebyshev filter, which are used in signal processing and image analysis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Filter Design: Designing filters that meet specific requirements, such as the Butterworth filter or the Chebyshev filter, which are used in signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Factorization: A method for factorizing a transfer function matrix into a product of two matrices, which is used in control theory to design and analyze control systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spectral Factorization: A method for factorizing a transfer function matrix into a product of two matrices, which is used in control theory to design and analyze control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Perturbation Theory: A branch of linear algebra that studies the effects of small changes in the parameters of a linear system on its behavior.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Perturbation Theory: A branch of linear algebra that studies the effects of small changes in the parameters of a linear system on its behavior.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Perturbation Theory: A branch of linear algebra that studies the behavior of linear systems with multiple time scales or small parameters.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8798713707638073
      },
      {
        "text": "Stability Analysis: A branch of linear algebra that studies the stability of linear systems, using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8128372367122256
      },
      {
        "text": "Perturbation Theory: This branch of linear algebra studies the effects of small changes in the parameters of a linear system on its behavior.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9297754939949225
      }
    ]
  },
  {
    "representative_text": "Gaussian Processes: A probabilistic framework for modeling and analyzing complex systems, which can be used in control theory to design and analyze control systems.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian Processes: A probabilistic framework for modeling and analyzing complex systems, which can be used in control theory to design and analyze control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Model Predictive Control (MPC): A control strategy that uses linear algebra and optimization techniques to predict the behavior of a system over a finite horizon and optimize a performance criterion.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Model Predictive Control (MPC): A control strategy that uses linear algebra and optimization techniques to predict the behavior of a system over a finite horizon and optimize a performance criterion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Model-Based Predictive Control (MBPC): This is a control strategy that uses linear algebra and optimization techniques to predict the behavior of a system over a finite horizon and optimize a performance criterion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9173317923523281
      }
    ]
  },
  {
    "representative_text": "Nonlinear Control Theory: A branch of control theory that studies the behavior of nonlinear systems using linear algebra and optimization techniques.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Nonlinear Control Theory: A branch of control theory that studies the behavior of nonlinear systems using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear-Nonlinear Control Theory: This branch of control theory studies the behavior of nonlinear systems using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.954143291830522
      }
    ]
  },
  {
    "representative_text": "State-Space Theory: A framework for modeling and analyzing complex systems using linear algebra and optimization techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "State-Space Theory: A framework for modeling and analyzing complex systems using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Transfer Function Theory: A framework for modeling and analyzing complex systems using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8556272853781233
      },
      {
        "text": "State-Space Theory: This theory is used to model and analyze complex systems using linear algebra and optimization techniques. It is widely used in control theory and is a fundamental tool for analyzing and designing control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8797917308275618
      },
      {
        "text": "Transfer Function Theory: This theory is used to model and analyze complex systems using linear algebra and optimization techniques. It is widely used in control theory and is a fundamental tool for analyzing and designing control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.897492085144957
      }
    ]
  },
  {
    "representative_text": "Robust Control Theory: A branch of control theory that studies the behavior of linear systems in the presence of uncertainty or noise, using linear algebra and optimization techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Robust Control Theory: A branch of control theory that studies the behavior of linear systems in the presence of uncertainty or noise, using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Observer Theory: A branch of linear algebra that studies the behavior of observers in control systems, using linear algebra and optimization techniques.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Observer Theory: A branch of linear algebra that studies the behavior of observers in control systems, using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Observer Theory: This branch of linear algebra studies the behavior of observers in control systems, using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9513713880131939
      }
    ]
  },
  {
    "representative_text": "Filter Theory: A branch of linear algebra that studies the behavior of filters in control systems, using linear algebra and optimization techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Filter Theory: A branch of linear algebra that studies the behavior of filters in control systems, using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Key agreement protocols: Methods for securely establishing a shared secret key between parties, such as Diffie-Hellman key exchange and key agreement protocols like Elliptic Curve Diffie-Hellman (ECDH).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Key agreement protocols: Methods for securely establishing a shared secret key between parties, such as Diffie-Hellman key exchange and key agreement protocols like Elliptic Curve Diffie-Hellman (ECDH).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Zero-knowledge proofs: A method for verifying the truth of a statement without revealing any information about the statement itself, which can be used in cryptographic protocols.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Zero-knowledge proofs: A method for verifying the truth of a statement without revealing any information about the statement itself, which can be used in cryptographic protocols.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Homomorphic encryption schemes: Advanced homomorphic encryption schemes like Brakerski-Gentry-Vaikuntanathan (BGV) and Brakerski-Gentry-Vaikuntanathan- Vaikuntanathan (BGV-V) that provide stronger security guarantees.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Homomorphic encryption schemes: Advanced homomorphic encryption schemes like Brakerski-Gentry-Vaikuntanathan (BGV) and Brakerski-Gentry-Vaikuntanathan- Vaikuntanathan (BGV-V) that provide stronger security guarantees.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Homomorphic Encryption Schemes with High-Order Homomorphisms: This includes advanced homomorphic encryption schemes that provide higher-order homomorphisms, such as the higher-order homomorphism in the Brakerski-Gentry-Vaikuntanathan (BGV) scheme.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.894250181317974
      },
      {
        "text": "Homomorphic encryption schemes with high-order homomorphisms: Exploring advanced homomorphic encryption schemes that provide higher-order homomorphisms, such as the higher-order homomorphism in the Brakerski-Gentry-Vaikuntanathan (BGV) scheme, is crucial for enabling secure computation on large datasets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9049176181619946
      },
      {
        "text": "Homomorphic encryption schemes with multiple levels of homomorphism: Advanced homomorphic encryption schemes that provide multiple levels of homomorphism, enabling secure computation on encrypted data with varying degrees of complexity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8822031583992529
      },
      {
        "text": "Homomorphic encryption schemes with high-order homomorphisms: Advanced homomorphic encryption schemes that provide higher-order homomorphisms, enabling secure computation on large datasets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9504224575956901
      },
      {
        "text": "Homomorphic encryption schemes with high-order homomorphisms for secure multi-party computation: Advanced homomorphic encryption schemes that provide higher-order homomorphisms, enabling secure computation on encrypted data with varying degrees of complexity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9505909750854031
      },
      {
        "text": "Homomorphic Encryption Schemes with High-Order Homomorphisms for Large-Datasets: This includes the analysis of homomorphic encryption schemes with high-order homomorphisms, enabling secure computation on large datasets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9324472466321609
      }
    ]
  },
  {
    "representative_text": "Side-channel attacks: Techniques for exploiting information about the implementation of cryptographic algorithms, such as timing attacks and power analysis attacks.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Side-channel attacks: Techniques for exploiting information about the implementation of cryptographic algorithms, such as timing attacks and power analysis attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Side-channel attacks on cryptographic hardware: Techniques for exploiting information about the implementation of cryptographic hardware, such as timing attacks and power analysis attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9211662530438829
      }
    ]
  },
  {
    "representative_text": "Secure multi-party computation protocols: Advanced protocols like the Bulletproofs protocol, which provide more efficient and secure computation on private data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Secure multi-party computation protocols: Advanced protocols like the Bulletproofs protocol, which provide more efficient and secure computation on private data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Secure multi-party computation protocols with homomorphic encryption: Investigating the intersection of secure multi-party computation and homomorphic encryption, including protocols that enable secure computation on encrypted data, is an area of active research.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8121584427599722
      },
      {
        "text": "Secure multi-party computation protocols with homomorphic encryption: Protocols that enable secure multi-party computation using homomorphic encryption, allowing parties to verify the correctness of computations without revealing their individual inputs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.887483241455082
      },
      {
        "text": "Secure multi-party computation protocols with homomorphic encryption: This includes the analysis of secure multi-party computation protocols with homomorphic encryption, enabling secure computation on encrypted data without revealing individual inputs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9437873730322708
      }
    ]
  },
  {
    "representative_text": "Lattice-based cryptography: A method for cryptographic computations using lattices, which can be used to create more secure cryptographic schemes.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lattice-based cryptography: A method for cryptographic computations using lattices, which can be used to create more secure cryptographic schemes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Code-based cryptography: A method for cryptographic computations using error-correcting codes, which can be used to create more secure cryptographic schemes.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Code-based cryptography: A method for cryptographic computations using error-correcting codes, which can be used to create more secure cryptographic schemes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Multivariate cryptography: A method for cryptographic computations using multivariate polynomials, which can be used to create more secure cryptographic schemes.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Multivariate cryptography: A method for cryptographic computations using multivariate polynomials, which can be used to create more secure cryptographic schemes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Multivariate Cryptography with Advanced Key Generation Methods: This includes methods for generating and analyzing keys for multivariate cryptographic schemes, such as the use of lattice-based key generation methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8394448931141396
      },
      {
        "text": "Multivariate cryptography with advanced key generation methods: Investigating novel key generation methods for multivariate cryptographic schemes, such as lattice-based key generation methods, is an area of ongoing research.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8974297860233917
      },
      {
        "text": "Multivariate cryptography with advanced key generation methods: Novel key generation methods for multivariate cryptographic schemes, such as lattice-based key generation methods, that improve security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9047739281116425
      },
      {
        "text": "Multivariate cryptography with advanced key generation methods: Investigating novel key generation methods for multivariate cryptographic schemes, such as lattice-based key generation methods, to improve security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9517867176616623
      },
      {
        "text": "Multivariate Cryptography with Advanced Key Generation Methods for Efficient Key Exchange: This includes methods for generating and analyzing keys for multivariate cryptographic schemes, such as lattice-based key generation methods, to improve security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9480857401038149
      },
      {
        "text": "Multivariate cryptography with advanced key generation methods for efficient key exchange: This includes the analysis of multivariate cryptography with advanced key generation methods, such as lattice-based key generation methods, for improving security and efficiency in key exchange.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9140001788263843
      }
    ]
  },
  {
    "representative_text": "Quantum-resistant cryptography: Methods for creating cryptographic schemes that are resistant to quantum computer attacks, such as lattice-based cryptography and code-based cryptography.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Quantum-resistant cryptography: Methods for creating cryptographic schemes that are resistant to quantum computer attacks, such as lattice-based cryptography and code-based cryptography.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Post-Quantum Cryptography: This includes cryptographic schemes that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.903475090973735
      },
      {
        "text": "Quantum-resistant cryptographic techniques: Exploring alternative cryptographic techniques that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, is crucial for ensuring the long-term security of cryptographic systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8008917897636603
      },
      {
        "text": "Quantum-Resistant Cryptographic Techniques for Secure Communication Networks: This includes exploring alternative cryptographic techniques that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, to ensure long-term security in secure communication networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8074939975114372
      },
      {
        "text": "Quantum-resistant cryptographic techniques for secure communication networks: Methods for exploring alternative cryptographic techniques that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, to ensure long-term security in secure communication networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.878676389067953
      },
      {
        "text": "Quantum-resistant cryptographic techniques for secure communication protocols: Methods for exploring alternative cryptographic techniques that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, to ensure long-term security in secure communication protocols.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9190685413195661
      },
      {
        "text": "Quantum-resistant cryptographic techniques for secure key exchange: Methods for exploring alternative cryptographic techniques that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, to ensure long-term security in secure key exchange.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8373089066264735
      },
      {
        "text": "Quantum-resistant cryptographic techniques for secure key exchange: This includes the analysis of quantum-resistant cryptographic techniques, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, for secure key exchange.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8472488818843688
      },
      {
        "text": "Quantum-resistant cryptographic techniques for secure communication networks: This includes the analysis of quantum-resistant cryptographic techniques, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, for secure communication networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8865322132768103
      },
      {
        "text": "Quantum-resistant cryptographic techniques for secure communication over high-speed and high-throughput networks: This includes the analysis of quantum-resistant cryptographic techniques, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, for secure communication over high-speed and high-throughput networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8399123748474522
      },
      {
        "text": "Quantum-Resistant Cryptographic Techniques for Secure Communication Protocols: This includes the analysis of quantum-resistant cryptographic techniques, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, for secure communication protocols.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.909616686244149
      }
    ]
  },
  {
    "representative_text": "Side-channel resistant cryptography: Methods for creating cryptographic schemes that are resistant to side-channel attacks, such as using secure coding practices and side-channel resistant algorithms.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Side-channel resistant cryptography: Methods for creating cryptographic schemes that are resistant to side-channel attacks, such as using secure coding practices and side-channel resistant algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Side-Channel Secure Multi-Party Computation: This includes methods for secure multi-party computation using side-channel resistant algorithms and secure coding practices to prevent side-channel attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8306377136547993
      },
      {
        "text": "Side-Channel Attacks on Advanced Cryptographic Hardware: This includes methods for analyzing and defending against side-channel attacks on advanced cryptographic hardware, such as the use of secure coding practices and side-channel resistant algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8471263335559598
      },
      {
        "text": "Side-Channel Secure Key Agreement Protocols: This includes methods for securely establishing shared secret keys between parties, such as the use of side-channel resistant algorithms and secure coding practices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8279638299716232
      },
      {
        "text": "Side-channel attack resistance: Understanding the principles and techniques for resisting side-channel attacks, such as timing attacks and power analysis attacks, is essential for designing and implementing secure cryptographic systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.846362635409608
      },
      {
        "text": "Side-channel attack resistance techniques: Various techniques used to prevent or mitigate side-channel attacks, such as secure coding practices, side-channel resistant algorithms, and countermeasures like masking or masking-based key recovery.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8680956203084433
      },
      {
        "text": "Side-channel attacks on advanced cryptographic hardware: Techniques for analyzing and defending against side-channel attacks on advanced cryptographic hardware, such as secure coding practices and side-channel resistant algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8660701182102688
      },
      {
        "text": "Side-Channel Attack Resistance Techniques for Advanced Cryptographic Hardware: This includes methods for analyzing and defending against side-channel attacks on advanced cryptographic hardware, such as secure coding practices, side-channel resistant algorithms, and countermeasures like masking or masking-based key recovery.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9011345044591886
      },
      {
        "text": "Key Recovery Attacks on Advanced Cryptographic Schemes: This includes methods for analyzing and defending against key recovery attacks on advanced cryptographic schemes, such as secure coding practices, side-channel resistant algorithms, and countermeasures like masking or masking-based key recovery.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8322708913707784
      },
      {
        "text": "Side-channel attack resistance techniques for advanced cryptographic hardware: Methods for analyzing and defending against side-channel attacks on advanced cryptographic hardware, such as secure coding practices, side-channel resistant algorithms, and countermeasures like masking or masking-based key recovery.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9209964780626214
      },
      {
        "text": "Side-channel attack resistance techniques for cryptographic hardware: This includes the analysis of side-channel attack resistance techniques, such as secure coding practices, side-channel resistant algorithms, and countermeasures like masking or masking-based key recovery, for protecting cryptographic hardware against side-channel attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9323083331632317
      }
    ]
  },
  {
    "representative_text": "Secure communication protocols: Protocols like the Secure Sockets Layer/Transport Layer Security (SSL/TLS) protocol, which provide secure communication over the internet.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Secure communication protocols: Protocols like the Secure Sockets Layer/Transport Layer Security (SSL/TLS) protocol, which provide secure communication over the internet.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Secure Communication Protocols with High-Speed and High-Throughput: This includes protocols, such as the Transport Layer Security (TLS) protocol, that provide secure communication over high-speed and high-throughput networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8022106606970435
      }
    ]
  },
  {
    "representative_text": "Digital watermarking: A method for embedding a hidden message or watermark into a digital file, which can be used to authenticate the file and detect tampering.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Digital watermarking: A method for embedding a hidden message or watermark into a digital file, which can be used to authenticate the file and detect tampering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Ciphertext-only attacks: Techniques for attacking cryptographic schemes using only the ciphertext, rather than the plaintext.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Ciphertext-only attacks: Techniques for attacking cryptographic schemes using only the ciphertext, rather than the plaintext.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Chosen-ciphertext attacks: Techniques for attacking cryptographic schemes by selecting the ciphertext to be encrypted.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Chosen-ciphertext attacks: Techniques for attacking cryptographic schemes by selecting the ciphertext to be encrypted.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "(u + v, w) = (u, w) + (v, w) for all u, v, w in V",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "(u + v, w) = (u, w) + (v, w) for all u, v, w in V",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "(cu, v) = c(u, v) for all c in F and u, v in V",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "(cu, v) = c(u, v) for all c in F and u, v in V",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "(u, v) = (v, u) for all u, v in V",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "(u, v) = (v, u) for all u, v in V",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Vector: A vector v in a vector space V is said to be singular if the only linear combination of v with itself is the zero vector, i.e., v + v = 0.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Vector: A vector v in a vector space V is said to be singular if the only linear combination of v with itself is the zero vector, i.e., v + v = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lagrange's Interpolation Formula: Lagrange's interpolation formula is a method for finding the coefficients of a polynomial that interpolates a set of given points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Lagrange's Interpolation Formula: Lagrange's interpolation formula is a method for finding the coefficients of a polynomial that interpolates a set of given points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Lagrange Interpolation Formula: A method for interpolating a polynomial using a set of points, which can be used to calculate the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8324127670001071
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Span of the Dual Space: A set of vectors in V is said to be linearly independent if and only if the only linear combination of the vectors with coefficients in the dual space V is the zero vector. The span of the dual space V is the set of all linear functionals on V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and Span of the Dual Space: A set of vectors in V is said to be linearly independent if and only if the only linear combination of the vectors with coefficients in the dual space V is the zero vector. The span of the dual space V is the set of all linear functionals on V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation Properties: Properties of linear transformations, such as linearity, injectivity, surjectivity, and the relationship between the kernel and image.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformation Properties: Properties of linear transformations, such as linearity, injectivity, surjectivity, and the relationship between the kernel and image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations with Non-Trivial Kernel or Image: A linear transformation with a non-trivial kernel or image, and its properties, such as the rank and nullity of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8497224324163581
      },
      {
        "text": "The definition and properties of a linear transformation, including the kernel and image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8951181430122322
      }
    ]
  },
  {
    "representative_text": "Change of Basis Theorem: A theorem that states that if a linear transformation has a basis in one vector space, it also has a basis in any other vector space with the same dimension.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Change of Basis Theorem: A theorem that states that if a linear transformation has a basis in one vector space, it also has a basis in any other vector space with the same dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Change of Basis Theorem: This theorem states that if a basis for a vector space is changed, then the dimension of the vector space remains the same.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9007972808916068
      }
    ]
  },
  {
    "representative_text": "Orthogonality and its Applications: The concept of orthogonality and its applications in solving systems of linear equations and finding the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonality and its Applications: The concept of orthogonality and its applications in solving systems of linear equations and finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Inner Product Spaces and their Applications: The concept of inner product spaces and their applications in solving systems of linear equations and finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.868710218063147
      }
    ]
  },
  {
    "representative_text": "Minimax Theorem: A theorem that describes the properties of the eigenvalues and eigenvectors of a matrix, which is useful in solving systems of linear equations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Minimax Theorem: A theorem that describes the properties of the eigenvalues and eigenvectors of a matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Minimax Theorem: A theorem that relates the rank and nullity of a linear transformation to its eigenvalues and eigenvectors, which is useful in understanding the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8650541854732181
      }
    ]
  },
  {
    "representative_text": "Rayleigh-Ritz Method: A method for solving systems of linear equations by approximating the solution using a set of orthogonal basis vectors, which is useful in solving systems of linear equations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Rayleigh-Ritz Method: A method for solving systems of linear equations by approximating the solution using a set of orthogonal basis vectors, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Least Squares Method: A method for solving systems of linear equations by minimizing the sum of the squared errors between the observed and predicted values, which is useful in solving systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Least Squares Method: A method for solving systems of linear equations by minimizing the sum of the squared errors between the observed and predicted values, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Least Squares Method: A method for finding the best-fitting linear combination of a set of vectors to a given set of data points by minimizing the sum of the squared errors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8515098103964994
      },
      {
        "text": "Least Squares Method (continued): The least squares method is a method for finding the best-fitting linear combination of a set of vectors to a given set of data points by minimizing the sum of the squared errors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9041163016058436
      },
      {
        "text": "Least Squares Method and Determinants: The least squares method is a method for finding the best-fitting linear combination of a set of vectors to a given set of data points by minimizing the sum of the squared errors. The determinant of a matrix can be used to find the least squares solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8194572962940125
      },
      {
        "text": "Least Squares Method and Orthogonal Projections: The least squares method is a method for finding the best-fitting linear combination of a set of vectors to a given set of data points by minimizing the sum of the squared errors. Orthogonal projections are a method for projecting a vector onto a subspace by finding the orthogonal projection of the vector onto a basis for the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8417809410087785
      },
      {
        "text": "Least Squares Method and Dimension: The least squares method is a method for finding the best-fitting linear combination of a set of vectors to a given set of data points by minimizing the sum of the squared errors. The determinant of a matrix can be used to find the least squares solution, which can also be used to find the dimension of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8582466642028398
      },
      {
        "text": "Least Squares Method: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8658246699546709
      }
    ]
  },
  {
    "representative_text": "The Inverse of an Orthogonal Matrix: A theorem that describes the properties of the inverse of an orthogonal matrix, which is useful in solving systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Inverse of an Orthogonal Matrix: A theorem that describes the properties of the inverse of an orthogonal matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Determinant of an Orthogonal Matrix: A theorem that describes the properties of the determinant of an orthogonal matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9187633975908008
      }
    ]
  },
  {
    "representative_text": "The Modulus of a Matrix: A theorem that describes the properties of the modulus of a matrix, which is useful in solving systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Modulus of a Matrix: A theorem that describes the properties of the modulus of a matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Moore-Penrose Inverse of a Matrix: A theorem that describes the properties of the Moore-Penrose inverse of a matrix, which is useful in solving systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Moore-Penrose Inverse of a Matrix: A theorem that describes the properties of the Moore-Penrose inverse of a matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Sylvester Equation: An equation that describes the properties of the eigenvalues and eigenvectors of a matrix, which is useful in solving systems of linear equations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Sylvester Equation: An equation that describes the properties of the eigenvalues and eigenvectors of a matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Sylvester Matrix: A matrix that describes the properties of the eigenvalues and eigenvectors of a matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9023669607621252
      }
    ]
  },
  {
    "representative_text": "The Spectral Theorem: A theorem that describes the properties of the eigenvalues and eigenvectors of a matrix, which is useful in solving systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Spectral Theorem: A theorem that describes the properties of the eigenvalues and eigenvectors of a matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Unitary Triangularization Theorem: A theorem that describes the properties of the eigenvalues and eigenvectors of a matrix, which is useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9017795354100596
      }
    ]
  },
  {
    "representative_text": "The Pythagorean Theorem for Inner Product Spaces: A theorem that describes the properties of the inner product and the norm of a vector space, which is useful in understanding the properties of inner products.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Pythagorean Theorem for Inner Product Spaces: A theorem that describes the properties of the inner product and the norm of a vector space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Hilbert-Schmidt Theorem: A theorem that describes the properties of the inner product and the norm of a vector space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9197414658917537
      },
      {
        "text": "The Pontryagin Duality Theorem: A theorem that describes the properties of the inner product and the norm of a vector space, which is useful in understanding the properties of inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9217230555447412
      }
    ]
  },
  {
    "representative_text": "Schmidt Normal Form: A linear transformation can be decomposed into a direct sum of two linear transformations, one of which is an orthogonal projection onto a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Schmidt Normal Form: A linear transformation can be decomposed into a direct sum of two linear transformations, one of which is an orthogonal projection onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Symmetric and Skew-Symmetric Bilinear Forms: A bilinear form can be classified as symmetric or skew-symmetric based on its properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Symmetric and Skew-Symmetric Bilinear Forms: A bilinear form can be classified as symmetric or skew-symmetric based on its properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bilinear Maps and Linear Transformations: A linear transformation between two vector spaces can be represented by a bilinear map, which is a function that is linear in each argument. This concept is essential in understanding the relationship between linear transformations and bilinear maps.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bilinear Maps and Linear Transformations: A linear transformation between two vector spaces can be represented by a bilinear map, which is a function that is linear in each argument. This concept is essential in understanding the relationship between linear transformations and bilinear maps.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singularity of Matrices: A discussion of the concept of singularity of matrices, including the relationship between singularity and invertibility.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Singularity of Matrices: A discussion of the concept of singularity of matrices, including the relationship between singularity and invertibility.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singularity of a Matrix: The singularity of a matrix is related to the concept of span in terms of the null space and range of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.803680728819492
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Computation for Non-Symmetric Matrices: A discussion of methods for computing eigenvalues and eigenvectors of non-symmetric matrices, such as the QR algorithm.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Computation for Non-Symmetric Matrices: A discussion of methods for computing eigenvalues and eigenvectors of non-symmetric matrices, such as the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Methods for Computing Eigenvalues and Eigenvectors of Non-Symmetric Matrices: This includes the discussion of numerical methods for computing eigenvalues and eigenvectors of non-symmetric matrices, such as the use of the QR algorithm and the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8984859088485765
      },
      {
        "text": "Eigenvalue Computation using the QR Algorithm for Non-Symmetric Matrices: This is an extension of the QR algorithm for symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8530677180527286
      }
    ]
  },
  {
    "representative_text": "Numerical Linear Algebra Software: A discussion of software packages that implement numerical linear algebra algorithms, such as MATLAB, NumPy, and SciPy.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Numerical Linear Algebra Software: A discussion of software packages that implement numerical linear algebra algorithms, such as MATLAB, NumPy, and SciPy.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Linear Algebra Software:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8183396423625093
      }
    ]
  },
  {
    "representative_text": "Linear Algebra Applications in Computer Graphics: A discussion of the applications of linear algebra in computer graphics, such as 3D modeling and animation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra Applications in Computer Graphics: A discussion of the applications of linear algebra in computer graphics, such as 3D modeling and animation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra Applications in Machine Learning: A discussion of the applications of linear algebra in machine learning, such as principal component analysis (PCA) and singular value decomposition (SVD).",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 19,
    "detailed_sources": [
      {
        "text": "Linear Algebra Applications in Machine Learning: A discussion of the applications of linear algebra in machine learning, such as principal component analysis (PCA) and singular value decomposition (SVD).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Machine Learning: Applying linear algebra techniques to machine learning tasks, such as kernel methods, linear regression, and neural networks, is essential in modern data analysis and artificial intelligence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8551297730332081
      },
      {
        "text": "Linear Algebraic Techniques for Machine Learning: This includes techniques such as the use of linear algebra to perform dimensionality reduction, and to train machine learning models.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8930435877530432
      },
      {
        "text": "Linear Algebraic Techniques for Data Analysis: This includes techniques such as the use of linear algebra to perform dimensionality reduction, and to solve problems of data visualization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8641379256392058
      },
      {
        "text": "Semi-Supervised Learning: This area of research involves using linear algebra techniques such as eigenvalue decomposition and eigenvector decomposition to improve the performance of machine learning models on semi-supervised datasets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8043615680853018
      },
      {
        "text": "Geometric Interpretation of Machine Learning Algorithms: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to interpret the behavior of machine learning algorithms and improve their performance.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.868992872946077
      },
      {
        "text": "Kernel Methods for Non-Linear Regression: This area of research involves using linear algebra techniques such as eigendecomposition and eigenvector decomposition to improve the performance of non-linear regression models.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8139412369643924
      },
      {
        "text": "Linear Algebra for Deep Learning: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize deep learning models for tasks such as image recognition and natural language processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8215480981869339
      },
      {
        "text": "Linear Algebra for Reinforcement Learning: This area of research involves using linear algebra techniques such as eigendecomposition and eigenvector decomposition to analyze and optimize reinforcement learning models for tasks such as game playing and robotics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8234528191684005
      },
      {
        "text": "Manifold Learning for High-Dimensional Data: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to reduce the dimensionality of high-dimensional data while preserving its underlying structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8447542387364975
      },
      {
        "text": "Linear Algebra for Machine Learning with Non-Linear Transformations: Applying linear algebra techniques to machine learning tasks with non-linear transformations, such as the kernel method and the support vector machine (SVM), is essential in modern machine learning systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8442937894520454
      },
      {
        "text": "Semi-Supervised Learning with Linear Algebra Techniques: This area of research involves using linear algebra techniques such as eigenvalue decomposition and eigenvector decomposition to improve the performance of machine learning models on semi-supervised datasets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8186635719601478
      },
      {
        "text": "Linear Algebra for Natural Language Processing: Text Classification and Sentiment Analysis: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize natural language processing algorithms for tasks such as text classification and sentiment analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8121214293816241
      },
      {
        "text": "Linear Algebra for Machine Learning: Linear algebra is used in machine learning tasks such as kernel methods, linear regression, and neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8568232851657449
      },
      {
        "text": "Linear Algebra for Data Analysis: Linear algebra is used in data analysis tasks such as principal component analysis (PCA), singular value decomposition (SVD), and eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8312327321895092
      },
      {
        "text": "The use of linear algebra in machine learning: Linear algebra is used in a wide range of applications in machine learning, including neural networks, support vector machines, and principal component analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8735045835049526
      },
      {
        "text": "Linear Algebraic Methods for Machine Learning: Techniques such as linear regression, logistic regression, and neural networks-based machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8190719909383959
      },
      {
        "text": "Machine Learning: Linear algebra is used in machine learning to perform tasks such as dimensionality reduction, clustering, and regression analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.849348685105562
      },
      {
        "text": "Machine Learning with Dimensionality Reduction: Machine learning is a field of study that deals with the use of algorithms to analyze and interpret complex data. Dimensionality reduction is a technique for reducing the number of features in a dataset while preserving the most important information. Linear algebra is used in machine learning for tasks such as dimensionality reduction, clustering, and regression analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8595028086842817
      }
    ]
  },
  {
    "representative_text": "Linear Algebra Applications in Optimization: A discussion of the applications of linear algebra in optimization, such as linear programming and convex optimization.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra Applications in Optimization: A discussion of the applications of linear algebra in optimization, such as linear programming and convex optimization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra Applications in Signal Processing: A discussion of the applications of linear algebra in signal processing, such as filter design and image processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Linear Algebra Applications in Signal Processing: A discussion of the applications of linear algebra in signal processing, such as filter design and image processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Signal Processing and Image Analysis: Linear algebra is a fundamental tool for signal processing and image analysis. It is used in various tasks such as filtering, convolution, and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.852879542164106
      },
      {
        "text": "Linear Algebra for Data Compression: Linear algebra is used in data compression tasks such as image compression, audio compression, and text compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8175362807096709
      },
      {
        "text": "Linear Algebra for Signal Processing: Linear algebra is used in signal processing tasks such as signal filtering, signal convolution, and signal compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9489217192536978
      },
      {
        "text": "Linear Algebra for Signal Processing with Non-Stationary Signals: Linear algebra is used in signal processing tasks with non-stationary signals such as signal filtering, signal convolution, and signal compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8785471855764647
      },
      {
        "text": "Signal Processing: Linear algebra is used in signal processing to perform tasks such as filtering, convolution, and Fourier analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9273588465580023
      },
      {
        "text": "Signal Processing with Filtering and Convolution: Signal processing is a field of study that deals with the analysis and processing of signals. Filtering and convolution are techniques for processing and analyzing signals. Linear algebra is used in signal processing for tasks such as filtering, convolution, and Fourier analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8802044099506514
      }
    ]
  },
  {
    "representative_text": "Duality Theorem: A fundamental theorem in linear algebra that establishes a relationship between the kernel and image of a linear transformation, as well as the dual transformation and its kernel.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Duality Theorem: A fundamental theorem in linear algebra that establishes a relationship between the kernel and image of a linear transformation, as well as the dual transformation and its kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Duality Theorem: The duality theorem is a fundamental result in linear algebra that establishes a relationship between the kernel and image of a linear transformation, as well as the dual transformation and its kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9300144161553224
      }
    ]
  },
  {
    "representative_text": "Kernel of the Dual Transformation: The kernel of the dual transformation, which is a subspace of the dual space of the original domain, and its relationship with the image of the original transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Kernel of the Dual Transformation: The kernel of the dual transformation, which is a subspace of the dual space of the original domain, and its relationship with the image of the original transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Image of the Dual Transformation: The image of the dual transformation, which is a subspace of the dual space of the codomain, and its relationship with the kernel of the original transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.92870042616851
      },
      {
        "text": "Dual Basis and Dual Basis for a Linear Transformation (Alternative): Alternative representations of the dual basis in relation to the image and kernel of a linear transformation, including its orthogonality and the fact that the dual basis of the domain can be used to find the image of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8372024156351441
      }
    ]
  },
  {
    "representative_text": "Cyclic Subspaces: A subspace generated by a single vector under the action of a linear transformation, and its properties, such as its dimension and relationship with the kernel and image.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cyclic Subspaces: A subspace generated by a single vector under the action of a linear transformation, and its properties, such as its dimension and relationship with the kernel and image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Extremal Linear Transformations: A linear transformation that is \"extreme\" in some sense, such as being one-to-one or onto, and its relationship with the kernel and image.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Extremal Linear Transformations: A linear transformation that is \"extreme\" in some sense, such as being one-to-one or onto, and its relationship with the kernel and image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Extremal Linear Transformations with Trivial Image or Kernel: A linear transformation T: V -> W has a trivial image or kernel if the image or kernel of T is the zero subspace. Understanding these cases is crucial in understanding the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8360602288230979
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Subspaces: The relationship between linear transformations and subspaces, including the fact that a linear transformation can be represented as a linear combination of its matrix columns.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Subspaces: The relationship between linear transformations and subspaces, including the fact that a linear transformation can be represented as a linear combination of its matrix columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant and Perturbation Theory: The determinant of a matrix A can be used to study the behavior of A near a given point using perturbation theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant and Perturbation Theory: The determinant of a matrix A can be used to study the behavior of A near a given point using perturbation theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant and Non-Orthogonal Linear Transformations: The determinant of a matrix A representing a non-orthogonal linear transformation is not equal to the determinant of A^T. This is a subtle nuance of determinants and non-orthogonal linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Determinant and Non-Orthogonal Linear Transformations: The determinant of a matrix A representing a non-orthogonal linear transformation is not equal to the determinant of A^T. This is a subtle nuance of determinants and non-orthogonal linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant and Non-Linear Transformations: The determinant of a matrix A representing a non-linear transformation is not equal to the determinant of A^T. This is a subtle nuance of determinants and non-linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9304247707702129
      },
      {
        "text": "Determinant of a Matrix with a Non-Orthogonal Transformation: The determinant of a matrix can change under a non-orthogonal transformation. This is an important concept in linear algebra and has applications in various fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8673189028749692
      },
      {
        "text": "Theorem on the Determinant of a Matrix with a Non-Symmetric Transformation: This theorem states that the determinant of a matrix can change under a non-symmetric transformation. This has important implications for the calculation of determinants and the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8107580786281339
      },
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues and Non-Orthogonal Linear Transformations: The determinant of a matrix with complex eigenvalues and non-orthogonal linear transformations is not equal to the determinant of the matrix with orthogonal linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8538887731622565
      },
      {
        "text": "The Determinant of a Matrix with a Non-Linear Transformation Matrix: The determinant of a matrix can change under a non-linear transformation matrix. This is an important concept in linear algebra and has applications in various fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9203591093993206
      },
      {
        "text": "The Determinant of a Matrix with a Non-Orthogonal Basis: The determinant of a matrix can change under a non-orthogonal basis. This has important implications for the calculation of determinants and the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8997995261802945
      },
      {
        "text": "The Determinant of a Matrix with a Non-Symmetric Transformation Matrix: The determinant of a matrix can change under a non-symmetric transformation matrix. This has important implications for the calculation of determinants and the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.945884108034851
      },
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Linear Transformations and Non-Constant Coefficients: The determinant of a matrix with non-orthogonal linear transformations and non-constant coefficients is not necessarily equal to the determinant of the matrix with orthogonal linear transformations and constant coefficients.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.908110940716753
      }
    ]
  },
  {
    "representative_text": "Determinant and Non-Full Rank: The determinant of a matrix A with non-full rank is not equal to 0. This is a subtle nuance of determinants and non-full rank.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant and Non-Full Rank: The determinant of a matrix A with non-full rank is not equal to 0. This is a subtle nuance of determinants and non-full rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Chains and Cyclic Matrices: A matrix chain is a sequence of matrices where each matrix is multiplied by the previous one, and a cyclic matrix is a matrix where the result of multiplying it by itself is the original matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Chains and Cyclic Matrices: A matrix chain is a sequence of matrices where each matrix is multiplied by the previous one, and a cyclic matrix is a matrix where the result of multiplying it by itself is the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block Diagonal Matrices: A block diagonal matrix is a matrix where the diagonal blocks are square matrices, and the off-diagonal blocks are zero matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block Diagonal Matrices: A block diagonal matrix is a matrix where the diagonal blocks are square matrices, and the off-diagonal blocks are zero matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Clustering: Eigenvalue clustering is a technique used to group eigenvalues into clusters based on their values and the structure of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Clustering: Eigenvalue clustering is a technique used to group eigenvalues into clusters based on their values and the structure of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Quadratic Equations: Matrix quadratic equations are equations where the unknown is a matrix, and the coefficients are polynomials.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Quadratic Equations: Matrix quadratic equations are equations where the unknown is a matrix, and the coefficients are polynomials.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix-Tree Theorem: The matrix-tree theorem is a theorem that relates the eigenvalues of a matrix to the number of trees that can be constructed from its edges.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Matrix-Tree Theorem: The matrix-tree theorem is a theorem that relates the eigenvalues of a matrix to the number of trees that can be constructed from its edges.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix-Tree Theorem and its Applications: This involves the study of the matrix-tree theorem, which relates the eigenvalues of a matrix to the number of trees that can be constructed from its edges, and its applications in various fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8807586357297669
      }
    ]
  },
  {
    "representative_text": "Computing the Eigenvectors of a Matrix using the QR Algorithm: The QR algorithm can also be used to compute the eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Computing the Eigenvectors of a Matrix using the QR Algorithm: The QR algorithm can also be used to compute the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computing the Eigenvectors of a Matrix using the QR Algorithm with Shift and Invert: This method can also be used to compute the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9248873386922002
      },
      {
        "text": "Computing the Eigenvectors of a Matrix using the Arnoldi Method: The Arnoldi method can also be used to compute the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.870010576145995
      },
      {
        "text": "Computing the Eigenvectors of a Matrix using the Krylov Subspace Method: The Krylov subspace method can also be used to compute the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8803063873899106
      }
    ]
  },
  {
    "representative_text": "Congruent Matrices: Two matrices A and B are said to be congruent if there exists an invertible matrix P such that B = P^TAP. This concept is useful in solving systems of linear equations and finding the inverse of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Congruent Matrices: Two matrices A and B are said to be congruent if there exists an invertible matrix P such that B = P^TAP. This concept is useful in solving systems of linear equations and finding the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Congruent Matrices: Two matrices A and B are said to be congruent if there exists an invertible matrix P such that B = P^TAP.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9289054631516717
      },
      {
        "text": "Congruent Matrices and Their Properties: The properties of congruent matrices, including their relationship to invertibility and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8468109663092394
      }
    ]
  },
  {
    "representative_text": "The Schur Complement: The Schur complement is a submatrix of a block matrix that is used to compute the determinant and inverse of the block matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Schur Complement: The Schur complement is a submatrix of a block matrix that is used to compute the determinant and inverse of the block matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Determinant of a Matrix with a Complex Eigenvalue: The determinant of a matrix with a complex eigenvalue can be calculated using the formula det(A) = e^(tr(A)) \\* det(A - λI), where tr(A) is the trace of the matrix A and λ is the eigenvalue.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Determinant of a Matrix with a Complex Eigenvalue: The determinant of a matrix with a complex eigenvalue can be calculated using the formula det(A) = e^(tr(A)) \\* det(A - λI), where tr(A) is the trace of the matrix A and λ is the eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues: The determinant of a matrix with complex eigenvalues can be calculated using the formula det(A) = ∏(λi), where λi are the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9192380380498275
      }
    ]
  },
  {
    "representative_text": "The Properties of Determinants under Elementary Row Operations: The determinant of a matrix changes sign when two rows are interchanged, and it remains unchanged when a row is multiplied by a scalar or added to another row.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Properties of Determinants under Elementary Row Operations: The determinant of a matrix changes sign when two rows are interchanged, and it remains unchanged when a row is multiplied by a scalar or added to another row.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Properties of Determinants under Elementary Column Operations: The determinant of a matrix changes sign when two columns are interchanged, and it remains unchanged when a column is multiplied by a scalar or added to another column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9316461940977538
      },
      {
        "text": "Properties of Determinants under Simultaneous Row and Column Operations (Generalized): A more general version of the properties of determinants under elementary row and column operations, which involves using the properties of the matrix and the operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8652827479477535
      },
      {
        "text": "Row and Column Operations: Understanding how row and column operations affect the determinant of a matrix, including the properties of determinants under row and column operations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.90933146230465
      },
      {
        "text": "Invariance of the Determinant Under Row Operations: The determinant of a matrix remains unchanged under elementary row operations such as multiplying a row by a scalar, adding a multiple of one row to another row, and interchanging two rows.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9071547113887795
      }
    ]
  },
  {
    "representative_text": "The Determinant of a Matrix with a Rank-Deficient Row or Column: The determinant of a matrix with a rank-deficient row or column is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Determinant of a Matrix with a Rank-Deficient Row or Column: The determinant of a matrix with a rank-deficient row or column is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inverse of a Matrix with Complex Eigenvalues: If A is a square matrix with complex eigenvalues, then the inverse of A is also a square matrix with complex eigenvalues.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Inverse of a Matrix with Complex Eigenvalues: If A is a square matrix with complex eigenvalues, then the inverse of A is also a square matrix with complex eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Inverse of a Matrix with Complex Eigenvalues: The calculation of the inverse of a matrix with complex eigenvalues, including the concept of the inverse of a matrix with complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8451762496018869
      },
      {
        "text": "Properties of the Inverse of a Matrix with Complex Eigenvalues: The properties of the inverse of a matrix with complex eigenvalues, including the concept of the inverse of a matrix with complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9173955479702244
      },
      {
        "text": "The Inverse of a Matrix with Complex Eigenvalues: The properties of the inverse of a matrix with complex eigenvalues, such as the existence of a complex inverse, is an important concept in understanding the behavior of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8747369800923979
      },
      {
        "text": "The Inverse of a Matrix with Complex Eigenvalues: This topic deals with the properties of the inverse of a matrix with complex eigenvalues, including the existence of a complex inverse and the relationship between the eigenvalues and the inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8973068693677988
      },
      {
        "text": "The Inverse of a Matrix with Complex Eigenvalues: The calculation of the inverse of a matrix with complex eigenvalues, including the concept of the inverse of a matrix with complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8934196442236407
      }
    ]
  },
  {
    "representative_text": "Eigenvalue of a Matrix with Non-Unitary Eigenvectors: The eigenvalue of a matrix A is not necessarily unique when the eigenvectors are not unitary.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue of a Matrix with Non-Unitary Eigenvectors: The eigenvalue of a matrix A is not necessarily unique when the eigenvectors are not unitary.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Conjugate Eigenvalues: If A is a square matrix with complex eigenvalues, then the conjugate of each eigenvalue is also an eigenvalue of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Conjugate Eigenvalues: If A is a square matrix with complex eigenvalues, then the conjugate of each eigenvalue is also an eigenvalue of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hessenberg Matrix: A Hessenberg matrix is a square matrix with non-zero entries only above the subdiagonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hessenberg Matrix: A Hessenberg matrix is a square matrix with non-zero entries only above the subdiagonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Sturm's Theorem: This theorem states that a polynomial has a real root if and only if it changes sign.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sturm's Theorem: This theorem states that a polynomial has a real root if and only if it changes sign.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gerschgorin Discs: These are discs that contain all the eigenvalues of a matrix A, and are used to estimate the eigenvalues of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Gerschgorin Discs: These are discs that contain all the eigenvalues of a matrix A, and are used to estimate the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gershgorin Circles: These are circles that contain all the eigenvalues of a matrix A, and are used to estimate the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8768571816854909
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Non-Integer Number of Rows and Columns: While the determinant of a square matrix is well-defined, the determinant of a non-square matrix is not, but there are extensions to the concept that allow us to define the determinant for non-square matrices in certain contexts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Non-Integer Number of Rows and Columns: While the determinant of a square matrix is well-defined, the determinant of a non-square matrix is not, but there are extensions to the concept that allow us to define the determinant for non-square matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Properties of Determinants for Non-Square Matrices: While the determinant of a square matrix is well-defined, the determinant of a non-square matrix is not. However, there are properties of determinants that can be extended to non-square matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9146228404581331
      },
      {
        "text": "Properties of Determinants for Non-Square Matrices: Determinants are defined only for square matrices, but there are some properties and relationships that can be derived for non-square matrices, such as the determinant of a non-square matrix being zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.924149486701703
      },
      {
        "text": "Matrix Inversion with Non-Integer Number of Rows and Columns: While the determinant of a square matrix is well-defined, the determinant of a non-square matrix is not. However, there are properties of determinants that can be extended to non-square matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9087145777879
      }
    ]
  },
  {
    "representative_text": "Matrix Norms and Condition Numbers: These are scalar values that can be used to describe the \"size\" of a matrix and the relative \"stability\" of linear transformations represented by the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Matrix Norms and Condition Numbers: These are scalar values that can be used to describe the \"size\" of a matrix and the relative \"stability\" of linear transformations represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Condition Number of a Matrix: The condition number of a matrix is a scalar value that can be used to describe the \"size\" of a matrix and the relative \"stability\" of linear transformations represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8072708136834631
      }
    ]
  },
  {
    "representative_text": "Inverse of a Matrix with a Non-Invertible Rank: This is a concept that extends the idea of invertibility to matrices with non-invertible ranks, which can be useful in certain applications.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inverse of a Matrix with a Non-Invertible Rank: This is a concept that extends the idea of invertibility to matrices with non-invertible ranks, which can be useful in certain applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gauss-Jordan Elimination for Non-Square Matrices: While the Gauss-Jordan elimination method is typically applied to square matrices, there are extensions to the method that allow us to apply it to non-square matrices in certain contexts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Gauss-Jordan Elimination for Non-Square Matrices: While the Gauss-Jordan elimination method is typically applied to square matrices, there are extensions to the method that allow us to apply it to non-square matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "LU Decomposition for Non-Square Matrices: Similar to the Gauss-Jordan elimination method, there are extensions to the LU decomposition method that allow us to apply it to non-square matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8503126663631196
      },
      {
        "text": "Gaussian Elimination for Non-Symmetric Matrices: While the Gauss-Jordan elimination method is typically applied to symmetric matrices, there are extensions to the method that allow us to apply it to non-symmetric matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9074483210641722
      },
      {
        "text": "LU Decomposition for Non-Symmetric Matrices: Similar to the Gauss-Jordan elimination method, there are extensions to the LU decomposition method that allow us to apply it to non-symmetric matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9096958214548634
      }
    ]
  },
  {
    "representative_text": "Adjoint Method for Non-Square Matrices: The adjoint method is typically applied to square matrices, but there are extensions to the method that allow us to apply it to non-square matrices in certain contexts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Adjoint Method for Non-Square Matrices: The adjoint method is typically applied to square matrices, but there are extensions to the method that allow us to apply it to non-square matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Adjoint Method for Non-Symmetric Matrices: The adjoint method is typically applied to symmetric matrices, but there are extensions to the method that allow us to apply it to non-symmetric matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9465905839596553
      }
    ]
  },
  {
    "representative_text": "Matrix Inversion Theorems for Non-Square Matrices: While the inverse of a matrix is typically defined only for square matrices, there are theorems that extend the concept of invertibility to non-square matrices in certain contexts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Matrix Inversion Theorems for Non-Square Matrices: While the inverse of a matrix is typically defined only for square matrices, there are theorems that extend the concept of invertibility to non-square matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inversion Theorems for Non-Square Matrices with Non-Integer Determinant: While the inverse of a matrix is typically defined only for square matrices with non-zero determinants, there are theorems that extend the concept of invertibility to non-square matrices with non-integer determinants in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8936264116599183
      }
    ]
  },
  {
    "representative_text": "Properties of Matrix Inverses, such as the Inverse of a Matrix with a Non-Zero Determinant: These are properties of matrix inverses that can be used to describe the behavior of matrix inverses in certain contexts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Properties of Matrix Inverses, such as the Inverse of a Matrix with a Non-Zero Determinant: These are properties of matrix inverses that can be used to describe the behavior of matrix inverses in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Properties of Matrix Inverses with Non-Integer Determinant: While the inverse of a matrix is typically defined only for matrices with non-zero determinants, there are properties of matrix inverses that can be extended to matrices with non-integer determinants in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8441710008284448
      },
      {
        "text": "Matrix Inversion and Transpose: The relationship between the inverse of a matrix and its transpose, as well as the properties of the transpose of an inverse matrix, is crucial in understanding the behavior of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8083619863135254
      },
      {
        "text": "The Relationship between Determinants and Inverses: Understanding the relationship between determinants and inverses, such as the fact that a matrix is invertible if and only if its determinant is non-zero, is essential in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8798012900592348
      },
      {
        "text": "The Invertibility of a Matrix with a Non-Positive Determinant: Understanding the relationship between the determinant and invertibility of a matrix with a non-positive determinant is crucial in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.841843852814839
      },
      {
        "text": "The Invertibility of a Matrix with a Non-Positive Eigenvalue: Understanding the relationship between the eigenvalues and invertibility of a matrix is crucial in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8305043743567844
      },
      {
        "text": "The Invertibility of a Matrix with a Zero Eigenvalue: Understanding the relationship between the eigenvalues and invertibility of a matrix is crucial in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8257698915126777
      },
      {
        "text": "The concept of a \"generalized inverse\": A generalized inverse of a matrix is a matrix that satisfies certain properties, which is related to the determinant of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8417786939084914
      },
      {
        "text": "The Invertibility of a Matrix with a Non-Positive Determinant: This topic deals with the relationship between the determinant and invertibility of a matrix, including the existence of a non-positive determinant and the implications for the invertibility of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8554293116822497
      }
    ]
  },
  {
    "representative_text": "Subspace Properties, such as the Nullspace and Row/Column Space of a Matrix: These are properties of subspaces that can be used to describe the behavior of matrices in certain contexts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Subspace Properties, such as the Nullspace and Row/Column Space of a Matrix: These are properties of subspaces that can be used to describe the behavior of matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computing the SVD of a matrix with a large number of singular values: The SVD decomposition may not converge to a well-defined result for matrices with a large number of singular values. Specialized algorithms, such as the truncated SVD, may be needed to handle such cases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computing the SVD of a matrix with a large number of singular values: The SVD decomposition may not converge to a well-defined result for matrices with a large number of singular values. Specialized algorithms, such as the truncated SVD, may be needed to handle such cases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Relationship between SVD and the Moore-Penrose Inverse: The SVD is closely related to the Moore-Penrose inverse of a matrix. In fact, the Moore-Penrose inverse can be computed using the SVD of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relationship between SVD and the Moore-Penrose Inverse: The SVD is closely related to the Moore-Penrose inverse of a matrix. In fact, the Moore-Penrose inverse can be computed using the SVD of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The SVD of a matrix with complex entries: The SVD of a matrix with complex entries can be computed using a similar approach to the real case. However, the resulting matrices U, Σ, and V may have complex entries.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "The SVD of a matrix with complex entries: The SVD of a matrix with complex entries can be computed using a similar approach to the real case. However, the resulting matrices U, Σ, and V may have complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The SVD of a matrix with a non-symmetric matrix: The SVD of a matrix with a non-symmetric matrix can be computed using a similar approach to the real case. However, the resulting matrices U, Σ, and V may have complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9001269109037686
      },
      {
        "text": "The Singular Value Decomposition (SVD) of a Matrix with Complex Eigenvalues: The SVD of a matrix A into the product A = U Σ V^T is typically considered for real matrices. However, it is not explicitly stated whether the SVD can be generalized to complex matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8244905442289445
      },
      {
        "text": "Properties of the SVD of a Matrix with a Non-Symmetric Matrix: The SVD of a matrix with a non-symmetric matrix can be computed using a similar approach to the real case. However, the resulting matrices U, Σ, and V may have complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9399755249034616
      },
      {
        "text": "Orthogonality of the SVD with Complex Eigenvalues: Although the SVD is typically considered for real matrices, it is essential to explore whether the SVD can be generalized to complex matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.858337694338785
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix with Complex Entries: This involves the study of the SVD of a matrix with complex entries, which is a generalization of the SVD of a real matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8376130003499118
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix with Complex Eigenvalues: An extension of SVD to matrices with complex eigenvalues, which involves using the properties of the matrix and the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8761275640353599
      },
      {
        "text": "Matrices with complex entries: While the SVD of a matrix with complex entries can be computed using a similar approach to the real case, there is a need to discuss the properties and implications of complex SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9349769301798225
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix with Complex Eigenvalues: The SVD can be generalized to complex matrices, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.899210231831403
      },
      {
        "text": "Numerical Stability of SVD with Complex Entries: The SVD of a matrix with complex entries can be computed using a similar approach to the real case. However, the resulting matrices U, Σ, and V may have complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9035014129583555
      },
      {
        "text": "Numerical Methods for SVD Computation with Complex Entries: The SVD of a matrix with complex entries can be computed using a similar approach to the real case. However, the resulting matrices U, Σ, and V may have complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9270192923982347
      }
    ]
  },
  {
    "representative_text": "Computing the SVD of a matrix with high rank using the Eckart-Young theorem: The Eckart-Young theorem provides a way to compute the SVD of a matrix with high rank using a different approach than the QR algorithm or Householder transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Computing the SVD of a matrix with high rank using the Eckart-Young theorem: The Eckart-Young theorem provides a way to compute the SVD of a matrix with high rank using a different approach than the QR algorithm or Householder transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eckart-Young Theorem: This theorem provides a way to compute the SVD of a matrix with high rank using a different approach than the QR algorithm or Householder transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.958192607919341
      }
    ]
  },
  {
    "representative_text": "The SVD of a matrix with a non-invertible matrix: The SVD of a matrix with a non-invertible matrix can be computed using the Moore-Penrose inverse of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The SVD of a matrix with a non-invertible matrix: The SVD of a matrix with a non-invertible matrix can be computed using the Moore-Penrose inverse of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "SVD of a matrix with a non-invertible matrix: The SVD of a matrix with a non-invertible matrix can be computed using the Moore-Penrose inverse of the matrix. This is an important application of the SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9101970193187916
      },
      {
        "text": "Singular Value Decomposition (SVD) for Non-Symmetric Matrices: The SVD for non-symmetric matrices, which may provide a more robust method for finding the inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8100075527786323
      }
    ]
  },
  {
    "representative_text": "Relationship between SVD and the singular value decomposition of a matrix transpose: The SVD of a matrix transpose is closely related to the SVD of the original matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relationship between SVD and the singular value decomposition of a matrix transpose: The SVD of a matrix transpose is closely related to the SVD of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fundamental Theorem of Linear Algebra: This theorem states that every linear transformation has an eigenvector, and it provides a way to diagonalize a matrix using eigenvectors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Fundamental Theorem of Linear Algebra: This theorem states that every linear transformation has an eigenvector, and it provides a way to diagonalize a matrix using eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Fundamental Theorem of Linear Algebra with a Non-Orthogonal Basis: This is a more general version of the Fundamental Theorem of Linear Algebra, which states that every square matrix can be diagonalized using a non-orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8062789752349477
      }
    ]
  },
  {
    "representative_text": "Minimal Polynomial and Characteristic Polynomial Relationship: The minimal polynomial and characteristic polynomial are related in that the minimal polynomial divides the characteristic polynomial. This relationship can be used to determine the diagonalizability of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minimal Polynomial and Characteristic Polynomial Relationship: The minimal polynomial and characteristic polynomial are related in that the minimal polynomial divides the characteristic polynomial. This relationship can be used to determine the diagonalizability of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Role of Eigenvector Combinations: Eigenvectors can be combined to form new eigenvectors. This can be used to construct a basis for the vector space spanned by the eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Role of Eigenvector Combinations: Eigenvectors can be combined to form new eigenvectors. This can be used to construct a basis for the vector space spanned by the eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Use of Eigenvectors in Linear Combinations: Eigenvectors can be used to form linear combinations of vectors. This can be used to analyze the behavior of linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8902835790547939
      },
      {
        "text": "The Use of Eigenvectors in Matrix Decomposition: Eigenvectors can be used to decompose a matrix into a product of simpler matrices. This can be used to analyze the behavior of linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8812953471534357
      },
      {
        "text": "Eigenvector Combinations and Orthogonality: Eigenvectors can be combined to form new eigenvectors, and understanding the properties of these combinations is crucial in many applications, including linear programming and optimization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8872840671011094
      }
    ]
  },
  {
    "representative_text": "The Effect of Linear Transformations on Eigenvectors: Linear transformations can affect the eigenvectors of a matrix. This can be used to analyze the behavior of linear systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Effect of Linear Transformations on Eigenvectors: Linear transformations can affect the eigenvectors of a matrix. This can be used to analyze the behavior of linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Role of Orthogonality in Eigenvector Analysis: Orthogonality plays a crucial role in eigenvector analysis. This can be used to analyze the behavior of linear systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Role of Orthogonality in Eigenvector Analysis: Orthogonality plays a crucial role in eigenvector analysis. This can be used to analyze the behavior of linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Relationship between Eigenvalues and the Matrix Power: The eigenvalues of a matrix are related to the power of the matrix. This can be used to analyze the behavior of linear systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Relationship between Eigenvalues and the Matrix Power: The eigenvalues of a matrix are related to the power of the matrix. This can be used to analyze the behavior of linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Eigenvalues and the Matrix Exponentiation: The matrix exponentiation is a method for computing the powers of a matrix. It is related to the computation of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8397547183672579
      },
      {
        "text": "The relationship between eigenvalues and the matrix exponential: The matrix exponential e^(A) is a fundamental concept in a matrix is not explicitly mentioned in theorepsilon",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8262303697924768
      }
    ]
  },
  {
    "representative_text": "The Use of Eigenvectors in Linear Programming: Eigenvectors can be used in linear programming to optimize linear objective functions. This can be used to analyze the behavior of linear systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Use of Eigenvectors in Linear Programming: Eigenvectors can be used in linear programming to optimize linear objective functions. This can be used to analyze the behavior of linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Role of Eigenvectors in Linear Programming: Eigenvectors can be used in linear programming to optimize linear objective functions. Eigenvectors play a crucial role in finding the optimal solution to linear programming problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9487974019529765
      }
    ]
  },
  {
    "representative_text": "Kronecker's Theorem: This theorem states that the eigenvalues of a matrix A are the same as the eigenvalues of the matrix AA^T (A^T is the transpose of A), where A^T is the transpose of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kronecker's Theorem: This theorem states that the eigenvalues of a matrix A are the same as the eigenvalues of the matrix AA^T (A^T is the transpose of A), where A^T is the transpose of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rayleigh's Theorem: This theorem states that if a matrix A has a positive definite matrix, then the sum of the squares of its eigenvalues is greater than or equal to the sum of the squares of its eigenvectors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Rayleigh's Theorem: This theorem states that if a matrix A has a positive definite matrix, then the sum of the squares of its eigenvalues is greater than or equal to the sum of the squares of its eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Rayleigh's Theorem (Alternative Proof): Rayleigh's theorem states that the sum of the squares of the eigenvalues of a matrix is greater than or equal to the sum of the squares of the corresponding eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9379228656387512
      },
      {
        "text": "The Rayleigh's Theorem (Alternative Method): Rayleigh's theorem can be used to prove that the sum of the squares of the eigenvalues of a matrix is greater than or equal to the sum of the squares of the corresponding eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9357738464407475
      },
      {
        "text": "The Rayleigh's Theorem (Alternative Method): Rayleigh's theorem can be used to prove that the sum of the squares of the eigenvalues of a matrix is greater than or equal to the sum of the squares of the corresponding eigenvectors. An alternative method is not mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8753151099776788
      }
    ]
  },
  {
    "representative_text": "The Eigenspace: The eigenspace of a matrix A is the set of all vectors that are mapped to a scaled version of themselves by the linear transformation. It is given by V(λ) = {v: Av = λv}.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Eigenspace: The eigenspace of a matrix A is the set of all vectors that are mapped to a scaled version of themselves by the linear transformation. It is given by V(λ) = {v: Av = λv}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenspace: This is the set of all linear combinations of the eigenvectors of a matrix. The eigenspace is a subspace of the vector space that the matrix operates on.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8734108460462018
      },
      {
        "text": "Eigenspace: Explore eigenspace, which is the set of all linear combinations of the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9095879425319071
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Algebra: This theorem states that every non-constant polynomial equation has at least one complex root.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Algebra: This theorem states that every non-constant polynomial equation has at least one complex root.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Spectral Mapping Theorem: This theorem states that the eigenvalues of a matrix A are the same as the eigenvalues of a matrix B if and only if A and B have the same characteristic polynomial.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Spectral Mapping Theorem: This theorem states that the eigenvalues of a matrix A are the same as the eigenvalues of a matrix B if and only if A and B have the same characteristic polynomial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Spectral Mapping Theorem: This theorem states that if λ is an eigenvalue of matrix A and λ' is a root of the minimal polynomial of A, then λ' is an eigenvalue of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9142291129136053
      },
      {
        "text": "Spectral Mapping Theorem: This theorem states that if A and B are square matrices and λ is an eigenvalue of A, then the eigenvalues of B are the images of the eigenvalues of A under the spectral mapping function f(λ) = λ^p, where p is the dimension of the matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9208925756745946
      },
      {
        "text": "The Spectral Mapping Theorem: The spectral mapping theorem states that if λ is an eigenvalue of matrix A and λ' is a root of the minimal polynomial of A, then λ' is an eigenvalue of A. This theorem is crucial in understanding the properties of a matrix and its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9407351050967363
      },
      {
        "text": "The Spectral Mapping Theorem: This theorem states that the eigenvalues of a linear transformation $T$ map to the eigenvalues of the matrix representation of $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9034870692071226
      }
    ]
  },
  {
    "representative_text": "The Smith Normal Form: This theorem states that a matrix A can be transformed into a diagonal matrix using a similarity transformation, where each diagonal entry is a polynomial in the eigenvalues of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Smith Normal Form: This theorem states that a matrix A can be transformed into a diagonal matrix using a similarity transformation, where each diagonal entry is a polynomial in the eigenvalues of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lagrange Multiplier Method: This method is used to find the eigenvectors of a matrix A by solving a system of equations involving the Lagrange multipliers.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lagrange Multiplier Method: This method is used to find the eigenvectors of a matrix A by solving a system of equations involving the Lagrange multipliers.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Characterization of Nilpotent Matrices: Nilpotent matrices are square matrices N such that N^k = 0 for some positive integer k. They can be characterized using their eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Characterization of Nilpotent Matrices: Nilpotent matrices are square matrices N such that N^k = 0 for some positive integer k. They can be characterized using their eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Polynomials: Orthogonal polynomials are polynomials that are orthogonal to each other with respect to a given inner product. They can be used to approximate the eigenvalues and eigenvectors of a matrix A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Orthogonal Polynomials: Orthogonal polynomials are polynomials that are orthogonal to each other with respect to a given inner product. They can be used to approximate the eigenvalues and eigenvectors of a matrix A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Polynomials: Orthogonal polynomials are a set of mathematical objects that are used to represent the orthogonal behavior of a signal or function. They have applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8057821135252466
      },
      {
        "text": "Orthogonal Polynomials: Orthogonal polynomials are a set of polynomials that are orthogonal to each other with respect to a given inner product. They can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8363056853527457
      },
      {
        "text": "Orthogonal polynomials: A set of polynomials that are orthogonal to each other with respect to a given inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.800294459844912
      },
      {
        "text": "Orthogonal Polynomials: A set of orthogonal polynomials, which are used in linear algebra to solve systems of linear equations and to approximate functions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9045300478968485
      }
    ]
  },
  {
    "representative_text": "Bessel's Inequality for Matrix Norms: Bessel's inequality for matrix norms states that the sum of the squares of the norms of the columns of a matrix A is less than or equal to the sum of the squares of the norms of the rows of A.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Bessel's Inequality for Matrix Norms: Bessel's inequality for matrix norms states that the sum of the squares of the norms of the columns of a matrix A is less than or equal to the sum of the squares of the norms of the rows of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bessel's Inequality for Matrix Norms: This is a variation of Bessel's inequality that deals with matrix norms instead of vector norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8599414552206252
      },
      {
        "text": "Bessel's Inequality for Matrix Norms: Bessel's inequality for matrix norms states that the sum of the squares of the norms of the columns of a matrix A is less than or equal to the sum of the squares of the norms of the rows of A. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9012431461902563
      },
      {
        "text": "Min-Max Theorem for Matrix Norms: This theorem states that the sum of the squares of the matrix norms of the columns of a matrix A is less than or equal to the sum of the squares of the matrix norms of the rows of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8053059528379438
      },
      {
        "text": "Bessel's Inequality for Non-Symmetric Matrices: This is an extension of Bessel's inequality for symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8375614810613834
      }
    ]
  },
  {
    "representative_text": "Rayleigh's Formula for the Eigenvalues of a Matrix: Rayleigh's formula states that the eigenvalues of a matrix A are given by the formula: λ = (Tr(A)) / (n), where n is the dimension of A and Tr(A) is the trace of A.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Rayleigh's Formula for the Eigenvalues of a Matrix: Rayleigh's formula states that the eigenvalues of a matrix A are given by the formula: λ = (Tr(A)) / (n), where n is the dimension of A and Tr(A) is the trace of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bessel's Formula for the Eigenvalues of a Matrix: This formula states that the eigenvalues of a matrix A are given by the formula: λ = (Tr(A)) / (n), where n is the dimension of A and Tr(A) is the trace of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8701640178464735
      },
      {
        "text": "Rayleigh's Formula for Eigenvalues: Rayleigh's formula states that the eigenvalues of a matrix A are given by the formula: λ = (Tr(A)) / (n), where n is the dimension of A and Tr(A) is the trace of A. This is an important concept in linear algebra, as it can be used to estimate the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9337898362722132
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and Linear Algebraic Identities: The study of orthogonal complements involves exploring linear algebraic identities that relate to orthogonality. These identities can be used to simplify expressions and prove theorems in linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and Linear Algebraic Identities: The study of orthogonal complements involves exploring linear algebraic identities that relate to orthogonality. These identities can be used to simplify expressions and prove theorems in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and Category Theory: The orthogonal complement of a subspace can be viewed as a concept in category theory, which studies the properties of mathematical structures and their relationships. The study of orthogonal complements can provide insights into the category theory of linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and Category Theory: The orthogonal complement of a subspace can be viewed as a concept in category theory, which studies the properties of mathematical structures and their relationships. The study of orthogonal complements can provide insights into the category theory of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and Intersections of Subspaces: The study of orthogonal complements involves exploring the intersections of subspaces, which can be used to prove theorems and simplify expressions in linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and Intersections of Subspaces: The study of orthogonal complements involves exploring the intersections of subspaces, which can be used to prove theorems and simplify expressions in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Complement and Sum of Subspaces: The study of orthogonal complements involves exploring the sum of subspaces, which can be used to prove theorems and simplify expressions in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9270119467463247
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and Orthogonal Decomposition of Tensor Products: The orthogonal complement of a subspace is closely related to the orthogonal decomposition of tensor products, which can be used to study the properties of tensor products in linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and Orthogonal Decomposition of Tensor Products: The orthogonal complement of a subspace is closely related to the orthogonal decomposition of tensor products, which can be used to study the properties of tensor products in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonality of Orthogonal Matrices and Linear Transformations: The properties of orthogonal matrices and their relationship with linear transformations, including the preservation of the inner product and the fact that orthogonal matrices represent linear isomorphisms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonality of Orthogonal Matrices and Linear Transformations: The properties of orthogonal matrices and their relationship with linear transformations, including the preservation of the inner product and the fact that orthogonal matrices represent linear isomorphisms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Matrices and Their Applications: Studying orthogonal matrices, including their properties and applications in linear algebra, and how they relate to linear transformations and matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8611824226192357
      }
    ]
  },
  {
    "representative_text": "Characterization of Orthogonal Matrices: The characterization of orthogonal matrices, including the fact that they can be represented as a product of Householder transformations or Givens rotations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Characterization of Orthogonal Matrices: The characterization of orthogonal matrices, including the fact that they can be represented as a product of Householder transformations or Givens rotations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of Orthogonal Diagonalization Algorithms: The numerical stability of various algorithms for orthogonal diagonalization, including the QR algorithm and the Jacobi method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Stability of Orthogonal Diagonalization Algorithms: The numerical stability of various algorithms for orthogonal diagonalization, including the QR algorithm and the Jacobi method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Applications of Orthogonal Diagonalization in Signal Processing and Data Analysis: The applications of orthogonal diagonalization in signal processing and data analysis, including the use of orthogonal diagonalization for filtering out noise from signals and reducing the dimensionality of high-dimensional data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Applications of Orthogonal Diagonalization in Signal Processing and Data Analysis: The applications of orthogonal diagonalization in signal processing and data analysis, including the use of orthogonal diagonalization for filtering out noise from signals and reducing the dimensionality of high-dimensional data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Applications of Orthogonal Diagonalization in Machine Learning and Statistics: The applications of orthogonal diagonalization in machine learning and statistics, including the use of orthogonal diagonalization for dimensionality reduction and feature extraction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8786124078914671
      },
      {
        "text": "Applications of Orthogonal Diagonalization in Signal Processing and Data Analysis (Advanced): Orthogonal diagonalization has various applications in signal processing and data analysis, including the use of orthogonal diagonalization for filtering out noise from signals and reducing the dimensionality of high-dimensional data, which can be advanced using techniques such as wavelet analysis and independent component analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8669126973282701
      },
      {
        "text": "Applications of Orthogonal Diagonalization in Machine Learning and Statistics (Advanced): Orthogonal diagonalization has various applications in machine learning and statistics, including the use of orthogonal diagonalization for dimensionality reduction and feature extraction, which can be advanced using techniques such as principal component analysis and independent component analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9396697743200255
      }
    ]
  },
  {
    "representative_text": "Computational Complexity of Finding Orthogonal Diagonalization Conditions: The computational complexity of various algorithms for finding the conditions under which a matrix can be orthogonally diagonalized.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Computational Complexity of Finding Orthogonal Diagonalization Conditions: The computational complexity of various algorithms for finding the conditions under which a matrix can be orthogonally diagonalized.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computational Complexity of Orthogonal Diagonalization Algorithms: The computational complexity of orthogonal diagonalization algorithms, which can vary depending on the algorithm used and the size of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8828634758635552
      }
    ]
  },
  {
    "representative_text": "Orthogonality of the Gram-Schmidt Process: The Gram-Schmidt process is a method for orthonormalizing a set of orthogonal vectors. However, it is not explicitly stated whether the Gram-Schmidt process preserves orthogonality.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Orthogonality of the Gram-Schmidt Process: The Gram-Schmidt process is a method for orthonormalizing a set of orthogonal vectors. However, it is not explicitly stated whether the Gram-Schmidt process preserves orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Gram-Schmidt Process and Orthogonality: The Gram-Schmidt process is a method for orthonormalizing a set of orthogonal vectors. However, it is necessary to explore whether the Gram-Schmidt process preserves orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9381042603970242
      },
      {
        "text": "The Schmidt Process and Orthogonality: The Schmidt process is a method used to orthogonalize a vector. However, it is necessary to explore whether the Schmidt process preserves orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9013682568658739
      },
      {
        "text": "The Schmidt Process and Orthogonalization: The Schmidt process is used to orthogonalize a vector. However, it is essential to explore whether this process can be generalized to orthogonalize matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9143688142695564
      },
      {
        "text": "Orthonormalization of a Matrix using the Gram-Schmidt Process: This process is used to orthonormalize a set of orthogonal vectors. However, it is not explicitly stated whether the Gram-Schmidt process preserves orthogonality when applied to a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8554121649625196
      }
    ]
  },
  {
    "representative_text": "The Inverse of an Orthogonal Matrix: Given an orthogonal matrix A, it is not explicitly stated whether the inverse of A is also orthogonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Inverse of an Orthogonal Matrix: Given an orthogonal matrix A, it is not explicitly stated whether the inverse of A is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Inverse of an Orthogonal Matrix: Given an orthogonal matrix A, it is necessary to investigate whether the inverse of A is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.891652697719719
      },
      {
        "text": "Properties of the Inverse of an Orthogonal Matrix: Given an orthogonal matrix A, it is necessary to investigate the properties of its inverse and whether it is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9403856376882169
      }
    ]
  },
  {
    "representative_text": "The Orthogonality of Orthogonal Complements: Given a vector u and a subspace V, it is not explicitly stated whether the orthogonal complement of u with respect to V is also orthogonal.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Orthogonality of Orthogonal Complements: Given a vector u and a subspace V, it is not explicitly stated whether the orthogonal complement of u with respect to V is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonality of Orthogonal Complements: Given a vector u and a subspace V, it is essential to verify whether the orthogonal complement of u with respect to V is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8889595772937152
      }
    ]
  },
  {
    "representative_text": "The Orthogonalization of a Matrix using the QR Decomposition: The QR decomposition is a factorization of a matrix A into the product A = QR, where Q is an orthogonal matrix and R is an upper triangular matrix. However, it is not explicitly stated whether this decomposition is also orthogonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Orthogonalization of a Matrix using the QR Decomposition: The QR decomposition is a factorization of a matrix A into the product A = QR, where Q is an orthogonal matrix and R is an upper triangular matrix. However, it is not explicitly stated whether this decomposition is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Properties of the QR Decomposition: The QR decomposition is a factorization of a matrix A into the product A = QR, where Q is an orthogonal matrix and R is an upper triangular matrix. However, it is essential to explore the properties of the QR decomposition and its relationship to orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8932682328974186
      },
      {
        "text": "The QR Decomposition and its relation to Orthogonal Diagonalization: The QR decomposition is a factorization of a matrix into the product of an orthogonal matrix and an upper triangular matrix. This decomposition is related to orthogonal diagonalization as it provides a way to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8425763857350523
      }
    ]
  },
  {
    "representative_text": "The Orthogonality of the Eigenvalue Decomposition: Given a matrix A, the eigenvalue decomposition is a factorization of A into the product A = U Λ U^T, where U is an orthogonal matrix and Λ is a diagonal matrix containing the eigenvalues. However, it is not explicitly stated whether this decomposition is also orthogonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Orthogonality of the Eigenvalue Decomposition: Given a matrix A, the eigenvalue decomposition is a factorization of A into the product A = U Λ U^T, where U is an orthogonal matrix and Λ is a diagonal matrix containing the eigenvalues. However, it is not explicitly stated whether this decomposition is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Orthogonality of the Spectral Theorem: The spectral theorem states that a matrix A can be diagonalized by an orthogonal matrix, i.e., A = U Λ U^T, where U is an orthogonal matrix and Λ is a diagonal matrix containing the eigenvalues of A. However, it is not explicitly stated whether this theorem implies that U is orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8286875033679834
      },
      {
        "text": "Orthogonality of the Eigenvalue Decomposition: The eigenvalue decomposition is a factorization of a matrix A into the product A = U Λ U^T, where U is an orthogonal matrix and Λ is a diagonal matrix containing the eigenvalues. It is necessary to verify whether this decomposition is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8497882695981347
      },
      {
        "text": "The Spectral Theorem and Orthogonality: The spectral theorem states that a matrix A can be diagonalized by an orthogonal matrix, i.e., A = U Λ U^T, where U is an orthogonal matrix and Λ is a diagonal matrix containing the eigenvalues of A. It is crucial to verify whether this theorem implies that U is orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8541965990728816
      },
      {
        "text": "Eigenvalue Decomposition and Spectral Theorem: Eigenvalue decomposition is a method for decomposing a matrix into a product of orthogonal matrices and diagonal matrices. The Spectral Theorem states that every square matrix can be diagonalized using an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8410525321774732
      }
    ]
  },
  {
    "representative_text": "The Orthogonalization of a Matrix using the Householder Transformation: The Householder transformation is a mathematical operation used to orthogonalize a vector. However, it is not explicitly stated whether this transformation can be generalized to orthogonalize matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Orthogonalization of a Matrix using the Householder Transformation: The Householder transformation is a mathematical operation used to orthogonalize a vector. However, it is not explicitly stated whether this transformation can be generalized to orthogonalize matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonalization of a Matrix using the Householder Transformation: The Householder transformation is a mathematical operation used to orthogonalize a vector. However, it is essential to explore whether this transformation can be generalized to orthogonalize matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.928714091346944
      },
      {
        "text": "The Householder Transformation and Orthogonality: The Householder transformation is a mathematical operation used to orthogonalize a vector. However, it is essential to explore whether this transformation can be generalized to orthogonalize matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9618119585524019
      }
    ]
  },
  {
    "representative_text": "The Orthogonality of the Polar Decomposition: Given a matrix A, the polar decomposition is a factorization of A into the product A = U ||A|| V, where U is an orthogonal matrix and V is a vector in the range of A. However, it is not explicitly stated whether this decomposition is also orthogonal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Orthogonality of the Polar Decomposition: Given a matrix A, the polar decomposition is a factorization of A into the product A = U ||A|| V, where U is an orthogonal matrix and V is a vector in the range of A. However, it is not explicitly stated whether this decomposition is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonality of the Polar Decomposition: The polar decomposition is a factorization of a matrix A into the product A = U ||A|| V, where U is an orthogonal matrix and V is a vector in the range of A. It is necessary to investigate whether this decomposition is also orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9165693642105988
      },
      {
        "text": "The Polar Decomposition and Orthogonality: The polar decomposition is a factorization of a matrix A into the product A = U ||A|| V, where U is an orthogonal matrix and V is a vector in the range of A. However, it is essential to explore the properties of the polar decomposition and its relationship to orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9680392743294317
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt Extension to Hilbert Spaces: The Gram-Schmidt process can be extended to Hilbert spaces, which are infinite-dimensional vector spaces with an inner product.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt Extension to Hilbert Spaces: The Gram-Schmidt process can be extended to Hilbert spaces, which are infinite-dimensional vector spaces with an inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gram-Schmidt Process with Non-Standard Inner Products for Hilbert Spaces: This includes the extension of the Gram-Schmidt process to Hilbert spaces, which are infinite-dimensional vector spaces with an inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.913502995918815
      },
      {
        "text": "Gram-Schmidt Process for Hilbert Spaces with Non-Standard Inner Products: Extend the Gram-Schmidt process to Hilbert spaces with non-standard inner products, such as the Frobenius norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8619760844134964
      },
      {
        "text": "Gram-Schmidt Process for Non-Standard Inner Product Spaces: Extend the Gram-Schmidt process to inner product spaces that are not the standard Euclidean space, such as spaces with different norms or non-standard inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9089057452643874
      },
      {
        "text": "Gram-Schmidt Process for Non-standard Inner Products in Banach Spaces: Extend the Gram-Schmidt process to inner products that are not the standard inner product, such as the Frobenius norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9318528327977016
      },
      {
        "text": "Gram-Schmidt Process for Non-Linear Hilbert Spaces: Apply the Gram-Schmidt process to non-linear Hilbert spaces, which can be represented by non-linear functions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8930868909674397
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt Process for Non-Convex Optimization: The Gram-Schmidt process can be used to solve non-convex optimization problems, where the goal is to find the optimal solution among a set of possible solutions.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt Process for Non-Convex Optimization: The Gram-Schmidt process can be used to solve non-convex optimization problems, where the goal is to find the optimal solution among a set of possible solutions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gram-Schmidt Process for Non-Convex Optimization: This is the use of the Gram-Schmidt process to solve non-convex optimization problems, where the goal is to find the optimal solution among a set of possible solutions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9364078423134947
      },
      {
        "text": "Gram-Schmidt Process for Non-Convex Optimization: This topic examines the use of the Gram-Schmidt process to solve non-convex optimization problems, which can be used in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9416615591916266
      },
      {
        "text": "Gram-Schmidt Process for Non-Convex Optimization in Hilbert Spaces: This involves using the Gram-Schmidt process to solve non-convex optimization problems in Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9225984310333275
      },
      {
        "text": "Gram-Schmidt Process for Non-convex Optimization in Banach Spaces: Investigate the application of the Gram-Schmidt process to non-convex optimization problems in Banach spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.909776733777008
      },
      {
        "text": "Gram-Schmidt Process for Non-convex Optimization in Non-standard Inner Product Spaces: Explore the application of the Gram-Schmidt process to non-convex optimization problems in non-standard inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9207933732427487
      },
      {
        "text": "Gram-Schmidt Process for Non-Convex Optimization Problems: Use the Gram-Schmidt process to solve non-convex optimization problems, which can be challenging to solve using traditional optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9631497278390055
      },
      {
        "text": "Gram-Schmidt Process for Non-Convex Optimization Problems in Hilbert Spaces: Use the Gram-Schmidt process to solve non-convex optimization problems in Hilbert spaces, which can be challenging to solve using traditional optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9323805867914173
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt Process for Non-Symmetric Matrices: The Gram-Schmidt process can be used to orthogonalize non-symmetric matrices, which are matrices that do not have the same transpose as their conjugate transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt Process for Non-Symmetric Matrices: The Gram-Schmidt process can be used to orthogonalize non-symmetric matrices, which are matrices that do not have the same transpose as their conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gram-Schmidt Process for Non-Orthogonal Projection Matrices: The Gram-Schmidt process can be used to orthogonalize non-orthogonal projection matrices, which are matrices that project vectors onto a subspace, but do not preserve the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9115199725072743
      },
      {
        "text": "Gram-Schmidt Process for Non-Orthogonal Projection Matrices: This is the use of the Gram-Schmidt process to orthogonalize non-orthogonal projection matrices, which are matrices that project vectors onto a subspace, but do not preserve the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9123558442734061
      },
      {
        "text": "Gram-Schmidt Process for Non-Symmetric Matrices: This is the use of the Gram-Schmidt process to orthogonalize non-symmetric matrices, which are matrices that do not have the same transpose as their conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9403704631263343
      },
      {
        "text": "Gram-Schmidt Process for Non-Orthogonal Projection Matrices: This topic provides a more general framework for orthogonalizing non-orthogonal projection matrices, which can be used in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8949311765575607
      },
      {
        "text": "Gram-Schmidt Process for Non-Symmetric Matrices: This topic explores the use of the Gram-Schmidt process to orthogonalize non-symmetric matrices, which can be used in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9104962851289728
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt Process for Non-Linear Vector Spaces: The Gram-Schmidt process can be used to orthogonalize vectors in non-linear vector spaces, which are vector spaces that do not have a linear structure.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt Process for Non-Linear Vector Spaces: The Gram-Schmidt process can be used to orthogonalize vectors in non-linear vector spaces, which are vector spaces that do not have a linear structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gram-Schmidt Process for Non-Linear Vector Spaces: This is the use of the Gram-Schmidt process to orthogonalize vectors in non-linear vector spaces, which are vector spaces that do not have a linear structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9302724212561997
      },
      {
        "text": "Gram-Schmidt Process for Non-Linear Vector Spaces: This topic examines the use of the Gram-Schmidt process to orthogonalize vectors in non-linear vector spaces, which can be used in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9203897216610553
      },
      {
        "text": "Gram-Schmidt Process for Non-Linear Hilbert Spaces: This involves using the Gram-Schmidt process to orthogonalize vectors in non-linear Hilbert spaces, which are Hilbert spaces that do not have a linear structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9292865512915502
      },
      {
        "text": "Gram-Schmidt Process for Non-linear Vector Spaces: Explore the application of the Gram-Schmidt process to non-linear vector spaces, where the data can be represented by non-linear functions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8950620665853701
      },
      {
        "text": "Gram-Schmidt Process for Non-linear Hilbert Spaces: Explore the application of the Gram-Schmidt process to non-linear Hilbert spaces, where the data can be represented by non-linear functions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8857317970706597
      },
      {
        "text": "Gram-Schmidt Process for Non-Linear Data Analysis: Apply the Gram-Schmidt process to analyze non-linear data, which can be represented by non-linear functions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.879857004145943
      }
    ]
  },
  {
    "representative_text": "Inversion of a Matrix using Cofactor Expansion: While the inverse of a matrix can be calculated using the adjoint matrix, there are also methods that use cofactor expansion to calculate the inverse of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inversion of a Matrix using Cofactor Expansion: While the inverse of a matrix can be calculated using the adjoint matrix, there are also methods that use cofactor expansion to calculate the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Determinant of Zero: There are some properties and relationships that can be derived for determinants of matrices with a determinant of zero, such as the determinant of a matrix with a determinant of zero being zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Determinant of Zero: There are some properties and relationships that can be derived for determinants of matrices with a determinant of zero, such as the determinant of a matrix with a determinant of zero being zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with a Non-Diagonal Dominant Matrix: There are some properties and relationships that can be derived for determinants of matrices with non-diagonal dominant matrices, such as the determinant of a matrix with a non-diagonal dominant matrix being zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8543301911904297
      },
      {
        "text": "Determinant of a Matrix with a Large Number of Variables: There are some properties and relationships that can be derived for determinants of matrices with a large number of variables, such as the determinant of a matrix with a large number of variables being zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8559586999957598
      },
      {
        "text": "Properties of Determinants for Non-Square Matrices: There are some properties and relationships that can be derived for determinants of non-square matrices, such as the determinant of a non-square matrix being zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8972477540521621
      },
      {
        "text": "Determinant of a Matrix with a Determinant of Zero: There are some properties and relationships that can be derived for determinants of matrices with a determinant of zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.909760975499369
      },
      {
        "text": "Determinant of a Matrix with a Non-Diagonal Dominant Matrix and a Determinant of Zero: There are some properties and relationships that can be derived for determinants of matrices with non-diagonal dominant matrices and a determinant of zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9290758635763186
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Decomposition: Eigenvalue decomposition is a factorization method that can be used to decompose a matrix into the product of three matrices: P, Λ, and P^(-1). The determinant of a matrix can be related to the eigenvalues in the eigenvalue decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Decomposition: Eigenvalue decomposition is a factorization method that can be used to decompose a matrix into the product of three matrices: P, Λ, and P^(-1). The determinant of a matrix can be related to the eigenvalues in the eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Methods for Calculating Determinants: There are some numerical methods that can be used to calculate the determinant of a matrix, such as the Gauss-Jordan elimination method and the LU decomposition method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Numerical Methods for Calculating Determinants: There are some numerical methods that can be used to calculate the determinant of a matrix, such as the Gauss-Jordan elimination method and the LU decomposition method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Techniques for Determinants: This includes various numerical techniques, such as the use of iterative methods or approximation algorithms, to calculate the determinant of a large matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8510385065295422
      },
      {
        "text": "Numerical Methods for Calculating Determinants of Large Matrices: There are some numerical methods that can be used to calculate the determinant of large matrices, such as the LU decomposition method and the Cholesky decomposition method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9306864010040414
      },
      {
        "text": "Advanced Numerical Methods for Calculating Determinants: There are some advanced numerical methods that can be used to calculate the determinant of a matrix, such as the QR algorithm and the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9129117838231292
      },
      {
        "text": "Computational Techniques for Determinants with Large Matrices: There are various numerical techniques that can be used to calculate the determinant of large matrices, including iterative methods, approximation algorithms, and parallel computing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9364734332856455
      },
      {
        "text": "Computational Techniques for Determinants with Non-Square Matrices: There are various numerical techniques that can be used to calculate the determinant of non-square matrices, including iterative methods, approximation algorithms, and parallel computing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8461484084819874
      },
      {
        "text": "Computational Techniques for Determinants with Non-Linear Transformations: There are various numerical techniques that can be used to calculate the determinant of matrices with non-linear transformations, including iterative methods, approximation algorithms, and parallel computing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.847649878486272
      },
      {
        "text": "Numerical Methods for Calculating Determinants of Non-Square Matrices: There are some numerical methods that can be used to calculate the determinant of non-square matrices, such as the Gauss-Jordan elimination method and the LU decomposition method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8976347555737532
      },
      {
        "text": "Advanced Numerical Methods for Calculating Determinants of Large Matrices: There are some advanced numerical methods that can be used to calculate the determinant of large matrices, such as the QR algorithm and the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9262351071311635
      },
      {
        "text": "Numerical Methods for Calculating Determinants of High-Dimensional Matrices: There are some numerical methods that can be used to calculate the determinant of high-dimensional matrices, such as the QR algorithm and the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9275703296563715
      }
    ]
  },
  {
    "representative_text": "Sylvester's Law of Inertia for Non-Square Matrices: Sylvester's law of inertia can be extended to non-square matrices by using the cofactor expansion along a row or column.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sylvester's Law of Inertia for Non-Square Matrices: Sylvester's law of inertia can be extended to non-square matrices by using the cofactor expansion along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Perron-Frobenius Theorem for Non-Square Matrices: The Perron-Frobenius theorem can be extended to non-square matrices by using the eigenvalue decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Perron-Frobenius Theorem for Non-Square Matrices: The Perron-Frobenius theorem can be extended to non-square matrices by using the eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gershgorin Circle Theorem for Non-Square Matrices: The Gershgorin circle theorem can be extended to non-square matrices by using the eigenvalue decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Gershgorin Circle Theorem for Non-Square Matrices: The Gershgorin circle theorem can be extended to non-square matrices by using the eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gerschgorin's Theorem for Non-Square Matrices: The extension of Gerschgorin's theorem to non-square matrices, which describes the location of eigenvalues of a non-square matrix based on the elements of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8942366713755137
      }
    ]
  },
  {
    "representative_text": "Jacobi's Formula for Non-Square Matrices: Jacobi's formula can be extended to non-square matrices by using the cofactor expansion along a row or column.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jacobi's Formula for Non-Square Matrices: Jacobi's formula can be extended to non-square matrices by using the cofactor expansion along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Laplace's Theorem for Non-Square Matrices: Laplace's theorem can be extended to non-square matrices by using the cofactor expansion along a row or column.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Laplace's Theorem for Non-Square Matrices: Laplace's theorem can be extended to non-square matrices by using the cofactor expansion along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cauchy-Binet Formula for Non-Square Matrices: The Cauchy-Binet formula can be extended to non-square matrices by using the cofactor expansion along a row or column.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cauchy-Binet Formula for Non-Square Matrices: The Cauchy-Binet formula can be extended to non-square matrices by using the cofactor expansion along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Schur's Theorem for Non-Square Matrices: Schur's theorem can be extended to non-square matrices by using the eigenvalue decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Schur's Theorem for Non-Square Matrices: Schur's theorem can be extended to non-square matrices by using the eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Schur's Triangularization Theorem for Non-Square Matrices: The extension of Schur's triangularization theorem to non-square matrices, which states that any non-square matrix can be triangularized.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8748940260892156
      }
    ]
  },
  {
    "representative_text": "Nakayama's Lemma: This is a theorem that states that if a matrix A satisfies the condition that the product of A and its right null space is zero, then A has an inverse if and only if its determinant is non-zero.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Nakayama's Lemma: This is a theorem that states that if a matrix A satisfies the condition that the product of A and its right null space is zero, then A has an inverse if and only if its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gelfand's Theorem: This theorem states that if a matrix A is invertible, then its determinant can be expressed as a sum of products of its eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gelfand's Theorem: This theorem states that if a matrix A is invertible, then its determinant can be expressed as a sum of products of its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Rank: This is a concept that is closely related to determinants, as the rank of a matrix is equal to the number of non-zero eigenvalues (or the number of linearly independent rows/columns).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Matrix Rank: This is a concept that is closely related to determinants, as the rank of a matrix is equal to the number of non-zero eigenvalues (or the number of linearly independent rows/columns).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The relationship between the eigenvalues of a matrix and its rank: The rank of a matrix is equal to the number of non-zero eigenvalues, but this is not explicitly mentioned as a theorem or property.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8064268754654724
      },
      {
        "text": "Relationship between Determinants and Matrix Rank: The determinant of a matrix is related to its rank, and there are some theorems that establish this relationship.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8324278590053293
      },
      {
        "text": "The Connection between Matrix Rank and Determinant: The relationship between the rank of a matrix and its determinant is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8795085981859574
      }
    ]
  },
  {
    "representative_text": "Theorem on the Determinant of a Matrix with a Zero Eigenvalue: This theorem states that if a matrix A has a zero eigenvalue, then its determinant is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Theorem on the Determinant of a Matrix with a Zero Eigenvalue: This theorem states that if a matrix A has a zero eigenvalue, then its determinant is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem on the Determinant of a Matrix with a Non-Zero Eigenvalue: This theorem states that if a matrix A has a non-zero eigenvalue, then its determinant is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8757652714126432
      },
      {
        "text": "Theorem on the Determinant of a Matrix with a Zero Column: This theorem states that if a matrix has a zero column, then its determinant is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8824483955601337
      },
      {
        "text": "Gershgorin's Theorem: This theorem states that if a matrix has a non-zero determinant, then it has a non-zero eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8531547279764212
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Non-Orthogonal Basis: The determinant of a matrix with a non-orthogonal basis can be computed using the Gram-Schmidt process.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Basis: The determinant of a matrix with a non-orthogonal basis can be computed using the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with a Non-Orthogonal Basis: The determinant of a matrix with a non-orthogonal basis can be calculated using the formula for the determinant of a matrix with a non-orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8767636355807189
      },
      {
        "text": "Determinant of a Matrix with Non-Standard Basis: The determinant of a matrix with non-standard basis can be calculated using the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9301143251166751
      }
    ]
  },
  {
    "representative_text": "Application of Determinants and Inverses in Machine Learning: Determinants and inverses are used in machine learning algorithms such as principal component analysis and singular value decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Application of Determinants and Inverses in Machine Learning: Determinants and inverses are used in machine learning algorithms such as principal component analysis and singular value decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The use of determinants in machine learning: Determinants are used in machine learning to analyze and design machine learning algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8853898298853404
      }
    ]
  },
  {
    "representative_text": "Computing Determinants using QR Decomposition: The process of decomposing a matrix into orthogonal (Q) and upper triangular (R) matrices, and using this decomposition to compute the determinant.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "Computing Determinants using QR Decomposition: The process of decomposing a matrix into orthogonal (Q) and upper triangular (R) matrices, and using this decomposition to compute the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Determinant of a Matrix with a Given Rank and Condition Number using the QR Algorithm: A method for computing the determinant of a matrix with a given rank and condition number using the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.833296075450929
      },
      {
        "text": "The Determinant of a Matrix with a Given Rank and Determinant using the LU Decomposition: A method for computing the determinant of a matrix with a given rank and determinant using the LU decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8799016709028591
      },
      {
        "text": "The Determinant of a Matrix with a Given Rank and Determinant using the Schur Decomposition: A method for computing the determinant of a matrix with a given rank and determinant using the Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9318592684499873
      },
      {
        "text": "The Relationship between the Determinant of a Matrix and its Schur Decomposition: A method for computing the determinant of a matrix using its Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9027897873387154
      },
      {
        "text": "The Relationship between the Determinant of a Matrix and its QR Decomposition: A method for computing the determinant of a matrix using its QR decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9367408327453736
      },
      {
        "text": "The Determinant of a Matrix Product using the Schur Decomposition Method: A method for computing the determinant of a matrix product using the Schur decomposition method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9211449859275072
      },
      {
        "text": "The Determinant of a Matrix Product using the LU Decomposition Method: A method for computing the determinant of a matrix product using the LU decomposition method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9082479405225385
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the QR Algorithm: A method for computing the determinant of a matrix product using the inversion of a matrix using the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9013395150045174
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Schur Decomposition with a Given Initial Guess: A method for computing the determinant of a matrix product using the inversion of a matrix using the Schur decomposition with a given initial guess.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9244262036577185
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the LU Decomposition with a Given Initial Guess: A method for computing the determinant of a matrix product using the inversion of a matrix using the LU decomposition with a given initial guess.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9120332269719262
      },
      {
        "text": "Computing Determinants using QR Decomposition with a Given Initial Guess: The QR decomposition method can be used to compute the determinant of a matrix with a given initial guess, which can be useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.832952076133427
      }
    ]
  },
  {
    "representative_text": "Determinants and the Jordan Canonical Form: The relationship between the determinant of a matrix and its Jordan canonical form, including the properties of the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinants and the Jordan Canonical Form: The relationship between the determinant of a matrix and its Jordan canonical form, including the properties of the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Inverse of a Matrix using the QR Algorithm: The process of computing the inverse of a matrix using the QR algorithm, which is an iterative method for computing the inverse of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "The Inverse of a Matrix using the QR Algorithm: The process of computing the inverse of a matrix using the QR algorithm, which is an iterative method for computing the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Inverse of a Matrix using the Moore-Penrose Inverse: The process of computing the inverse of a matrix using the Moore-Penrose inverse, which is a generalization of the inverse matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8202289041446322
      },
      {
        "text": "Matrix Inversion using the QR Algorithm: An iterative method for finding the inverse of a matrix by using the QR algorithm to find the eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.884497729806892
      },
      {
        "text": "The Inversion of a Matrix using the QR Algorithm with a Given Initial Guess: A method for computing the inverse of a matrix using the QR algorithm, which allows for a given initial guess.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9018722577103284
      },
      {
        "text": "The Inversion of a Matrix with a Non-Standard Linear Transformation using the QR Algorithm: A method for computing the inverse of a matrix with a non-standard linear transformation using the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8805949173169865
      },
      {
        "text": "The Inversion of a Matrix with a Non-Standard Linear Independence using the QR Algorithm: A method for computing the inverse of a matrix with non-standard linear independence using the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8849885067312188
      },
      {
        "text": "The Inversion of a Matrix with a Non-Standard Eigenvalue using the QR Algorithm: A method for computing the inverse of a matrix with a non-standard eigenvalue using the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9298212899580724
      },
      {
        "text": "The Inversion of a Matrix with a Non-Standard Singular Value using the QR Algorithm: A method for computing the inverse of a matrix with a non-standard singular value using the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.928096091673275
      },
      {
        "text": "The Inversion of a Matrix with a Non-Standard Jordan Canonical Form using the QR Algorithm: A method for computing the inverse of a matrix with a non-standard Jordan canonical form using the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9153475514621225
      },
      {
        "text": "The Inversion of a Matrix with a Non-Standard Spectral Theorem using the QR Algorithm: A method for computing the inverse of a matrix with a non-standard spectral theorem using the QR algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9240446188412855
      },
      {
        "text": "Computing Determinants using the Inversion of a Matrix using the QR Algorithm: The QR algorithm is an iterative method for computing the inverse of a matrix, which can be useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8649567774772293
      }
    ]
  },
  {
    "representative_text": "Determinants and the Spectral Theorem: The relationship between the determinant of a matrix and its spectral theorem, including the properties of the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Determinants and the Spectral Theorem: The relationship between the determinant of a matrix and its spectral theorem, including the properties of the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinants and the Singular Value Decomposition: The relationship between the determinant of a matrix and its singular value decomposition, including the properties of the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8406244849783355
      }
    ]
  },
  {
    "representative_text": "Complex Eigenvalues and Eigenvectors: The relationship between complex eigenvalues and eigenvectors, including the concept of conjugate eigenvectors and the properties of complex matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Complex Eigenvalues and Eigenvectors: The relationship between complex eigenvalues and eigenvectors, including the concept of conjugate eigenvectors and the properties of complex matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue and Eigenvector Properties for Complex Eigenvalues: The properties and behavior of complex eigenvalues and eigenvectors, including their relationships with the matrix exponential and the Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8682861982263461
      }
    ]
  },
  {
    "representative_text": "Matrix Inversion using Polar Decomposition: The relationship between matrix inversion and polar decomposition, including the concept of the polar decomposition of a matrix and its applications in matrix inversion.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Matrix Inversion using Polar Decomposition: The relationship between matrix inversion and polar decomposition, including the concept of the polar decomposition of a matrix and its applications in matrix inversion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inversion using Polar Decomposition: A method for computing the inverse of a matrix using polar decomposition, which is a factorization of the matrix into a product of an orthogonal matrix and a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9031133246017597
      },
      {
        "text": "Polar Decomposition: This is a factorization of a matrix into the product of a positive semidefinite matrix and an orthogonal matrix. This decomposition can be used to find the inverse of a matrix, as the orthogonal matrix can be inverted easily.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8948405789646128
      }
    ]
  },
  {
    "representative_text": "Eigendecomposition of a Matrix with Non-Orthogonal Eigenvectors: The calculation of the eigendecomposition of a matrix with non-orthogonal eigenvectors, including the concept of the eigendecomposition of a matrix with non-orthogonal eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Eigendecomposition of a Matrix with Non-Orthogonal Eigenvectors: The calculation of the eigendecomposition of a matrix with non-orthogonal eigenvectors, including the concept of the eigendecomposition of a matrix with non-orthogonal eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvector Decomposition with Non-Unitary Eigenvectors: Understanding the properties of eigenvector decomposition with non-unitary eigenvectors is essential in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8247189781015082
      },
      {
        "text": "Eigenvector Decomposition with a Non-Unitary Basis: Understanding the properties of eigenvector decomposition with a non-unitary basis is essential in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9254503693665047
      }
    ]
  },
  {
    "representative_text": "Spectral Radius and Matrix Convergence: The concept of the spectral radius and its relationship to matrix convergence, including the idea that a matrix converges to a diagonal matrix if and only if its spectral radius is less than 1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Spectral Radius and Matrix Convergence: The concept of the spectral radius and its relationship to matrix convergence, including the idea that a matrix converges to a diagonal matrix if and only if its spectral radius is less than 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Spectral Radius and Matrix Convergence: A concept that describes the convergence of a matrix to a diagonal matrix based on its spectral radius.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9032664885934687
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Perturbation Theory: The theory of eigenvalue and eigenvector perturbation, including the concept of perturbation of eigenvalues and eigenvectors and its applications in numerical linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Perturbation Theory: The theory of eigenvalue and eigenvector perturbation, including the concept of perturbation of eigenvalues and eigenvectors and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Perturbation Theory for Eigenvalues and Eigenvectors: A theory of perturbation of eigenvalues and eigenvectors, which is an extension of the power method and other eigenvalue computation techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8817755149979943
      },
      {
        "text": "Perturbation Theory for Eigenvalues of Non-Symmetric Matrices: This is an extension of the perturbation theory for eigenvalues of symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.800191454909421
      }
    ]
  },
  {
    "representative_text": "Computational Complexity of Eigenvalue and Eigenvector Computation: The analysis of the computational complexity of eigenvalue and eigenvector computation, including the time and space complexity of various algorithms for computing eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Computational Complexity of Eigenvalue and Eigenvector Computation: The analysis of the computational complexity of eigenvalue and eigenvector computation, including the time and space complexity of various algorithms for computing eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theoretical Results on the Computational Complexity of Eigenvalue Computation: This includes the study of the theoretical results on the computational complexity of eigenvalue computation, such as the upper and lower bounds on the computational complexity of eigenvalue computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8634091203220073
      },
      {
        "text": "Computational Complexity of Power Method Computation: This includes the analysis of the computational complexity of algorithms for computing eigenvalues and eigenvectors using the power method, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8761525239562947
      },
      {
        "text": "Computational Complexity of QR Algorithm Computation: This includes the analysis of the computational complexity of algorithms for computing eigenvalues and eigenvectors using the QR algorithm, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9037939765672521
      },
      {
        "text": "Computational Complexity of Jacobi-Davidson Algorithm Computation: This includes the analysis of the computational complexity of algorithms for computing eigenvalues and eigenvectors using the Jacobi-Davidson algorithm, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8959727003078322
      },
      {
        "text": "Computational Complexity of Schur Decomposition Computation: The computational complexity of algorithms for computing Schur decomposition, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8382537521730367
      },
      {
        "text": "Computational Complexity of Jacobi-Davidson Algorithm Computation for Large-Scale Matrices: The analysis of the computational complexity of algorithms for computing eigenvalues and eigenvectors using the Jacobi-Davidson algorithm for large-scale matrices, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8080555804301877
      },
      {
        "text": "Computational Complexity of Power Iteration Computation for Large-Scale Matrices: The analysis of the computational complexity of algorithms for computing eigenvalues and eigenvectors using the power iteration for large-scale matrices, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8439177899007526
      },
      {
        "text": "Computational Complexity of Non-Symmetric Matrix Eigenvalue Computation: The analysis of the computational complexity of algorithms for computing eigenvalues and eigenvectors of non-symmetric matrices, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.901419484420027
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Interlacing Theorem: The interlacing theorem, which states that the eigenvalues of a matrix are interlaced, i.e., the eigenvalues of the upper triangular part of a matrix are greater than or equal to the eigenvalues of the lower triangular part of the matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Interlacing Theorem: The interlacing theorem, which states that the eigenvalues of a matrix are interlaced, i.e., the eigenvalues of the upper triangular part of a matrix are greater than or equal to the eigenvalues of the lower triangular part of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gershgorin's Circle Theorem for Non-Symmetric Matrices: Gershgorin's circle theorem for non-symmetric matrices, which provides an upper bound on the absolute value of the eigenvalues of a non-symmetric matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gershgorin's Circle Theorem for Non-Symmetric Matrices: Gershgorin's circle theorem for non-symmetric matrices, which provides an upper bound on the absolute value of the eigenvalues of a non-symmetric matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Linearly Dependent Sets: It is essential to understand that linear independence and linear dependence are not mutually exclusive concepts. A set of vectors can be both linearly independent and linearly dependent, depending on the context and the specific properties of the vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and Linearly Dependent Sets: It is essential to understand that linear independence and linear dependence are not mutually exclusive concepts. A set of vectors can be both linearly independent and linearly dependent, depending on the context and the specific properties of the vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Concept of Linear Independence in Higher-Dimensional Spaces: In higher-dimensional spaces, linear independence is a more complex concept. A set of vectors can be linearly independent in a higher-dimensional space even if they are not linearly independent in a lower-dimensional space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Concept of Linear Independence in Higher-Dimensional Spaces: In higher-dimensional spaces, linear independence is a more complex concept. A set of vectors can be linearly independent in a higher-dimensional space even if they are not linearly independent in a lower-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Concept of Linear Dependence in Higher-Dimensional Spaces: In higher-dimensional spaces, linear dependence is a more complex concept than linear independence, and it requires a deeper understanding of the properties of vectors and linear combinations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9199569430972467
      },
      {
        "text": "Linear Independence and the Concept of Linear Dependence in Higher-Dimensional Spaces: In higher-dimensional spaces, linear dependence is a more complex concept than linear independence. A set of vectors is said to be linearly dependent if at least one vector in the set can be expressed as a linear combination of the other vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9231314138451863
      }
    ]
  },
  {
    "representative_text": "The Relationship between Linear Independence and the Rank-Nullity Theorem: The rank-nullity theorem is a fundamental theorem in linear algebra that states that the rank of a linear transformation is equal to the dimension of the range, and the nullity of the transformation is equal to the dimension of the kernel. Linear independence is closely related to the rank-nullity theorem, as a set of vectors is linearly independent if and only if the null space of the corresponding matrix has a trivial solution.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 18,
    "detailed_sources": [
      {
        "text": "The Relationship between Linear Independence and the Rank-Nullity Theorem: The rank-nullity theorem is a fundamental theorem in linear algebra that states that the rank of a linear transformation is equal to the dimension of the range, and the nullity of the transformation is equal to the dimension of the kernel. Linear independence is closely related to the rank-nullity theorem, as a set of vectors is linearly independent if and only if the null space of the corresponding matrix has a trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Rank-Nullity Theorem: The relationship between linear independence and the rank-nullity theorem. Specifically, the rank of a matrix is equal to the dimension of the span of its linearly independent columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8881517270305346
      },
      {
        "text": "The Rank-Nullity Theorem and Linear Independence: The rank-nullity theorem is related to linear independence. Specifically, the rank of a matrix (the dimension of its row space) plus the nullity of the matrix (the dimension of its null space) is equal to the number of columns of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8738807882184613
      },
      {
        "text": "Rank-Nullity Theorem: The rank-nullity theorem states that the rank of a linear transformation is equal to the dimension of the range of the transformation, and the nullity of a linear transformation is equal to the dimension of the null space of the transformation. This theorem is closely related to the concept of span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8631133728859152
      },
      {
        "text": "The relationship between linear independence and the rank-nullity theorem: This concept involves the relationship between linear independence and the rank-nullity theorem, which states that the sum of the rank and nullity of a linear transformation is equal to the dimension of the domain vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8959967013851396
      },
      {
        "text": "The Connection between Linear Independence and the Rank-Nullity Theorem in Infinite-Dimensional Spaces: The rank-nullity theorem still holds for infinite-dimensional vector spaces, but the dimensions of the kernel and range are not necessarily finite, and linear independence plays a crucial role in understanding the rank and nullity of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8636635291237805
      },
      {
        "text": "Linear Independence and the Fundamental Theorem of Finitely Generated Abelian Groups: The concept of linear independence can be related to the fundamental theorem of finitely generated abelian groups, particularly in the context of the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8426110901699334
      },
      {
        "text": "Linear Independence and the Rank-Nullity Theorem: The rank-nullity theorem states that the rank of a matrix is equal to the dimension of its column space, and the nullity of a matrix is equal to the dimension of its null space. Understanding the relationship between linear independence and the rank-nullity theorem is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9572997923511766
      },
      {
        "text": "Relationship between the Basis Theorem and the Rank-Nullity Theorem: The basis theorem states that a set of vectors is a basis of a vector space if and only if it is linearly independent and spans the entire vector space. The rank-nullity theorem states that the rank of a matrix is equal to the dimension of its column space, and the nullity of a matrix is equal to the dimension of its null space. Understanding the relationship between the basis theorem and the rank-nullity theorem is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.826900185550703
      },
      {
        "text": "The Rank-Nullity Theorem for Linear Transformations: This theorem states that the rank and nullity of a linear transformation are additive. This theorem has implications for the linear independence of vectors in the image and kernel of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8573125710652661
      },
      {
        "text": "The relationship between linear independence and the rank-nullity theorem: This relationship is mentioned earlier, but it's essential to understand how the rank-nullity theorem is related to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9260090725586817
      },
      {
        "text": "Linear Independence and the Rank-Nullity Theorem: The concept of linear independence can be related to the rank-nullity theorem, particularly in the context of the dimension theorem and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9279779691097468
      },
      {
        "text": "Relationship between Linear Independence and the Rank-Nullity Theorem for Infinite-Dimensional Spaces: This concept involves the relationship between linear independence and the rank-nullity theorem in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8791832451202471
      },
      {
        "text": "The Rank-Nullity Theorem for Linear Transformations: The rank-nullity theorem has implications for the linear independence of vectors in the image and kernel of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9016634528706977
      },
      {
        "text": "The Relationship Between Linear Independence and the Rank-Nullity Theorem in Infinite-Dimensional Vector Spaces: The rank-nullity theorem and its relationship with linear independence in infinite-dimensional vector spaces is essential to understand in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9422490123870649
      },
      {
        "text": "Linear Independence and the Connection between Linear Independence and the Rank-Nullity Theorem in Higher-Dimensional Spaces: The rank-nullity theorem is a fundamental theorem in linear algebra that states that the rank of a linear transformation is equal to the dimension of the range, and the nullity of the transformation is equal to the dimension of the kernel. Linear independence is closely related to the rank-nullity theorem, as a set of vectors is linearly independent if and only if the null space of the corresponding matrix has a trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9384785375768196
      },
      {
        "text": "The connection between linear independence and the rank-nullity theorem for linear transformations: The rank-nullity theorem states that the rank and nullity of a linear transformation are additive. This theorem has implications for the linear independence of vectors in the image and kernel of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9365987313490771
      },
      {
        "text": "**Linear Independence and the Rank-Nullity Theorem in a set of Linear Independence of Linear Independence of a set of Linear Independence of theore theore theore theore theore than other theore than in theore is a set of theore theore the Hie any higher-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8229126870946997
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Concept of Linear Independence in Quadratic Spaces: Quadratic spaces are a type of vector space that involves quadratic forms. Linear independence in quadratic spaces is a more complex concept than in linear spaces, and it requires a deeper understanding of quadratic forms and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Concept of Linear Independence in Quadratic Spaces: Quadratic spaces are a type of vector space that involves quadratic forms. Linear independence in quadratic spaces is a more complex concept than in linear spaces, and it requires a deeper understanding of quadratic forms and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Role of Quadratic Forms: Quadratic forms are an important concept in linear algebra, and they play a crucial role in understanding linear independence, especially in the context of quadratic spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8335091716152176
      },
      {
        "text": "Linear Independence of Quadratic Spaces: Quadratic spaces involve quadratic forms, and linear independence in these spaces is a complex concept that requires a deeper understanding of quadratic forms and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.937245505801027
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination and Linear Independence: The connection between Gaussian elimination and linear independence. Specifically, the row-echelon form obtained through Gaussian elimination can be used to determine linear independence of vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination and Linear Independence: The connection between Gaussian elimination and linear independence. Specifically, the row-echelon form obtained through Gaussian elimination can be used to determine linear independence of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of \"Gaussian Elimination and Linear Independence\": This concept relates to the connection between Gaussian elimination and linear independence, and understanding it is crucial in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8400173455635374
      },
      {
        "text": "Linear Independence and the Reduced Row Echelon Form: The concept of linear independence can be related to reduced row echelon form, particularly in the context of Gaussian elimination and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8961823826947927
      },
      {
        "text": "Gaussian Elimination and Linear Independence with Respect to a Given Basis: The connection between Gaussian elimination and linear independence with respect to a given basis is essential to understand in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9219144829188485
      },
      {
        "text": "Gaussian Elimination and Linear Independence in Infinite-Dimensional Vector Spaces: The connection between Gaussian elimination and linear independence in infinite-dimensional vector spaces is essential to understand in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9188415929281782
      },
      {
        "text": "Gaussian Elimination for Infinite-Dimensional Matrices: A method for transforming an infinite-dimensional matrix into row echelon form, which is useful for determining linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8243640448586687
      }
    ]
  },
  {
    "representative_text": "Minimal Spanning Sets: A set of vectors that spans a vector space and has the minimum number of vectors necessary to do so. This concept is related to linear independence and basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Minimal Spanning Sets: A set of vectors that spans a vector space and has the minimum number of vectors necessary to do so. This concept is related to linear independence and basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of \"Minimal Spanning Sets\": A set of vectors that spans a vector space and has the minimum number of vectors necessary to do so.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8748111315161278
      }
    ]
  },
  {
    "representative_text": "The concept of \"Trivial Span\" and \"Non-Trivial Span\": A set of vectors that spans a vector space and is either trivial (contains only the zero vector) or non-trivial (contains at least one non-zero vector).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The concept of \"Trivial Span\" and \"Non-Trivial Span\": A set of vectors that spans a vector space and is either trivial (contains only the zero vector) or non-trivial (contains at least one non-zero vector).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of \"Trivial Span\" and \"Non-Trivial Span\": A set of vectors that spans a vector space and is either trivial (contains only the zero vector) or non-trivial (contains at least one non-zero vector) is essential to understand in the context of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9212458858023409
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Dimension: The dimension of a vector space is related to the linear independence of its basis. Specifically, the dimension of a vector space is equal to the number of linearly independent vectors in a basis for the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Linear Independence and Dimension: The dimension of a vector space is related to the linear independence of its basis. Specifically, the dimension of a vector space is equal to the number of linearly independent vectors in a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace Dimension: The dimension of a subspace is related to the linear independence of its basis. Specifically, the dimension of a subspace is equal to the number of linearly independent vectors in a basis for the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9024565046268596
      },
      {
        "text": "The relationship between linear independence and the dimension of a vector space: The dimension of a vector space is equal to the maximum number of linearly independent vectors, but it's also equal to the number of vectors in a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9151827778952072
      },
      {
        "text": "The relationship between linear independence and the dimension of a subspace: The dimension of a subspace is the number of vectors in a basis of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9100596586340568
      },
      {
        "text": "Dimension Implies Basis Implies Linear Independence: The dimension of a vector space is equal to the number of vectors in a basis for the space if and only if the basis is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8998400244601219
      },
      {
        "text": "Subspace Dimension: The dimension of a subspace, which is the number of linearly independent vectors in the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.839445608378724
      },
      {
        "text": "The relationship between linear independence and the dimension of a vector space: The dimension of a vector space is equal to the maximum number of linearly independent vectors, and this relationship is essential to understand in the context of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9081958288292523
      }
    ]
  },
  {
    "representative_text": "The Singular Value Decomposition (SVD): The SVD of a matrix is related to linear independence. Specifically, the SVD of a matrix can be used to diagonalize the matrix and find its eigenvalues and eigenvectors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "The Singular Value Decomposition (SVD): The SVD of a matrix is related to linear independence. Specifically, the SVD of a matrix can be used to diagonalize the matrix and find its eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Values and Linear Independence: We need to consider how singular values relate to linear independence, particularly in the context of SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8470762501320858
      },
      {
        "text": "Singular Value Decomposition (SVD): SVD is a factorization technique that decomposes a matrix into three matrices: U, Σ, and V. The relationship between SVD and linear independence is crucial in understanding matrix factorization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8701711860773655
      },
      {
        "text": "Linear Independence and the SVD: The SVD is a factorization technique that decomposes a matrix into three matrices: U, Σ, and V. Understanding the relationship between linear independence and the SVD is crucial in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9432016892055517
      },
      {
        "text": "Rank of a Matrix and Linear Independence: We need to consider how the rank of a matrix relates to linear independence, particularly in the context of linear transformations and SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8581396513435722
      },
      {
        "text": "Singular Value Decomposition (SVD) and Finite-Dimensional Vector Spaces: The SVD theorem provides a connection between linear transformations and their representations as matrices. This theorem has important implications for linear algebra and its applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8326260961385988
      },
      {
        "text": "Singular Value Decomposition (SVD) and Eigenvalues: The SVD of a matrix and its relationship to eigenvalues is an important concept in understanding the properties of matrices and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8667537665100207
      },
      {
        "text": "Theoretical Background of Singular Value Decomposition (SVD): This includes the study of the mathematical foundations of SVD, such as the properties of SVD and the relationship between SVD and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8153755875347553
      },
      {
        "text": "The Connection between Eigenvalues and the Singular Value Decomposition: The relationship between eigenvalues and the singular value decomposition (SVD) is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8588163209504389
      },
      {
        "text": "Linear Transformations and the Singular Value Decomposition (SVD) of a Linear Transformation: The SVD of a linear transformation is a factorization of the transformation matrix into three matrices: U, Σ, and V. Understanding the SVD can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.859575984370508
      },
      {
        "text": "Linear Transformations and the Singular Value Decomposition (SVD) of a Linear Transformation (Alternative): Alternative representations of the SVD, such as the singular value decomposition of a linear transformation, can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8382107752681355
      },
      {
        "text": "Linear Transformations and the Singular Value Decomposition (SVD) of a Linear Transformation (Relationship with Kernel and Image): The relationship between the SVD and the kernel and image of a linear transformation can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8677475372677702
      },
      {
        "text": "Linear Transformations and Singular Value Decomposition (SVD): The SVD of a matrix can be used to analyze the properties of linear transformations, including the rank and nullity of a transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8924876688199532
      },
      {
        "text": "Singular Value Decomposition (SVD) and Eigenvalues: The relationship between SVD and eigenvalues, which is a fundamental concept in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8661854015167786
      }
    ]
  },
  {
    "representative_text": "The Eigenvalue Decomposition: The eigenvalue decomposition of a matrix is related to linear independence. Specifically, the eigenvalue decomposition of a matrix can be used to diagonalize the matrix and find its eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Eigenvalue Decomposition: The eigenvalue decomposition of a matrix is related to linear independence. Specifically, the eigenvalue decomposition of a matrix can be used to diagonalize the matrix and find its eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span Implies Basis: The concept of span implies the existence of a basis for the vector space, but it is not necessarily true that a set of vectors that spans a vector space is also a basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span Implies Basis: The concept of span implies the existence of a basis for the vector space, but it is not necessarily true that a set of vectors that spans a vector space is also a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Characterization of Linear Independence using the Null Space: A set of vectors is linearly independent if and only if the null space of the transformation is trivial, i.e., the only vector in the null space is the zero vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Characterization of Linear Independence using the Null Space: A set of vectors is linearly independent if and only if the null space of the transformation is trivial, i.e., the only vector in the null space is the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Null Space and Column Space Theorems: Theorems that relate the null space and column space of a linear transformation can be used to determine linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8218804135521935
      },
      {
        "text": "The Relationship Between Linear Dependence and the Null Space: If the columns of a matrix are linearly dependent, the null space of the matrix is non-trivial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8502697383604594
      },
      {
        "text": "The relationship between linear independence and the null space: A set of vectors is linearly independent if and only if the null space of the corresponding matrix is trivial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9441127774161853
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space in Infinite-Dimensional Vector Spaces: Examining the relationship between linear independence and the null space of a matrix in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.856532442228851
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space: Examining the relationship between linear independence and the null space of a matrix, which involves the use of linear independence to define a notion of dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9242394588677905
      },
      {
        "text": "The Relationship between Linear Independence and the Nullity of a Matrix: If a matrix has a non-zero nullity, then its row space and column space are linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8959121944810012
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space of a Linear Transformation: The relationship between linear independence and the null space of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8890446915897308
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space of a Matrix: The relationship between linear independence and the null space of a matrix, particularly in the context of finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9412375884494933
      }
    ]
  },
  {
    "representative_text": "Characterization of Span using the Range: A set of vectors spans a vector space if and only if the range of the transformation is equal to the entire space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Characterization of Span using the Range: A set of vectors spans a vector space if and only if the range of the transformation is equal to the entire space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Additivity: A set of vectors is linearly independent if and only if the span of the union of the sets is equal to the union of the spans of the individual sets.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Additivity: A set of vectors is linearly independent if and only if the span of the union of the sets is equal to the union of the spans of the individual sets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Existence of a Basis for a Vector Space with a Non-Trivial Null Space: While the Basis Theorem states that a set of linearly independent vectors is a basis for a vector space if and only if its span is the entire vector space, it does not explicitly address the existence of a basis for a vector space with a non-trivial null space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Existence of a Basis for a Vector Space with a Non-Trivial Null Space: While the Basis Theorem states that a set of linearly independent vectors is a basis for a vector space if and only if its span is the entire vector space, it does not explicitly address the existence of a basis for a vector space with a non-trivial null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of a Basis for a Vector Space with a Linearly Dependent Set of Vectors: While the Basis Theorem states that a set of linearly independent vectors is a basis for a vector space if and only if its span is the entire vector space, it does not explicitly address the existence of a basis for a vector space with a linearly dependent set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9432964081128138
      },
      {
        "text": "Existence of a Linear Transformation with a Trivial Null Space: While the Basis Theorem states that a set of linearly independent vectors is a basis for a vector space if and only if its span is the entire vector space, it does not explicitly address the existence of a linear transformation with a trivial null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.924463320383522
      }
    ]
  },
  {
    "representative_text": "Linear Independence of the Span of a Set of Vectors: While the Linear Independence Theorem states that a set of vectors is linearly independent if and only if its span is the entire vector space, it does not explicitly address the linear independence of the span of a set of vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence of the Span of a Set of Vectors: While the Linear Independence Theorem states that a set of vectors is linearly independent if and only if its span is the entire vector space, it does not explicitly address the linear independence of the span of a set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence of a Set of Vectors in a Finite-Dimensional Vector Space with a Bounded Basis: The concept of linear independence can be extended to include finite-dimensional vector spaces with bounded bases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 18,
    "detailed_sources": [
      {
        "text": "Linear Independence of a Set of Vectors in a Finite-Dimensional Vector Space with a Bounded Basis: The concept of linear independence can be extended to include finite-dimensional vector spaces with bounded bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of a Set of Vectors in a Vector Space with a Non-Standard Basis: The concept of linear independence can be extended to include vector spaces with non-standard bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8965599178518115
      },
      {
        "text": "Linear Independence of a Set of Vectors in a Vector Space with a Bounded Linear Independence: The concept of linear independence can be extended to include vector spaces with bounded linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9601869280246984
      },
      {
        "text": "The Concept of Linear Independence of a Vector Space with Respect to a Given Basis: This concept involves determining whether a vector space is linearly independent with respect to a given basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8502066966797269
      },
      {
        "text": "The Concept of Linear Independence of a Vector Space with Respect to a Linear Transformation: This concept involves determining whether a vector space is linearly independent with respect to a given linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8460774249127633
      },
      {
        "text": "The Concept of Linear Independence of a Matrix with Respect to a Given Basis: This concept involves determining whether a matrix is linearly independent with respect to a given basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.881674217718731
      },
      {
        "text": "The Concept of a Linearly Independent Set of Vectors in a Vector Space: A set of vectors can be linearly independent in one vector space but not in another.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9022237798167283
      },
      {
        "text": "The Concept of Linear Independence in the Context of Differential Equations: This concept involves understanding the properties of linear independence in the context of differential equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8211566701777637
      },
      {
        "text": "Linear Independence and the Linear Independence of a Set of Matrices: The concept of linear independence can be extended to sets of matrices, particularly in the context of linear independence and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9001643581399772
      },
      {
        "text": "Linear Independence and the Linear Independence of a Set of Polynomials: The concept of linear independence can be extended to sets of polynomials, particularly in the context of linear independence and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8978520677553927
      },
      {
        "text": "Linear Independence and the Linear Independence of a Set of Rational Functions: The concept of linear independence can be extended to sets of rational functions, particularly in the context of linear independence and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8881455651767338
      },
      {
        "text": "Linear Independence and the Linear Independence of a Set of Trigonometric Series: The concept of linear independence can be extended to sets of trigonometric series, particularly in the context of linear independence and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8734868244670232
      },
      {
        "text": "Dual Basis: A basis of the dual space, which is related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8047128324974179
      },
      {
        "text": "Concept of Linear Independence in the Context of Vector Spaces with Respect to a Given Orthogonal Basis: This concept involves determining whether a vector space is linearly independent with respect to a given orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8987946277667974
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Bases: A theorem that generalizes the linear independence of a set of functions to non-standard bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8223843648396225
      },
      {
        "text": "Linear Algebraic Independence: The study of linear algebraic independence of a set of vectors, which is a fundamental concept in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.84740553663127
      },
      {
        "text": "Linear Independence of a Set of Matrices: The concept of linear independence can be:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8646062149094125
      },
      {
        "text": "The Linear Independence of a Matrix: This concept is not explicitly,",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8790520601462157
      }
    ]
  },
  {
    "representative_text": "Linear Independence of a Set of Vectors in a Vector Space with a Special Structure: The concept of linear independence can be extended to include vector spaces with special structures, such as symmetric or skew-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Linear Independence of a Set of Vectors in a Vector Space with a Special Structure: The concept of linear independence can be extended to include vector spaces with special structures, such as symmetric or skew-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence in Vector Spaces with Non-Standard Structures: The concept of linear independence can be extended to vector spaces with non-standard structures, such as symmetric or skew-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9568070938435086
      },
      {
        "text": "Linear Independence of a Set of Vectors with Non-Standard Properties: This concept examines the linear independence of a set of vectors with non-standard properties, such as non-standard norms or inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8658170611015443
      },
      {
        "text": "Linear Independence of a Set of Vectors in a Non-Standard Vector Space: This concept explores the linear independence of a set of vectors in a non-standard vector space, where the usual properties of linear independence may not hold.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.911711506587563
      },
      {
        "text": "Linear Independence of a Set of Vectors in a Non-Standard Metric Space: This concept studies the linear independence of a set of vectors in a non-standard metric space, where the usual properties of linear independence may not hold.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9195844074285386
      },
      {
        "text": "Linear Independence of a Set of Vectors in a Non-Standard Fractal Space: This concept explores the linear independence of a set of vectors in a non-standard fractal space, where the usual properties of linear independence may not hold.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.922333807883856
      },
      {
        "text": "Linear Independence of a Set of Vectors in a Vector Space with a Non-Standard Structure: This theorem generalizes the linear independence of a set of vectors in a vector space with a non-standard structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8960382509282852
      },
      {
        "text": "Linear Independence of a Set of Vectors with Non-Standard Linear Transformations: This concept generalizes the linear independence of a set of vectors to non-standard linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8948567838103384
      }
    ]
  },
  {
    "representative_text": "Basis for a Vector Space with a Non-Standard Metric: The concept of a basis can be extended to vector spaces with non-standard metrics, such as Riemannian manifolds.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 23,
    "detailed_sources": [
      {
        "text": "Basis for a Vector Space with a Non-Standard Metric: The concept of a basis can be extended to vector spaces with non-standard metrics, such as Riemannian manifolds.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Inner Product: The concept of a basis can be extended to vector spaces with non-standard inner products, such as inner product spaces over non-Real fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8836347686714761
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Norm: The concept of a basis can be extended to vector spaces with non-standard norms, such as normed vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9323226840503593
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Topology: The concept of a basis can be extended to vector spaces with non-standard topologies, such as topological vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9232983387833085
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Basis Element: The concept of a basis can be extended to vector spaces with non-standard basis elements, such as basis elements with non-standard properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9329618924042387
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Linear Transformation: The concept of a basis can be extended to vector spaces with non-standard linear transformations, such as linear transformations with non-standard properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9364699939003758
      },
      {
        "text": "Gloss on Basis Theorem for Non-Standard Vector Spaces: The Basis Theorem can be extended to non-standard vector spaces, where the concept of a basis is more complex due to the non-standard nature of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8582648743605863
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Basis Element: The concept of a basis can be extended to vector spaces with non-standard basis elements, such as basis elements with non-standard properties. This includes the study of basis elements with non-integer or non-rational values.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.915749349103536
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Inner Product: The concept of a basis can be extended to vector spaces with non-standard inner products, such as inner product spaces over non-Real fields. This includes the study of bases in complex vector spaces and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9036200572351587
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Norm: The concept of a basis can be extended to vector spaces with non-standard norms, such as normed vector spaces. This includes the study of bases in normed vector spaces and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9361655754373748
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Metric: The concept of a basis can be extended to vector spaces with non-standard metrics, such as Riemannian manifolds. This includes the study of bases in Riemannian manifolds and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9021898223690636
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Topology: The concept of a basis can be extended to vector spaces with non-standard topologies, such as topological vector spaces. This includes the study of bases in topological vector spaces and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9303024607656633
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Linear Transformation: The concept of a basis can be extended to vector spaces with non-standard linear transformations, such as linear transformations with non-standard properties. This includes the study of bases in vector spaces with non-standard linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.933314269533414
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Metric: In a vector space equipped with a non-standard metric, the concept of a basis and its relationship with the dimension of the space may be different from the standard metric case.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8781615441651556
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Dimension: This concept extends the concept of a basis to vector spaces with non-standard dimensions, such as infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9256685199464774
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Metric - Geometric Interpretation: This concept provides a geometric interpretation of the basis of a vector space with a non-standard metric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8692569435876771
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Inner Product - Geometric Interpretation: This concept provides a geometric interpretation of the basis of a vector space with a non-standard inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8540789393905373
      },
      {
        "text": "Basis of a vector space with a non-standard inner product: The existence of a basis for a vector space with a non-standard inner product is not explicitly addressed in the existing points. This concept is essential in understanding the properties of bases in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.852819346841028
      },
      {
        "text": "Geometric interpretation of basis in non-standard metric spaces: The concept of basis is closely related to geometric properties. However, the existing points do not provide a geometric interpretation of basis in non-standard metric spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8020002605271557
      },
      {
        "text": "Geometric Interpretation of Basis in Non-Standard Metric Spaces: The concept of basis is closely related to geometric properties, and a geometric interpretation of basis in non-standard metric spaces is essential for understanding the properties of bases in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.880374718377442
      },
      {
        "text": "Topological Implications of Basis in Non-Standard Vector Spaces: The concept of basis is closely related to topological properties, and the topological implications of basis in non-standard vector spaces are essential for understanding the properties of bases in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8888883570238566
      },
      {
        "text": "Basis of a vector space with a non-standard norm: The existence of a basis for a vector space with a non-standard norm is not explicitly addressed in the existing points. This concept is essential in understanding the properties of bases in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8841966864350951
      },
      {
        "text": "Basis of a vector space with a non-standard metric: The existence of a basis for a vector space with a non-standard metric is not explicitly addressed in the existing points. This concept is essential in understanding the properties of bases in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8988312291949491
      }
    ]
  },
  {
    "representative_text": "The concept of a \"Spanning Set\": A spanning set is a set of vectors that spans a vector space, but may not be linearly independent. It is a complementary concept to linear independence and is used in many applications in linear algebra and other fields.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The concept of a \"Spanning Set\": A spanning set is a set of vectors that spans a vector space, but may not be linearly independent. It is a complementary concept to linear independence and is used in many applications in linear algebra and other fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of the Complement of a Set: The span of the complement of a set of vectors is the entire vector space, which is related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8202054574219606
      },
      {
        "text": "The concept of a \"Spanning Set of Bases\": A spanning set of bases is a set of bases that spans a vector space, but may not be linearly independent. It is a complementary concept to linear independence and is used in many applications in linear algebra and other fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9155521222765315
      },
      {
        "text": "Weak Basis: A weak basis is a set of vectors that spans the space, but may not be linearly independent. This concept is important in understanding the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8383645695629152
      },
      {
        "text": "The concept of a \"Spanning Set of Linearly Dependent Vectors\": A set of linearly dependent vectors that spans a vector space is a set of vectors that can be expressed as a linear combination of the other vectors. This concept is related to the concept of a spanning set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8693766586513911
      }
    ]
  },
  {
    "representative_text": "Existence of a Spanning Set for an Infinite-Dimensional Vector Space: If V is an infinite-dimensional vector space, then V has a spanning set for V, but this spanning set may not be linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Existence of a Spanning Set for an Infinite-Dimensional Vector Space: If V is an infinite-dimensional vector space, then V has a spanning set for V, but this spanning set may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Reduction of a Matrix: A process of finding a basis for a subspace of a vector space by performing a series of row or column operations on a matrix, which helps in understanding linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Reduction of a Matrix: A process of finding a basis for a subspace of a vector space by performing a series of row or column operations on a matrix, which helps in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The process of finding a basis for a subspace of a vector space using Gaussian elimination: A method used to reduce a matrix to row echelon form, which helps in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8017870139571999
      }
    ]
  },
  {
    "representative_text": "Vector Space Closure Properties: The vector space closure properties, such as commutativity of addition, associativity of addition, distributivity of scalar multiplication over vector addition, distributivity of scalar multiplication over scalar addition, existence of additive identity, and existence of additive inverse, are fundamental in understanding linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector Space Closure Properties: The vector space closure properties, such as commutativity of addition, associativity of addition, distributivity of scalar multiplication over vector addition, distributivity of scalar multiplication over scalar addition, existence of additive identity, and existence of additive inverse, are fundamental in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Free Basis and Basis Theorem Variations: There are variations of the Free Basis Theorem, such as the Free Basis Theorem with Null Space, which relate to the concept of linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Free Basis and Basis Theorem Variations: There are variations of the Free Basis Theorem, such as the Free Basis Theorem with Null Space, which relate to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of Free Basis: There is a relationship between the linear independence of a free basis and the dimension of the vector space. Understanding this relationship can help in extending the free basis to a basis for the entire space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.807963476253352
      }
    ]
  },
  {
    "representative_text": "Linear Independence of a Spanning Set: A spanning set of a vector space is linearly independent if and only if the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has only the trivial solution.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence of a Spanning Set: A spanning set of a vector space is linearly independent if and only if the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ has only the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dimension of a Linearly Dependent Set: The dimension of a linearly dependent set is less than or equal to the number of vectors in the set.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension of a Linearly Dependent Set: The dimension of a linearly dependent set is less than or equal to the number of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Minimax Theorems: Minimax theorems, which provide bounds on the best approximation of a subspace by a linear combination of vectors, could be applied to the context of linear independence and span.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minimax Theorems: Minimax theorems, which provide bounds on the best approximation of a subspace by a linear combination of vectors, could be applied to the context of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis Extension using Linear Programming: Linear programming techniques can be used to find the best basis extension for a free basis. This approach could provide a more efficient method for extending the free basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis Extension using Linear Programming: Linear programming techniques can be used to find the best basis extension for a free basis. This approach could provide a more efficient method for extending the free basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Connection between Linear Independence and Dimension: There is a close relationship between linear independence and dimension, which can be explored further. For example, understanding the relationship between the dimension of a vector space and the linear independence of its basis could provide additional insights.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "The Connection between Linear Independence and Dimension: There is a close relationship between linear independence and dimension, which can be explored further. For example, understanding the relationship between the dimension of a vector space and the linear independence of its basis could provide additional insights.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Connection between Linear Independence and the Null Space: Understanding the relationship between linear independence and the null space of a matrix could provide additional insights into the concept of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8930664119570355
      },
      {
        "text": "The Relationship between Linear Independence and the Orthogonal Projection: Understanding the relationship between linear independence and orthogonal projections could provide additional insights into the concept of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9088484371952807
      },
      {
        "text": "The Relationship between Linear Independence and Null Space in Infinite-Dimensional Vector Spaces: Understanding the relationship between linear independence and the null space of a matrix can provide insights into the properties of linear transformations in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8703132653542052
      },
      {
        "text": "Relationship between Linear Independence and the Complement Theorem: The concept of linear independence can be related to the complement theorem, particularly in the context of the dimension theorem and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8645180364677947
      },
      {
        "text": "Linear Independence and the Duality Theorem: The concept of linear independence can be related to the duality theorem, particularly in the context of the dimension theorem and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8808212256282482
      },
      {
        "text": "Relationship between Linear Independence and the Existence of a Basis: The concept of linear independence can be related to the existence of a basis, particularly in the context of the dimension theorem and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9003981243850372
      },
      {
        "text": "Linear Independence and the Null Space and Column Space Theorems: The concept of linear independence can be related to the null space and column space theorems, particularly in the context of the dimension theorem and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9351271371165485
      },
      {
        "text": "Linear Independence and the Pivot and Non-Pivot Columns: The concept of linear independence can be related to pivot and non-pivot columns, particularly in the context of Gaussian elimination and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8749432008974878
      },
      {
        "text": "Linear Independence and the Linear Dependence of a Set of Vectors: The concept of linear independence can be related to linear dependence, particularly in the context of the dimension theorem and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9049789952885654
      },
      {
        "text": "Krylov Subspace and Linear Independence: The Krylov subspace is closely related to linear independence, and its properties can be used to understand the relationship between linear independence and the dimension of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8100171572663946
      }
    ]
  },
  {
    "representative_text": "The Role of Symmetric Matrices: Symmetric matrices play a crucial role in many linear algebra concepts, including linear independence and span. Exploring their applications and properties could be beneficial.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Role of Symmetric Matrices: Symmetric matrices play a crucial role in many linear algebra concepts, including linear independence and span. Exploring their applications and properties could be beneficial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Role of Semi-Simple Matrices: Semi-simple matrices, which are diagonalizable, play a crucial role in many linear algebra concepts, including linear independence and span. Exploring their applications and properties could be beneficial.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Role of Semi-Simple Matrices: Semi-simple matrices, which are diagonalizable, play a crucial role in many linear algebra concepts, including linear independence and span. Exploring their applications and properties could be beneficial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Role of Semi-Simple Matrices: Semi-simple matrices, which are diagonalizable, play a crucial role in many linear algebra concepts, including linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9504166561078883
      }
    ]
  },
  {
    "representative_text": "The Use of Combinatorial Methods: Combinatorial methods, such as the use of combinatorial designs or the concept of matroids, could be applied to the context of linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Use of Combinatorial Methods: Combinatorial methods, such as the use of combinatorial designs or the concept of matroids, could be applied to the context of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The application of combinatorial methods in linear independence and span: Combinatorial methods, such as the use of combinatorial designs or the concept of matroids, can be applied to the context of linear independence and span. This approach could provide additional insights into the relationships between these concepts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9389661219206848
      },
      {
        "text": "Combinatorial Methods for Linear Independence and Span: Combinatorial methods, such as the use of combinatorial designs or the concept of matroids, can be applied to the context of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9606832838727961
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Span for Infinite-Dimensional Vector Spaces: This concept extends the idea of linear independence implying span for finite-dimensional spaces to infinite-dimensional spaces, where the concept of basis is more nuanced.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Span for Infinite-Dimensional Vector Spaces: This concept extends the idea of linear independence implying span for finite-dimensional spaces to infinite-dimensional spaces, where the concept of basis is more nuanced.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence Implies Span for Infinite-Dimensional Spaces: This concept involves the relationship between linear independence and span in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9436089470469399
      },
      {
        "text": "Linear Independence Implies Span for Infinite-Dimensional Vector Spaces: We need to explore the implications of having a non-empty span on linear independence for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9062678244693319
      },
      {
        "text": "Linear Independence Implies Span Theorem Variations for Infinite-Dimensional Vector Spaces: We need to explore the implications of having a non-empty span on linear independence for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9133823876833557
      },
      {
        "text": "Linear Independence Implies Span and Span Implies Linear Independence in Infinite-Dimensional Vector Spaces: While the statements are true for infinite-dimensional vector spaces, understanding the nuances of linear independence in this context is crucial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9215725486831285
      },
      {
        "text": "Linear Independence Implies Span for Infinite-Dimensional Vector Spaces (Part 2): This involves extending the idea of linear independence implying span for finite-dimensional spaces to infinite-dimensional spaces, where the concept of basis is more nuanced.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9372201316140563
      },
      {
        "text": "Linear Independence Implies Span for Infinite-Dimensional Vector Spaces: The implications of having a non-empty span on linear independence in infinite-dimensional vector spaces are not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9021160375772437
      },
      {
        "text": "Linear Independence Implies Span: The Dimensionality Consequence for Infinite-Dimensional Spaces: This concept involves the relationship between linear independence and span in infinite-dimensional vector spaces, where the dimension of the span is equal to the number of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9073127074396901
      },
      {
        "text": "Linear Independence Implies Span in Infinite-Dimensional Vector Spaces: This statement is true for infinite-dimensional vector spaces, but it's essential to understand the nuances of linear independence in this context.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.926331510864518
      },
      {
        "text": "Linear Independence Implies Span for Infinite-Dimensional Vector Spaces with Non-Standard Bases: This concept extends the idea of linear independence implying span for this is there are quite a vector spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9117458376607162
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Span Theorem Variations: While we have the theorem stating that linear independence implies span, we also need to consider variations such as \"If the span of a set is a subspace, then the set is linearly independent\" or \"If a set of vectors spans a subspace, then the set is linearly independent\".",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Span Theorem Variations: While we have the theorem stating that linear independence implies span, we also need to consider variations such as \"If the span of a set is a subspace, then the set is linearly independent\" or \"If a set of vectors spans a subspace, then the set is linearly independent\".",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis and Span Theorem for Infinite-Dimensional Vector Spaces: While we have the theorem for finite-dimensional vector spaces, we need to consider the case for infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Basis and Span Theorem for Infinite-Dimensional Vector Spaces: While we have the theorem for finite-dimensional vector spaces, we need to consider the case for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace Span Theorem for Infinite-Dimensional Vector Spaces: We need to explore the implications of the subspace span theorem for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8503425557001896
      },
      {
        "text": "Subspace Span Theorem for Infinite-Dimensional Vector Spaces: The implications of the subspace span theorem for infinite-dimensional vector spaces are not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8607801535805335
      }
    ]
  },
  {
    "representative_text": "Linear Independence of a Set with a Zero Vector: We need to explore the implications of having a zero vector in a set of vectors on linear independence.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Independence of a Set with a Zero Vector: We need to explore the implications of having a zero vector in a set of vectors on linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of a Set with a Linear Combination of Vectors: We need to explore the implications of having a linear combination of vectors on linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8893872396183253
      }
    ]
  },
  {
    "representative_text": "Basis and Span Theorem for Vector Spaces with a Non-Empty Span: We need to consider the implications of having a non-empty span on the basis and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis and Span Theorem for Vector Spaces with a Non-Empty Span: We need to consider the implications of having a non-empty span on the basis and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of the Conjugate Transpose: The span of the conjugate transpose of a matrix is equal to the row space of the original matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span of the Conjugate Transpose: The span of the conjugate transpose of a matrix is equal to the row space of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Concept of Linear Dependence in the Context of Inner Product Spaces: Similar to linear independence, linear dependence in inner product spaces involves determining whether a set of vectors can be expressed as a linear combination of other vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "The Concept of Linear Dependence in the Context of Inner Product Spaces: Similar to linear independence, linear dependence in inner product spaces involves determining whether a set of vectors can be expressed as a linear combination of other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Concept of Linear Dependence of a Vector Space with Respect to a Given Basis: Similar to linear independence, linear dependence in a vector space with respect to a given basis involves determining whether a set of vectors can be expressed as a linear combination of other vectors with respect to that basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8836444747619907
      },
      {
        "text": "The Concept of Linear Dependence of a Vector Space with Respect to a Linear Transformation: Similar to linear independence, linear dependence in a vector space with respect to a linear transformation involves determining whether a set of vectors can be expressed as a linear combination of other vectors with respect to that transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9318818266482523
      },
      {
        "text": "The Concept of Linear Dependence of a Matrix with Respect to a Given Basis: Similar to linear independence, linear dependence in a matrix with respect to a given basis involves determining whether a set of matrices can be expressed as a linear combination of other matrices with respect to that basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9114587284153569
      },
      {
        "text": "The Concept of Linear Dependence in the Context of Differential Equations: Similar to linear independence, linear dependence in differential equations involves determining whether a set of equations can be expressed as a linear combination of other equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8664095907687572
      },
      {
        "text": "Concept of Linear Dependence in the Context of Vector Spaces with Respect to a Given Orthogonal Basis: Similar to linear independence, linear dependence in a vector space with respect to a given orthogonal basis involves determining whether a set of vectors can be expressed as a linear combination of other vectors with respect to that basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.917759871850367
      }
    ]
  },
  {
    "representative_text": "The Relationship Between Linear Independence and the Image of a Linear Transformation: The image of a linear transformation is closely related to linear independence. If the columns of a matrix are linearly independent, the image of the linear transformation is the entire codomain.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Relationship Between Linear Independence and the Image of a Linear Transformation: The image of a linear transformation is closely related to linear independence. If the columns of a matrix are linearly independent, the image of the linear transformation is the entire codomain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship Between Linear Dependence and the Image of a Linear Transformation: If the columns of a matrix are linearly dependent, the image of the linear transformation is a proper subspace of the codomain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8723530193213783
      }
    ]
  },
  {
    "representative_text": "Spherical Harmonics and Texture Mapping: Spherical harmonics are a mathematical tool used in texture mapping to generate high-frequency details on 3D models. This technique is crucial in computer graphics for tasks such as terrain rendering and atmosphere simulation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Spherical Harmonics and Texture Mapping: Spherical harmonics are a mathematical tool used in texture mapping to generate high-frequency details on 3D models. This technique is crucial in computer graphics for tasks such as terrain rendering and atmosphere simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Spherical Harmonics and Their Applications: Spherical harmonics are a mathematical tool used in computer graphics and game development to generate high-frequency details on 3D models.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8874802960736035
      }
    ]
  },
  {
    "representative_text": "Clipping and Culling: Clipping and culling are techniques used in computer graphics to optimize rendering performance by removing objects that are outside the camera's view frustum. Linear algebra is used to implement these techniques efficiently.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Clipping and Culling: Clipping and culling are techniques used in computer graphics to optimize rendering performance by removing objects that are outside the camera's view frustum. Linear algebra is used to implement these techniques efficiently.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Global Illumination and Bump Mapping: Global illumination is a technique used to simulate the way light interacts with objects in 3D space, taking into account indirect lighting and ambient occlusion. Bump mapping is a technique used to simulate the way objects have surface details such as wrinkles and pores.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Global Illumination and Bump Mapping: Global illumination is a technique used to simulate the way light interacts with objects in 3D space, taking into account indirect lighting and ambient occlusion. Bump mapping is a technique used to simulate the way objects have surface details such as wrinkles and pores.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Normal Estimation and Smoothing: Normal estimation is a technique used to estimate the normal vectors of 3D models, which is essential for tasks such as lighting and shading. Smoothing is a technique used to smooth out the normal vectors to reduce noise and artifacts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Normal Estimation and Smoothing: Normal estimation is a technique used to estimate the normal vectors of 3D models, which is essential for tasks such as lighting and shading. Smoothing is a technique used to smooth out the normal vectors to reduce noise and artifacts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "3D Reconstruction and Denoising: 3D reconstruction and denoising are techniques used in computer graphics to reconstruct 3D models from 2D images or videos. Linear algebra is used to implement these techniques efficiently.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "3D Reconstruction and Denoising: 3D reconstruction and denoising are techniques used in computer graphics to reconstruct 3D models from 2D images or videos. Linear algebra is used to implement these techniques efficiently.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Span: Understanding the concepts of linear independence and span, which are crucial in signal processing and image analysis, particularly in the context of filtering and image representation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and Span: Understanding the concepts of linear independence and span, which are crucial in signal processing and image analysis, particularly in the context of filtering and image representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Pseudoinverses: Pseudoinverses of matrices with non-full rank, particularly in the context of image processing and computer vision, where linear algebra techniques are used for tasks like image segmentation and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Pseudoinverses: Pseudoinverses of matrices with non-full rank, particularly in the context of image processing and computer vision, where linear algebra techniques are used for tasks like image segmentation and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Properties of Matrices: Understanding the spectral properties of matrices, such as the eigenvalues and eigenvectors of a matrix, is essential in signal processing and image analysis, particularly in the context of filtering and image compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Spectral Properties of Matrices: Understanding the spectral properties of matrices, such as the eigenvalues and eigenvectors of a matrix, is essential in signal processing and image analysis, particularly in the context of filtering and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Spectral Properties of Matrices and Their Applications in Signal Processing and Image Analysis: Understanding the spectral properties of matrices, such as the eigenvalues and eigenvectors of a matrix, and their applications in signal processing and image analysis, including tasks like filtering, convolution, and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8960791545293851
      }
    ]
  },
  {
    "representative_text": "Wavelet Analysis and Multiresolution Representation: Understanding wavelet analysis and its applications in signal processing and image analysis, including tasks like image compression, de-noising, and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Wavelet Analysis and Multiresolution Representation: Understanding wavelet analysis and its applications in signal processing and image analysis, including tasks like image compression, de-noising, and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Principal Component Analysis (PCA) Variants: Understanding PCA variants, such as PCA with non-linear transformations and PCA with non-standard norms, and their applications in signal processing and image analysis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Principal Component Analysis (PCA) Variants: Understanding PCA variants, such as PCA with non-linear transformations and PCA with non-standard norms, and their applications in signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Optimization Techniques in Signal Processing and Image Analysis: Optimization techniques, such as linear programming and convex optimization, are used in signal processing and image analysis for tasks like image compression, de-noising, and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Optimization Techniques in Signal Processing and Image Analysis: Optimization techniques, such as linear programming and convex optimization, are used in signal processing and image analysis for tasks like image compression, de-noising, and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Optimization Techniques in Signal Processing and Image Analysis: Optimization techniques, such as linear programming and convex optimization, are used in signal processing, are used in signal processing and Linear Programming is not only mention optimization techniques like the spectral methods are used in Signal processing techniques, are used in Signal processing and linear programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9166607049334129
      }
    ]
  },
  {
    "representative_text": "Continuous-Time Control Theory: This branch of control theory deals with systems that are continuous in time, and it is based heavily on linear algebra and differential equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Continuous-Time Control Theory: This branch of control theory deals with systems that are continuous in time, and it is based heavily on linear algebra and differential equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Discrete-Time Control Theory: This branch of control theory deals with systems that are discrete in time, and it is based heavily on linear algebra and difference equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Discrete-Time Control Theory: This branch of control theory deals with systems that are discrete in time, and it is based heavily on linear algebra and difference equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Decomposition: This is a technique used to diagonalize a matrix and is widely used in control theory to analyze and design control systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Decomposition: This is a technique used to diagonalize a matrix and is widely used in control theory to analyze and design control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Modal Analysis: This is a technique used to analyze the frequency response of a system and is widely used in control theory to design and analyze control systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Modal Analysis: This is a technique used to analyze the frequency response of a system and is widely used in control theory to design and analyze control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Transfer Function Poles and Zeros: These are fundamental concepts in control theory that describe the behavior of a system in the frequency domain.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Transfer Function Poles and Zeros: These are fundamental concepts in control theory that describe the behavior of a system in the frequency domain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Transfer Function Poles and Zeros: Fundamental concepts in control theory that describe the behavior of a system in the frequency domain, used to design and analyze control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9260111053299749
      }
    ]
  },
  {
    "representative_text": "Bode Plot: This is a graphical representation of the frequency response of a system and is widely used in control theory to analyze and design control systems.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bode Plot: This is a graphical representation of the frequency response of a system and is widely used in control theory to analyze and design control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lattice-based secure multi-party computation: A method for secure multi-party computation using lattices.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lattice-based secure multi-party computation: A method for secure multi-party computation using lattices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Code-based secure multi-party computation: A method for secure multi-party computation using error-correcting codes.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Code-based secure multi-party computation: A method for secure multi-party computation using error-correcting codes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Multivariate-based secure multi-party computation: A method for secure multi-party computation using multivariate polynomials.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9014471881357506
      }
    ]
  },
  {
    "representative_text": "Quantum-resistant secure multi-party computation: A method for secure multi-party computation that is resistant to quantum computer attacks.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Quantum-resistant secure multi-party computation: A method for secure multi-party computation that is resistant to quantum computer attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Quantum-Resistant Secure Multi-Party Computation: This includes methods for designing and analyzing secure multi-party computation protocols that are resistant to quantum computer attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9256569973291735
      },
      {
        "text": "Quantum-resistant secure multi-party computation: Methods for designing and analyzing secure multi-party computation protocols that are resistant to quantum computer attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9684108341371277
      },
      {
        "text": "Quantum-Resistant Secure Multi-Party Computation with Homomorphic Encryption: This includes methods for designing and analyzing secure multi-party computation protocols that are resistant to quantum computer attacks using homomorphic encryption.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9155797399130836
      },
      {
        "text": "Quantum-Resistant Cryptographic Techniques for Secure Multi-Party Computation: This includes exploring alternative cryptographic techniques that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, to ensure long-term security.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8637416626607833
      },
      {
        "text": "Quantum-resistant cryptographic techniques for secure multi-party computation: Methods for exploring alternative cryptographic techniques that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, to ensure long-term security in secure multi-party computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8790802947301524
      },
      {
        "text": "Quantum-resistant cryptographic techniques for secure multi-party computation: This includes the analysis of quantum-resistant cryptographic techniques, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, for secure multi-party computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9187958662199178
      }
    ]
  },
  {
    "representative_text": "The definition and properties of a semi-inner product, including the relationship with the inner product.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The definition and properties of a semi-inner product, including the relationship with the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Semi-inner Products",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9220075105180393
      }
    ]
  },
  {
    "representative_text": "The definition and properties of the least squares method, including the relationship with the orthogonal projection of a vector onto a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "The definition and properties of the least squares method, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of the normal equation, including the properties of the normal equation and the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8438241902743775
      },
      {
        "text": "The definition and properties of the Procrustean theorem, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.856775784284427
      },
      {
        "text": "The definition and properties of the Pythagorean theorem for inner product spaces, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8154912552258649
      },
      {
        "text": "The definition and properties of the Cauchy-Schwarz inequality for inner product spaces, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8217956538578772
      },
      {
        "text": "The definition and properties of the parallelogram law for inner product spaces, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8780607922748593
      },
      {
        "text": "The definition and properties of the Bessel's inequality for inner product spaces, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8456166965429123
      },
      {
        "text": "The definition and properties of the Plancherel's theorem for inner product spaces, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.879020467573189
      },
      {
        "text": "The definition and properties of the Fourier transform of an inner product space, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8170166060526899
      },
      {
        "text": "The definition and properties of the Fourier series of an inner product space, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8424532158463107
      },
      {
        "text": "The definition and properties of the Fourier integral theorem, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8591754403925742
      },
      {
        "text": "The definition and properties of the Fourier integral of a vector in an inner product space, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8674700964527735
      },
      {
        "text": "The definition and properties of the Pontryagin duality theorem, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8370158175599937
      },
      {
        "text": "Inner Product Spaces and Orthogonality: The relationship between inner product spaces, orthogonality, and projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8460554798623462
      }
    ]
  },
  {
    "representative_text": "The definition and properties of the Rayleigh-Ritz method, including the relationship with the orthogonal projection of a vector onto a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The definition and properties of the Rayleigh-Ritz method, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of the Rayleigh quotient, including the properties of the Rayleigh quotient and the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8690836550714753
      },
      {
        "text": "Orthogonal Projection Matrices and the Rayleigh Quotient: Computing eigenvalues of a matrix using the Rayleigh quotient, with applications in orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8544457924074804
      }
    ]
  },
  {
    "representative_text": "The definition and properties of the inverse of an orthogonal matrix, including the relationship with the orthogonal projection of a vector onto a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The definition and properties of the inverse of an orthogonal matrix, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The definition and properties of the determinant of an orthogonal matrix, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8721870457716937
      }
    ]
  },
  {
    "representative_text": "The definition and properties of the Moore-Penrose inverse of a matrix, including the relationship with the orthogonal projection of a vector onto a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The definition and properties of the Moore-Penrose inverse of a matrix, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The definition and properties of the Sylvester equation, including the relationship with the orthogonal projection of a vector onto a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The definition and properties of the Sylvester equation, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Sylvester Equation:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8113199516340386
      }
    ]
  },
  {
    "representative_text": "The definition and properties of the spectral theorem, including the relationship with the orthogonal projection of a vector onto a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The definition and properties of the spectral theorem, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The definition and properties of the unitary triangularization theorem, including the relationship with the orthogonal projection of a vector onto a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The definition and properties of the unitary triangularization theorem, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The definition and properties of the Hilbert-Schmidt theorem, including the relationship with the orthogonal projection of a vector onto a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The definition and properties of the Hilbert-Schmidt theorem, including the relationship with the orthogonal projection of a vector onto a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cyclic Vectors and Cyclic Subspaces: A vector v is a cyclic vector of a linear transformation T if the set {T^n(v) | n ∈ ℕ} is linearly independent. A subspace W is a cyclic subspace of a linear transformation T if there exists a vector v in W such that the set {T^n(v) | n ∈ ℕ} is linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cyclic Vectors and Cyclic Subspaces: A vector v is a cyclic vector of a linear transformation T if the set {T^n(v) | n ∈ ℕ} is linearly independent. A subspace W is a cyclic subspace of a linear transformation T if there exists a vector v in W such that the set {T^n(v) | n ∈ ℕ} is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Linear Algebra: Every linear transformation T from a vector space V to a vector space W is equivalent to a diagonalizable matrix, and the rank of T is equal to the dimension of the column space of T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Linear Algebra: Every linear transformation T from a vector space V to a vector space W is equivalent to a diagonalizable matrix, and the rank of T is equal to the dimension of the column space of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Fundamental Theorem of Linear Algebra for Vector Spaces: This theorem states that every linear transformation T from a vector space V to itself is equivalent to a diagonalizable matrix, and the rank of T is equal to the dimension of the column space of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9520445354686122
      }
    ]
  },
  {
    "representative_text": "Vector Space Closure under Scalar Multiplication by Scalar Products: A vector space may have a non-trivial scalar product, which can affect the closure under scalar multiplication.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector Space Closure under Scalar Multiplication by Scalar Products: A vector space may have a non-trivial scalar product, which can affect the closure under scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Vector Spaces: This theorem states that every vector space has a basis, but the proof is more involved than the corresponding theorem for linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Vector Spaces: This theorem states that every vector space has a basis, but the proof is more involved than the corresponding theorem for linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Concept of a Finite-Dimensional Subspace: A subspace of a vector space can be finite-dimensional, which affects the basis and dimension of the subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Concept of a Finite-Dimensional Subspace: A subspace of a vector space can be finite-dimensional, which affects the basis and dimension of the subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Dimension of a Vector Space as a Linear Combination of Other Vector Spaces: The dimension of a vector space can be expressed as a linear combination of the dimensions of other vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Dimension of a Vector Space as a Linear Combination of Other Vector Spaces: The dimension of a vector space can be expressed as a linear combination of the dimensions of other vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Dimension of a Vector Space as a Sum of Linearly Independent Subspaces: The dimension of a vector space can be expressed as the sum of the dimensions of linearly independent subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.877367811870404
      },
      {
        "text": "The Dimension of a Vector Space as a Product of Linearly Independent Subspaces: The dimension of a vector space can be expressed as the product of the dimensions of linearly independent subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9151338558426607
      },
      {
        "text": "The Dimension of a Vector Space as a Minkowski Sum of Linearly Independent Subspaces: The dimension of a vector space can be expressed as the Minkowski sum of the dimensions of linearly independent subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9287281419217763
      }
    ]
  },
  {
    "representative_text": "The Rank-Nullity Theorem for Complex Vector Spaces: This theorem is similar to the one mentioned above but for complex vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Rank-Nullity Theorem for Complex Vector Spaces: This theorem is similar to the one mentioned above but for complex vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Basis and Dimension of Complex Vector Spaces: This includes the properties of basis and dimension for complex vector spaces, such as the fact that a set of complex vectors is a basis for a complex vector space if and only if it is linearly independent and spans the vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Basis and Dimension of Complex Vector Spaces: This includes the properties of basis and dimension for complex vector spaces, such as the fact that a set of complex vectors is a basis for a complex vector space if and only if it is linearly independent and spans the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Inner Product - Properties: This includes the study of properties of bases in complex vector spaces and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8665749509608671
      }
    ]
  },
  {
    "representative_text": "The Linear Transformation and Matrix Representation for Complex Vectors: This includes the properties of linear transformations and matrix representation for complex vectors, such as the fact that a linear transformation can be represented by a matrix if and only if the transformation is a linear combination of basis vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Transformation and Matrix Representation for Complex Vectors: This includes the properties of linear transformations and matrix representation for complex vectors, such as the fact that a linear transformation can be represented by a matrix if and only if the transformation is a linear combination of basis vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector Space Properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Vector Space Properties:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Properties of vector spaces:",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9527978527997666
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Least Squares Method:",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Least Squares Method:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The model is represented by a linear equation, and the coefficients are estimated using the method of least squares.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8025277616981882
      }
    ]
  },
  {
    "representative_text": "Rayleigh-Ritz Method:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Rayleigh-Ritz Method:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Unitary Triangularization Theorem:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Unitary Triangularization Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Triangularization Theorem",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9307772236237526
      }
    ]
  },
  {
    "representative_text": "The Procrustean Theorem:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Procrustean Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Pythagorean Theorem for Inner Product Spaces:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Pythagorean Theorem for Inner Product Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Parallelogram Law for Inner Product Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.924154710591294
      },
      {
        "text": "Inner Product Spaces and Orthogonormality: Inner Product: Theorem: This theorem: This theorem: This theorem: Inner Product: Inner Product Spaces: Theorem for Inner Product of a :",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8147683002829729
      }
    ]
  },
  {
    "representative_text": "The Cauchy-Schwarz Inequality for Inner Product Spaces:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Cauchy-Schwarz Inequality for Inner Product Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Bessel's Inequality for Inner Product Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9043388724671904
      },
      {
        "text": "The properties of inner product spaces, including the Cauchy-Schwarz inequality and the Gram-Schmidt process",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8273209812383806
      }
    ]
  },
  {
    "representative_text": "The Plancherel's Theorem for Inner Product Spaces:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "The Plancherel's Theorem for Inner Product Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Fourier Transform of an Inner Product Space:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8477494048756987
      },
      {
        "text": "The Fourier Series of an Inner Product Space:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.929017648128314
      },
      {
        "text": "The Fourier Integral Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8496256197285168
      },
      {
        "text": "Fourier Integral Theorem",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8799892073607918
      },
      {
        "text": "Plancherel's Theorem",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8900773115864031
      },
      {
        "text": "Fourier Transform of an Inner Product Space",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8757627430573529
      }
    ]
  },
  {
    "representative_text": "The Fourier Transform of a Vector:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Fourier Transform of a Vector:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Fourier Series of a Vector:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.917821495872944
      },
      {
        "text": "The Fourier Integral of a Vector:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9337231931814449
      }
    ]
  },
  {
    "representative_text": "The Pontryagin Duality Theorem:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Pontryagin Duality Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis Extension Theorem Variations: There are variations of the basis extension theorem, such as the fact that if a basis for a subspace is extended to a basis for the original vector space, then the extended basis spans the original space, and this theorem can be applied to larger vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Basis Extension Theorem Variations: There are variations of the basis extension theorem, such as the fact that if a basis for a subspace is extended to a basis for the original vector space, then the extended basis spans the original space, and this theorem can be applied to larger vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Non-standard basis extension theorems: The existing Basis Extension Theorem states that adding a new vector to a basis results in a new basis unless the new vector is a linear combination of the original vectors. However, this theorem may not hold in non-standard vector spaces with non-standard dimensions or linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8126659800685576
      },
      {
        "text": "Non-standard basis theorems: The Basis Theorem can be extended to non-standard vector spaces, where the concept of a basis is more complex due to the non-standard nature of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8931871086536032
      }
    ]
  },
  {
    "representative_text": "Intersections of Subspaces: The intersection of two subspaces is a subspace, but the knowledge points do not explicitly state the conditions for this to be true.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Intersections of Subspaces: The intersection of two subspaces is a subspace, but the knowledge points do not explicitly state the conditions for this to be true.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of the Intersection of Two Subspaces: The span of the intersection of two subspaces is a subspace, but the knowledge points do not explicitly state the conditions for this to be true.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9166801451727769
      },
      {
        "text": "Intersections of Subspaces with Different Dimensions: The intersection of two subspaces with different dimensions can be a subspace, but its dimension may not be the minimum of the dimensions of the two subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.82355322654066
      }
    ]
  },
  {
    "representative_text": "Direct Sum of Infinite Subspaces: The direct sum of infinite subspaces is not explicitly mentioned in the knowledge points.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Direct Sum of Infinite Subspaces: The direct sum of infinite subspaces is not explicitly mentioned in the knowledge points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Direct Sum of Infinite-Dimensional Vector Spaces: The relationship between the direct sum of infinite-dimensional vector spaces and the dimension of the resulting space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8030787964383435
      }
    ]
  },
  {
    "representative_text": "Basis Vectors are Linearly Independent Implies Span: The knowledge points do not explicitly state the condition that a basis of linearly independent vectors spans a vector space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Basis Vectors are Linearly Independent Implies Span: The knowledge points do not explicitly state the condition that a basis of linearly independent vectors spans a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear independence of bases in non-standard vector spaces: The concept of linear independence is crucial in defining a basis. However, the existing points do not explicitly address linear independence in non-standard vector spaces, where the usual properties of linear independence may not hold.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.800828614252157
      },
      {
        "text": "Linear independence of a set of vectors in a non-standard fractal space: The concept of linear independence is crucial in defining a basis. However, the existing points do not explicitly address linear independence in non-standard fractal spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8682120260196333
      }
    ]
  },
  {
    "representative_text": "Null Space and Image of a Linear Transformation: The knowledge points do not explicitly state the relationship between the null space and image of a linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Null Space and Image of a Linear Transformation: The knowledge points do not explicitly state the relationship between the null space and image of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of the Image of a Linear Transformation: The relationship between the dimension of the image of a linear transformation and the original vector space is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8229223414718718
      }
    ]
  },
  {
    "representative_text": "Quotient Space of a Vector Space: The quotient space of a vector space is not explicitly mentioned in the knowledge points.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quotient Space of a Vector Space: The quotient space of a vector space is not explicitly mentioned in the knowledge points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Subspace Invariant: The knowledge points do not explicitly state the conditions for a linear transformation to be subspace invariant.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Subspace Invariant: The knowledge points do not explicitly state the conditions for a linear transformation to be subspace invariant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Isomorphism between Vector Spaces and Quotient Spaces: The knowledge points do not explicitly state the conditions for two vector spaces to be isomorphic.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Isomorphism between Vector Spaces and Quotient Spaces: The knowledge points do not explicitly state the conditions for two vector spaces to be isomorphic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Products and Linear Transformations: The knowledge points do not explicitly state the relationship between tensor products and linear transformations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Tensor Products and Linear Transformations: The knowledge points do not explicitly state the relationship between tensor products and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bilinear Forms and Linear Transformations: The knowledge points do not explicitly state the relationship between bilinear forms and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8530689174496289
      }
    ]
  },
  {
    "representative_text": "Tensor Decomposition and Linear Transformations: The knowledge points do not explicitly state the relationship between tensor decomposition and linear transformations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Tensor Decomposition and Linear Transformations: The knowledge points do not explicitly state the relationship between tensor decomposition and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Tensor Decomposition and Linear Algebra: The knowledge points do not explicitly state the relationship between tensor decomposition and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9536305163853196
      },
      {
        "text": "Tensor Networks and Linear Algebra: The knowledge points do not explicitly state the relationship between tensor networks and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8720842511131474
      },
      {
        "text": "Tensor Decomposition and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between tensor decomposition and finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9168883595982213
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) and Tensor Decomposition: The knowledge points do not explicitly state the relationship between SVD and tensor decomposition.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) and Tensor Decomposition: The knowledge points do not explicitly state the relationship between SVD and tensor decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Finite-Dimensional Vector Spaces and the Rank-Nullity Theorem: The knowledge points do not explicitly state the relationship between the fundamental theorem of finite-dimensional vector spaces and the rank-nullity theorem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Finite-Dimensional Vector Spaces and the Rank-Nullity Theorem: The knowledge points do not explicitly state the relationship between the fundamental theorem of finite-dimensional vector spaces and the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Rank-Nullity Theorem and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between the rank-nullity theorem and finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9447273110894951
      }
    ]
  },
  {
    "representative_text": "The Cayley-Hamilton Theorem and the Jordan Decomposition Theorem: The knowledge points do not explicitly state the relationship between the Cayley-Hamilton theorem and the Jordan decomposition theorem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Cayley-Hamilton Theorem and the Jordan Decomposition Theorem: The knowledge points do not explicitly state the relationship between the Cayley-Hamilton theorem and the Jordan decomposition theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Theorem and Linear Algebra: The knowledge points do not explicitly state the relationship between the spectral theorem and linear algebra.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Spectral Theorem and Linear Algebra: The knowledge points do not explicitly state the relationship between the spectral theorem and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Spectral Theorem and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between the spectral theorem and finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8996259027720324
      },
      {
        "text": "Eigenvalue Decomposition and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between eigenvalue decomposition and finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8472200369809213
      }
    ]
  },
  {
    "representative_text": "Eigenvalues and Eigenvectors in Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the conditions for a matrix to have eigenvalues and eigenvectors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalues and Eigenvectors in Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the conditions for a matrix to have eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Gram-Schmidt Process for Infinite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between the Gram-Schmidt process and infinite-dimensional vector spaces.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Gram-Schmidt Process for Infinite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between the Gram-Schmidt process and infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product Spaces and Orthogonality in Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the conditions for an inner product space to be finite-dimensional.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner Product Spaces and Orthogonality in Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the conditions for an inner product space to be finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Cayley-Hamilton Theorem and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between the Cayley-Hamilton theorem and finite-dimensional vector spaces.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Cayley-Hamilton Theorem and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between the Cayley-Hamilton theorem and finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cayley-Hamilton Theorem for Infinite-Dimensional Vector Spaces: The Cayley-Hamilton theorem for infinite-dimensional vector spaces may not be applicable, as the characteristic polynomial may not be defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8017200710397991
      },
      {
        "text": "The Cayley-Hamilton Theorem for Infinite-Dimensional Vector Spaces: The Cayley-Hamilton theorem for infinite-dimensional vector spaces states that a matrix satisfies its own characteristic polynomial, but the characteristic polynomial may not be defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9113500149522742
      },
      {
        "text": "Cayley-Hamilton Theorem and its Extensions: The Cayley-Hamilton theorem states that every square matrix satisfies its own characteristic equation. However, there are extensions of this theorem, such as the generalized Cayley-Hamilton theorem and the theorem for matrices with complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8430734903766284
      }
    ]
  },
  {
    "representative_text": "The Jordan Decomposition Theorem and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between the Jordan decomposition theorem and finite-dimensional vector spaces.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Jordan Decomposition Theorem and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between the Jordan decomposition theorem and finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between SVD and finite-dimensional vector spaces.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the relationship between SVD and finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bilinear Forms and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the conditions for a bilinear form to be finite-dimensional.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bilinear Forms and Finite-Dimensional Vector Spaces: The knowledge points do not explicitly state the conditions for a bilinear form to be finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bilinear Transformation: A generalization of linear transformations to bilinear transformations, which are functions that preserve both addition and scalar multiplication.",
    "is_in_domain": false,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Bilinear Transformation: A generalization of linear transformations to bilinear transformations, which are functions that preserve both addition and scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bilinear Transformation: A bilinear transformation from a vector space V to a vector space W is a function that satisfies certain properties, such as linearity in both arguments.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8685389781338156
      },
      {
        "text": "Theorem on the Bilinear Transformation: This theorem states that a bilinear transformation from a vector space V to a vector space W is a function that satisfies certain properties, such as linearity in both arguments.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8589548653297895
      },
      {
        "text": "The Bilinear Transformation Theorem: This theorem states that a bilinear transformation is a linear transformation that preserves the bilinear form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8952707884242433
      },
      {
        "text": "The bilinear transformation theorem states that a bilinear transformation is a linear transformation that preserves the bilinear form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8471682522529187
      },
      {
        "text": "Bilinear transformations: A type of linear transformation that can be represented as a linear combination of inner products, which can be used to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8220166087009182
      },
      {
        "text": "Bilinear Transformation with Applications: A type of linear transformation that can be represented as a linear combination of inner products, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8525777071333249
      }
    ]
  },
  {
    "representative_text": "Riemannian Metric: A concept from differential geometry that generalizes the notion of matrix multiplication to the context of Riemannian manifolds.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Riemannian Metric: A concept from differential geometry that generalizes the notion of matrix multiplication to the context of Riemannian manifolds.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Logarithm: A concept that generalizes the logarithm to matrices, which can be used to study the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Logarithm: A concept that generalizes the logarithm to matrices, which can be used to study the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Exponentiation: A concept that generalizes the exponential function to matrices, which can be used to study the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Exponentiation: A concept that generalizes the exponential function to matrices, which can be used to study the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lie Algebra: A mathematical structure that generalizes the notion of linear transformations to Lie groups, which are groups that are locally Euclidean and have a smooth structure.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lie Algebra: A mathematical structure that generalizes the notion of linear transformations to Lie groups, which are groups that are locally Euclidean and have a smooth structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Analysis: A branch of mathematics that generalizes the notion of linear transformations to tensors, which are multi-linear functions that can be used to study the geometry and topology of vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Tensor Analysis: A branch of mathematics that generalizes the notion of linear transformations to tensors, which are multi-linear functions that can be used to study the geometry and topology of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Tensor Analysis and Vector Spaces: Tensor analysis is a branch of mathematics that deals with the study of tensors, which are mathematical objects that can be used to represent linear transformations and other mathematical objects. Vector spaces play a crucial role in tensor analysis, particularly in the representation of tensor fields.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8029913787481116
      },
      {
        "text": "Tensor Analysis and Vector Spaces: Tensor analysis is a branch of mathematics that deals with the study of tensors, which are mathematical objects that can be used to represent linear transformations and other mathematical objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9619281328747535
      },
      {
        "text": "Tensor Analysis and Linear Algebra: The study of tensor analysis and its relationship to linear algebra, which is a fundamental concept in mathematics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8365586810951546
      }
    ]
  },
  {
    "representative_text": "Differential Operators: A concept that generalizes the notion of linear transformations to differential operators, which are functions that take a vector space as input and produce another vector space as output.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Differential Operators: A concept that generalizes the notion of linear transformations to differential operators, which are functions that take a vector space as input and produce another vector space as output.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Theory: A branch of mathematics that studies the eigenvalues and eigenvectors of linear transformations, which can be used to study the properties of vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Spectral Theory: A branch of mathematics that studies the eigenvalues and eigenvectors of linear transformations, which can be used to study the properties of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Operator Theory: A branch of mathematics that studies the properties of linear operators on vector spaces, which can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8536293027367576
      }
    ]
  },
  {
    "representative_text": "Quantum Linear Algebra: A branch of mathematics that generalizes the notion of linear transformations to quantum mechanics, which is a mathematical framework that describes the behavior of particles at the atomic and subatomic level.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quantum Linear Algebra: A branch of mathematics that generalizes the notion of linear transformations to quantum mechanics, which is a mathematical framework that describes the behavior of particles at the atomic and subatomic level.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Algebraic Topology: A branch of mathematics that studies the topological properties of vector spaces, which can be used to study the properties of linear transformations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Algebraic Topology: A branch of mathematics that studies the topological properties of vector spaces, which can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Homology Theory: A branch of mathematics that studies the topological properties of vector spaces, which can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9273755941837944
      },
      {
        "text": "K-theory: A branch of mathematics that studies the topological properties of vector spaces, which can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9002122798151704
      }
    ]
  },
  {
    "representative_text": "Category Theory: A branch of mathematics that studies the properties of linear transformations in a categorical framework, which can be used to study the properties of vector spaces.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Category Theory: A branch of mathematics that studies the properties of linear transformations in a categorical framework, which can be used to study the properties of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Groups: A branch of mathematics that studies the properties of linear transformations in the context of group theory, which can be used to study the properties of vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Groups: A branch of mathematics that studies the properties of linear transformations in the context of group theory, which can be used to study the properties of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dual Basis and Dual Basis for a Linear Transformation (Alternative): This could involve discussing the properties of the dual basis in relation to the image and kernel of a linear transformation, including its orthogonality and the fact that the dual basis of the domain can be used to find the image of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dual Basis and Dual Basis for a Linear Transformation (Alternative): This could involve discussing the properties of the dual basis in relation to the image and kernel of a linear transformation, including its orthogonality and the fact that the dual basis of the domain can be used to find the image of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Fundamental Theorem of Linear Algebra: This could involve discussing the relationship between linear transformations and the fundamental theorem of linear algebra, including how the theorem can be used to establish a relationship between the kernel and image of a linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Fundamental Theorem of Linear Algebra: This could involve discussing the relationship between linear transformations and the fundamental theorem of linear algebra, including how the theorem can be used to establish a relationship between the kernel and image of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and the Conjugate Transpose Theorem: This could involve discussing the relationship between linear transformations and the conjugate transpose theorem, including how the theorem can be used to establish a relationship between the kernel and image of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8734032405090393
      },
      {
        "text": "Linear Transformations and the Trace Theorem: This could involve exploring the relationship between linear transformations and the trace theorem, including how the theorem can be used to determine the properties of a linear transformation, such as its rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.83938639837008
      },
      {
        "text": "Linear Transformations and the Index of a Linear Transformation: This could involve discussing the relationship between linear transformations and the index of a linear transformation, including how the index can be used to determine the properties of the transformation, such as its rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8182111604929497
      },
      {
        "text": "Linear Transformations and the Index of a Linear Transformation (Relationship with Kernel and Image): This could involve exploring the relationship between the index of a linear transformation and its kernel and image, including how the index can be used to determine the properties of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8788694642403099
      },
      {
        "text": "Linear Transformations and the Minimal Polynomial Theorem: This could involve exploring the relationship between linear transformations and the minimal polynomial theorem, including how the theorem can be used to determine the properties of a linear transformation, such as its rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732671222323471
      },
      {
        "text": "Linear Transformations and the Minimal Polynomial of a Linear Transformation: This could involve discussing the relationship between linear transformations and the minimal polynomial of a linear transformation, including how the minimal polynomial can be used to determine the properties of the transformation, such as its rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8745086783499866
      },
      {
        "text": "Linear Transformations and the Index of a Linear Transformation (Alternative): This concept is related to the index of a linear transformation and provides a measure of the \"size\" of the kernel and image of T. However, there are alternative representations, such as the rank-nullity theorem, which can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8387679635026317
      },
      {
        "text": "Linear Transformations and the Duality Theorem: The duality theorem is a fundamental result in linear algebra that establishes a relationship between the kernel and image of a linear transformation, as well as the dual transformation and its kernel. Understanding the properties of the duality theorem can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8120624959363462
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Inverse Function Theorem: This could involve exploring the relationship between linear transformations and the inverse function theorem, including how the theorem can be used to determine the existence and properties of the inverse of a linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Inverse Function Theorem: This could involve exploring the relationship between linear transformations and the inverse function theorem, including how the theorem can be used to determine the existence and properties of the inverse of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Spectral Mapping Theorem: This could involve discussing the relationship between linear transformations and the spectral mapping theorem, including how the theorem can be used to establish a relationship between the eigenvalues and eigenvectors of a linear transformation and its matrix representation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Spectral Mapping Theorem: This could involve discussing the relationship between linear transformations and the spectral mapping theorem, including how the theorem can be used to establish a relationship between the eigenvalues and eigenvectors of a linear transformation and its matrix representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and the Spectral Mapping Theorem (Alternative): This theorem states that the eigenvalues of a linear transformation T: V -> V are the same as the eigenvalues of its matrix representation. However, there are alternative representations, such as the Jordan decomposition, which can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8503110288256452
      },
      {
        "text": "Linear Transformations and the Spectral Mapping Theorem (Alternative): Alternative representations of the spectral mapping theorem, such as the Jordan decomposition, can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8849630400939428
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Spectral Radius Theorem: This could involve exploring the relationship between linear transformations and the spectral radius theorem, including how the theorem can be used to determine the properties of a linear transformation, such as its rank and nullity.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Spectral Radius Theorem: This could involve exploring the relationship between linear transformations and the spectral radius theorem, including how the theorem can be used to determine the properties of a linear transformation, such as its rank and nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Jordan Decomposition Theorem: This could involve discussing the relationship between linear transformations and the Jordan decomposition theorem, including how the theorem can be used to establish a relationship between the eigenvalues and eigenvectors of a linear transformation and its matrix representation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Jordan Decomposition Theorem: This could involve discussing the relationship between linear transformations and the Jordan decomposition theorem, including how the theorem can be used to establish a relationship between the eigenvalues and eigenvectors of a linear transformation and its matrix representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations with a Trivial Image or Kernel and Non-Zero Rank or Nullity: The rank and nullity of a linear transformation can be zero even when the image or kernel is non-trivial, depending on the dimension of the domain and codomain.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations with a Trivial Image or Kernel and Non-Zero Rank or Nullity: The rank and nullity of a linear transformation can be zero even when the image or kernel is non-trivial, depending on the dimension of the domain and codomain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Orthogonal Projections: The relationship between linear transformations and orthogonal projections can be studied using determinants. Specifically, the determinant of the orthogonal projection matrix can be used to determine the behavior of the linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Orthogonal Projections: The relationship between linear transformations and orthogonal projections can be studied using determinants. Specifically, the determinant of the orthogonal projection matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Orthogonal Decomposition: The relationship between linear transformations and orthogonal decomposition can be studied using determinants. Specifically, the determinant of the orthogonal decomposition matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9346461236733139
      },
      {
        "text": "Linear Transformation and Orthogonal Diagonalization: The relationship between linear transformations and orthogonal diagonalization can be studied using determinants. Specifically, the determinant of the orthogonal diagonalization matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9461832448607242
      },
      {
        "text": "Linear Transformation and Singular Value Decomposition with Non-Orthogonal Linear Transformations: The relationship between linear transformations and singular value decomposition with non-orthogonal linear transformations can be studied using determinants. Specifically, the determinant of the singular value decomposition matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9011783285785595
      },
      {
        "text": "Orthogonal Projection Matrices and the Determinant: The determinant of an orthogonal projection matrix can provide information about the subspace being projected onto. This property has implications for various applications and theorems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8272705043625141
      }
    ]
  },
  {
    "representative_text": "Determinant and Nullity of a Matrix: The determinant of a matrix is related to the nullity of the matrix. Specifically, the determinant of a matrix is equal to the determinant of its transpose, which is related to the nullity of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant and Nullity of a Matrix: The determinant of a matrix is related to the nullity of the matrix. Specifically, the determinant of a matrix is equal to the determinant of its transpose, which is related to the nullity of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Matrix Exponentiation: The relationship between linear transformations and matrix exponentiation can be studied using determinants. Specifically, the determinant of the matrix exponential can be used to determine the behavior of the linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Matrix Exponentiation: The relationship between linear transformations and matrix exponentiation can be studied using determinants. Specifically, the determinant of the matrix exponential can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Matrix Exponential: The study of the matrix exponential can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8191679293529106
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Jordan Canonical Form: The relationship between linear transformations and Jordan canonical form can be studied using determinants. Specifically, the determinant of the Jordan canonical form matrix can be used to determine the behavior of the linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Jordan Canonical Form: The relationship between linear transformations and Jordan canonical form can be studied using determinants. Specifically, the determinant of the Jordan canonical form matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Jordan Canonical Form with Non-Orthogonal Linear Transformations: The relationship between linear transformations and Jordan canonical form with non-orthogonal linear transformations can be studied using determinants. Specifically, the determinant of the Jordan canonical form matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9522291042577076
      },
      {
        "text": "Linear Transformation and Jordan Canonical Form with Non-Orthogonal Linear Transformations and Non-Constant Coefficients: The relationship between linear transformations and Jordan canonical form with non-orthogonal linear transformations and non-constant coefficients can be studied using determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9268781246549465
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Fourier Transform: The relationship between linear transformations and Fourier transform can be studied using determinants. Specifically, the determinant of the Fourier transform matrix can be used to determine the behavior of the linear transformation.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Fourier Transform: The relationship between linear transformations and Fourier transform can be studied using determinants. Specifically, the determinant of the Fourier transform matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Fourier Transform with Non-Orthogonal Linear Transformations and Non-Constant Coefficients: The relationship between linear transformations and Fourier transform with non-orthogonal linear transformations and non-constant coefficients can be studied using determinants. Specifically, the determinant of the Fourier transform matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9397986147894344
      },
      {
        "text": "Linear Transformation and Fourier Transform with Non-Orthogonal Linear Transformations and Non-Constant Coefficients: The relationship between linear transformations and Fourier transform with non-orthogonal linear transformations and non-constant coefficients can be studied using determinants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9252591388426811
      }
    ]
  },
  {
    "representative_text": "Matrices and Orthogonality: This involves the study of matrices that are orthogonal, meaning their transpose is their inverse, and the properties of orthogonal matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Matrices and Orthogonality: This involves the study of matrices that are orthogonal, meaning their transpose is their inverse, and the properties of orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonality: Orthogonality of vectors and matrices can be used to describe the properties of matrices, particularly in the context of linear transformations and stability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8007019285825397
      },
      {
        "text": "Matrix Orthogonality: Matrix orthogonality is a concept that describes the properties of a matrix in terms of its orthogonality with other matrices. It is related to the concept of orthogonality of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8423834517254429
      },
      {
        "text": "Orthogonal Decomposition of a Matrix: This involves the study of the orthogonal decomposition of a matrix, which is a method for decomposing a matrix into the sum of orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8112804880095057
      },
      {
        "text": "The Orthogonal Matrix: This topic deals with matrices that have orthogonal columns, which are useful in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8647196947521149
      },
      {
        "text": "The Unitary Matrix: This topic deals with matrices that have unitary columns, which are useful in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8436969881881708
      },
      {
        "text": "Matrix Orthogonality Conditions: Matrix orthogonality conditions are a concept that describes the properties of a matrix in terms of its orthogonality with other matrices. They are related to the concept of matrix stability and can be used to solve certain types of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8370470261677981
      },
      {
        "text": "The concept of a matrix orthogonal matrix and its relation to the orthogonal decomposition: The matrix orthogonal matrix is a matrix that satisfies a specific condition when multiplied by its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8778946090740821
      },
      {
        "text": "The concept of a matrix unitary matrix and its relation to the orthogonal decomposition: The matrix unitary matrix is a matrix that satisfies a specific condition when multiplied by its conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8503675054550117
      },
      {
        "text": "Matrix Orthogonality Conditions: These are conditions that a matrix must satisfy in order to be orthogonal. Orthogonal matrices have many important applications in linear algebra, such as in the context of eigenvectors and eigenspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8867322637631649
      },
      {
        "text": "Orthogonal Matrix Orthogonality Conditions: The conditions under which two orthogonal matrices are orthogonal to each other, including the fact that the columns of one matrix must be orthogonal to the rows of the other matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8106989508762926
      }
    ]
  },
  {
    "representative_text": "Quadratic Forms: This involves the study of quadratic forms, which are expressions of the form x^T A x, where A is a symmetric matrix, and the properties of quadratic forms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Quadratic Forms: This involves the study of quadratic forms, which are expressions of the form x^T A x, where A is a symmetric matrix, and the properties of quadratic forms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Complement and the Study of Orthogonal Quadratic Forms: This involves studying the properties of orthogonal quadratic forms, which are quadratic forms that are orthogonal to each other with respect to a certain inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8242432236594381
      },
      {
        "text": "Quadratic Form Theory: Quadratic form theory is a branch of mathematics that deals with quadratic forms and their properties.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8070958882098241
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Properties: This involves the study of properties of eigenvalues, such as the spectral radius, the spectral norm, and the eigenvalue decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvalue Properties: This involves the study of properties of eigenvalues, such as the spectral radius, the spectral norm, and the eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvector and Eigenvalue Theory: This involves studying the properties of eigenvectors and eigenvalues, including the definition of eigenvectors and eigenvalues, eigenvector properties, and eigenvalue properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.843400550886956
      }
    ]
  },
  {
    "representative_text": "Matrix Inversion using the Sherman-Morrison Formula: This involves the study of the Sherman-Morrison formula, which is a formula for inverting a matrix with a rank-one update.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Matrix Inversion using the Sherman-Morrison Formula: This involves the study of the Sherman-Morrison formula, which is a formula for inverting a matrix with a rank-one update.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inversion using the Sherman-Morrison Formula with Rank-One Update and Non-Real Entries: This involves the study of the Sherman-Morrison formula with rank-one update and non-real entries, which is a method for inverting a matrix with a rank-one update.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.944780307676241
      }
    ]
  },
  {
    "representative_text": "Matrix Exponentiation using the Cayley-Hamilton Theorem: This involves the study of the Cayley-Hamilton theorem, which states that every square matrix satisfies its own characteristic equation, and the application of this theorem to compute the matrix exponential.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Matrix Exponentiation using the Cayley-Hamilton Theorem: This involves the study of the Cayley-Hamilton theorem, which states that every square matrix satisfies its own characteristic equation, and the application of this theorem to compute the matrix exponential.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Exponentiation using the Cayley-Hamilton Theorem with Shift and Invert: This involves the study of the Cayley-Hamilton theorem with shift and invert, which is an extension of the Cayley-Hamilton theorem for computing the matrix exponential.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9222342183084833
      },
      {
        "text": "Matrix Exponentiation using the Resolvent: This involves the study of the resolvent, which is a method for computing the matrix exponential using a power series expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8662630545966432
      },
      {
        "text": "Matrix Exponentiation using the Matrix Logarithm: This involves the study of the matrix logarithm, which is a method for computing the matrix exponential using a power series expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8988356117781345
      },
      {
        "text": "Matrix Exponentiation using the Matrix Polar Decomposition: This involves the study of the matrix polar decomposition, which is a method for computing the matrix exponential using a power series expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9198147766667135
      },
      {
        "text": "Matrix Exponentiation using the Power Method: This involves the study of the power method for computing the matrix exponential using a power series expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9180071941748092
      }
    ]
  },
  {
    "representative_text": "Cramer's Rule for Systems of Non-Linear Equations: This involves the study of Cramer's rule for systems of non-linear equations, which is an extension of Cramer's rule for systems of linear equations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cramer's Rule for Systems of Non-Linear Equations: This involves the study of Cramer's rule for systems of non-linear equations, which is an extension of Cramer's rule for systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computing the Eigenvalues of a Matrix using the Arnoldi Method with Shift: This involves the study of the Arnoldi method with shift, which is an extension of the Arnoldi method for computing the eigenvalues of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Computing the Eigenvalues of a Matrix using the Arnoldi Method with Shift: This involves the study of the Arnoldi method with shift, which is an extension of the Arnoldi method for computing the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the Krylov Subspace Method with Shift: This involves the study of the Krylov subspace method with shift, which is an extension of the Krylov subspace method for computing the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9124244990915565
      }
    ]
  },
  {
    "representative_text": "Newton's Sums: Newton's sums are a method for finding the elementary symmetric polynomials of a matrix, which can be used to calculate the determinant and other properties of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Newton's Sums: Newton's sums are a method for finding the elementary symmetric polynomials of a matrix, which can be used to calculate the determinant and other properties of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Newton's Sums for a Matrix with Complex Eigenvalues: An extension of Newton's sums to matrices with complex eigenvalues, which involves using the properties of the matrix and the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8425513230764395
      }
    ]
  },
  {
    "representative_text": "Companion Matrices: Companion matrices are a type of matrix that is used to represent polynomials and are related to the eigenvalues and eigenvectors of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Companion Matrices: Companion matrices are a type of matrix that is used to represent polynomials and are related to the eigenvalues and eigenvectors of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Companion Matrices for a Matrix with Complex Eigenvalues: A more general version of companion matrices, which involves using the properties of the matrix and the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8231499220302966
      }
    ]
  },
  {
    "representative_text": "Schur Complement Properties: This includes the properties of the Schur complement, such as its relationship to the determinant and inverse of the block matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Schur Complement Properties: This includes the properties of the Schur complement, such as its relationship to the determinant and inverse of the block matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Schur complement properties: The Schur complement has various properties, including being used to compute the determinant and inverse of a block matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.936919043857876
      }
    ]
  },
  {
    "representative_text": "The Cayley-Hamilton Theorem for a Non-Square Matrix: This is a more general version of the Cayley-Hamilton Theorem, which states that every square matrix satisfies its own characteristic equation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Cayley-Hamilton Theorem for a Non-Square Matrix: This is a more general version of the Cayley-Hamilton Theorem, which states that every square matrix satisfies its own characteristic equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Schur Complement and Its Relationship to the Determinant and Inverse of a Block Matrix: This includes the properties of the Schur complement and its relationship to the determinant and inverse of a block matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "The Schur Complement and Its Relationship to the Determinant and Inverse of a Block Matrix: This includes the properties of the Schur complement and its relationship to the determinant and inverse of a block matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Block Matrix Inversion using the Schur Complement for Block Diagonal Matrices: This is a method for inverting a block diagonal matrix using the Schur complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8403548928927942
      },
      {
        "text": "Determinant of a Block Matrix using the Schur Complement for Block Diagonal Matrices: This is a method for calculating the determinant of a block diagonal matrix using the Schur complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9058680738942246
      },
      {
        "text": "Block Matrix Determinants using the Schur Complement (Special Case): A method for calculating the determinant of a block matrix using the Schur complement, which is a submatrix that is obtained by removing a row and a column from the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8999147233819025
      },
      {
        "text": "Block Matrix Inversion using the Schur Complement for Non-Symmetric Matrices (Generalized): A method for inverting a block matrix using the Schur complement, which is a submatrix that is obtained by removing a row and a column from the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8999815308296584
      },
      {
        "text": "Determinant of a Block Matrix using the Schur Complement for Block Diagonal Matrices (Generalized): A method for calculating the determinant of a block diagonal matrix using the Schur complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9312596275538427
      },
      {
        "text": "Orthogonal Projection Matrices and the Schur Complement: Computing the determinant of a block matrix using the Schur complement, with applications in orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8691017228635083
      }
    ]
  },
  {
    "representative_text": "The Schur Complement: This is a matrix used to estimate the eigenvalues of a matrix, which is useful in understanding the properties of eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Schur Complement: This is a matrix used to estimate the eigenvalues of a matrix, which is useful in understanding the properties of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Connection between Eigenvalues and the Schur Complement: The relationship between eigenvalues and the Schur complement of a matrix is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8504119881272147
      }
    ]
  },
  {
    "representative_text": "The Conjugate Gradient Method: This is an algorithm used to find the eigenvalues and eigenvectors of a matrix by iteratively applying a conjugate gradient method.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Conjugate Gradient Method: This is an algorithm used to find the eigenvalues and eigenvectors of a matrix by iteratively applying a conjugate gradient method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Arnoldi Iteration: This is an algorithm used to find the eigenvalues and eigenvectors of a matrix by iteratively applying an Arnoldi iteration method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9082255884665629
      }
    ]
  },
  {
    "representative_text": "The Jacobi Iteration: This is an algorithm used to find the eigenvalues and eigenvectors of a matrix by iteratively applying a Jacobi iteration method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Jacobi Iteration: This is an algorithm used to find the eigenvalues and eigenvectors of a matrix by iteratively applying a Jacobi iteration method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Eigenvalues using the Jacobi Iteration Method: The Jacobi iteration method is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves iteratively applying a series of orthogonal transformations to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9168696684844386
      }
    ]
  },
  {
    "representative_text": "The Krylov Subspace Method: This is a technique used to estimate the eigenvalues of a matrix by applying a Krylov subspace method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Krylov Subspace Method: This is a technique used to estimate the eigenvalues of a matrix by applying a Krylov subspace method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Krylov Subspace Method: The Krylov subspace method is a technique used to estimate the eigenvalues of a matrix. It involves iteratively applying a QR decomposition and using the resulting matrices to estimate the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8885888539738587
      }
    ]
  },
  {
    "representative_text": "Eigenvalues and Eigenvectors: While mentioned, eigenvalues and eigenvectors are not fully explored. They are scalar values and vectors, respectively, that are used to describe the properties of a matrix, particularly in the context of linear transformations and stability.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalues and Eigenvectors: While mentioned, eigenvalues and eigenvectors are not fully explored. They are scalar values and vectors, respectively, that are used to describe the properties of a matrix, particularly in the context of linear transformations and stability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Jordan Block: Jordan block is a square matrix with a specific structure that is used to describe the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jordan Block: Jordan block is a square matrix with a specific structure that is used to describe the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Square Root: Matrix square root is a method for finding the square root of a matrix, which can be useful in solving certain types of linear equations and systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Matrix Square Root: Matrix square root is a method for finding the square root of a matrix, which can be useful in solving certain types of linear equations and systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Square: Matrix square is a method for finding the square of a matrix, which can be useful in solving certain types of linear equations and systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9012593985919215
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) for Non-Square Matrices: While mentioned, SVD for non-square matrices is not fully explored. It is a factorization of a matrix into the product of three matrices, which can be used to describe the properties of the matrix and solve certain types of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) for Non-Square Matrices: While mentioned, SVD for non-square matrices is not fully explored. It is a factorization of a matrix into the product of three matrices, which can be used to describe the properties of the matrix and solve certain types of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Stability: Matrix stability is a concept that describes the behavior of a matrix in the presence of small perturbations. It is related to the concept of eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Matrix Stability: Matrix stability is a concept that describes the behavior of a matrix in the presence of small perturbations. It is related to the concept of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Stabilization: This is a concept in linear algebra that studies how a matrix behaves in the presence of small perturbations. The stabilization of a matrix is important in many applications, such as control theory and numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8319520531191882
      }
    ]
  },
  {
    "representative_text": "Quadratic Forms: Quadratic forms are a way to describe the properties of a matrix in terms of quadratic expressions. They are related to the concept of eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Quadratic Forms: Quadratic forms are a way to describe the properties of a matrix in terms of quadratic expressions. They are related to the concept of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Quadratic Forms: Quadratic forms are mathematical objects that can be used to represent the quadratic behavior of a matrix. They have applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8569030723475118
      },
      {
        "text": "Quadratic Forms: A quadratic form is a function that takes a vector as input and returns a scalar value. Quadratic forms are used to study the properties of matrices and to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8654451754427654
      }
    ]
  },
  {
    "representative_text": "The Cayley-Hamilton Theorem for Determinant: The Cayley-Hamilton Theorem can be generalized to the determinant, which states that det(A) = det(A^n) for any positive integer n.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Cayley-Hamilton Theorem for Determinant: The Cayley-Hamilton Theorem can be generalized to the determinant, which states that det(A) = det(A^n) for any positive integer n.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Inverse Eigenvalue Theorem: This theorem states that if A is a diagonalizable matrix, then the inverse of A is diagonalizable and has the same eigenvalues as A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Inverse Eigenvalue Theorem: This theorem states that if A is a diagonalizable matrix, then the inverse of A is diagonalizable and has the same eigenvalues as A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Eigenvalues using the Inverse Eigenvalue Theorem: The inverse eigenvalue theorem states that if A is a diagonalizable matrix, then the inverse of A is diagonalizable and has the same eigenvalues as A. This theorem can be used to compute eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8423865454928487
      }
    ]
  },
  {
    "representative_text": "The Relationship between Eigenvalues and the Residue Theorem: The residue theorem is a result from complex analysis that relates the eigenvalues of a matrix to the residues of a function.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Relationship between Eigenvalues and the Residue Theorem: The residue theorem is a result from complex analysis that relates the eigenvalues of a matrix to the residues of a function.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Eigenvalues using the Residue Theorem: The residue theorem is a result from complex analysis that relates the eigenvalues of a matrix to the residues of a function. This theorem can be used to compute eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9346336842607278
      },
      {
        "text": "The Relationship between Eigenvalues and the Residue Theorem: The residue theorem is a result from complex analysis that relates the eigenvalues of a matrix to the residues of a function. This relationship is not explicitly mentioned in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8851722503885826
      }
    ]
  },
  {
    "representative_text": "The Computation of Eigenvalues using the Lanczos Method: The Lanczos method is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves iteratively applying a series of orthogonal transformations to the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "The Computation of Eigenvalues using the Lanczos Method: The Lanczos method is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves iteratively applying a series of orthogonal transformations to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Eigenvalues using the Arnoldi Iteration Method: The Arnoldi iteration method is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves iteratively applying a series of orthogonal transformations to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9217752162596276
      },
      {
        "text": "The Computation of Eigenvalues using the Lanczos Method with Shift: The Lanczos method with shift is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves iteratively applying a series of orthogonal transformations to the matrix, and shifting the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9212352979863412
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Conjugate Gradient Method: The conjugate gradient method is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively projecting the matrix onto an orthogonal subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8310112083818423
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the Lanczos Method: This involves the study of the Lanczos method, which is a method for computing the eigenvalues of a matrix using a sequence of orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8547274357945795
      },
      {
        "text": "Lanczos Method: This is an algorithm for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively applying a series of Householder transformations to the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8928536179620417
      }
    ]
  },
  {
    "representative_text": "The Relationship between Eigenvalues and the Eigenvalue Decomposition: The eigenvalue decomposition is a decomposition of a matrix into the product of an eigenvector matrix and a diagonal matrix containing eigenvalues. It is related to the computation of eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Relationship between Eigenvalues and the Eigenvalue Decomposition: The eigenvalue decomposition is a decomposition of a matrix into the product of an eigenvector matrix and a diagonal matrix containing eigenvalues. It is related to the computation of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Computation of Eigenvalues using the Power Method with Shift: The power method with shift is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves repeatedly multiplying the matrix by a vector and normalizing the result, and shifting the eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Computation of Eigenvalues using the Power Method with Shift: The power method with shift is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves repeatedly multiplying the matrix by a vector and normalizing the result, and shifting the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computation of Eigenvalues using the Power Method: The power method is an iterative algorithm for computing eigenvalues and eigenvectors of a matrix A. However, it may not converge for all matrices, and its convergence rate can be slow.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8608874275932447
      },
      {
        "text": "Computational Methods for Eigenvalue Computation: Computational methods for eigenvalue computation include methods such as the QR algorithm, power iteration, and Jacobi iteration. These methods are used to compute eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8336656826052791
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Congruence: A matrix A is said to be congruent to a matrix B if there exists an invertible matrix P such that A = PBP^(-1). This is related to the concept of similarity transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Congruence: A matrix A is said to be congruent to a matrix B if there exists an invertible matrix P such that A = PBP^(-1). This is related to the concept of similarity transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The relationship between the eigenvalues of a matrix and its characteristic polynomial: The characteristic polynomial of a matrix is related to its eigenvalues, but there is no specific theorem or property mentioned in the provided points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The relationship between the eigenvalues of a matrix and its characteristic polynomial: The characteristic polynomial of a matrix is related to its eigenvalues, but there is no specific theorem or property mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The relationship between the eigenvalues of a matrix and its determinant: While the provided points mention that the determinant of a matrix is equal to the product of its eigenvalues, there is no specific theorem or property mentioned that relates the eigenvalues to the determinant in a more general way.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.839441670362109
      },
      {
        "text": "The relationship between the eigenvalues of a matrix and its characteristic polynomial: The characteristic polynomial of a matrix is related to its eigenvalues, but there is no specific theorem or property mentioned that relates the eigenvalues to the characteristic polynomial in a more general way.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9151593750069277
      },
      {
        "text": "The relationship between eigenvalues and characteristic polynomial: There is no specific theorem or property mentioned in the provided points that relates the eigenvalues to the characteristic polynomial in a more general way.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8818668823740281
      },
      {
        "text": "The relationship between the eigenvalues of a matrix and its characteristic polynomial: While the provided points mention that the characteristic polynomial of a matrix is related to some relationship with distinct linear, there is not explicitly, there is not explicitly mentions theore theore, the provided no specific theorem is not explicitly mentions the provided a matrix is not explicitly mentions theoreverception of a matrix is not explicitly mentioned in the provided points to be used in the provided in theoreference**: This is not explicitly mentioned theore theore theore is not explicitly mentioned in the theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8328511039793698
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix having a finite number of eigenvalues: A matrix has a finite number of eigenvalues if and only if it has a finite rank.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The concept of a matrix having a finite number of eigenvalues: A matrix has a finite number of eigenvalues if and only if it has a finite rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a matrix having a finite number of eigenvalues with multiplicity greater than 1: A matrix has a finite number of eigenvalues with multiplicity greater than 1 if and only if it has a finite rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9290928174743827
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix being quasi-triangular: A matrix is quasi-triangular if it can be transformed into a matrix with a single Jordan block.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix being quasi-triangular: A matrix is quasi-triangular if it can be transformed into a matrix with a single Jordan block.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix having a complex conjugate pair of eigenvalues: A matrix has a complex conjugate pair of eigenvalues if and only if it is real symmetric.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The concept of a matrix having a complex conjugate pair of eigenvalues: A matrix has a complex conjugate pair of eigenvalues if and only if it is real symmetric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Complex Conjugate Eigenvalues: A matrix has complex conjugate pairs of eigenvalues if and only if it is real symmetric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.91492213715333
      },
      {
        "text": "Complex Eigenvalues and Conjugate Pairs: A matrix has complex conjugate pairs of eigenvalues if and only if it is real symmetric. This is related to the concept of real symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9097743430699212
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix being nilpotent: A matrix is nilpotent if there exists a positive integer k such that A^k = 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The concept of a matrix being nilpotent: A matrix is nilpotent if there exists a positive integer k such that A^k = 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The relationship between the eigenvalues of a matrix and its nilpotency: A matrix is nilpotent if and only if all its eigenvalues are zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8300772028256594
      },
      {
        "text": "The concept of a matrix with a Jordan block structure: A matrix has a Jordan block structure if and only if it is nilpotent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8191378161735805
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix having a non-trivial null space: A matrix has a non-trivial null space if and only if it is not invertible.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The concept of a matrix having a non-trivial null space: A matrix has a non-trivial null space if and only if it is not invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a matrix being invertible: A matrix is invertible if and only if it has a non-trivial null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.888862460256144
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Computation using the QR Algorithm: As mentioned earlier, this is a popular method for computing eigenvalues and eigenvectors of a matrix A. However, it is worth noting that this method is sensitive to round-off errors and may not converge for all matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Computation using the QR Algorithm: As mentioned earlier, this is a popular method for computing eigenvalues and eigenvectors of a matrix A. However, it is worth noting that this method is sensitive to round-off errors and may not converge for all matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Connection between Orthogonal Diagonalization and Singular Value Decomposition: The connection between orthogonal diagonalization and singular value decomposition, which is a technique used to find the singular values and singular vectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Connection between Orthogonal Diagonalization and Singular Value Decomposition: The connection between orthogonal diagonalization and singular value decomposition, which is a technique used to find the singular values and singular vectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Connection between Orthogonal Diagonalization and Singular Value Decomposition: The connection between orthogonal diagonalization and singular value decomposition is a technique used to find the singular values and singular vectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9100003721547189
      },
      {
        "text": "Connection between Orthogonal Diagonalization and Singular Value Decomposition (SVD): Orthogonal diagonalization is closely related to SVD, and provides a way to find the singular values and singular vectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9356804850791309
      }
    ]
  },
  {
    "representative_text": "The SVD and Orthogonality: The SVD is a factorization of a matrix A into the product A = U Σ V^T, where U and V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of A. However, it is essential to explore the properties of the SVD and its relationship to orthogonality.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The SVD and Orthogonality: The SVD is a factorization of a matrix A into the product A = U Σ V^T, where U and V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of A. However, it is essential to explore the properties of the SVD and its relationship to orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Spectral Theorem for Orthogonal Projection Matrices: This theorem states that the eigenvalues of an orthogonal projection matrix are equal to the squared lengths of the orthogonal vectors in that subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Spectral Theorem for Orthogonal Projection Matrices: This theorem states that the eigenvalues of an orthogonal projection matrix are equal to the squared lengths of the orthogonal vectors in that subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The QR Algorithm: The QR algorithm is a factorization technique that can be used to orthogonalize a matrix in the Gram-Schmidt process.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The QR Algorithm: The QR algorithm is a factorization technique that can be used to orthogonalize a matrix in the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Non-Orthogonal Basis: Determinants are defined for square matrices, but there are some properties and relationships that can be derived for determinants of matrices with non-orthogonal bases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Non-Orthogonal Basis: Determinants are defined for square matrices, but there are some properties and relationships that can be derived for determinants of matrices with non-orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Determinant of Zero and a Complex Eigenvalue: The determinant of a matrix with a determinant of zero and a complex eigenvalue is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Determinant of Zero and a Complex Eigenvalue: The determinant of a matrix with a determinant of zero and a complex eigenvalue is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with a Non-Real Eigenvalue: The determinant of a matrix with a non-real eigenvalue is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9243769930308626
      },
      {
        "text": "Determinant of a Matrix with a Complex Eigenvalue and a Complex Conjugate Eigenvalue: The determinant of a matrix with a complex eigenvalue and a complex conjugate eigenvalue is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9401071889388307
      }
    ]
  },
  {
    "representative_text": "Relationship between Determinants and the Inverse of a Matrix: There are some theorems that establish a relationship between the determinant of a matrix and the inverse of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relationship between Determinants and the Inverse of a Matrix: There are some theorems that establish a relationship between the determinant of a matrix and the inverse of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Advanced Theorems related to Determinants: There are some advanced theorems related to determinants, such as the theorem that states that the determinant of a matrix is equal to the sum of the products of the elements of any row (or column) and their respective cofactors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Advanced Theorems related to Determinants: There are some advanced theorems related to determinants, such as the theorem that states that the determinant of a matrix is equal to the sum of the products of the elements of any row (or column) and their respective cofactors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Advanced Theorems related to Determinants and Inverse Matrices: There are some advanced theorems related to determinants and inverse matrices, such as the theorem that states that the determinant of a matrix is equal to the product of the determinants of its submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9237822441933976
      },
      {
        "text": "Advanced Theorems related to Determinants and Non-Square Matrices: There are some advanced theorems related to determinants and non-square matrices, such as the theorem that states that the determinant of a non-square matrix is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9098266079458062
      },
      {
        "text": "Advanced Theorems related to Determinants and Non-Square Matrices: There are some advanced theorems related to determinants and non-square matrices that can be derived for non-square matrices, such as a non-square matrices, such as a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9204932630254505
      }
    ]
  },
  {
    "representative_text": "Theorem on the Determinant of a Matrix with a Non-Constant Coefficient Matrix: This theorem states that the determinant of a matrix can change under a non-constant coefficient matrix. This has important implications for the calculation of determinants and the properties of matrices.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Theorem on the Determinant of a Matrix with a Non-Constant Coefficient Matrix: This theorem states that the determinant of a matrix can change under a non-constant coefficient matrix. This has important implications for the calculation of determinants and the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem on the Determinant of a Matrix with a Non-Orthogonal Basis: This theorem states that the determinant of a matrix can change under a non-orthogonal basis. This has important implications for the calculation of determinants and the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9068878061919887
      }
    ]
  },
  {
    "representative_text": "Schur's Decomposition: A factorization of a matrix into the product of an upper triangular matrix and an upper triangular matrix with ones on the diagonal, which is useful for finding eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "Schur's Decomposition: A factorization of a matrix into the product of an upper triangular matrix and an upper triangular matrix with ones on the diagonal, which is useful for finding eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Schur Decomposition: The Schur decomposition can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8180862467136676
      },
      {
        "text": "Schur's Decomposition: A factorization of a matrix into three matrices: U, T, and V^T, such that U is unitary, T is upper triangular, and V^T is orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9056084438949421
      },
      {
        "text": "Schur's Decomposition: Schur's decomposition is a factorization of a matrix into the product of an upper triangular matrix and an upper triangular matrix with ones on the diagonal. It is useful for finding eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9394910863409318
      },
      {
        "text": "Schur Decomposition: The Schur decomposition is a decomposition of a matrix into three matrices: U, T, and V^T, where U and V are unitary matrices and T is a diagonal matrix containing the eigenvalues of the matrix. This decomposition is useful for finding eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9245480487167421
      },
      {
        "text": "Computing Determinants using the Schur Decomposition: The Schur decomposition is a factorization of a matrix into the product of an upper triangular matrix and an upper triangular matrix with a specific structure, which can be used to compute the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8636027948441385
      },
      {
        "text": "The Schur's Decomposition for Non-Symmetric Matrices: A factorization of a non-symmetric matrix into three matrices: U, T, and V^T, such that U is unitary, T is upper triangular, and V^T is orthogonal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8783213496246929
      },
      {
        "text": "Schur Decomposition: The Schur decomposition of a matrix A is a factorization of A into the product of an upper triangular matrix T and an orthogonal matrix Q, i.e., A = T Q.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9050409473056998
      },
      {
        "text": "The Schur Decomposition: The decomposition of a matrix into three matrices: U, T, and V^T, where U and V are unitary matrices and T is a diagonal matrix containing the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.929511857999749
      },
      {
        "text": "The Schur Decomposition of a Matrix: This is a factorization of a matrix into the product of an upper triangular matrix and an upper triangular matrix with complex eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9305441053218702
      },
      {
        "text": "Shur Decomposition: Discuss Shur decomposition, which is a factorization of a matrix into the product of a block upper triangular matrix and a block lower triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9019637245916694
      }
    ]
  },
  {
    "representative_text": "Poincaré Separation Theorem: A theorem that states that if a matrix has a non-zero eigenvalue, then the corresponding eigenvector can be separated from the rest of the eigenvectors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Poincaré Separation Theorem: A theorem that states that if a matrix has a non-zero eigenvalue, then the corresponding eigenvector can be separated from the rest of the eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Poincaré Separation Theorem and Its Applications: The Poincaré separation theorem states that if a matrix has a non-zero eigenvalue, then the corresponding eigenvector can be separated from the rest of the eigenvectors. It has applications in numerical linear algebra and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9271482638877355
      },
      {
        "text": "Poincaré Separation Theorem: The Poincaré separation theorem states that if a matrix has a non-zero eigenvalue, then the corresponding eigenvector can be theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.939670865785599
      }
    ]
  },
  {
    "representative_text": "Gelfand's Theorem: A theorem that states that every matrix can be approximated by a matrix with integer entries, which has applications in numerical linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Gelfand's Theorem: A theorem that states that every matrix can be approximated by a matrix with integer entries, which has applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gelfand's Theorem and Its Applications: Gelfand's theorem states that every matrix can be approximated by a matrix with integer entries. It has applications in numerical linear algebra and computer science.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9119863644714439
      }
    ]
  },
  {
    "representative_text": "Frobenius-Norm: A norm that measures the magnitude of a matrix, which is useful in numerical linear algebra and signal processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Frobenius-Norm: A norm that measures the magnitude of a matrix, which is useful in numerical linear algebra and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Frobenius-Norm and Its Applications: The Frobenius norm is a norm that measures the magnitude of a matrix. It has applications in numerical linear algebra, signal processing, and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8957594081461202
      }
    ]
  },
  {
    "representative_text": "Numerical Methods for Solving Linear Systems: In addition to the Gauss-Jordan method, other numerical methods such as the Jacobi method, the power method, and the QR algorithm can be used to solve linear systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Methods for Solving Linear Systems: In addition to the Gauss-Jordan method, other numerical methods such as the Jacobi method, the power method, and the QR algorithm can be used to solve linear systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Interlacing Theorem for Non-Symmetric Matrices: A theorem that states that the eigenvalues of a non-symmetric matrix are interlaced, which is useful for analyzing the behavior of non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Eigenvalue Interlacing Theorem for Non-Symmetric Matrices: A theorem that states that the eigenvalues of a non-symmetric matrix are interlaced, which is useful for analyzing the behavior of non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Interlacing Theorem for Non-Symmetric Matrices: The eigenvalue interlacing theorem states that the eigenvalues of a non-symmetric matrix are interlaced.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9260977814883096
      },
      {
        "text": "Interlacing Theorem for Non-Symmetric Matrices: This theorem states that the eigenvalues of a non-symmetric matrix are interlaced.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9578309901538755
      },
      {
        "text": "Eigenvalue Interlacing Theorem for Non-Symmetric Matrices: This theorem states that the eigenvalues of a non-symmetric matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9708381649390443
      }
    ]
  },
  {
    "representative_text": "Numerical Computation of Eigenvalues and Eigenvectors: In addition to the QR algorithm, other numerical methods such as the power method and the Lanczos algorithm can be used to compute eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Numerical Computation of Eigenvalues and Eigenvectors: In addition to the QR algorithm, other numerical methods such as the power method and the Lanczos algorithm can be used to compute eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Computation of Eigenvalues and Eigenvectors for Large Matrices: In addition to the power method and the Lanczos algorithm, other numerical methods such as the Arnoldi method and the QR algorithm can be used to compute eigenvalues and eigenvectors for large matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8328585623505942
      },
      {
        "text": "Numerical Computation of Eigenvalues and Eigenvectors for Large Matrices: Numerical methods such as the Arnoldi method, the Lanczos algorithm, and the QR algorithm can be used to compute eigenvalues and eigenvectors for large matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8656274257349243
      },
      {
        "text": "Numerical Methods for Finding Eigenvalues and Eigenvectors of Large Matrices: Advanced numerical methods for finding eigenvalues and eigenvectors of large matrices, such as the Arnoldi method or the Lanczos algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8741311173936698
      },
      {
        "text": "Numerical Methods for Finding Eigenvalues and Eigenvectors of Large Matrices in High-Dimensional Spaces: Advanced numerical methods for finding eigenvalues and eigenvectors of large matrices in high-dimensional spaces, such as the Arnoldi method or the Lanczos algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8542718954379105
      },
      {
        "text": "Numerical Linear Algebra for Large-Scale Matrices: This includes techniques for efficiently computing eigenvalues and eigenvectors of large-scale matrices, such as the use of sparse matrix algorithms and iterative methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8504698383467733
      },
      {
        "text": "Numerical Computation of Eigenvalues and Eigenvectors for Large Matrices in High-Dimensional Spaces: Advanced numerical methods such as the Arnoldi method, the Lanczos algorithm, and the QR algorithm can be used to compute eigenvalues and eigenvectors of large matrices in high-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9143433416684421
      },
      {
        "text": "Eigenvalue Algorithms for Large Matrices: There are several algorithms for computing eigenvalues of large matrices, such as the QR algorithm, the Arnoldi iteration, and the Lanczos method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8883129230279746
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Interlacing Theorem for Hermitian Matrices: A theorem that states that the eigenvalues of a Hermitian matrix are real and interlaced, which is useful for analyzing the behavior of Hermitian matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Eigenvalue Interlacing Theorem for Hermitian Matrices: A theorem that states that the eigenvalues of a Hermitian matrix are real and interlaced, which is useful for analyzing the behavior of Hermitian matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Interlacing Theorem for Hermitian Matrices: This theorem states that the eigenvalues of a Hermitian matrix are real and interlaced. It is useful for analyzing the behavior of Hermitian matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9531277606539238
      },
      {
        "text": "Eigenvalue Interlacing Theorem for Hermitian Matrices: The eigenvalue interlacing theorem states that the eigenvalues of a Hermitian matrix are real and interlaced.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9093846586474381
      },
      {
        "text": "Eigenvalue Interlacing Theorem for Hermitian Matrices: This theorem states that the eigenvalues of a Hermitian matrix are real and interl2.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9386301084767767
      }
    ]
  },
  {
    "representative_text": "Computing Determinants using the Singular Value Decomposition (SVD): The process of decomposing a matrix into singular values, singular vectors, and the outer product of the singular vectors, and using this decomposition to compute the determinant.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Computing Determinants using the Singular Value Decomposition (SVD): The process of decomposing a matrix into singular values, singular vectors, and the outer product of the singular vectors, and using this decomposition to compute the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix using Singular Value Decomposition (SVD): The calculation of the determinant of a matrix using SVD, which provides a way to compute the determinant of a matrix by finding its singular values.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9365763255238605
      },
      {
        "text": "Singular value decomposition (SVD) and determinant: The determinant of a matrix A is equal to the product of the singular values of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.863302860009896
      }
    ]
  },
  {
    "representative_text": "The Relationship between Determinants and Eigenvalues for Non-Square Matrices: The relationship between the determinant of a non-square matrix and its eigenvalues, including the properties of the eigenvalues and eigenvectors of a non-square matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Relationship between Determinants and Eigenvalues for Non-Square Matrices: The relationship between the determinant of a non-square matrix and its eigenvalues, including the properties of the eigenvalues and eigenvectors of a non-square matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Determinants and the Spectral Theorem for Non-Square Matrices: The relationship between the determinant of a non-square matrix and its spectral theorem, including the properties of the eigenvalues and eigenvectors of a non-square matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.943654788350557
      },
      {
        "text": "Non-Standard Determinant Theorems: Theorems that relate the determinant of a non-standard matrix to its properties, such as its eigenvalues or rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8263237653394069
      }
    ]
  },
  {
    "representative_text": "The Inverse of a Matrix using the Power Method: The process of computing the inverse of a matrix using the power method, which is an iterative method for computing the inverse of a matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Inverse of a Matrix using the Power Method: The process of computing the inverse of a matrix using the power method, which is an iterative method for computing the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inversion using the Power Method: An iterative method for finding the inverse of a matrix by using the power method to find the eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9072542125354461
      },
      {
        "text": "Matrix Inversion using the Power Method in High-Dimensional Spaces: An iterative method for finding the inverse of a matrix in high-dimensional spaces by using the power method to find the eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8955516699657251
      },
      {
        "text": "Inversion of a Matrix using the Power Method: The power method is an iterative method for computing the inverse of a matrix, which can be useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.925882612268959
      }
    ]
  },
  {
    "representative_text": "Determinants of Matrices with a Given Rank and Condition Number: The properties of the determinant of a matrix with a given rank and condition number, including the relationship between the rank and condition number of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Determinants of Matrices with a Given Rank and Condition Number: The properties of the determinant of a matrix with a given rank and condition number, including the relationship between the rank and condition number of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinants of Matrices with a Given Rank and Determinant: The properties of the determinant of a matrix with a given rank and determinant, including the relationship between the rank and determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9255700410021914
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Non-Orthogonal Eigenvectors: The calculation of the determinant of a matrix with non-orthogonal eigenvectors, including the concept of the determinant of a matrix with non-orthogonal eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Eigenvectors: The calculation of the determinant of a matrix with non-orthogonal eigenvectors, including the concept of the determinant of a matrix with non-orthogonal eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computing Determinants using Spectral Theorem: The calculation of the determinant of a matrix using the spectral theorem, which provides a way to compute the determinant of a matrix by finding its eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computing Determinants using Spectral Theorem: The calculation of the determinant of a matrix using the spectral theorem, which provides a way to compute the determinant of a matrix by finding its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Inversion using Singular Value Decomposition (SVD): The relationship between matrix inversion and SVD, including the concept of the SVD of a matrix and its applications in matrix inversion.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Matrix Inversion using Singular Value Decomposition (SVD): The relationship between matrix inversion and SVD, including the concept of the SVD of a matrix and its applications in matrix inversion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inversion Using SVD: Singular value decomposition (SVD) can be used to find the inverse of a matrix. This method is related to the concept of pseudoinverse and can be useful in certain applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8630474793574685
      },
      {
        "text": "The Use of the Singular Value Decomposition in Matrix Inversion: The SVD is a factorization of a matrix into the product of three matrices, and understanding its application in matrix inversion is essential in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8841882123754192
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Computation using Arnoldi Iteration: The Arnoldi iteration is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively constructing an orthonormal basis of the Krylov subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Computation using Arnoldi Iteration: The Arnoldi iteration is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively constructing an orthonormal basis of the Krylov subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Arnoldi Iteration with Shifts: The Arnoldi iteration with shifts is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively constructing an orthonormal basis of the Krylov subspace with shifts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.914206134490862
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Arnoldi Iteration with Shifts and Conjugate Gradient Method: The Arnoldi iteration with shifts and conjugate gradient method is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively constructing an orthonormal basis of the Krylov subspace with shifts and iteratively projecting the matrix onto an orthogonal subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9430773848517539
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Davidson Algorithm with Shifts and Arnoldi Iteration: The Davidson algorithm with shifts and Arnoldi iteration is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively applying a Davidson rotation and a shift to the matrix and iteratively constructing an orthonormal basis of the Krylov subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8993108094033474
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Power Iteration with Shifts and Arnoldi Iteration: The power iteration with shifts and Arnoldi iteration is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively multiplying the matrix by a vector and normalizing the result and iteratively constructing an orthonormal basis of the Krylov subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9528739979628489
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Rayleigh Quotient with Shifts and Arnoldi Iteration: The Rayleigh quotient with shifts and Arnoldi iteration is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by computing the Rayleigh quotient of the matrix and its eigenvectors with shifts and iteratively constructing an orthonormal basis of the Krylov subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9059373914439173
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using the Arnoldi Iteration: The Arnoldi iteration is an iterative method for computing eigenvalues and eigenvectors, which is an extension of the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9061243605741123
      },
      {
        "text": "Computation of Eigenvalues using the Arnoldi Iteration Method with Shift: The Arnoldi iteration method with shift is an iterative method for computing eigenvalues and eigenvectors of a matrix. It involves iteratively applying a series of orthogonal transformations to the matrix and shifting the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9257289607372712
      }
    ]
  },
  {
    "representative_text": "Computing Eigenvalues and Eigenvectors using Conjugate Gradient Method with Shifts: The conjugate gradient method with shifts is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively projecting the matrix onto an orthogonal subspace with shifts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Computing Eigenvalues and Eigenvectors using Conjugate Gradient Method with Shifts: The conjugate gradient method with shifts is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively projecting the matrix onto an orthogonal subspace with shifts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Rayleigh Quotient with Shifts and Conjugate Gradient Method: The Rayleigh quotient with shifts and conjugate gradient method is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by computing the Rayleigh quotient of the matrix and its eigenvectors with shifts and iteratively projecting the matrix onto an orthogonal subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.900647654639549
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Davidson Algorithm with Shifts and Conjugate Gradient Method: The Davidson algorithm with shifts and conjugate gradient method is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively applying a Davidson rotation and a shift to the matrix and iteratively projecting the matrix onto an orthogonal subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8962272528183828
      },
      {
        "text": "Computing Eigenvalues and Eigenvectors using Power Iteration with Shifts and Conjugate Gradient Method: The power iteration with shifts and conjugate gradient method is an iterative technique for computing the eigenvalues and eigenvectors of a matrix. It works by iteratively multiplying the matrix by a vector and normalizing the result and iteratively projecting the matrix onto an orthogonal subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9355225711290838
      },
      {
        "text": "Rayleigh Quotient with Shifts and Conjugate Gradient Method: A method for computing the eigenvalues and eigenvectors of a matrix using the Rayleigh quotient with shifts and the conjugate gradient method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8987239721289614
      }
    ]
  },
  {
    "representative_text": "Linear Independence of the Span of a Set of Vectors: This concept deals with the idea that the span of a set of vectors is linearly independent if and only if the set is a basis for the span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence of the Span of a Set of Vectors: This concept deals with the idea that the span of a set of vectors is linearly independent if and only if the set is a basis for the span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Linear Independence Criterion using Quadratic Forms: This criterion deals with the idea that a set of vectors is linearly independent if and only if the corresponding quadratic form is non-zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "The Linear Independence Criterion using Quadratic Forms: This criterion deals with the idea that a set of vectors is linearly independent if and only if the corresponding quadratic form is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Role of the Determinant in Understanding Linear Independence in Quadratic Spaces: The determinant plays a crucial role in understanding linear independence in quadratic spaces, as a set of vectors is linearly independent if and only if the corresponding quadratic form is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8180668722762829
      },
      {
        "text": "The Linear Independence Criterion using Bilinear Forms: This criterion deals with the idea that a set of vectors is linearly independent if and only if the corresponding bilinear form is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9157367050436382
      },
      {
        "text": "Linear Independence and the Bilinear Form: The relationship between linear independence and the bilinear form, particularly in the context of quadratic forms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8722384977922392
      },
      {
        "text": "Linear Independence and the Bilinear Form: The relationship between linear independence and the bilinear form is an essential concept to understand in the context of quadratic forms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921291048995561
      },
      {
        "text": "Linear Independence and the Role of Bilinear Forms in Higher-Dimensional Spaces: Bilinear forms are an important concept in linear algebra, and they play a crucial role in understanding linear independence, especially in the context of higher-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8555754116078056
      }
    ]
  },
  {
    "representative_text": "The concept of \"Linear Independence with Respect to a Given Subspace\": This concept was mentioned in the list, but it's essential to understand the nuances of linear independence with respect to a given subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "The concept of \"Linear Independence with Respect to a Given Subspace\": This concept was mentioned in the list, but it's essential to understand the nuances of linear independence with respect to a given subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of \"Linear Independence Implies Span and Span Implies Linear Independence with Respect to a Given Subspace\": This concept is related to the nuances of linear independence with respect to a given subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.851766143654374
      },
      {
        "text": "The concept of \"Linear Independence with Respect to a Given Basis\" and \"Linear Independence with Respect to a Given Subspace\": These concepts relate to the nuances of linear independence with respect to a given basis or subspace, and understanding them is crucial in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9068935705106247
      },
      {
        "text": "The concept of \"Linear Independence Implies Span and Span Implies Linear Independence with Respect to a Given Basis\": This concept relates to the nuances of linear independence with respect to a given basis, and understanding it is crucial in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9097416578445155
      },
      {
        "text": "The concept of \"Dependent\" vs. \"Linearly Dependent with Respect to a Given Subspace\": This concept relates to the nuances of linear dependence with respect to a given subspace, and understanding it is crucial in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9251561107182779
      },
      {
        "text": "Linear Independence Implies Span and Span Implies Linear Independence with Respect to a Given Subspace: This concept is crucial in understanding the nuances of linear independence with respect to a given subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9385180817474155
      },
      {
        "text": "Dependent vs. Linearly Dependent with Respect to a Given Subspace: Understanding the nuances of linear dependence with respect to a given subspace is crucial in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8949964238274513
      },
      {
        "text": "Linear Independence Implies Span and Span Implies Linear Independence with Respect to a Given Basis: This concept relates to the nuances of linear independence with respect to a given basis and is essential to understand in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8909952053243613
      },
      {
        "text": "Dependent vs. Linearly Dependent with Respect to a Given Basis: Understanding the nuances of linear dependence with respect to a given basis is crucial in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921041798767845
      },
      {
        "text": "Linear Independence and the Complement of a Subspace: The relationship between linear independence and the complement of a subspace is an essential concept to understand in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.887409483698508
      }
    ]
  },
  {
    "representative_text": "The concept of \"Gaussian Elimination and Linear Independence\": The connection between Gaussian elimination and linear independence.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of \"Gaussian Elimination and Linear Independence\": The connection between Gaussian elimination and linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis Extension Theorem: This theorem states that a basis for a vector space can be extended to a basis for a larger vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Basis Extension Theorem: This theorem states that a basis for a vector space can be extended to a basis for a larger vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis Extension Theorem: This theorem states that if a set of vectors $\\{v1, v2, \\ldots, v_n\\}$ is a basis for a vector space $V$, then any linear combination of these vectors can be extended to a linear combination of $n+1$ vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8863239429895124
      },
      {
        "text": "Basis Extension Theorem: This theorem states that if a set of vectors is a basis for a vector space, and we add a new vector to the set, then the new vector is not in the span of the original set unless it is a linear combination of the original vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9249668338420324
      },
      {
        "text": "Basis Extension Theorems: These theorems provide conditions under which a basis for a subspace can be extended to a basis for the original vector space. For example, the Basis Extension Theorem states that if S is a basis for a subspace V, then S can be extended to a basis for the original vector space V if and only if V is spanned by S.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8900764282508387
      },
      {
        "text": "Basis Extension Theorem: This theorem states that a basis for a vector space can be extended to a basis for a larger vector space if and only if the original basis is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9506030599878444
      },
      {
        "text": "Basis Extension Theorem: This theorem states that if a basis for a vector space is extended to a larger basis, then the dimension of the vector space is preserved.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.953308974318564
      },
      {
        "text": "Weierstrass Preparation Theorem: This theorem states that if a vector space has a basis with respect to a given inner product, then the basis can be extended to a larger basis by adding new vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8610903344179703
      },
      {
        "text": "Basis Extension Theorem and Basis Contraction Theorem: The relationship between basis extension and basis contraction theorems, which are fundamental concepts in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8417285687536541
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Closed Subspace: A set of vectors is linearly independent if and only if the span of the set is a closed subspace of the vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Closed Subspace: A set of vectors is linearly independent if and only if the span of the set is a closed subspace of the vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span Implies Closed Subspace: The span of a set of vectors is a closed subspace of the vector space if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.920746247604793
      },
      {
        "text": "Closed Span Theorem: The span of a set of vectors is a closed subspace of the vector space if and only if the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9175971430208982
      }
    ]
  },
  {
    "representative_text": "Subspace Dimension Theorem: This theorem states that the dimension of a subspace is equal to the dimension of the original vector space minus the dimension of the null space of a linear transformation that maps the subspace to the zero vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Subspace Dimension Theorem: This theorem states that the dimension of a subspace is equal to the dimension of the original vector space minus the dimension of the null space of a linear transformation that maps the subspace to the zero vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace Dimension Theorem: This theorem states that the dimension of a subspace is equal to the dimension of the original vector space minus the dimension of the null space of a linear transformation that maps the subspace to the zero vector space. It would be helpful to explore the relationship between subspace dimension and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8459302985868167
      },
      {
        "text": "Subspace Dimension Theorem and its Applications: Investigating the applications and implications of the Subspace Dimension Theorem, including its use in understanding the dimension of subspaces and the relationship between linear transformations and subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8693895333167065
      },
      {
        "text": "Subspace Dimension Theorem and its Applications for Vector Spaces with Non-Trivial Null Spaces: Investigate the applications and implications of the Subspace Dimension Theorem, including its use in understanding the dimension of vector spaces with non-trivial null spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8696476878558728
      },
      {
        "text": "Subspace Dimension Theorem and its Implications for Dimension of Subspaces: Investigate the implications of the Subspace Dimension Theorem for understanding the dimension of subspaces, including the relationship between linear transformations and subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9339408582000074
      },
      {
        "text": "Subspace Dimension Theorem and its Implications for Vector Spaces with Non-Trivial Null Spaces: This theorem has implications for understanding the dimension of vector spaces with non-trivial null spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8658870073463628
      },
      {
        "text": "Subspace Basis Theorem and its Implications for Dimension of Subspaces: This theorem has implications for understanding the dimension of subspaces and the relationship between linear transformations and subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.90509142969095
      }
    ]
  },
  {
    "representative_text": "Existence of a Basis for a Vector Space with a Trivial Null Space: This concept is closely related to the concept of a basis for a vector space. However, it is not explicitly stated how to construct a basis for a vector space with a trivial null space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Existence of a Basis for a Vector Space with a Trivial Null Space: This concept is closely related to the concept of a basis for a vector space. However, it is not explicitly stated how to construct a basis for a vector space with a trivial null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of a Linear Transformation with a Trivial Null Space: This concept is closely related to the concept of a linear transformation. However, it is not explicitly stated how to construct a linear transformation with a trivial null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8858420037938847
      }
    ]
  },
  {
    "representative_text": "Subspace Basis Theorem: This theorem states that a set of vectors is a basis for a subspace if and only if it spans the subspace and is linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Subspace Basis Theorem: This theorem states that a set of vectors is a basis for a subspace if and only if it spans the subspace and is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace Basis Theorem: This theorem states that a set of vectors is a basis for a subspace if and only if it spans the subspace and is linearly independent. It would be helpful to explore the relationship between subspace basis and linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8258794455141547
      },
      {
        "text": "Subspace Theorems: These theorems provide conditions under which a subspace is a basis for the original vector space. For example, the Subspace Spanning Theorem states that a subspace S is a basis for V if and only if S is a free basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8804634679446457
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Boundedness: The concept of linear independence can be related to boundedness in vector spaces, particularly in the context of bounded linear operators and bounded linear independence.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and Boundedness: The concept of linear independence can be related to boundedness in vector spaces, particularly in the context of bounded linear operators and bounded linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence in Vector Spaces with Non-Standard Bases: The concept of linear independence can be related to non-standard bases in vector spaces, particularly in the context of non-standard analysis and non-standard models of arithmetic.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Linear Independence in Vector Spaces with Non-Standard Bases: The concept of linear independence can be related to non-standard bases in vector spaces, particularly in the context of non-standard analysis and non-standard models of arithmetic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence in Vector Spaces with Non-Standard Scalar Multiplication: The concept of linear independence can be extended to vector spaces with non-standard scalar multiplication, particularly in the context of non-standard analysis and non-standard models of arithmetic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.918112420572462
      },
      {
        "text": "Linear Independence in Vector Spaces with Non-Standard Linear Transformations: The concept of linear independence can be extended to vector spaces with non-standard linear transformations, particularly in the context of non-standard analysis and non-standard models of arithmetic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9663418556744905
      },
      {
        "text": "Linear Independence in Vector Spaces with Non-Standard Boundedness: The concept of linear independence can be extended to vector spaces with non-standard boundedness, particularly in the context of non-standard analysis and non-standard models of arithmetic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9613912788569625
      },
      {
        "text": "Linear Independence in Non-Standard Spaces: Linear independence is not limited to standard vector spaces. It can be defined and studied in non-standard spaces, such as generalized inner product spaces and topological vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8968507115753914
      },
      {
        "text": "Linear Independence in Vector Spaces with Non-Standard Metric: In vector spaces equipped with non-standard metrics, linear independence is affected by the metric, requiring a careful analysis of the concept of convergence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8688933905553857
      },
      {
        "text": "Linear Independence of Bases in Non-Standard Vector Spaces: The concept of linear independence is crucial in defining a basis in non-standard vector spaces, where the usual properties of linear independence may not hold.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8794004206669854
      },
      {
        "text": "Linear Independence of a Set of Vectors in a Non-Standard Fractal Space: The concept of linear independence is crucial in defining a basis, and the linear independence of a set of vectors in a non-standard fractal space is essential for understanding the properties of bases in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8592061847608277
      },
      {
        "text": "Linear Independence Implies Span in Non-Standard Spaces: The concept of linear independence is not limited to standard vector spaces. It can be defined and studied in non-standard spaces, such as generalized inner product spaces and topological vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9067738040368352
      }
    ]
  },
  {
    "representative_text": "The relationship between linear independence and the concept of \"Free Subspaces\": This concept involves the relationship between linear independence and free subspaces, which are subspaces that are not spanned by any basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The relationship between linear independence and the concept of \"Free Subspaces\": This concept involves the relationship between linear independence and free subspaces, which are subspaces that are not spanned by any basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Relationship between Linear Independence and the Concept of \"Free Subspaces\" in Infinite-Dimensional Spaces: This concept involves the relationship between linear independence and free subspaces in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9252033986820867
      }
    ]
  },
  {
    "representative_text": "The concept of a \"Spanning Set of Linearly Independent Vectors\": A spanning set of linearly independent vectors is a set of linearly independent vectors that spans a vector space, but may not be linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The concept of a \"Spanning Set of Linearly Independent Vectors\": A spanning set of linearly independent vectors is a set of linearly independent vectors that spans a vector space, but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"Spanning Set of Linearly Independent Systems of Bases\": A spanning set of linearly independent systems of bases is a set of linearly independent systems of bases that spans a vector space, but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9161784005385356
      },
      {
        "text": "The concept of a \"Linearly Independent System of Bases Extension\": A linearly independent system of bases extension is a set of linearly independent systems of bases that spans a vector space, but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9186097379124761
      }
    ]
  },
  {
    "representative_text": "Cyclic Spanning Sets: A set of vectors {v1, v2, ..., vn} is called a cyclic spanning set for a vector space V if the span of {v1, v2, ..., vn} is equal to the span of {v1 + v2 + ... + vn, v1, v2, ..., vn-1}.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Cyclic Spanning Sets: A set of vectors {v1, v2, ..., vn} is called a cyclic spanning set for a vector space V if the span of {v1, v2, ..., vn} is equal to the span of {v1 + v2 + ... + vn, v1, v2, ..., vn-1}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "A set of vectors {v1, v2, ..., vn} is called a cyclic spanning set for a vector space V if the span of {v1, v2, ..., vn} is equal to the span of {v1 + v2 + ... + vn, v1, v2, ..., vn-1}.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9143054330115429
      }
    ]
  },
  {
    "representative_text": "Existence of a Basis for a Vector Space with Infinite-Dimensional Vector Space: If V is an infinite-dimensional vector space, then V has a basis for V, but this basis may not be finite.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Existence of a Basis for a Vector Space with Infinite-Dimensional Vector Space: If V is an infinite-dimensional vector space, then V has a basis for V, but this basis may not be finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "If V is an infinite-dimensional vector space, then V has a basis for V, but this basis may not be finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9253069163163598
      },
      {
        "text": "Existence of a Basis for a Vector Space with Infinite-Dimensional Vector Space:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.888135867393701
      },
      {
        "text": "Existence of a Basis for a Vector Space with Infinite-Dimensional Vector Space: This theorem states that if V is an infinite-dimensional vector space, then V has a basis for V, but this basis may not be finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.912548790668117
      },
      {
        "text": "Existence of a Basis for a Vector Space with Finite-Dimensional Vector Space: This theorem: This theorem: This theorem: This theorem: This theorem: This theorem: This theorem: This theorem: This theorem states that V: This theorem: This theorem states that a basis Implies Basis Implies Linear Independence Implies Linear Independence Theorem: This theorem: This theorem for Infinite-Dimensional Vector space",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8639793736497081
      },
      {
        "text": "Existence of a Basis for an Infinite-Dimensional Vector Space: This theorem states that if V is an infinite-dimensional vector space, then V has been taken in",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9361622894242946
      }
    ]
  },
  {
    "representative_text": "Free Basis and the Null Space: The null space of a free basis is the entire vector space. Understanding the relationship between the null space and the free basis is essential in linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Free Basis and the Null Space: The null space of a free basis is the entire vector space. Understanding the relationship between the null space and the free basis is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of the Span of a Set: The span of the span of a set of vectors is the entire vector space. Understanding the relationship between the span of a set and the span of its span is essential in linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Span of the Span of a Set: The span of the span of a set of vectors is the entire vector space. Understanding the relationship between the span of a set and the span of its span is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of the Span of a Set: The span of the span of a set of vectors is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9012905904264858
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Complement of a Set: The complement of a set of vectors is the set of vectors that are not in the set. Understanding the relationship between linear independence and the complement of a set is essential in linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Complement of a Set: The complement of a set of vectors is the set of vectors that are not in the set. Understanding the relationship between linear independence and the complement of a set is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Free Basis and the Orthogonal Complement: The orthogonal complement of a free basis is the entire vector space. Understanding the relationship between the orthogonal complement and the free basis is essential in linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Free Basis and the Orthogonal Complement: The orthogonal complement of a free basis is the entire vector space. Understanding the relationship between the orthogonal complement and the free basis is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Free Basis and Orthogonal Complement: A free basis of a vector space has an orthogonal complement that is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9137312084635862
      }
    ]
  },
  {
    "representative_text": "Free Basis and the Dimension of the Complement: The dimension of the complement of a free basis is equal to the dimension of the entire vector space minus the dimension of the free basis. Understanding the relationship between the free basis and the dimension of the complement is essential in linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Free Basis and the Dimension of the Complement: The dimension of the complement of a free basis is equal to the dimension of the entire vector space minus the dimension of the free basis. Understanding the relationship between the free basis and the dimension of the complement is essential in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Weierstrass Approximation Theorem: This theorem states that any continuous function on a closed interval can be uniformly approximated by polynomials. This theorem has implications for the linear independence of polynomials in a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Weierstrass Approximation Theorem: This theorem states that any continuous function on a closed interval can be uniformly approximated by polynomials. This theorem has implications for the linear independence of polynomials in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Linear Independence and the Mittag-Leffler Theorem: The Mittag-Leffler theorem states that any analytic function on a domain can be uniformly approximated by polynomials. This theorem has implications for the linear independence of polynomials in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8175979752227504
      },
      {
        "text": "The Relationship between Linear Independence and the Mittag-Leffler Theorem: The Mittag-Leffler theorem has implications for the linear independence of polynomials in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9027085692963919
      },
      {
        "text": "Weierstrass Approximation Theorem: The Weierstrass approximation theorem has implications for the linear independence of polynomials in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9071282457101076
      },
      {
        "text": "The Weierstrass Approximation Theorem and Linear Independence in Vector Spaces**: The Weierstrass approximation theorem states that any continuous function on a closed interval can be uniformly approximated by polynomials. This theorem has implications for the linear independence of polynomials in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9370619039605619
      }
    ]
  },
  {
    "representative_text": "Poincaré's Lemma: This lemma states that if a differential form on a manifold is exact, then it can be expressed as the differential of another form. This theorem has implications for the linear independence of forms in differential geometry.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Poincaré's Lemma: This lemma states that if a differential form on a manifold is exact, then it can be expressed as the differential of another form. This theorem has implications for the linear independence of forms in differential geometry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Poincaré's Lemma: Poincaré's lemma has implications for the linear independence of forms in differential geometry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8967566565885466
      },
      {
        "text": "The Poincaré's Lemma and Linear Independence in Differential Geometry**: Poincaré's lemma states that if a differential form on a manifold is exact, then it can be expressed as the differential of another form. This theorem has implications for the linear independence of forms in differential geometry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9554229650912198
      },
      {
        "text": "The relationship between linear independence and the Poincaré's lemma: Poincaré's lemma states that if a differential form on a manifold is exact, then it can be expressed as the differential of another form. This theorem has implications for the linear independence of forms in differential geometry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9358908380594952
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Algebra: This theorem states that every non-constant polynomial has at least one complex root. This theorem has implications for the linear independence of polynomials in a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Algebra: This theorem states that every non-constant polynomial has at least one complex root. This theorem has implications for the linear independence of polynomials in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Fundamental Theorem of Algebra: The fundamental theorem of algebra has implications for the linear independence of polynomials in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9115614944033861
      },
      {
        "text": "The Fundamental Theorem of Algebra and Linear Independence in Vector Spaces**: The fundamental theorem of algebra states that every non-constant polynomial has at least one complex root. This theorem has implications for the linear independence of polynomials in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9411227631410735
      }
    ]
  },
  {
    "representative_text": "The Dimension Theorem: This theorem states that the dimension of a vector space is equal to the maximum number of linearly independent vectors in the space. This theorem has implications for the linear independence of vectors in a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "The Dimension Theorem: This theorem states that the dimension of a vector space is equal to the maximum number of linearly independent vectors in the space. This theorem has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Dimension Theorem: The dimension theorem states that the dimension of a vector space is equal to the maximum number of linearly independent vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9138691420407272
      },
      {
        "text": "The Dimension Theorem and Linear Independence in Infinite Dimensional Spaces**: The dimension theorem states that the dimension of a vector space is equal to the maximum number of linearly independent vectors in the space. This theorem has implications for the linear independence of vectors in an infinite dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8773715944591813
      },
      {
        "text": "The Dimension Theorem for Infinite-Dimensional Spaces: This theorem states that the dimension of an infinite-dimensional vector space is equal to the number of linearly independent vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8864913710681877
      },
      {
        "text": "The Dimension Theorem for Vector Spaces with Respect to a Given Basis: This theorem states that the dimension of a vector space with respect to a given basis is equal to the number of linearly independent vectors in the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8908467104215839
      },
      {
        "text": "The Dimension Theorem for Vector Spaces with Respect to a Linear Transformation: The dimension theorem states that the dimension of a vector space with respect to a given linear transformation is equal to the number of linearly independent vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.931453469964695
      },
      {
        "text": "The Dimension of a Banach Space: This theorem states that the dimension of a Banach space is the maximum number of linearly independent vectors in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8441071882098076
      },
      {
        "text": "Dimension of a Vector Space is Equal to the Dimension of a Basis: This theorem states that the dimension of a vector space is not explicitly stated that theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8137080814923949
      }
    ]
  },
  {
    "representative_text": "The Hahn-Banach Theorem: This theorem states that any linear functional on a vector space can be extended to a linear functional on a larger vector space. This theorem has implications for the linear independence of vectors in a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "The Hahn-Banach Theorem: This theorem states that any linear functional on a vector space can be extended to a linear functional on a larger vector space. This theorem has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The role of the Hahn-Banach theorem in linear independence: The Hahn-Banach theorem provides a way to extend linear independence from finite-dimensional spaces to infinite-dimensional spaces. It states that if a set of vectors is linearly independent in a finite-dimensional space, then it is also linearly independent in the corresponding infinite-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.859723486324472
      },
      {
        "text": "The Hahn-Banach Theorem: The Hahn-Banach theorem has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9089159390868218
      },
      {
        "text": "The Hahn-Banach Theorem and Linear Independence in Infinite-Dimensional Vector Spaces: The Hahn-Banach theorem provides a way to extend linear independence from finite-dimensional spaces to infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.917801961659974
      },
      {
        "text": "The Hahn-Banach Theorem and Linear Independence in Vector Spaces**: The Hahn-Banach theorem states that any linear functional on a vector space can be extended to a linear functional on a larger vector space. This theorem has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9536778227263583
      },
      {
        "text": "Relationship between Linear Independence and the Dimension of a Vector Space in Infinite-Dimensional Vector Spaces (Hahn-Banach Theorem): This theorem generalizes the relationship between linear independence and the dimension of a vector space in finite-dimensional vector spaces to infinite-dimensional vector spaces, using the Hahn-Banach theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8920871368932475
      },
      {
        "text": "The Hahn-Banach Theorem: This theorem extends the concept of linear independence to infinite-dimensional vector spaces by providing a way to extend linear functionals to the entire space while preserving their norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9136788709486656
      },
      {
        "text": "The relationship between linear independence and the Hahn-Banach theorem: The Hahn-Banach theorem states that any linear functional on a vector space can be extended to a linear functional on a larger vector space. This theorem has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9036369020457051
      },
      {
        "text": "Linear Independence and the Role of Hahn-Banach Theorem in Operator Algebras: The Hahn-Banach theorem provides a way to extend linear independence from finite-dimensional spaces to infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9276339217632694
      },
      {
        "text": "Hahn-Banach Theorem: The relationship between linear independence and the dimension of a vector space in infinite-dimensional vector spaces using the Hahn-Banach theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9180304194780351
      }
    ]
  },
  {
    "representative_text": "The Uniform Boundedness Principle: This principle states that any collection of bounded linear operators on a Banach space must be uniformly bounded. This principle has implications for the linear independence of vectors in a Banach space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Uniform Boundedness Principle: This principle states that any collection of bounded linear operators on a Banach space must be uniformly bounded. This principle has implications for the linear independence of vectors in a Banach space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Uniform Boundedness Principle: The uniform boundedness principle has implications for the linear independence of vectors in a Banach space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9385308612978828
      },
      {
        "text": "The Uniform Boundedness Principle: This theorem states that if a family of bounded linear operators from a Banach space to a normed space is pointwise bounded, then it is uniformly bounded. This can be applied to show that a vector space has a basis if and only if every family of linearly independent subsets has a uniform bound.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8300658056318584
      },
      {
        "text": "The Uniform Boundedness Principle for Infinite-Dimensional Vector Spaces: This theorem states that if a family of linear operators from a Banach space to a normed space is pointwise bounded, then it is uniformly bounded.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8561405123389753
      },
      {
        "text": "The application of the uniform boundedness principle in linear independence and span: The uniform boundedness principle states that any collection of bounded linear operators on a Banach space must be uniformly bounded. This principle has implications for the linear independence of vectors in a Banach space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9166648088435212
      }
    ]
  },
  {
    "representative_text": "The Minimax Theorem for Linear Operators: This theorem states that the norm of a linear operator is equal to the minimum of the maximum norm of the operator and the minimum norm of the operator. This theorem has implications for the linear independence of vectors in a Banach space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Minimax Theorem for Linear Operators: This theorem states that the norm of a linear operator is equal to the minimum of the maximum norm of the operator and the minimum norm of the operator. This theorem has implications for the linear independence of vectors in a Banach space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Minimax Theorems for Linear Operators: The minimax theorem has implications for the linear independence of vectors in a Banach space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8875677125258481
      },
      {
        "text": "The Minimax Theorem for Infinite Dimensional Spaces**: The minimax theorem states that the norm of a linear operator is equal to the minimum of the maximum norm of the operator and the minimum norm of the operator. This theorem has implications for the linear independence of vectors in an infinite dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9365040494768251
      },
      {
        "text": "The Minimax Theorems for Linear Operators in Infinite Dimensional Spaces**: Minimax theorems have implications for the linear independence of vectors in an infinite dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8889359609837153
      }
    ]
  },
  {
    "representative_text": "The Role of Compact Operators in Linear Independence: Compact operators play a crucial role in many linear algebra concepts, including linear independence. Exploring their applications and properties could be beneficial.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Role of Compact Operators in Linear Independence: Compact operators play a crucial role in many linear algebra concepts, including linear independence. Exploring their applications and properties could be beneficial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Compact Operators and Linear Independence: Compact operators play a crucial role in many linear algebra concepts, including linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9366496386216105
      }
    ]
  },
  {
    "representative_text": "The Connection between Linear Independence and the Ergodic Theorem: The ergodic theorem states that any sequence of random variables can be approximated by a sequence of independent random variables. This theorem has implications for the linear independence of vectors in a vector space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Connection between Linear Independence and the Ergodic Theorem: The ergodic theorem states that any sequence of random variables can be approximated by a sequence of independent random variables. This theorem has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Connection between Linear Independence and the Ergodic Theorem: The ergodic theorem has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9232470198573488
      },
      {
        "text": "The Relationship between Linear Independence and the Ergodic Theorem in Infinite Dimensional Spaces**: The ergodic theorem states that any sequence of random variables can be approximated by a sequence of independent random variables. This theorem has implications for the linear independence of vectors in an infinite dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9343950917913972
      }
    ]
  },
  {
    "representative_text": "The Interplay between Linear Independence and the Lefschetz Fixed Point Theorem: The Lefschetz fixed point theorem states that any continuous function on a compact space has a fixed point. This theorem has implications for the linear independence of vectors in a vector space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Interplay between Linear Independence and the Lefschetz Fixed Point Theorem: The Lefschetz fixed point theorem states that any continuous function on a compact space has a fixed point. This theorem has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Interplay between Linear Independence and the Lefschetz Fixed Point Theorem: The Lefschetz fixed point theorem has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9532972960047829
      }
    ]
  },
  {
    "representative_text": "The Impact of Linear Independence on the Existence of a Basis in Infinite-Dimensional Vector Spaces: Exploring the impact of linear independence on the existence of a basis in infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 39,
    "detailed_sources": [
      {
        "text": "The Impact of Linear Independence on the Existence of a Basis in Infinite-Dimensional Vector Spaces: Exploring the impact of linear independence on the existence of a basis in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Role of Linear Independence in the Study of Linear Transformations in Infinite-Dimensional Spaces: Investigating the role of linear independence in the study of linear transformations in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8627148774776602
      },
      {
        "text": "The Impact of Linear Independence on the Properties of an Infinite-Dimensional Hilbert Space: Exploring the impact of linear independence on the properties of an infinite-dimensional Hilbert space, particularly in the context of inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8783973222885214
      },
      {
        "text": "Relationship between Linear Independence and the Complement Theorem in Infinite-Dimensional Vector Spaces: A theorem that generalizes the relationship between linear independence and the complement theorem in finite-dimensional vector spaces to infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8392779613432724
      },
      {
        "text": "Relationship between Linear Independence and the Existence of a Basis in Infinite-Dimensional Vector Spaces: A theorem that generalizes the relationship between linear independence and the existence of a basis in finite-dimensional vector spaces to infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9132446931360035
      },
      {
        "text": "Relationship between Linear Independence and the Dimension of a Vector Space in Infinite-Dimensional Vector Spaces: A theorem that generalizes the relationship between linear independence and the dimension of a vector space in finite-dimensional vector spaces to infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9409985765715152
      },
      {
        "text": "Linear Independence and Bases in Infinite-Dimensional Vector Spaces (Part 2): This involves exploring the relationship between linear independence and the existence of a basis in infinite-dimensional vector spaces, particularly in the context of Hilbert spaces and inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.84205408318736
      },
      {
        "text": "The Role of Linear Independence in the Study of Infinite-Dimensional Vector Spaces (Part 2): This involves investigating the role of linear independence in the study of infinite-dimensional vector spaces, particularly in the context of Hilbert spaces and inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8892749325020691
      },
      {
        "text": "The Impact of Linear Independence on the Existence of a Basis in Infinite-Dimensional Vector Spaces (Part 2): This involves exploring the impact of linear independence on the existence of a basis in infinite-dimensional vector spaces, particularly in the context of Hilbert spaces and inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8825388540975837
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space in Infinite-Dimensional Vector Spaces (Part 2): This involves examining the relationship between linear independence and the null space of a matrix in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8942718297584202
      },
      {
        "text": "Linear Independence and the Dimension of a Hilbert Space (Part 2): This involves exploring the impact of linear independence on the dimension of a Hilbert space, which involves the relationship between the dimension of a Hilbert space and the number of linearly independent vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8658978295278177
      },
      {
        "text": "The Impact of Linear Independence on the Properties of an Infinite-Dimensional Hilbert Space (Part 2): This involves exploring the impact of linear independence on the properties of an infinite-dimensional Hilbert space, particularly in the context of inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9029844771324662
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space in Infinite-Dimensional Hilbert Spaces: This involves examining the relationship between linear independence and the null space of a matrix in infinite-dimensional Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.86154152032123
      },
      {
        "text": "The Impact of Linear Independence on the Properties of Infinite-Dimensional Spaces: This involves exploring the impact of linear independence on the properties of infinite-dimensional spaces, particularly in the context of Hilbert spaces and inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.902876732788247
      },
      {
        "text": "The Relationship between Linear Independence and the Dimension of a Vector Space: The relationship between linear independence and the dimension of a vector space, particularly in the context of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9084110083337397
      },
      {
        "text": "Linear Independence and the Extension Theorem: The relationship between linear independence and the extension theorem, particularly in the context of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8965022520235053
      },
      {
        "text": "Relationship between Linear Independence and the Existence of a Basis in Infinite-Dimensional Vector Spaces (Zorn's Lemma): This theorem generalizes the relationship between linear independence and the existence of a basis in finite-dimensional vector spaces to infinite-dimensional vector spaces, using Zorn's Lemma.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8377690379083276
      },
      {
        "text": "Relationship between Linear Independence and the Complement Theorem in Infinite-Dimensional Vector Spaces (Internal Hom): This theorem generalizes the relationship between linear independence and the complement theorem in finite-dimensional vector spaces to infinite-dimensional vector spaces, using internal hom.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8315189834987365
      },
      {
        "text": "Relationship between Linear Independence and the Dimension of a Vector Space in Infinite-Dimensional Vector Spaces (Schroeder-Whitney Theorem): This theorem generalizes the relationship between linear independence and the dimension of a vector space in finite-dimensional vector spaces to infinite-dimensional vector spaces, using the Schroeder-Whitney theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8379415884097277
      },
      {
        "text": "The Impact of Linear Independence on the Properties of a Linear Operator in Infinite-Dimensional Spaces: Investigating the impact of linear independence on the properties of a linear operator in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8754781710108942
      },
      {
        "text": "The Impact of Linear Independence on the Properties of a Topological Vector Space: Investigating the impact of linear independence on the properties of a topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8515170396785465
      },
      {
        "text": "Relationship between Linear Independence and the Existence of a Basis in Infinite-Dimensional Vector Spaces (Kuratowski's Fixed Point Theorem): This theorem generalizes the relationship between linear independence and the existence of a basis in finite-dimensional vector spaces to infinite-dimensional vector spaces, using Kuratowski's fixed point theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8266042326386287
      },
      {
        "text": "Relationship between Linear Independence and the Dimension of a Vector Space in Infinite-Dimensional Vector Spaces (Internal Hom): This theorem generalizes the relationship between linear independence and the dimension of a vector space in finite-dimensional vector spaces to infinite-dimensional vector spaces, using internal hom.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8885943833047659
      },
      {
        "text": "Relationship between Linear Independence and the Null Space in Infinite-Dimensional Spaces with Non-Standard Metrics: Investigating the relationship between linear independence and the null space of a matrix in infinite-dimensional spaces with non-standard metrics, such as F-adic or p-adic metrics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8012968716927442
      },
      {
        "text": "Relationship between Linear Independence and the Rank of a Matrix in Infinite-Dimensional Spaces with Non-Standard Norms: Examining the relationship between linear independence and the rank of a matrix in infinite-dimensional spaces with non-standard norms, such as p-norms or F-norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8246186199133999
      },
      {
        "text": "Linear Independence and the Properties of a Fuzzy Linear Operator: Investigating the properties of a fuzzy linear operator in infinite-dimensional spaces, which involves the concept of linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8392647156040192
      },
      {
        "text": "Relationship between Linear Independence and the Null Space in Infinite-Dimensional Spaces with Non-Standard Topologies: Examining the relationship between linear independence and the null space of a matrix in infinite-dimensional spaces with non-standard topologies, such as Fréchet topologies.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8610231110888651
      },
      {
        "text": "Impact of Non-Standard Seminorms on Linear Independence in Infinite-Dimensional Spaces: Investigating the impact of non-standard seminorms on linear independence in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8540578169541907
      },
      {
        "text": "Relationship between Linear Independence and the Rank of a Matrix in Infinite-Dimensional Banach Spaces: Examining the relationship between linear independence and the rank of a matrix in infinite-dimensional Banach spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8785418423671976
      },
      {
        "text": "Relationship between Linear Independence and the Null Space in Infinite-Dimensional Banach Spaces with Non-Standard Norms: Examining the relationship between linear independence and the null space of a matrix in infinite-dimensional Banach spaces with non-standard norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8925158017137338
      },
      {
        "text": "Schroeder-Whitney Theorem: The relationship between linear independence and the dimension of a vector space in infinite-dimensional vector spaces using the Schroeder-Whitney theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8641941201514534
      },
      {
        "text": "The Relationship between Linear Independence and the Rank of a Matrix in Infinite-Dimensional Spaces with Non-Standard Norms: Examining the relationship between linear independence and the rank of a matrix in infinite-dimensional spaces with non-standard norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8862994676414133
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space in Infinite-Dimensional Spaces with Non-Standard Topologies: Examining the relationship between linear independence and the null space of a matrix in infinite-dimensional spaces with non-standard topologies.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9144317932017152
      },
      {
        "text": "Linear independence in Banach spaces with non-standard norms: Examine the relationship between linear independence and the rank of a matrix in Banach spaces with non-standard norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8192299082259658
      },
      {
        "text": "The relationship between linear independence and the rank of a matrix in infinite-dimensional spaces: Examine the relationship between linear independence and the rank of a matrix in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8873714983671493
      },
      {
        "text": "The relationship between linear independence and the dimension of a Hilbert space: Examine the relationship between linear independence and the dimension of a Hilbert space in infinite-dimensional Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8857549697154588
      },
      {
        "text": "Linear Independence in Fréchet Spaces with Non-Standard Bases: Investigating the properties of linear independence in Fréchet spaces with non-standard bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8676008006223876
      },
      {
        "text": "Linear Independence of Linear Independence of Linear Independence in Linear Independence: Exploring the relationship between Linear Independence in Linear Independence of Linear Independence: Investigating linear independence in Infinite-Dimensional Hilbert spaces: Investigating linear independence of Linear Independence in infinite-Dimensional Hilbert spaces: Investigating theoreseveral Independence:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8457432874380026
      },
      {
        "text": "Theorelated to be used in a few more detailed explanation of Linear Independence of a basis for Infinite-Dimensional Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8591521092114888
      }
    ]
  },
  {
    "representative_text": "Span Implies Linear Independence for Vector Spaces with a Trivial Span: We need to consider how the span of a trivial subspace relates to linear independence.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Span Implies Linear Independence for Vector Spaces with a Trivial Span: We need to consider how the span of a trivial subspace relates to linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span Implies Linear Independence for Vector Spaces with a Non-Trivial Span: Investigate the relationship between the span of a non-trivial subspace and linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9101336145620731
      }
    ]
  },
  {
    "representative_text": "Singular Values and Linear Independence for Infinite-Dimensional Vector Spaces: We need to consider how singular values relate to linear independence, particularly in the context of infinite-dimensional vector spaces.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Singular Values and Linear Independence for Infinite-Dimensional Vector Spaces: We need to consider how singular values relate to linear independence, particularly in the context of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Decomposition (SVD) and Linear Independence for Infinite-Dimensional Vector Spaces: We need to consider how SVD relates to linear independence, particularly in the context of infinite-dimensional vector spaces and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8972002471466249
      },
      {
        "text": "Singular Values and Linear Independence for Infinite-Dimensional Vector Spaces: The relationship between singular values and linear independence in infinite-dimensional vector spaces is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8647304259982808
      },
      {
        "text": "Singular Values and Linear Independence for Infinite-Dimensional Vector Spaces: Examine the relationship between singular values and linear independence in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9136671145260967
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Finitely Generated Abelian Groups: This theorem states that every finitely generated abelian group can be expressed as a direct sum of cyclic groups. This concept is related to the concept of linear independence in infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Finitely Generated Abelian Groups: This theorem states that every finitely generated abelian group can be expressed as a direct sum of cyclic groups. This concept is related to the concept of linear independence in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Schur's Lemma: This lemma states that if T and S are two linear transformations between finite-dimensional vector spaces, and T and S commute (i.e., TS = ST), then T and S are scalar multiples of each other. This concept is related to the concept of linear independence and basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Schur's Lemma: This lemma states that if T and S are two linear transformations between finite-dimensional vector spaces, and T and S commute (i.e., TS = ST), then T and S are scalar multiples of each other. This concept is related to the concept of linear independence and basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Schur's Theorem on Linear Independence: This theorem states that if T and S are two linear transformations between finite-dimensional vector spaces, and T and S commute (i.e., TS = ST), then T and S are scalar multiples of each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8402006713476098
      }
    ]
  },
  {
    "representative_text": "The Wedderburn-Artin Theorem: This theorem states that every finite-dimensional algebra over a field is isomorphic to a matrix algebra over a division algebra. This concept is related to the concept of linear independence and basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Wedderburn-Artin Theorem: This theorem states that every finite-dimensional algebra over a field is isomorphic to a matrix algebra over a division algebra. This concept is related to the concept of linear independence and basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Relationship Between Linear Independence and the Eigenvalues of a Matrix: The eigenvalues of a matrix are closely related to linear independence. If the columns of a matrix are linearly independent, the eigenvalues of the matrix are non-zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Relationship Between Linear Independence and the Eigenvalues of a Matrix: The eigenvalues of a matrix are closely related to linear independence. If the columns of a matrix are linearly independent, the eigenvalues of the matrix are non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship Between Linear Dependence and the Eigenvalues of a Matrix: If the columns of a matrix are linearly dependent, the eigenvalues of the matrix are zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8397782906961013
      },
      {
        "text": "The Relationship between Linear Independence and the Eigenvalues of a Matrix: If a matrix has a non-zero eigenvalue, then its row space and column space are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9320122357959952
      }
    ]
  },
  {
    "representative_text": "The Relationship Between Linear Dependence and the Gram-Schmidt Process: If the columns of a matrix are linearly dependent, the Gram-Schmidt process does not produce an orthogonal basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Relationship Between Linear Dependence and the Gram-Schmidt Process: If the columns of a matrix are linearly dependent, the Gram-Schmidt process does not produce an orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cayley-Menger Determinant: This is a determinant used to calculate the volume of a tetrahedron, which is a common problem in computer graphics and game development.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cayley-Menger Determinant: This is a determinant used to calculate the volume of a tetrahedron, which is a common problem in computer graphics and game development.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Physics-Based Modeling of Materials: This includes techniques such as the use of linear algebra to model the mechanical properties of materials, and to simulate the way materials interact with light.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Physics-Based Modeling of Materials: This includes techniques such as the use of linear algebra to model the mechanical properties of materials, and to simulate the way materials interact with light.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computer-Aided Design (CAD) and Computer-Aided Manufacturing (CAM): This includes techniques such as the use of linear algebra to perform 3D modeling, and to simulate the behavior of complex systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computer-Aided Design (CAD) and Computer-Aided Manufacturing (CAM): This includes techniques such as the use of linear algebra to perform 3D modeling, and to simulate the behavior of complex systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Geometry: This includes techniques such as the use of linear algebra to perform geometric transformations, and to solve geometric problems such as finding the intersection of two curves.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Computational Geometry: This includes techniques such as the use of linear algebra to perform geometric transformations, and to solve geometric problems such as finding the intersection of two curves.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Geometry and Topology: Computational geometry and topology involve the use of linear algebra to solve geometric problems, such as finding the intersection of two curves, and to study the properties of geometric shapes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8500980998419654
      },
      {
        "text": "Computational Geometry and Voronoi Diagrams: Computational geometry is a field that uses linear algebra to solve geometric problems, such as finding the intersection of two curves. Voronoi diagrams are a type of computational geometry problem that involves dividing space into regions based on the proximity to a set of points. This is useful in computer graphics for tasks such as terrain rendering and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8471935935456054
      },
      {
        "text": "Computational Geometry and Convex Hulls: Computational geometry is a field that uses linear algebra to solve geometric problems, such as finding the intersection of two curves. Convex hulls are a type of geometric problem that involves finding the convex shape of a set of points. This is useful in computer graphics for tasks such as object recognition and image registration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9291935663113555
      },
      {
        "text": "Computational Geometry and Its Applications: Computational geometry is a field that uses linear algebra to solve geometric problems, and it has many applications in computer graphics and game development.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9271473468451052
      },
      {
        "text": "Computational Geometry and Its Applications in 3D Reconstruction: Computational geometry is used in 3D reconstruction to solve problems like finding the intersection of two curves and reconstructing 3D objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8287683634542116
      }
    ]
  },
  {
    "representative_text": "Linear System Solving and Least Squares: This includes techniques such as the use of linear algebra to solve linear systems of equations, and to perform least squares regression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Linear System Solving and Least Squares: This includes techniques such as the use of linear algebra to solve linear systems of equations, and to perform least squares regression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Methods for Linear Algebra: This includes techniques such as the use of linear algebra to solve systems of linear equations, and to perform numerical integration and differentiation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8281335063268265
      },
      {
        "text": "Linear System Inversion and Pseudoinversion: This includes techniques such as the use of linear algebra to invert and pseudoinvert matrices, and to solve systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8617811067843528
      },
      {
        "text": "Numerical Methods for Linear Algebra: Numerical methods, such as the use of linear algebra to solve systems of linear equations and perform numerical integration and differentiation, are essential for tasks such as physics-based modeling and simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8343038846057415
      },
      {
        "text": "Computational Methods for Solving Linear Systems: This topic deals with various computational methods used to solve linear systems, such as Gaussian elimination, LU decomposition, and Cholesky decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8149720113205765
      }
    ]
  },
  {
    "representative_text": "Computational Fluid Dynamics (CFD): This includes techniques such as the use of linear algebra to solve systems of partial differential equations, and to simulate the behavior of fluids.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Fluid Dynamics (CFD): This includes techniques such as the use of linear algebra to solve systems of partial differential equations, and to simulate the behavior of fluids.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Techniques for Robotics: This includes techniques such as the use of linear algebra to perform kinematic analysis, and to solve problems of motion planning and control.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Techniques for Robotics: This includes techniques such as the use of linear algebra to perform kinematic analysis, and to solve problems of motion planning and control.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebraic Techniques for Robotics: Linear algebraic techniques, such as the use of eigenvectors and singular value decomposition (SVD), are used in robotics to perform kinematic analysis and solve problems of motion planning and control.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.892943552151683
      },
      {
        "text": "Linear Algebraic Methods for Robotics: Linear algebraic methods for robotics are used in computer graphics and game development to perform kinematic analysis and solve problems of motion planning and control. Linear algebra is used to optimize tasks such as collision detection and response.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8650238399850779
      },
      {
        "text": "Linear Algebraic Computation for Robotics: Linear algebraic computation is used to solve problems in robotics in computer graphics and game development. These methods involve using techniques such as eigenvalue decomposition to solve problems involving robotics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8691547204337675
      },
      {
        "text": "Linear Algebraic Computation for Robotics: Linear algebraic computation is used in robotics to solve problems like kinematic analysis and motion planning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9329574903266122
      },
      {
        "text": "Linear Algebraic Methods for Robotics: Techniques such as kinematic analysis, motion planning, and control system design.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8606079472407447
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Techniques for Optimization: This includes techniques such as the use of linear algebra to perform optimization problems, and to solve problems of linear programming.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Techniques for Optimization: This includes techniques such as the use of linear algebra to perform optimization problems, and to solve problems of linear programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The connection between linear algebra and optimization: Linear algebra provides a framework for solving optimization problems, including linear programming and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8603530158303636
      }
    ]
  },
  {
    "representative_text": "Krylov Subspace Methods: These methods are used to solve large linear systems and are particularly useful in machine learning and data science for tasks such as principal component analysis and neural network optimization.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Krylov Subspace Methods: These methods are used to solve large linear systems and are particularly useful in machine learning and data science for tasks such as principal component analysis and neural network optimization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Graph Neural Networks: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize graph neural networks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Graph Neural Networks: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize graph neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Optimization Techniques for Machine Learning: Linear algebra is used extensively in optimization techniques such as gradient descent, stochastic gradient descent, and Adam optimization, which are widely used in machine learning and data science.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Optimization Techniques for Machine Learning: Linear algebra is used extensively in optimization techniques such as gradient descent, stochastic gradient descent, and Adam optimization, which are widely used in machine learning and data science.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Factorization: This technique is used to reduce the dimensionality of large matrices and is widely used in machine learning and data science for tasks such as collaborative filtering and recommender systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Factorization: This technique is used to reduce the dimensionality of large matrices and is widely used in machine learning and data science for tasks such as collaborative filtering and recommender systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bilinear and Trilinear Transformations: Bilinear and trilinear transformations are used in signal processing and image analysis for tasks like filtering and image compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Bilinear and Trilinear Transformations: Bilinear and trilinear transformations are used in signal processing and image analysis for tasks like filtering and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bilinear and Trilinear Transformations: Bilinear and trilinear transformations are used in signal processing and image analysis for tasks like filtering and image compression. However, a detailed explanation of these transformations and their applications is not provided.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8758045131228043
      }
    ]
  },
  {
    "representative_text": "Convolutive and Non-Convex Optimization: Convolutive and non-convex optimization techniques are used in signal processing and image analysis for tasks like image de-noising and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Convolutive and Non-Convex Optimization: Convolutive and non-convex optimization techniques are used in signal processing and image analysis for tasks like image de-noising and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Signal Separation and Reconstruction: Signal separation and reconstruction techniques, such as independent component analysis (ICA) and blind source separation (BSS), are used in signal processing and image analysis for tasks like noise reduction and image restoration.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Signal Separation and Reconstruction: Signal separation and reconstruction techniques, such as independent component analysis (ICA) and blind source separation (BSS), are used in signal processing and image analysis for tasks like noise reduction and image restoration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Image Denoising using Total Variation: Image denoising using total variation (TV) regularization is a technique used in signal processing and image analysis for tasks like image de-noising and super-resolution.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Image Denoising using Total Variation: Image denoising using total variation (TV) regularization is a technique used in signal processing and image analysis for tasks like image de-noising and super-resolution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis Pursuit and L1 Regularization: Basis pursuit and L1 regularization techniques are used in signal processing and image analysis for tasks like sparse signal recovery and image compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Basis Pursuit and L1 Regularization: Basis pursuit and L1 regularization techniques are used in signal processing and image analysis for tasks like sparse signal recovery and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis Pursuit: Basis pursuit is a mathematical technique that can be used to recover the sparsest representation of a signal or function. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8155004902913403
      }
    ]
  },
  {
    "representative_text": "Sparse Representation and Dictionary Learning: Sparse representation and dictionary learning techniques, such as the K-SVD algorithm, are used in signal processing and image analysis for tasks like image compression and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Sparse Representation and Dictionary Learning: Sparse representation and dictionary learning techniques, such as the K-SVD algorithm, are used in signal processing and image analysis for tasks like image compression and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Sparse Representation and Dictionary Learning Variants: Sparse representation and dictionary learning techniques have various variants like the K-SVD algorithm, the orthogonal matching pursuit (OMP) algorithm, and the basis pursuit algorithm. These variants have different applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.86098849301834
      }
    ]
  },
  {
    "representative_text": "Non-Stationary Signal Processing: Non-stationary signal processing techniques, such as the wavelet transform and the short-time Fourier transform, are used in signal processing and image analysis for tasks like signal analysis and image compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Non-Stationary Signal Processing: Non-stationary signal processing techniques, such as the wavelet transform and the short-time Fourier transform, are used in signal processing and image analysis for tasks like signal analysis and image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Signal Processing and Image Analysis with Non-Stationary Signals: Applying linear algebra techniques to signal processing and image analysis with non-stationary signals, such as the wavelet transform and the short-time Fourier transform, is essential in modern signal processing and image analysis systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8257317975524336
      },
      {
        "text": "Non-Stationary Signal Processing Variants: Non-stationary signal processing techniques, such as the wavelet transform and the short-time Fourier transform, have various variants like the continuous wavelet transform, the continuous short-time Fourier transform, and the time-frequency representation using the Short-Time Fourier Transform. These variants have different applications and are essential in signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8062514832122505
      },
      {
        "text": "Non-Stationary Signal Processing Variants: Non-stationary signal processing techniques, such as the continuous wavelet transform and the continuous short-time Fourier transform, have various variants that are used in signal processing and image analysis for tasks such as image compression and signal denoising.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.938750322902359
      }
    ]
  },
  {
    "representative_text": "Optimal Filtering and Smoothing: Optimal filtering and smoothing techniques, such as the Wiener filter and the Kalman filter, are used in signal processing and image analysis for tasks like signal denoising and image restoration.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Optimal Filtering and Smoothing: Optimal filtering and smoothing techniques, such as the Wiener filter and the Kalman filter, are used in signal processing and image analysis for tasks like signal denoising and image restoration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Image Segmentation using Level Sets: Image segmentation using level sets is a technique used in signal processing and image analysis for tasks like image segmentation and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Image Segmentation using Level Sets: Image segmentation using level sets is a technique used in signal processing and image analysis for tasks like image segmentation and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Subspace Analysis and Clustering: Subspace analysis and clustering techniques, such as the singular value decomposition (SVD) and the k-means algorithm, are used in signal processing and image analysis for tasks like data analysis and image segmentation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Subspace Analysis and Clustering: Subspace analysis and clustering techniques, such as the singular value decomposition (SVD) and the k-means algorithm, are used in signal processing and image analysis for tasks like data analysis and image segmentation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Pontryagin Minimum Principle: A theorem that characterizes the optimality of control systems in the context of optimal control theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pontryagin Minimum Principle: A theorem that characterizes the optimality of control systems in the context of optimal control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Routh-Hurwitz Stability Theorem: A theorem that provides a necessary and sufficient condition for the stability of a system in terms of its characteristic roots.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Routh-Hurwitz Stability Theorem: A theorem that provides a necessary and sufficient condition for the stability of a system in terms of its characteristic roots.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Least Squares Estimation: A method for estimating the parameters of a system by minimizing the difference between the observed and predicted outputs.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Least Squares Estimation: A method for estimating the parameters of a system by minimizing the difference between the observed and predicted outputs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Regularization Methods: A class of methods for regularizing the solution of a linear system, such as using a penalty term or a L1 norm.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Regularization Methods: A class of methods for regularizing the solution of a linear system, such as using a penalty term or a L1 norm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Regularization: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8522631659072226
      },
      {
        "text": "Tikhonov Regularization: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9219721902003915
      },
      {
        "text": "Nevanlinna Regularization: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9034821987888355
      },
      {
        "text": "Regularization with Applications: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9597452935813193
      }
    ]
  },
  {
    "representative_text": "Zero-Knowledge Proofs with Advanced Proof Systems: This includes advanced proof systems, such as the Bulletproofs protocol, that provide more efficient and secure zero-knowledge proofs.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Zero-Knowledge Proofs with Advanced Proof Systems: This includes advanced proof systems, such as the Bulletproofs protocol, that provide more efficient and secure zero-knowledge proofs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Zero-knowledge proof systems with advanced proof systems: Investigating advanced proof systems, such as the Bulletproofs protocol, that provide more efficient and secure zero-knowledge proofs is an area of ongoing research.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9230649192390336
      },
      {
        "text": "Advanced Zero-Knowledge Proof Systems with Multi-Party Computation: This includes the analysis of zero-knowledge proof systems with advanced proof systems, such as the Bulletproofs protocol, for enabling secure zero-knowledge proofs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9222740218201962
      }
    ]
  },
  {
    "representative_text": "Code-Based Cryptography with Optimal Parameters: This includes methods for optimizing the parameters of code-based cryptographic schemes, such as the Reed-Solomon code, to achieve optimal security and efficiency.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Code-Based Cryptography with Optimal Parameters: This includes methods for optimizing the parameters of code-based cryptographic schemes, such as the Reed-Solomon code, to achieve optimal security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Code-based cryptography with optimal parameters: Methods for optimizing the parameters of code-based cryptographic schemes, such as the Reed-Solomon code, to achieve optimal security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9556389516602962
      },
      {
        "text": "Code-Based Cryptography with Optimal Parameters: This includes the analysis of code-based cryptography with optimal parameters, such as the Reed-Solomon code, for achieving optimal security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9653060269015898
      }
    ]
  },
  {
    "representative_text": "Multivariate Cryptography with Advanced Attacks: This includes methods for analyzing and defending against advanced attacks on multivariate cryptographic schemes, such as the Weil and the Supremum attacks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Multivariate Cryptography with Advanced Attacks: This includes methods for analyzing and defending against advanced attacks on multivariate cryptographic schemes, such as the Weil and the Supremum attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Multivariate cryptography with advanced attack techniques: Methods for analyzing and defending against advanced attacks on multivariate cryptographic schemes, such as the Weil and the Supremum attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.912617797997908
      },
      {
        "text": "Multivariate Cryptography with Advanced Attack Techniques: This includes the analysis of multivariate cryptography with advanced attack techniques, such as the Weil and the Supremum attacks, for defending against advanced attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9685437597499575
      }
    ]
  },
  {
    "representative_text": "Quantum-Resistant Hash Functions: This includes methods for designing and analyzing hash functions that are resistant to quantum computer attacks.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Quantum-Resistant Hash Functions: This includes methods for designing and analyzing hash functions that are resistant to quantum computer attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Quantum-Resistant Cryptographic Hash Functions: This includes methods for designing and analyzing hash functions that are resistant to quantum computer attacks, such as quantum-resistant hash functions based on lattices or codes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9469887870326744
      },
      {
        "text": "Quantum-resistant cryptographic hash functions: Methods for designing and analyzing hash functions that are resistant to quantum computer attacks, such as quantum-resistant hash functions based on lattices or codes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9432749579332644
      },
      {
        "text": "Post-quantum cryptographic hash functions and their security guarantees: This includes the analysis of post-quantum cryptographic hash functions, such as quantum-resistant hash functions based on lattices or codes, and their security guarantees against quantum attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8992015364247017
      }
    ]
  },
  {
    "representative_text": "Digital Watermarking with Advanced Techniques: This includes advanced techniques for embedding hidden messages or watermarks into digital files, such as the use of steganography and spread-spectrum techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Digital Watermarking with Advanced Techniques: This includes advanced techniques for embedding hidden messages or watermarks into digital files, such as the use of steganography and spread-spectrum techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Digital watermarking with advanced techniques: Advanced techniques for embedding hidden messages or watermarks into digital files, such as steganography and spread-spectrum techniques, that improve security and authenticity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9369246854771259
      },
      {
        "text": "Digital Watermarking with Advanced Techniques for Authentication and Tamper Detection: This includes advanced techniques for embedding hidden messages or watermarks into digital files, such as steganography and spread-spectrum techniques, to improve security and authenticity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9279624430421878
      }
    ]
  },
  {
    "representative_text": "Lattice-Based Cryptography with Optimal Parameters: This includes methods for optimizing the parameters of lattice-based cryptographic schemes, such as the NTRU algorithm, to achieve optimal security and efficiency.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Lattice-Based Cryptography with Optimal Parameters: This includes methods for optimizing the parameters of lattice-based cryptographic schemes, such as the NTRU algorithm, to achieve optimal security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Lattice-based cryptography with optimal parameters: Methods for optimizing the parameters of lattice-based cryptographic schemes, such as the NTRU algorithm, to achieve optimal security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9394406216086275
      },
      {
        "text": "Lattice-based cryptography with optimal parameters: Methods for optimizing the parameters of lattice-based cryptographic schemes to achieve optimal security and efficiency by using optimal lattice structures.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8406657150841172
      },
      {
        "text": "Lattice-based cryptography with optimal decoding algorithms: Methods for optimizing the parameters of lattice-based cryptographic schemes, such as the NTRU algorithm, to achieve optimal security and efficiency by using optimal decoding algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9119556959348112
      },
      {
        "text": "Lattice-based cryptography with optimal lattice structures: This includes the analysis of lattice-based cryptography with optimal lattice structures, such as the NTRU algorithm, for achieving optimal security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9278533620501423
      },
      {
        "text": "Lattice-based cryptography with optimal decoding algorithms: This includes the analysis of lattice-based cryptography with optimal decoding algorithms, such as the NTRU algorithm, for achieving optimal security and efficiency by using optimal decoding algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9149417631010776
      }
    ]
  },
  {
    "representative_text": "Code-Based Cryptography with Efficient Decoding Algorithms: This includes methods for designing and analyzing efficient decoding algorithms for code-based cryptographic schemes, such as the Reed-Solomon code.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Code-Based Cryptography with Efficient Decoding Algorithms: This includes methods for designing and analyzing efficient decoding algorithms for code-based cryptographic schemes, such as the Reed-Solomon code.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Code-based cryptography with optimal decoding algorithms: Developing efficient decoding algorithms for code-based cryptographic schemes, such as the Reed-Solomon code, is essential for optimizing the security and efficiency of these systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.931252060357219
      },
      {
        "text": "Code-based cryptography with optimal decoding algorithms: Efficient decoding algorithms for code-based cryptographic schemes, such as the Reed-Solomon code, that optimize security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9166777975634597
      },
      {
        "text": "Code-Based Cryptography with Advanced Error Correction Codes: This includes methods for designing and analyzing advanced error correction codes, such as Reed-Solomon codes with minimum distance 3, for optimizing security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8511637718145331
      },
      {
        "text": "Code-based cryptography with efficient decoding algorithms: This includes the analysis of code-based cryptography with efficient decoding algorithms, such as the Reed-Solomon code, for optimizing security and efficiency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9617271023835143
      }
    ]
  },
  {
    "representative_text": "Projection Theorems",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Projection Theorems",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Givens Rotations",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Givens Rotations",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Block Givens Rotation with Applications: A method for solving systems of linear equations,",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8371957657420561
      }
    ]
  },
  {
    "representative_text": "Orthogonal Diagonalization",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Diagonalization",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Normal Equation",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Normal Equation",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gradient Descent Method",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gradient Descent Method",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rayleigh Quotient",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Rayleigh Quotient",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rayleigh-Ritz Theorem",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8823818981207164
      }
    ]
  },
  {
    "representative_text": "Moore-Penrose Inverse of a Matrix",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Moore-Penrose Inverse of a Matrix",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Pseudoinverse and Moore-Penrose Inverse:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9017482222254517
      }
    ]
  },
  {
    "representative_text": "Jacobi Method",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Jacobi Method",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gauss-Seidel Method",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9285364583140373
      }
    ]
  },
  {
    "representative_text": "Span of a Vector Space",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span of a Vector Space",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Error Analysis",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Error Analysis",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Convergence of Numerical Methods",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Convergence of Numerical Methods",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Stability of Numerical Methods",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8898577411858889
      },
      {
        "text": "Numerical methods for error estimation",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8257154949857881
      }
    ]
  },
  {
    "representative_text": "The property of a linear operator being one-to-one (injective) and onto (surjective)",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The property of a linear operator being one-to-one (injective) and onto (surjective)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The property of a linear operator being invertible",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8421912431902279
      },
      {
        "text": "The concept of a linear transformation being invertible if and only if it is both injective and surjective",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8511870952400822
      },
      {
        "text": "The properties of invertible linear transformations, including the concept of the inverse operator",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9063317158947437
      },
      {
        "text": "Invertibility of Linear Transformations:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9022118254449913
      }
    ]
  },
  {
    "representative_text": "The property of a linear operator being nilpotent",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The property of a linear operator being nilpotent",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The identity $(I - A)(I + A) = I^2 - A^2$",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The identity $(I - A)(I + A) = I^2 - A^2$",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The properties of the matrices U, Σ, and V^T",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The properties of the matrices U, Σ, and V^T",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The properties of linear functionals, including the concept of the adjoint operator",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The properties of linear functionals, including the concept of the adjoint operator",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Properties of Linear Operators:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8382711751646152
      },
      {
        "text": "The concept of dual basis for linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8450702153291345
      }
    ]
  },
  {
    "representative_text": "If V is a finite-dimensional vector space, then V has a basis for V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "If V is a finite-dimensional vector space, then V has a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of a Basis for a Vector Space with Finite-Dimensional Vector Space:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8716654188843933
      },
      {
        "text": "This concept of linearlycan be a basis for Finite-Dimensional vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8091309267442626
      }
    ]
  },
  {
    "representative_text": "Strong Basis Theorem: If {v1, v2, ..., vn} is a linearly independent set of vectors, then {v1, v2, ..., vn} is a basis for V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Strong Basis Theorem: If {v1, v2, ..., vn} is a linearly independent set of vectors, then {v1, v2, ..., vn} is a basis for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hilbert-Schmidt Theorem: The Hilbert-Schmidt theorem states that a linear transformation T from a finite-dimensional vector space V to itself is invertible if and only if the sum of the squares of the eigenvalues of T is finite.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hilbert-Schmidt Theorem: The Hilbert-Schmidt theorem states that a linear transformation T from a finite-dimensional vector space V to itself is invertible if and only if the sum of the squares of the eigenvalues of T is finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hilbert Space: A complete inner product space, which is a vector space with an inner product that satisfies certain properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Hilbert Space: A complete inner product space, which is a vector space with an inner product that satisfies certain properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Hilbert Space and Inner Product Spaces: Hilbert space is a complete inner product space, which is a vector space with an inner product that satisfies certain properties, such as linearity and positive definiteness.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8975773819344781
      }
    ]
  },
  {
    "representative_text": "Linear Functionals and the Trace: The sum of the eigenvalues of a matrix, which can be used to compute the trace of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Functionals and the Trace: The sum of the eigenvalues of a matrix, which can be used to compute the trace of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Birkhoff-Jordan Theorem: A theorem that states that every square matrix can be written as a product of orthogonal matrices and diagonal matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Birkhoff-Jordan Theorem: A theorem that states that every square matrix can be written as a product of orthogonal matrices and diagonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Birkhoff-Jordan Theorem and Jordan Canonical Form: The Birkhoff-Jordan Theorem states that every square matrix can be written as a product of orthogonal matrices and diagonal matrices. Jordan canonical form is a way of representing a matrix as a direct sum of Jordan blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8976100685720774
      }
    ]
  },
  {
    "representative_text": "Linearly Independent Span: The span of a set of linearly independent vectors, which is a subspace of the original vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linearly Independent Span: The span of a set of linearly independent vectors, which is a subspace of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-standard Scalar Multiplication: A non-standard scalar multiplication operation, which can affect the closure under scalar multiplication.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Non-standard Scalar Multiplication: A non-standard scalar multiplication operation, which can affect the closure under scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Vector Space with a Non-Standard Scalar Multiplication: A vector space with a non-standard scalar multiplication operation, which can affect the closure under scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8327201075057885
      },
      {
        "text": "Non-standard Scalar Multiplication and Non-standard Addition Operation: A non-standard scalar multiplication operation and a non-standard addition operation can affect the closure under scalar multiplication and addition, respectively.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9247196692491382
      },
      {
        "text": "Non-standard Vector Spaces: The study of vector spaces with non-standard scalar multiplication and addition operations, which can affect the closure under scalar multiplication and addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8263756152290864
      }
    ]
  },
  {
    "representative_text": "Non-standard Addition Operation: A non-standard addition operation, which can affect the closure under addition.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-standard Addition Operation: A non-standard addition operation, which can affect the closure under addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Vector Spaces with Non-standard Operations: A theorem that states that every vector space with a non-standard scalar multiplication and addition operation still has a basis and dimension.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Vector Spaces with Non-standard Operations: A theorem that states that every vector space with a non-standard scalar multiplication and addition operation still has a basis and dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quadratic Forms and Matrix Representation: The study of quadratic forms and their matrix representation can provide additional insights into the properties of vector spaces and linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quadratic Forms and Matrix Representation: The study of quadratic forms and their matrix representation can provide additional insights into the properties of vector spaces and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hilbert-Schmidt Operators: The study of Hilbert-Schmidt operators can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Hilbert-Schmidt Operators: The study of Hilbert-Schmidt operators can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and the Hilbert-Schmidt Operator: The Hilbert-Schmidt operator is a concept in linear algebra that provides a way to measure the \"size\" of a linear transformation. Understanding the Hilbert-Schmidt operator can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8637881567393069
      }
    ]
  },
  {
    "representative_text": "Fredholm Operators: The study of Fredholm operators can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Fredholm Operators: The study of Fredholm operators can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Projection and Fredholm Operators: This involves understanding how orthogonal projection matrices are used to represent Fredholm operators, which are a class of linear operators that have a finite-dimensional kernel and a continuous range.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8430234500580687
      }
    ]
  },
  {
    "representative_text": "The Spectral Theorem for Normal Matrices: The spectral theorem for normal matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Spectral Theorem for Normal Matrices: The spectral theorem for normal matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Spectral Theorem for Hermitian Matrices: The spectral theorem for Hermitian matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9315191522739354
      },
      {
        "text": "The Singular Value Decomposition of Complex Matrices: The study of the singular value decomposition of complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8172883159656879
      },
      {
        "text": "The Diagonalization of Complex Matrices: The study of the diagonalization of complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8719925544580392
      }
    ]
  },
  {
    "representative_text": "The Calculus of Variations: The study of the calculus of variations can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Calculus of Variations: The study of the calculus of variations can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Euler-Lagrange Equations: The study of the Euler-Lagrange equations can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Euler-Lagrange Equations: The study of the Euler-Lagrange equations can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Navier-Stokes Equations: The study of the Navier-Stokes equations can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Navier-Stokes Equations: The study of the Navier-Stokes equations can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Moore-Penrose Inverse of Complex Matrices: The study of the Moore-Penrose inverse of complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Moore-Penrose Inverse of Complex Matrices: The study of the Moore-Penrose inverse of complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Moore-Penrose Inverse of a Matrix with Complex Entries: The Moore-Penrose inverse of a matrix with complex entries is a matrix that satisfies the same properties as the Moore-Penrose inverse of a matrix with real entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8008416334598981
      },
      {
        "text": "Moore-Penrose Inverse of a Matrix with Non-Uniformly Distributed Eigenvalues: The Moore-Penrose inverse of a matrix with non-uniformly distributed eigenvalues is a matrix that satisfies the same properties as the Moore-Penrose inverse of a matrix with real entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8751083174027542
      }
    ]
  },
  {
    "representative_text": "The QR Decomposition of Complex Matrices: The study of the QR decomposition of complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The QR Decomposition of Complex Matrices: The study of the QR decomposition of complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The LU Decomposition of Complex Matrices: The study of the LU decomposition of complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9131056854658779
      },
      {
        "text": "The Polar Decomposition of Complex Matrices: The study of the polar decomposition of complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9228658959550989
      }
    ]
  },
  {
    "representative_text": "The Jordan Canonical Form for Complex Matrices: The study of the Jordan canonical form for complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Jordan Canonical Form for Complex Matrices: The study of the Jordan canonical form for complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Basis and Dimension of Complex Vector Spaces: The study of basis and dimension of complex vector spaces can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Basis and Dimension of Complex Vector Spaces: The study of basis and dimension of complex vector spaces can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Transformation and Matrix Representation for Complex Vectors: The study of linear transformation and matrix representation for complex vectors can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9011871677478056
      },
      {
        "text": "The Rank-Nullity Theorem for Complex Vector Spaces: The study of the rank-nullity theorem for complex vector spaces can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8685190859132457
      }
    ]
  },
  {
    "representative_text": "The Cayley-Hamilton Theorem for Complex Matrices: The study of the Cayley-Hamilton theorem for complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Cayley-Hamilton Theorem for Complex Matrices: The study of the Cayley-Hamilton theorem for complex matrices can provide a deeper understanding of the properties of linear transformations and their matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Methods:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Numerical Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Analysis:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.911496905292345
      }
    ]
  },
  {
    "representative_text": "Universal Property of Basis: Every vector space has a basis, and this basis has the universal property that any linear transformation from the vector space to any vector space can be represented by a matrix with respect to the basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Universal Property of Basis: Every vector space has a basis, and this basis has the universal property that any linear transformation from the vector space to any vector space can be represented by a matrix with respect to the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Universal Property of Basis for Infinite-Dimensional Vector Spaces: Every vector space has a basis, and this basis has the universal property that any linear transformation from the vector space to any vector space can be represented by a matrix with respect to the basis. This concept has important implications for linear algebra and its applications in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9159728028822469
      },
      {
        "text": "Universal Property of Basis for Infinite-Dimensional Vector Spaces with Infinite Basis: Every vector space with an infinite basis has a universal property, but the basis may not be a finite basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8970647864953656
      },
      {
        "text": "Universal Property of Vector Spaces: Every vector space V has a universal property, which states that for any linear transformation T from V to a vector space W, there exists a unique linear transformation S from V to W such that T = S ∘ I, where I is the identity transformation on V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8553929093579237
      }
    ]
  },
  {
    "representative_text": "Basis Extension for Infinite-Dimensional Vector Spaces: The basis extension theorem can be generalized to infinite-dimensional vector spaces. This means that if a basis for a subspace of an infinite-dimensional vector space is extended to a basis for the original vector space, then the extended basis spans the original space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Basis Extension for Infinite-Dimensional Vector Spaces: The basis extension theorem can be generalized to infinite-dimensional vector spaces. This means that if a basis for a subspace of an infinite-dimensional vector space is extended to a basis for the original vector space, then the extended basis spans the original space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis Extension Theorem for Infinite-Dimensional Vector Spaces: We need to explore the implications of the basis extension theorem for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9075012105707578
      },
      {
        "text": "Basis Extension Theorem for Infinite-Dimensional Vector Spaces: The implications of the basis extension theorem for infinite-dimensional vector spaces are not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8963146617929412
      }
    ]
  },
  {
    "representative_text": "Condition for a Subspace to be Closed under Linear Transformations: A subspace is closed under linear transformations if and only if the linear transformation maps the subspace to itself.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Condition for a Subspace to be Closed under Linear Transformations: A subspace is closed under linear transformations if and only if the linear transformation maps the subspace to itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Existence of a Bilinear Form on a Vector Space: Every vector space has a bilinear form, which can be used to represent linear transformations and matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Existence of a Bilinear Form on a Vector Space: Every vector space has a bilinear form, which can be used to represent linear transformations and matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonality and Bilinear Forms: Orthogonality is a fundamental concept in inner product spaces, where two vectors are orthogonal if their inner product is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Orthogonality and Bilinear Forms: Orthogonality is a fundamental concept in inner product spaces, where two vectors are orthogonal if their inner product is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Inner Product Spaces and Orthogonality in Infinite-Dimensional Vector Spaces: Inner product spaces are fundamental concepts in linear algebra, where an inner product space is a vector space with an inner product. Orthogonality is a fundamental concept in these spaces, where two vectors are orthogonal if their inner product is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8981828591068775
      },
      {
        "text": "Inner Product Spaces: Inner product spaces are a fundamental concept in linear algebra, and there are many theorems and properties related to them, such as the Cauchy-Schwarz inequality, the polarization identity, and the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8253839206652513
      },
      {
        "text": "The Inner Product Spaces: This concept is related to the orthogonal projections and states that an inner product space is a vector space equipped with an inner product, which can be used to define the concept of orthogonal transformations and orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8690297521097904
      },
      {
        "text": "Inner Product Spaces on Infinite-Dimensional Vector Spaces: Inner product spaces on infinite-dimensional vector spaces can be used to represent linear transformations and matrices, but their properties may differ from those on finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8058958724414701
      },
      {
        "text": "Inner Product Spaces and Orthogonality: Inner product spaces provide a fundamental connection between inner product spaces and orthogonality.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8746544808620469
      },
      {
        "text": "Characterization of Inner Product Spaces: This involves describing the properties of inner product spaces, including the definition of inner product spaces, inner product properties, and the characterization of inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8197541328386693
      },
      {
        "text": "Inner Product Spaces and Metrics: This involves studying the relationship between inner product spaces and metrics, including the definition of metrics, the properties of metrics, and the characterization of metric spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.808305773782899
      }
    ]
  },
  {
    "representative_text": "Tensor Networks and Linear Algebra: Tensor networks are a powerful tool for computing tensor decompositions and linear transformations. They have applications in various fields, including physics and computer science.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Tensor Networks and Linear Algebra: Tensor networks are a powerful tool for computing tensor decompositions and linear transformations. They have applications in various fields, including physics and computer science.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Tensor Decomposition and Linear Algebra: Tensor decomposition is a powerful tool for computing tensor products and linear transformations. It has applications in various fields, including physics and computer science.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9162118351001403
      },
      {
        "text": "Tensor Decomposition and Infinite-Dimensional Vector Spaces: Tensor decomposition is a powerful tool for computing tensor products and linear transformations. It has applications in various fields, including physics and computer science.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9378739696852023
      }
    ]
  },
  {
    "representative_text": "Cayley-Hamilton Theorem and Jordan Decomposition Theorem: The Cayley-Hamilton theorem and the Jordan decomposition theorem are fundamental theorems in linear algebra that provide connections between linear transformations and their representations as matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Cayley-Hamilton Theorem and Jordan Decomposition Theorem: The Cayley-Hamilton theorem and the Jordan decomposition theorem are fundamental theorems in linear algebra that provide connections between linear transformations and their representations as matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cayley-Hamilton Theorem and Jordan Decomposition Theorem in Infinite-Dimensional Vector Spaces: The Cayley-Hamilton theorem and the Jordan decomposition theorem are fundamental theorems in linear algebra that provide connections between linear transformations and their representations as matrices. These theorems have important implications for linear algebra and its applications in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9229565555416801
      },
      {
        "text": "The Cayley-Hamilton Theorem and the Jordan Decomposition Theorem: The Cayley-Hamilton theorem and the Jordan decomposition theorem provide a fundamental connection between linear transformations and their representations as matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9207528116851188
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Decomposition and Spectral Theorem: The eigenvalue decomposition theorem provides a connection between linear transformations and their representations as matrices. The spectral theorem is a fundamental theorem in linear algebra that provides a connection between linear transformations and their eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Eigenvalue Decomposition and Spectral Theorem: The eigenvalue decomposition theorem provides a connection between linear transformations and their representations as matrices. The spectral theorem is a fundamental theorem in linear algebra that provides a connection between linear transformations and their eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Decomposition and Infinite-Dimensional Vector Spaces: The eigenvalue decomposition theorem provides a connection between linear transformations and their representations as matrices. This theorem has important implications for linear algebra and its applications in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.838817364780372
      },
      {
        "text": "Eigenvalue Decomposition and Linear Algebra: Eigenvalue decomposition provides a fundamental connection between linear transformations and their representations as matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8684420280134624
      },
      {
        "text": "Linear Transformation and Eigenvalue Decomposition: Exploring the eigenvalue decomposition of a linear transformation, which provides a way to diagonalize the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8462886168350026
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt Process for Infinite-Dimensional Vector Spaces: The Gram-Schmidt process is a fundamental algorithm in linear algebra, where it is used to construct an orthonormal basis from a given basis. This process can be generalized to infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt Process for Infinite-Dimensional Vector Spaces: The Gram-Schmidt process is a fundamental algorithm in linear algebra, where it is used to construct an orthonormal basis from a given basis. This process can be generalized to infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Operator Identities:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Operator Identities:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations of Linear Operator Identities**:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9456360880478725
      },
      {
        "text": "Linear Operator Identities: Theorem: Axi:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8801365112646018
      }
    ]
  },
  {
    "representative_text": "Null Space and Column Space:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Null Space and Column Space:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Spectral Mapping Theorem: A theorem that describes the behavior of the eigenvalues of a matrix under a linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Spectral Mapping Theorem: A theorem that describes the behavior of the eigenvalues of a matrix under a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Spectral Mapping Theorem for Non-Symmetric Matrices: A theorem that describes the behavior of the eigenvalues of a non-symmetric matrix under a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9026527085273136
      },
      {
        "text": "The Spectral Mapping Theorem for Linear Operators: This theorem generalizes the spectral mapping theorem to linear operators and is essential in understanding the behavior of eigenvalues under linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8437378777502347
      },
      {
        "text": "Spectral Mapping Theorem for Non-Symmetric Matrices: This is an extension of the spectral mapping theorem for symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8700480616526951
      },
      {
        "text": "The Spectral Mapping Theorem: A (for Non-Symmetric Matrices for Non-Singular matrices: A theorem: A theorem**: A more generally applies to be generalized to theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8262127954943052
      }
    ]
  },
  {
    "representative_text": "The Inverse Function Theorem: A theorem that guarantees the existence and uniqueness of the inverse of a matrix, given certain conditions.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Inverse Function Theorem: A theorem that guarantees the existence and uniqueness of the inverse of a matrix, given certain conditions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Inverse Mapping Theorem: A theorem that guarantees the existence of a matrix inverse for a linear transformation, given certain conditions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9178585981141956
      },
      {
        "text": "The Inverse Mapping Theorem for Non-Symmetric Matrices: A theorem that guarantees the existence of a matrix inverse for a non-symmetric linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8746225838903472
      }
    ]
  },
  {
    "representative_text": "The Rank-Invariant Theorem: A theorem that describes the relationship between the rank of a matrix and the rank of its transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Rank-Invariant Theorem: A theorem that describes the relationship between the rank of a matrix and the rank of its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Matrix-Vector Product Formula: A formula that describes the matrix-vector product of a matrix and a vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Matrix-Vector Product Formula: A formula that describes the matrix-vector product of a matrix and a vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Matrix-Transpose Product Formula: A formula that describes the matrix-transpose product of a matrix and its transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Matrix-Transpose Product Formula: A formula that describes the matrix-transpose product of a matrix and its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Trace of a Matrix: A scalar value that can be computed from the diagonal elements of a square matrix, and has the property that tr(AB) = tr(A)tr(B).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Trace of a Matrix: A scalar value that can be computed from the diagonal elements of a square matrix, and has the property that tr(AB) = tr(A)tr(B).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Invariant Subspace Theorem for Non-Symmetric Matrices: A theorem that describes the relationship between the null space and the range of a non-symmetric matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Invariant Subspace Theorem for Non-Symmetric Matrices: A theorem that describes the relationship between the null space and the range of a non-symmetric matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Rank-Nullity Theorem for Non-Symmetric Matrices: A theorem that describes the relationship between the rank and nullity of a non-symmetric matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.877847641235429
      }
    ]
  },
  {
    "representative_text": "Fredholm Theorem: This theorem states that for a linear transformation T: V -> W between finite-dimensional vector spaces, the rank-nullity theorem holds, and the image of T is a closed subspace of W.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Fredholm Theorem: This theorem states that for a linear transformation T: V -> W between finite-dimensional vector spaces, the rank-nullity theorem holds, and the image of T is a closed subspace of W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fredholm Index: This concept is related to the Fredholm theorem and is defined as the difference between the rank and nullity of a linear transformation T: V -> W. It provides a measure of the \"size\" of the kernel and image of T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Fredholm Index: This concept is related to the Fredholm theorem and is defined as the difference between the rank and nullity of a linear transformation T: V -> W. It provides a measure of the \"size\" of the kernel and image of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Index of a Linear Transformation: The concept of the index of a linear transformation, which is related to the rank-nullity theorem and provides a measure of the \"size\" of the kernel and image of T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8508691617120439
      },
      {
        "text": "Linear Transformation and Index of a Linear Transformation (Alternative): Alternative representations of the index of a linear transformation, such as the rank-nullity theorem and its relationship to the index.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8474924790767878
      },
      {
        "text": "Linear Transformations and the Index of a Linear Transformation (Alternative): Alternative representations of the index of a linear transformation, such as the rank-nullity theorem and its relationship to the index, can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8743037287099956
      }
    ]
  },
  {
    "representative_text": "Cyclic Linear Transformations: A linear transformation T: V -> V is said to be cyclic if there exists a vector v in V such that the set of all images of v under T, denoted by {T^n(v) : n in N}, spans the entire space V. This concept is useful in understanding the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cyclic Linear Transformations: A linear transformation T: V -> V is said to be cyclic if there exists a vector v in V such that the set of all images of v under T, denoted by {T^n(v) : n in N}, spans the entire space V. This concept is useful in understanding the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Fundamental Theorem of Linear Algebra (Alternative): This theorem states that every linear transformation T: V -> W can be represented as a linear combination of its matrix columns. However, there are alternative representations, such as the diagonalization of a matrix, which can provide additional insights into the behavior of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Fundamental Theorem of Linear Algebra (Alternative): This theorem states that every linear transformation T: V -> W can be represented as a linear combination of its matrix columns. However, there are alternative representations, such as the diagonalization of a matrix, which can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and the Fundamental Theorem of Linear Algebra (Alternative): Alternative representations of the fundamental theorem of linear algebra, such as the diagonalization of a matrix, can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8957073782080971
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Minimal Polynomial Theorem (Alternative): This theorem states that the minimal polynomial of a linear transformation T: V -> W divides the characteristic polynomial of T. However, there are alternative representations, such as the minimal polynomial of the matrix representation, which can provide additional insights into the behavior of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Minimal Polynomial Theorem (Alternative): This theorem states that the minimal polynomial of a linear transformation T: V -> W divides the characteristic polynomial of T. However, there are alternative representations, such as the minimal polynomial of the matrix representation, which can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Minimal Polynomial Theorem (Alternative): Alternative representations of the minimal polynomial theorem, such as the minimal polynomial of the matrix representation and its relationship to the minimal polynomial theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8748972117885627
      }
    ]
  },
  {
    "representative_text": "Rank and Nullity for Linear Transformations with Non-Standard Inner Products: The concept of analyzing the rank and nullity of linear transformations with non-standard inner products, which is essential in understanding the properties of linear transformations in non-standard spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "Rank and Nullity for Linear Transformations with Non-Standard Inner Products: The concept of analyzing the rank and nullity of linear transformations with non-standard inner products, which is essential in understanding the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Null Space and Image of a Linear Transformation with a Non-Standard Metric: The concept of analyzing the null space and image of a linear transformation when the metric is non-standard, which can be used to analyze the rank and nullity of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8479807958894159
      },
      {
        "text": "Linear Transformations on Fuzzy Vector Spaces: The concept of analyzing the rank and nullity of linear transformations on fuzzy vector spaces, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8369447409563628
      },
      {
        "text": "Rank and Nullity for Linear Transformations with Non-Standard Inner Products in Banach Spaces: The concept of analyzing the rank and nullity of linear transformations with non-standard inner products in Banach spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8815337005194384
      },
      {
        "text": "Rank and Nullity for Linear Transformations with Non-Standard Structures in Differential Geometry: The concept of analyzing the rank and nullity of linear transformations with non-standard structures in differential geometry, which can be used to analyze the properties of linear transformations in higher-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8656117394732317
      },
      {
        "text": "Linear Transformations on Hilbert Spaces with Non-Standard Inner Products: The concept of analyzing the rank and nullity of linear transformations on Hilbert spaces with non-standard inner products, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9304781799589117
      },
      {
        "text": "Linear Transformations and Non-Standard Operator Algebras: The concept of analyzing the rank and nullity of linear transformations using non-standard operator algebras, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8737640495717183
      },
      {
        "text": "Rank and Nullity for Linear Transformations with Non-Standard Metric in Quantum Mechanics: The concept of analyzing the rank and nullity of linear transformations with non-standard metrics in quantum mechanics, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.910705789556493
      },
      {
        "text": "Linear Transformations on Riemannian Manifolds with Non-Standard Metrics: The concept of analyzing the rank and nullity of linear transformations on Riemannian manifolds with non-standard metrics, which can be used to analyze the properties of linear transformations in higher-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9042856944847093
      },
      {
        "text": "Linear Transformations and Non-Standard Topology: The concept of analyzing the rank and nullity of linear transformations using non-standard topology, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8857702816129783
      },
      {
        "text": "Rank and Nullity for Linear Transformations with Non-Standard Structures in Algebraic Topology: The concept of analyzing the rank and nullity of linear transformations with non-standard structures in algebraic topology, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9153186951298378
      },
      {
        "text": "Linear Transformations on Banach Spaces with Non-Standard Norms: The concept of analyzing the rank and nullity of linear transformations on Banach spaces with non-standard norms, which can be used to analyze the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9186161167248978
      },
      {
        "text": "Rank and Nullity of Linear Transformations in Topological Spaces: The concept of analyzing the rank and nullity of linear transformations when the image or kernel is degenerate in topological spaces, which is essential in understanding the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8981105531848443
      },
      {
        "text": "Rank and Nullity of Linear Transformations with Non-Standard Riemannian Metrics: The rank-nullity theorem can be used to analyze the properties of linear transformations with non-standard Riemannian metrics, which is essential in understanding the behavior of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8977701097600179
      }
    ]
  },
  {
    "representative_text": "Rank and Nullity for Linear Transformations with Non-Finite-Dimensional Domain or Codomain: The concept of analyzing the rank and nullity of linear transformations with non-finite-dimensional domain or codomain, which is essential in understanding the properties of linear transformations in infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Rank and Nullity for Linear Transformations with Non-Finite-Dimensional Domain or Codomain: The concept of analyzing the rank and nullity of linear transformations with non-finite-dimensional domain or codomain, which is essential in understanding the properties of linear transformations in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rank and Nullity for Linear Transformations with Degenerate Image or Kernel in Infinite-Dimensional Spaces: The concept of analyzing the rank and nullity of linear transformations when the image or kernel is degenerate in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8707229470313829
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Matrix Inversion: The relationship between linear transformations and matrix inversion can be studied using determinants. Specifically, the determinant of the inverse matrix can be used to determine the behavior of the linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Matrix Inversion: The relationship between linear transformations and matrix inversion can be studied using determinants. Specifically, the determinant of the inverse matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Matrix Inversion with Non-Orthogonal Linear Transformations: The relationship between linear transformations and matrix inversion with non-orthogonal linear transformations can be studied using determinants. Specifically, the determinant of the inverse matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9551283611503147
      },
      {
        "text": "Linear Transformations and the Modular Inverse: The modular inverse is a concept in linear algebra that provides a way to compute the inverse of a matrix modulo a certain matrix. Understanding the modular inverse can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8199460562892493
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Complex Eigenvalues and Non-Orthogonal Basis: The determinant of a matrix with complex eigenvalues and non-orthogonal basis is not equal to the determinant of the matrix with orthogonal basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues and Non-Orthogonal Basis: The determinant of a matrix with complex eigenvalues and non-orthogonal basis is not equal to the determinant of the matrix with orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues and Non-Orthogonal Decomposition: The determinant of a matrix with complex eigenvalues and non-orthogonal decomposition is not equal to the determinant of the matrix with orthogonal decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9237132862385701
      },
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues and Non-Constant Coefficients and Non-Orthogonal Decomposition: The determinant of a matrix with complex eigenvalues and non-constant coefficients and non-orthogonal decomposition is not necessarily equal to the determinant of the matrix with orthogonal decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9233165739570255
      },
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues and Non-Constant Coefficients and Non-Orthogonal Basis: The determinant of a matrix with complex eigenvalues and non-constant coefficients and non-orthogonal basis is not necessarily equal to the determinant of the matrix with orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9562688740048857
      },
      {
        "text": "Determinant of a Matrix with a Non-Orthogonal Decomposition (e.g., a decomposition into eigenvectors and eigenvalues): The determinant of a matrix with a non-orthogonal decomposition is not necessarily equal to the determinant of the matrix with orthogonal decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9045711979856682
      },
      {
        "text": "Determinant of a matrix with a non-orthogonal basis and a specific type of decomposition: The determinant of a matrix with a non-orthogonal basis and a specific type of decomposition, such as SVD, is not necessarily equal to the determinant of the matrix with an orthogonal basis and orthogonal decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8600793526092125
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Linearly Dependent Columns or Rows and Non-Linear Transformations: The determinant of a matrix with linearly dependent columns or rows and non-linear transformations is not equal to 0.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Linearly Dependent Columns or Rows and Non-Linear Transformations: The determinant of a matrix with linearly dependent columns or rows and non-linear transformations is not equal to 0.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Non-Linearly Independent Columns or Rows: The determinant of a matrix with non-linearly independent columns or rows is not necessarily equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9376587025781247
      },
      {
        "text": "Determinant of a Matrix with Non-Linearly Independent Columns or Rows and Non-Constant Coefficients: The determinant of a matrix with non-linearly independent columns or rows and non-constant coefficients is not necessarily equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9443920022035853
      },
      {
        "text": "Determinant of a Matrix with Non-Constant Coefficients and Non-Linearly Independent Columns or Rows and Non-Orthogonal Decomposition: The determinant of a matrix with non-constant coefficients and non-linearly independent columns or rows and non-orthogonal decomposition is not necessarily equal to zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.911004633388308
      }
    ]
  },
  {
    "representative_text": "Matrix Chain Decomposition: This is a method for decomposing a matrix into smaller sub-matrices, similar to the chain rule in calculus.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Chain Decomposition: This is a method for decomposing a matrix into smaller sub-matrices, similar to the chain rule in calculus.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cyclic Matrices and Their Properties: This involves the study of matrices that can be expressed as the product of a set of matrices, and the properties of these matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cyclic Matrices and Their Properties: This involves the study of matrices that can be expressed as the product of a set of matrices, and the properties of these matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computing the Eigenvectors of a Matrix using the QR Algorithm with Shift and Invert: This involves the study of the QR algorithm with shift and invert, which is an extension of the QR algorithm for computing the eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Computing the Eigenvectors of a Matrix using the QR Algorithm with Shift and Invert: This involves the study of the QR algorithm with shift and invert, which is an extension of the QR algorithm for computing the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computing the Eigenvalues of a Matrix using the QR Algorithm with Shift: This involves the study of the QR algorithm with shift, which is an extension of the QR algorithm for computing the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9398506589071521
      }
    ]
  },
  {
    "representative_text": "Poincaré Duality: A theorem in linear algebra that states that the rank of a matrix is equal to the rank of its transpose, and that the null space of a matrix is equal to the row space of its transpose.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Poincaré Duality: A theorem in linear algebra that states that the rank of a matrix is equal to the rank of its transpose, and that the null space of a matrix is equal to the row space of its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cramer's Rule for Non-Square Matrices: An extension of Cramer's rule to non-square matrices, which involves using the inverse of the matrix to find the solution.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Cramer's Rule for Non-Square Matrices: An extension of Cramer's rule to non-square matrices, which involves using the inverse of the matrix to find the solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cramer's Rule Extension: Cramer's rule is typically applied to square matrices. However, there are extensions to the rule that allow us to apply it to non-square matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8588751042283999
      }
    ]
  },
  {
    "representative_text": "The Invertibility Criterion using the Rank-Nullity Theorem (Generalized): A more general version of the invertibility criterion using the rank-nullity theorem, which involves using the properties of the matrix and the rank-nullity theorem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "The Invertibility Criterion using the Rank-Nullity Theorem (Generalized): A more general version of the invertibility criterion using the rank-nullity theorem, which involves using the properties of the matrix and the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Invertibility Criterion using the Inverse of the Adjugate Matrix (Generalized): A more general version of the invertibility criterion using the inverse of the adjugate matrix, which involves using the properties of the matrix and the inverse of the adjugate matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8368432018745336
      },
      {
        "text": "The Invertibility Criterion using the Null Space Rank: A criterion for determining the invertibility of a matrix using the rank of its null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8561254149999495
      },
      {
        "text": "The Invertibility Criterion using the Orthogonal Projections: A criterion for determining the invertibility of a matrix using orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732789762141304
      },
      {
        "text": "The Invertibility Criterion using the Null Space Rank: This is a criterion for determining the invertibility of a matrix using the rank of its null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8976237981622808
      },
      {
        "text": "The Invertibility Criterion using the Orthogonal Projections: This is a criterion for determining the invertibility of a matrix using orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8955962366426986
      },
      {
        "text": "The Invertibility Criterion using the Null Space Rank for Block Matrices: This is a criterion for determining the invertibility of a block matrix using the rank of its null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8972889051362536
      },
      {
        "text": "The Invertibility Criterion using the Orthogonal Projections for Block Matrices: This is a criterion for determining the invertibility of a block matrix using orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9042612429811598
      },
      {
        "text": "The Invertibility Criterion using the Null Space Rank for Block Matrices (Generalized): A criterion for determining the invertibility of a block matrix using the rank of its null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9208874107278311
      },
      {
        "text": "The Invertibility Criterion using the Orthogonal Projections for Block Matrices (Generalized): A criterion for determining the invertibility of a block matrix using orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9091383935736053
      },
      {
        "text": "Invertibility using the orthogonal projections: A matrix A is invertible if and only if its orthogonal projections have full rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8525875519527745
      }
    ]
  },
  {
    "representative_text": "The Determinant of a Matrix with a Zero Eigenvalue (Generalized): A more general version of the determinant of a matrix with a zero eigenvalue, which involves using the properties of the matrix and the eigenvalue.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Determinant of a Matrix with a Zero Eigenvalue (Generalized): A more general version of the determinant of a matrix with a zero eigenvalue, which involves using the properties of the matrix and the eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Conjugate Eigenvalue Problem: This problem deals with finding the eigenvalues of a matrix that are conjugates of each other.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Conjugate Eigenvalue Problem: This problem deals with finding the eigenvalues of a matrix that are conjugates of each other.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Positive Definite Matrix: This topic deals with matrices that are positive definite, which have a positive eigenvalue for all eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Positive Definite Matrix: This topic deals with matrices that are positive definite, which have a positive eigenvalue for all eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Positive Semi-Definite Matrix: This topic deals with matrices that are positive semi-definite, which have a non-negative eigenvalue for all eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9456757467602149
      },
      {
        "text": "Positive Definite Matrices: A positive definite matrix is a matrix that is always positive when multiplied by a non-zero vector. Positive definite matrices have many applications in linear algebra, including finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8252014614885648
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Pairs: Eigenvalue pairs are a concept that describes the properties of a matrix in terms of its eigenvalues and eigenvectors. They are related to the concept of diagonalization and Jordan block.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvalue Pairs: Eigenvalue pairs are a concept that describes the properties of a matrix in terms of its eigenvalues and eigenvectors. They are related to the concept of diagonalization and Jordan block.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Pairs for Non-Square Matrices: Eigenvalue pairs are a concept that describes the properties of a matrix in terms of its eigenvalues and eigenvectors. They are related to the concept of diagonalization and Jordan block, and can be used to solve systems of linear equations for non-square matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9220292499665017
      }
    ]
  },
  {
    "representative_text": "Block Triangular Matrices: Block triangular matrices are a type of matrix that has a specific structure. They are related to the concept of LU decomposition and can be used to solve certain types of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Block Triangular Matrices: Block triangular matrices are a type of matrix that has a specific structure. They are related to the concept of LU decomposition and can be used to solve certain types of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Block Triangular Matrices for Non-Square Matrices: Block triangular matrices are a type of matrix that has a specific structure. They are related to the concept of LU decomposition and can be used to solve systems of linear equations for non-square matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.943019300036423
      }
    ]
  },
  {
    "representative_text": "Matrix Inversion Theorems for Non-Orthogonal Bases: While the inverse of a matrix is typically defined only for orthogonal bases, there are theorems that extend the concept of invertibility to non-orthogonal bases in certain contexts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Inversion Theorems for Non-Orthogonal Bases: While the inverse of a matrix is typically defined only for orthogonal bases, there are theorems that extend the concept of invertibility to non-orthogonal bases in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block SVD: The SVD of a matrix with a large number of rows or columns can be computed using the block QR algorithm. This is an important extension of the standard SVD algorithm.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block SVD: The SVD of a matrix with a large number of rows or columns can be computed using the block QR algorithm. This is an important extension of the standard SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kronecker Product and Tensor Product: The Kronecker product and tensor product of two matrices A and B are new matrices formed by multiplying each element of A by the entire matrix B. This is an important concept that is related to the SVD algorithm.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kronecker Product and Tensor Product: The Kronecker product and tensor product of two matrices A and B are new matrices formed by multiplying each element of A by the entire matrix B. This is an important concept that is related to the SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Algebraic and Geometric Multiplicity: The relationship between algebraic and geometric multiplicities of eigenvalues is crucial in understanding the properties of a matrix. The algebraic multiplicity is the number of times an eigenvalue appears as a root of the characteristic equation, while the geometric multiplicity is the number of linearly independent eigenvectors corresponding to an eigenvalue.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Algebraic and Geometric Multiplicity: The relationship between algebraic and geometric multiplicities of eigenvalues is crucial in understanding the properties of a matrix. The algebraic multiplicity is the number of times an eigenvalue appears as a root of the characteristic equation, while the geometric multiplicity is the number of linearly independent eigenvectors corresponding to an eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Eigenvalues using the Algebraic and Geometric Multiplicity: The relationship between algebraic and geometric multiplicities of eigenvalues is crucial in understanding the properties of a matrix. The algebraic multiplicity is the number of times an eigenvalue appears as a root of the characteristic equation, while the geometric multiplicity is the number of linearly independent eigenvectors corresponding to an eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9289047467971099
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) and Eigenvalue Decomposition: SVD is a factorization of a matrix A into the product of three matrices: U, Σ, and V^T. Eigenvalue decomposition is a factorization of a matrix A into the product of a diagonal matrix D and an orthogonal matrix P, where D contains the eigenvalues of A. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) and Eigenvalue Decomposition: SVD is a factorization of a matrix A into the product of three matrices: U, Σ, and V^T. Eigenvalue decomposition is a factorization of a matrix A into the product of a diagonal matrix D and an orthogonal matrix P, where D contains the eigenvalues of A. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Singular Value Decomposition (SVD) and its relation to Orthogonal Diagonalization: The SVD is a factorization of a matrix into the product of three matrices: an orthogonal matrix, a diagonal matrix, and a transpose of an orthogonal matrix. This decomposition is related to orthogonal diagonalization as it provides a way to find the singular values and singular vectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8404471422553037
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix: The SVD of a matrix is a factorization of the matrix into the product of three matrices: an orthogonal matrix, a diagonal matrix, and a diagonal matrix. This concept is essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9018470693805434
      },
      {
        "text": "The Singular Value Decomposition (SVD) of a Matrix: The SVD of a matrix is a factorization of the matrix into the product of three matrices: an orthogonal matrix, a diagonal matrix, and a transpose of an orthogonal matrix. This decomposition is closely related to orthogonal diagonalization and is widely used in various applications of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9254974951657899
      }
    ]
  },
  {
    "representative_text": "Gelfand's Theorem: Gelfand's theorem states that if A is a square matrix with distinct eigenvalues, then A can be diagonalized using a similarity transformation. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Gelfand's Theorem: Gelfand's theorem states that if A is a square matrix with distinct eigenvalues, then A can be diagonalized using a similarity transformation. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Lanczos Theorem: The Lanczos theorem states that if A is a symmetric matrix with distinct eigenvalues, then A can be diagonalized using a similarity transformation. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.860459394596696
      }
    ]
  },
  {
    "representative_text": "Eigenvector Scaling: Eigenvector scaling is a concept in linear algebra that states that if v is an eigenvector of A corresponding to eigenvalue λ, then kv is an eigenvector of A corresponding to eigenvalue λk. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvector Scaling: Eigenvector scaling is a concept in linear algebra that states that if v is an eigenvector of A corresponding to eigenvalue λ, then kv is an eigenvector of A corresponding to eigenvalue λk. This is an important concept in linear algebra, as it can be used to analyze the behavior of matrices in various applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and the Fundamental Theorem of Hilbert Spaces: This theorem states that every Hilbert space has an orthogonal complement for every subspace, which is a fundamental result in functional analysis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and the Fundamental Theorem of Hilbert Spaces: This theorem states that every Hilbert space has an orthogonal complement for every subspace, which is a fundamental result in functional analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and the Spectral Theorem: This theorem states that every linear operator on a Hilbert space has a spectral decomposition, which involves the orthogonal complement of the null space and the range of the operator.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and the Spectral Theorem: This theorem states that every linear operator on a Hilbert space has a spectral decomposition, which involves the orthogonal complement of the null space and the range of the operator.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Spectral Theorem for Orthogonal Complement: If T: V → V is a linear transformation, then the orthogonal complement of T is the set of all eigenvectors of T corresponding to eigenvalues with absolute value 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8102784591624446
      },
      {
        "text": "Spectral Theorem for Orthogonal Complement: If T: V → V is a linear transformation, then the orthogonal complement of T is the set of all eigenv",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8926678072450003
      },
      {
        "text": "Spectral Theorem for Orthogonal Complement of Matrices: The spectral theorem can be extended to orthogonal complements of matrices, and its implications can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.815824466737366
      },
      {
        "text": "Orthogonal Projections and the Spectral Theorem: The spectral theorem can be used to find the orthogonal projections onto the eigenspaces of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8307989154560005
      },
      {
        "text": "Spectral Theorem for Orthogonal Complement of a Matrix using the Schmidt Process and Gram-Schmidt Process: The spectral theorem can be extended to orthogonal complements of a matrix using the Schmidt process and the Gram-Schmidt process, and its implications can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8536819167678003
      }
    ]
  },
  {
    "representative_text": "The Inverse of an Orthogonal Matrix: The inverse of an orthogonal matrix is equal to its transpose, i.e., A^(-1) = A^T.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Inverse of an Orthogonal Matrix: The inverse of an orthogonal matrix is equal to its transpose, i.e., A^(-1) = A^T.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Matrix Invertibility: An orthogonal matrix is invertible, and its inverse is equal to its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9070683041420966
      },
      {
        "text": "Orthogonal Matrix Inversion Properties: The properties of the inverse of an orthogonal matrix, including the fact that the inverse of an orthogonal matrix is equal to its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8861698019744779
      },
      {
        "text": "Orthogonal Matrix Transpose: The transpose of an orthogonal matrix is its inverse, i.e., A^T A = I.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9289453471471473
      }
    ]
  },
  {
    "representative_text": "The Determinant of an Orthogonal Matrix: The determinant of an orthogonal matrix is either 1 or -1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Determinant of an Orthogonal Matrix: The determinant of an orthogonal matrix is either 1 or -1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection and Clifford Algebras: This involves understanding how orthogonal projection matrices are used in Clifford algebras to represent geometric transformations and to compute geometric invariants.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection and Clifford Algebras: This involves understanding how orthogonal projection matrices are used in Clifford algebras to represent geometric transformations and to compute geometric invariants.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Use of Orthogonal Projection Matrices in Representation Theory: This involves understanding how orthogonal projection matrices are used to represent linear transformations in representation theory, such as in the study of group representations and character theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Use of Orthogonal Projection Matrices in Representation Theory: This involves understanding how orthogonal projection matrices are used to represent linear transformations in representation theory, such as in the study of group representations and character theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bessel's Inequality: This states that the sum of the squares of the coefficients in an orthogonal expansion is less than or equal to the square of the norm of the original vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bessel's Inequality: This states that the sum of the squares of the coefficients in an orthogonal expansion is less than or equal to the square of the norm of the original vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Relationship between Determinants and Eigenvectors: While the relationship between determinants and eigenvalues is well-established, there is also a relationship between determinants and eigenvectors. Specifically, the determinant of a matrix can be used to determine the eigenvectors of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Relationship between Determinants and Eigenvectors: While the relationship between determinants and eigenvalues is well-established, there is also a relationship between determinants and eigenvectors. Specifically, the determinant of a matrix can be used to determine the eigenvectors of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Determinants and Linear Transformations: While the relationship between determinants and eigenvalues is well-established, there is also a relationship between determinants and linear transformations. Specifically, the determinant of a matrix can be used to determine the linear transformation described by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9122211585840533
      }
    ]
  },
  {
    "representative_text": "The Determinant of a Matrix with a Singular Jacobian Matrix: This is a subtle nuance that states that the determinant of a matrix can change under a singular Jacobian matrix. This has important implications for the calculation of determinants and the properties of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Determinant of a Matrix with a Singular Jacobian Matrix: This is a subtle nuance that states that the determinant of a matrix can change under a singular Jacobian matrix. This has important implications for the calculation of determinants and the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with a Non-Linear Jacobian Matrix: This is a subtle nuance that states that the determinant of a matrix can change under a non-linear Jacobian matrix. This has important implications for the calculation of determinants and the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9531220285410891
      },
      {
        "text": "Determinant of a Matrix with a Non-Integrable Transformation Matrix: This is a subtle nuance that states that the determinant of a matrix can change under a non-integrable transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9140126990794246
      },
      {
        "text": "Determinant of a Matrix with a Singular Jacobian Matrix: This is a subtle nuance that states that the determinant of a matrix can change under a singular Jacobian matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9596325822276746
      },
      {
        "text": "Determinant of a Matrix with a Non-Orthogonal Transformation Matrix: This is a subtle nuance that states that the determinant of a matrix can change under a non-orthogonal transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9323405596648643
      }
    ]
  },
  {
    "representative_text": "The Determinant of a Matrix with a Non-Constant Coefficient Matrix: The determinant of a matrix can change under a non-constant coefficient matrix. This has important implications for the calculation of determinants and the properties of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "The Determinant of a Matrix with a Non-Constant Coefficient Matrix: The determinant of a matrix can change under a non-constant coefficient matrix. This has important implications for the calculation of determinants and the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Non-Constant Coefficients: The determinant of a matrix with non-constant coefficients is not necessarily equal to the determinant of the matrix with constant coefficients.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8857786721137985
      },
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues and Non-Constant Coefficients: The determinant of a matrix with complex eigenvalues and non-constant coefficients is not necessarily equal to the determinant of the matrix with constant coefficients.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8827155925147545
      },
      {
        "text": "Determinant of a Matrix with Non-Constant Coefficients and Non-Orthogonal Basis: The determinant of a matrix with non-constant coefficients and non-orthogonal basis is not necessarily equal to the determinant of the matrix with constant coefficients and orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8927980083979239
      },
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues: This is a subtle nuance that states that the determinant of a matrix can change under a non-constant coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9002593429475878
      },
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Decomposition and Non-Constant Coefficients: The determinant of a matrix with non-orthogonal decomposition and non-constant coefficients is not necessarily equal to the determinant of the matrix with orthogonal decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8952364955961385
      },
      {
        "text": "Determinant of a Matrix with a Non-Constant Coefficient Matrix: The determinant of a matrix with a non-constant coefficient matrix can be calculated using the formula for the determinant of a matrix with a non-constant coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8905758189132389
      },
      {
        "text": "Determinant of a Matrix with Non-Constant Coefficients and Non-Orthogonal Decomposition (with a specific type of decomposition, e.g., QR decomposition): The determinant of a matrix with non-constant coefficients and non-orthogonal decomposition is not necessarily equal to the determinant of the matrix with orthogonal decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9021972473461225
      },
      {
        "text": "Determinant of a Matrix with a Non-Constant Coefficient Matrix (e.g., a matrix with a non-constant coefficient matrix): The determinant of a matrix with a non-constant coefficient matrix is not necessarily equal to the determinant of the matrix with constant coefficients.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9633321734042652
      },
      {
        "text": "Determinant of a Matrix with a Non-Linear Decomposition (e.g., a decomposition into a polynomial and a linear term): The determinant of a matrix with a non-linear decomposition is not necessarily equal to the determinant of the matrix with linear terms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.874158564796519
      },
      {
        "text": "Determinant of a matrix with non-constant coefficients and a specific type of decomposition: The determinant of a matrix with non-constant coefficients and a specific type of decomposition, such as QR decomposition, is not necessarily equal to the determinant of the matrix with constant coefficients and orthogonal decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9009022735699319
      },
      {
        "text": "Determinant of a matrix with a non-constant coefficient matrix and a specific type of decomposition: The determinant of a matrix with a non-constant coefficient matrix and a specific type of decomposition, such as SVD, is not necessarily equal to the determinant of the matrix with constant coefficients and orthogonal decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8937691283472791
      },
      {
        "text": "Determinant of a matrix with a non-linear decomposition: The determinant of a matrix with a non-linear decomposition is not necessarily equal to the determinant of the matrix with linear terms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8907753699856115
      }
    ]
  },
  {
    "representative_text": "Jacobi Iteration Method: The Jacobi iteration method is an iterative algorithm for finding the inverse of a matrix by iteratively updating the matrix using a fixed-point iteration.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jacobi Iteration Method: The Jacobi iteration method is an iterative algorithm for finding the inverse of a matrix by iteratively updating the matrix using a fixed-point iteration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinants of Matrix Products with a Non-Standard Order: The determinant of a matrix product, where the order of the matrices is not standard (i.e., not the usual AB or BA), can be calculated using various methods, such as the permutation of the matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Determinants of Matrix Products with a Non-Standard Order: The determinant of a matrix product, where the order of the matrices is not standard (i.e., not the usual AB or BA), can be calculated using various methods, such as the permutation of the matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinants of Matrix Products with a Non-Standard Order and Non-Standard Decomposition: The determinant of a matrix product, where the order of the matrices is not standard and the decomposition is not standard (e.g., not LU or QR), can be calculated using various methods, such as the Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8849752326588074
      },
      {
        "text": "Determinants of Matrix Products with a Non-Standard Order and Non-Standard Block Structure: The determinant of a matrix product, where the order of the matrices is not standard and the matrix has a non-standard block structure (e.g., a block diagonal matrix with non-square blocks), can be calculated using various methods, such as the Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9372077879888896
      },
      {
        "text": "Determinants of Matrix Products with a Non-Standard Order and Non-Standard Linear Transformation: The determinant of a matrix product, where the order of the matrices is not standard and the linear transformation is not standard (e.g., not the usual addition or scalar multiplication), can be calculated using various methods, such as the Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9490318350570679
      },
      {
        "text": "Determinants of Matrix Products with a Non-Standard Order and Non-Standard Linear Independence: The determinant of a matrix product, where the order of the matrices is not standard and the linear independence of the columns or rows is not standard, can be calculated using various methods, such as the Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9544452752853324
      },
      {
        "text": "Determinants of Matrix Products with a Non-Standard Order and Non-Standard Eigenvalues: The determinant of a matrix product, where the order of the matrices is not standard and the eigenvalues are not standard (e.g., not real), can be calculated using various methods, such as the Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9648915150266126
      },
      {
        "text": "Determinants of Matrix Products with a Non-Standard Order and Non-Standard Jordan Canonical Form: The determinant of a matrix product, where the order of the matrices is not standard and the Jordan canonical form is not standard (e.g., not diagonalizable), can be calculated using various methods, such as the Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9632074922489273
      },
      {
        "text": "Determinants of Matrix Products with a Non-Standard Order and Non-Standard Spectral Theorem: The determinant of a matrix product, where the order of the matrices is not standard and the spectral theorem is not standard (e.g., not diagonalizable), can be calculated using various methods, such as the Schur decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9577209133445646
      },
      {
        "text": "Determinant of a Matrix Product with a Non-Standard Order and Non-Standard Decomposition: A method for computing the determinant of a matrix product where the order of the matrices is not standard and the decomposition is not standard (e.g., not LU or QR).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8542229303489388
      }
    ]
  },
  {
    "representative_text": "Theorem of Jacobson: A theorem that provides a relationship between the determinant of a matrix and its eigenvalues.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Theorem of Jacobson: A theorem that provides a relationship between the determinant of a matrix and its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem of Sylvester: A theorem that provides a relationship between the determinant of a matrix and its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9465877485748402
      },
      {
        "text": "Theorem of Vandermonde: A theorem that provides a relationship between the determinant of a matrix and its eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9522799692057939
      }
    ]
  },
  {
    "representative_text": "Vandermonde Matrix: A matrix whose determinant is a Vandermonde polynomial, which is a polynomial that has a specific structure and is used to compute the determinant of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vandermonde Matrix: A matrix whose determinant is a Vandermonde polynomial, which is a polynomial that has a specific structure and is used to compute the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of linear independence in projective spaces: In projective spaces, linear independence is defined differently than in vector spaces. A set of vectors in a projective space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The concept of linear independence in projective spaces: In projective spaces, linear independence is defined differently than in vector spaces. A set of vectors in a projective space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of linear independence in symmetric spaces: In symmetric spaces, linear independence is defined differently than in vector spaces. A set of vectors in a symmetric space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8542697350449687
      }
    ]
  },
  {
    "representative_text": "The connection between linear independence and the concept of linear dependence in generalized vector spaces: Generalized vector spaces, such as generalized inner product spaces, provide a way to generalize the concept of linear independence to spaces that are not necessarily inner product spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 12,
    "detailed_sources": [
      {
        "text": "The connection between linear independence and the concept of linear dependence in generalized vector spaces: Generalized vector spaces, such as generalized inner product spaces, provide a way to generalize the concept of linear independence to spaces that are not necessarily inner product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The connection between linear independence and the concept of linear dependence in metric spaces: Metric spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily inner product spaces. A set of vectors in a metric space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8360981235338496
      },
      {
        "text": "The concept of linear independence in normed spaces: Normed spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily finite-dimensional. A set of vectors in a normed space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8774112436777721
      },
      {
        "text": "The connection between linear independence and the concept of linear dependence in topological vector spaces: Topological vector spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily normed spaces. A set of vectors in a topological vector space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9063038495992765
      },
      {
        "text": "The concept of linear independence in C-algebras: C-algebras provide a way to generalize the concept of linear independence to spaces that are not necessarily Banach spaces. A set of vectors in a C*-algebra is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8153507202887824
      },
      {
        "text": "The connection between linear independence and the concept of linear dependence in Hilbert spaces: Hilbert spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily Banach spaces. A set of vectors in a Hilbert space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9078347102918327
      },
      {
        "text": "The concept of linear independence in operator algebras: Operator algebras provide a way to generalize the concept of linear independence to spaces that are not necessarily Banach spaces. A set of vectors in an operator algebra is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8752430553689728
      },
      {
        "text": "The Connection between Linear Independence and the Concept of Linear Dependence in Topological Vector Spaces: Topological vector spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily normed spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8980991727380365
      },
      {
        "text": "The Connection between Linear Independence and the Concept of Linear Dependence in Hilbert Spaces: Hilbert spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily Banach spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8895370046725031
      },
      {
        "text": "Linear Independence in Metric Spaces: Metric spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily inner product spaces. A set of vectors in a metric space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921033913504062
      },
      {
        "text": "Linear Independence in Hilbert Spaces: Hilbert spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily Banach spaces. A set of vectors in a Hilbert space is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9204207545055546
      },
      {
        "text": "Linear Independence and the Concept of Linear Dependence in Topological Vector Spaces: Topological vector spaces provide a way to generalize the concept of linear independence to spaces that are not necessarily normed spaces. A set of vectors in a topological vector in a set is said to be determined if theore the concept of a set of Linear Independence of linear independence in topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8519235489113508
      }
    ]
  },
  {
    "representative_text": "The role of the Mazur-Ulam theorem in linear independence: The Mazur-Ulam theorem provides a way to show that a set of vectors in a Banach space is linearly independent if and only if it is also a basis for the space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The role of the Mazur-Ulam theorem in linear independence: The Mazur-Ulam theorem provides a way to show that a set of vectors in a Banach space is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Mazur-Ulam Theorem in Banach Spaces: The Mazur-Ulam theorem provides a way to show that a set of vectors in a Banach space is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9491448577522629
      },
      {
        "text": "Linear Independence and the Role of Mazur-Ulam Theorem in Higher-Dimensional Spaces: The Mazur-Ulam theorem provides a way to show that a set of vectors in a topological vector space is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9479153763148689
      },
      {
        "text": "Linear Independence and the Mazur-Ulam Theorem for Infinite-Dimensional Vector Spaces: The Mazur-Ulam theorem provides a way to show that a set of vectors in an infinite-dimensional vector space is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9488727705800464
      },
      {
        "text": "Linear Independence and the Role of Mazur-Ulam Theorem in Operator Algebras: The Mazur-Ulam theorem provides a way to show that a set of vectors in an operator algebra is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9380285752370834
      }
    ]
  },
  {
    "representative_text": "The role of the Schur's theorem in linear independence: Schur's theorem provides a way to show that a set of vectors in a Banach space is linearly independent if and only if it is also a basis for the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The role of the Schur's theorem in linear independence: Schur's theorem provides a way to show that a set of vectors in a Banach space is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Role of Schur's Theorem in Higher-Dimensional Spaces: Schur's theorem provides a way to show that a set of vectors in a Banach space is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9453533881124598
      }
    ]
  },
  {
    "representative_text": "The role of the Krein-Milman theorem in linear independence: The Krein-Milman theorem provides a way to show that a set of vectors in a Banach space is linearly independent if and only if it is also a basis for the space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The role of the Krein-Milman theorem in linear independence: The Krein-Milman theorem provides a way to show that a set of vectors in a Banach space is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Krein-Milman Theorem and Linear Independence in Hilbert Spaces: The Krein-Milman theorem provides a way to show that a set of vectors in a Hilbert space is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9178890468422919
      },
      {
        "text": "Linear Independence and the Role of Krein-Milman Theorem in Higher-Dimensional Spaces: The Krein-Milman theorem provides a way to show that a set of vectors in a Banach space is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9562872307194994
      },
      {
        "text": "Linear Independence and the Role of Krein-Milman Theorem in Operator Algebras: The Krein-Milman theorem provides a way to show that a set of vectors in an operator algebra is linearly independent if and only if it is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9308982102391854
      }
    ]
  },
  {
    "representative_text": "Linear Independence of the Image of a Linear Transformation under a Composition of Transformations: If a set of vectors is linearly independent under a linear transformation T, then the image of the set under the composition of transformations T ∘ S is also linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Independence of the Image of a Linear Transformation under a Composition of Transformations: If a set of vectors is linearly independent under a linear transformation T, then the image of the set under the composition of transformations T ∘ S is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Independence of the Image of a Linear Transformation under a Linear Combination of Transformations: If a set of vectors is linearly independent under a linear transformation T, then the image of the set under a linear combination of transformations T ∘ S is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9387163150079669
      },
      {
        "text": "The Linear Independence of the Image of a Linear Transformation under a Composition of Transformations with a Non-Empty Kernel: If a set of vectors is linearly independent under a linear transformation T, then the image of the set under the composition of transformations T ∘ S with a non-empty kernel is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9447314359024984
      },
      {
        "text": "The Linear Independence of the Image of a Linear Transformation under a Composition of Linear Transformations with a Non-Empty Row Space: If a set of vectors is linearly independent under a linear transformation T, then the image of the set under the composition of transformations T ∘ S with a non-empty row space is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.96007627786431
      }
    ]
  },
  {
    "representative_text": "The Dimension of the Intersection of Two Subspaces: The dimension of the intersection of two subspaces is equal to the dimension of the smaller of the two subspaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Dimension of the Intersection of Two Subspaces: The dimension of the intersection of two subspaces is equal to the dimension of the smaller of the two subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Dimension of the Intersection of Two Subspaces with Different Dimensions and Non-Empty Null Spaces: The dimension of the intersection of two subspaces with different dimensions and non-empty null spaces is equal to the dimension of the smaller of the two subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9183061874294871
      }
    ]
  },
  {
    "representative_text": "The Linear Independence of the Span of a Linear Combination of Vectors: The span of a linear combination of vectors is linearly independent if and only if the linear combination is non-zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Linear Independence of the Span of a Linear Combination of Vectors: The span of a linear combination of vectors is linearly independent if and only if the linear combination is non-zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of a Linear Combination of Vectors and Linear Independence: We need to explore the relationship between the span of a linear combination of vectors and linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8795262267024386
      }
    ]
  },
  {
    "representative_text": "The Dimension of the Span of a Linear Combination of Vectors: The dimension of the span of a linear combination of vectors is equal to the number of non-zero coefficients in the linear combination.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Dimension of the Span of a Linear Combination of Vectors: The dimension of the span of a linear combination of vectors is equal to the number of non-zero coefficients in the linear combination.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Linear Independence of the Image of a Linear Transformation under a Scalar Multiple: If a set of vectors is linearly independent under a linear transformation T, then the image of the set under a scalar multiple of T is also linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Independence of the Image of a Linear Transformation under a Scalar Multiple: If a set of vectors is linearly independent under a linear transformation T, then the image of the set under a scalar multiple of T is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Dimension of the Span of a Vector Space with a Non-Empty Null Space: The dimension of the span of a vector space with a non-empty null space is equal to the number of linearly independent vectors in the null space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "The Dimension of the Span of a Vector Space with a Non-Empty Null Space: The dimension of the span of a vector space with a non-empty null space is equal to the number of linearly independent vectors in the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Dimension of the Span of a Vector Space with a Non-Empty Kernel: The dimension of the span of a vector space with a non-empty kernel is equal to the number of linearly independent vectors in the kernel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9220609023895323
      },
      {
        "text": "The Dimension of the Span of a Vector Space with a Non-Empty Row Space: The dimension of the span of a vector space with a non-empty row space is equal to the number of linearly independent vectors in the row space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9067877098810482
      },
      {
        "text": "The Dimension of the Span of a Vector Space with a Non-Empty Kernel and Non-Empty Row Space: The dimension of the span of a vector space with a non-empty kernel and non-empty row space is equal to the number of linearly independent vectors in the row space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.960075697864242
      },
      {
        "text": "The Dimension of the Span of a Vector Space with a Non-Empty Kernel and Non-Empty Row Space and Non-Empty Column Space: The dimension of the span of a vector space with a non-empty kernel and non-empty row space and non-empty column space is equal to the number of linearly independent vectors in the row space or the number of linearly independent vectors in the column space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9499476644810847
      },
      {
        "text": "The Dimension of the Span of a Vector Space with a Non-Empty Kernel and Non-Empty Row Space and Non-Empty Column Space with Different Dimensions: The dimension of the span of a vector space with a non-empty kernel and non-empty row space and non-empty column space with different dimensions is equal to the dimension of the smaller of the two subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9214076963404009
      }
    ]
  },
  {
    "representative_text": "The Linear Independence of the Span of a Set of Vectors under a Linear Transformation: The span of a set of vectors under a linear transformation T is linearly independent if and only if the set of vectors is linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Linear Independence of the Span of a Set of Vectors under a Linear Transformation: The span of a set of vectors under a linear transformation T is linearly independent if and only if the set of vectors is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Independence of the Span of a Set of Vectors under a Linear Transformation with a Non-Empty Kernel: The span of a set of vectors under a linear transformation with a non-empty kernel is linearly independent if and only if the set of vectors is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9238601941483642
      },
      {
        "text": "Linear Independence of a Set of Vectors under a Linear Transformation with a Non-Empty Row Space: The span of a set of vectors under a linear transformation with a non-empty row space is linearly independent if and only if the set of vectors is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9159581386535929
      },
      {
        "text": "The Linear Independence of a Set of Vectors under a Linear Transformation with a Non-Empty Column Space: The span of a set of vectors under a linear transformation with a non-empty column space is linearly independent if and only if the set of vectors is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9658971302982101
      }
    ]
  },
  {
    "representative_text": "The Linear Independence of the Image of a Linear Transformation under a Change of Basis: If a set of vectors is linearly independent under a linear transformation T, then the image of the set under a change of basis is also linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Independence of the Image of a Linear Transformation under a Change of Basis: If a set of vectors is linearly independent under a linear transformation T, then the image of the set under a change of basis is also linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Change of Basis Theorem: If a basis for a vector space is changed, then the span of the new basis is the same as the span of the original basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Change of Basis Theorem: If a basis for a vector space is changed, then the span of the new basis is the same as the span of the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Change of Basis Theorem and Orthogonal Complement: The change of basis theorem states that if a basis for a vector space is changed, then the span of the new basis is the same as the span of the original basis. This theorem also implies that the orthogonal complement of the span of the new basis is equal to the orthogonal complement of the span of the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.871461401791218
      },
      {
        "text": "Change of Basis Theorem and Dimension: The change of basis theorem states that if a basis for a vector space is changed, then the span of the new basis is the same as the span of the span of the spanned space. This theorem also implies that the dimension is equal to find the dimension theorem implies that the dimension of the dimension is not only implies the change of a set of a basis Implies Basis Extension Theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9030947808400579
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Condition Number: If a set of linearly independent vectors is used to define a linear transformation, then the condition number of the transformation is equal to the number of vectors in the set.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Condition Number: If a set of linearly independent vectors is used to define a linear transformation, then the condition number of the transformation is equal to the number of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span Implies Condition Number: If a set of vectors spans a vector space, then the condition number of the linear transformation associated with the set is equal to the number of vectors in the set.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span Implies Condition Number: If a set of vectors spans a vector space, then the condition number of the linear transformation associated with the set is equal to the number of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis Implies Condition Number: If a basis for a vector space is used to define a linear transformation, then the condition number of the transformation is equal to the number of vectors in the basis.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis Implies Condition Number: If a basis for a vector space is used to define a linear transformation, then the condition number of the transformation is equal to the number of vectors in the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dimension Implies Condition Number: If the dimension of a vector space is equal to the number of vectors in a basis for the space, then the condition number of the linear transformation associated with the basis is equal to the dimension of the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension Implies Condition Number: If the dimension of a vector space is equal to the number of vectors in a basis for the space, then the condition number of the linear transformation associated with the basis is equal to the dimension of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Dimension: If a set of linearly independent vectors is used to define a linear transformation, then the dimension of the range of the transformation is equal to the number of vectors in the set.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Dimension: If a set of linearly independent vectors is used to define a linear transformation, then the dimension of the range of the transformation is equal to the number of vectors in the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis Implies Dimension: If a basis for a vector space is used to define a linear transformation, then the dimension of the range of the transformation is equal to the number of vectors in the basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8710107269892808
      },
      {
        "text": "Dimension of the Span of a Linear Transformation's Image: The dimension of the image of a linear transformation is equal to the number of linearly independent vectors in the image.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8466248702750003
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination with Pivoting: A method for finding a basis for a vector space by row reducing a matrix to row echelon form, using pivoting to avoid division by zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination with Pivoting: A method for finding a basis for a vector space by row reducing a matrix to row echelon form, using pivoting to avoid division by zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gaussian Elimination with Pivoting (continued): Gaussian elimination with pivoting is a method for finding a basis for a vector space by row reducing a matrix to row echelon form, using pivoting to avoid division by zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9507966551347853
      },
      {
        "text": "Gaussian Elimination with Pivoting and Determinants: Gaussian elimination with pivoting is a method for finding a basis for a vector space by row reducing a matrix to row echelon form, using pivoting to avoid division by zero. This method can also be used to find the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9151107145009906
      },
      {
        "text": "Gaussian Elimination with Pivoting and Orthogonal Complement: Gaussian elimination with pivoting is a method for finding a basis for a vector space by row reducing a matrix to row echelon form, using pivoting to avoid division by zero. This method can also be used to find the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8992500778959622
      },
      {
        "text": "Gaussian Elimination with Pivoting and Dimension: Gaussian elimination with pivoting is a method for finding a basis for a vector space by row reducing a matrix to row echelon form, using pivoting to avoid division by zero. This method can also be used to find the dimension of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9265351683745517
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Linear Independence of a Set of Vectors with Scalar Multiples: The concept of linear independence can be extended to sets of vectors with scalar multiples, particularly in the context of linear independence and the existence of a basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Linear Independence of a Set of Vectors with Scalar Multiples: The concept of linear independence can be extended to sets of vectors with scalar multiples, particularly in the context of linear independence and the existence of a basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a \"Spanning Set of Linearly Independent Vectors with Respect to a Non-Standard Scalar Multiplication\": A spanning set of linearly independent vectors with respect to a non-standard scalar multiplication is a set of linearly independent vectors that spans a vector space with non-standard scalar multiplication.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The concept of a \"Spanning Set of Linearly Independent Vectors with Respect to a Non-Standard Scalar Multiplication\": A spanning set of linearly independent vectors with respect to a non-standard scalar multiplication is a set of linearly independent vectors that spans a vector space with non-standard scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"Spanning Set of Linearly Independent Vectors with Respect to a Non-Standard Vector Addition\": A spanning set of linearly independent vectors with respect to a non-standard vector addition is a set of linearly independent vectors that spans a vector space with non-standard vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9505008916917406
      },
      {
        "text": "The concept of a \"Spanning Set of Linearly Independent Vectors with Respect to a Non-Standard Vector Addition\": A set of linearly independent vectors that spans a vector space with non-standard vector addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9131328383548458
      },
      {
        "text": "The concept of a \"Spanning Set of Linearly Independent Vectors with Respect to a Non-Standard Scalar Multiplication\": A set of linearly independent vectors that spans a vector space with non-standard scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9608421523461775
      }
    ]
  },
  {
    "representative_text": "The concept of a \"Free Basis Extension\": A free basis extension is a basis for a vector space that can be extended to a larger basis by adding vectors from the original basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "The concept of a \"Free Basis Extension\": A free basis extension is a basis for a vector space that can be extended to a larger basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"Linearly Independent Basis Extension\": A linearly independent basis extension is a basis for a vector space that can be extended to a larger basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9144853474130474
      },
      {
        "text": "The concept of a \"Linearly Independent System of Bases Extension\": A linearly independent system of bases extension is a set of bases for a vector space that can be extended to a larger set of bases by adding vectors from the original set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9122659619901228
      },
      {
        "text": "The concept of a \"Free Basis Extension with Respect to a Non-Standard Scalar Multiplication\": A free basis extension with respect to a non-standard scalar multiplication is a basis for a vector space with non-standard scalar multiplication that can be extended to a larger basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8791716353207559
      },
      {
        "text": "The concept of a \"Free Basis Extension with Respect to a Non-Standard Vector Addition\": A free basis extension with respect to a non-standard vector addition is a basis for a vector space with non-standard vector addition that can be extended to a larger basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9422024590365996
      },
      {
        "text": "The concept of a \"Free Basis Extension with Respect to a Non-Standard Vector Addition\": A basis for a vector space with non-standard vector addition that can be extended to a larger basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9051130104966705
      },
      {
        "text": "The concept of a \"Free Basis Extension for Infinite-Dimensional Spaces with Non-Standard Scalar Multiplication\": A basis for an infinite-dimensional vector space with non-standard scalar multiplication that can be extended to a larger basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8786824455889057
      }
    ]
  },
  {
    "representative_text": "Subspace Spanning Theorem for Infinite-Dimensional Vector Spaces:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Subspace Spanning Theorem for Infinite-Dimensional Vector Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace Spanning Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9098410487136666
      }
    ]
  },
  {
    "representative_text": "Cyclic Spanning Sets:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cyclic Spanning Sets:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Free Basis Implies Basis:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Free Basis Implies Basis:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Null Space of a Linear Transformation with Non-Injective: Theorem: Theorem: Theorem for Infinite-Dimensional Vector Spaces: Theorem: Theorem: Theorem: Theorem for Linear Algebra: Theorem: Theorem for Infinite-Dimensional Vector Spaces: Theorem for Infinite-Dimensional vector spaces: Theorem for Infinite-Dimensional Vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8218915668637619
      },
      {
        "text": "Null Space and Column Space Theorems for Infinite-Dimensional Vector Spaces: The theorems that relate the null space and column space of a linear transformation in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8920716020448254
      },
      {
        "text": "Null Space and Column Space Theorems for Infinite-Dimensional Vector Spaces: The null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9130026032968195
      }
    ]
  },
  {
    "representative_text": "The Dimension of the Vector Space using the Inner Product: The dimension of a vector space can be found using the inner product, which is essential in understanding linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Dimension of the Vector Space using the Inner Product: The dimension of a vector space can be found using the inner product, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Dimension of the Span of a Set using the Inner Product: The dimension of the span of a set of vectors can be found using the inner product, which is essential in understanding linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9528008918400037
      }
    ]
  },
  {
    "representative_text": "Lagrange's Basis Theorem Variations: Exploring variations of Lagrange's basis theorem, such as for non-square matrices or for infinite dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lagrange's Basis Theorem Variations: Exploring variations of Lagrange's basis theorem, such as for non-square matrices or for infinite dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Null Space and Linear Independence for Infinite-Dimensional Vector Spaces: We need to consider the implications of having a non-empty null space on linear independence for infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Null Space and Linear Independence for Infinite-Dimensional Vector Spaces: We need to consider the implications of having a non-empty null space on linear independence for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Null Space and Linear Independence for Infinite-Dimensional Vector Spaces: The implications of having a non-empty null space on linear independence in infinite-dimensional vector spaces are not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8781656256784605
      },
      {
        "text": "Null Space and Linear Independence for Infinite-Dimensional Vector Spaces: Investigate the implications of having a non-empty null space on linear independence in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9479743185883086
      },
      {
        "text": "Null Space and Linear Independence for Infinite-Dimensional Vector Spaces: The null space of a linear transformation is a subspace of the original vector space, and linear independence is preserved in the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8849251664604292
      },
      {
        "text": "Relationship between the dimension of the null space and span for infinite-dimensional vector spaces: The relationship between the dimension of the null space and span in infinite-dimensional vector spaces is not explicitly addressed in Linear Independence for Infinite-Dimensional vector",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8771185772343488
      }
    ]
  },
  {
    "representative_text": "Higher-Order Linear Transformations: In computer graphics and game development, higher-order linear transformations (e.g., 3D rotations and translations) are essential for tasks such as object animation and simulation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Higher-Order Linear Transformations: In computer graphics and game development, higher-order linear transformations (e.g., 3D rotations and translations) are essential for tasks such as object animation and simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Geometric Transformations: Geometric transformations, such as rotations, translations, and scaling, are used in computer graphics and game development to manipulate 3D objects and simulate realistic physics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8303278899161317
      }
    ]
  },
  {
    "representative_text": "Non-Euclidean Geometry: Non-Euclidean geometry, particularly in the context of computer graphics, involves the use of curved spaces and non-orthogonal coordinate systems. This is crucial for tasks such as terrain rendering and atmosphere simulation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Euclidean Geometry: Non-Euclidean geometry, particularly in the context of computer graphics, involves the use of curved spaces and non-orthogonal coordinate systems. This is crucial for tasks such as terrain rendering and atmosphere simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fractal Geometry: Fractal geometry is a mathematical concept that involves the study of geometric patterns that repeat at different scales. In computer graphics, fractal geometry is used to generate realistic terrain and atmosphere.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Fractal Geometry: Fractal geometry is a mathematical concept that involves the study of geometric patterns that repeat at different scales. In computer graphics, fractal geometry is used to generate realistic terrain and atmosphere.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Fractal Geometry and Minkowski Sum: Fractal geometry is a mathematical concept that involves the study of geometric patterns that repeat at different scales. Minkowski sum is a technique used in fractal geometry to construct new shapes from existing shapes. This is useful in computer graphics for tasks such as terrain rendering and atmosphere simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8432034026109736
      },
      {
        "text": "Fractal Geometry and Minkowski Sum for 3D Reconstruction: Fractal geometry and Minkowski sum are used to solve problems involving 3D reconstruction in computer graphics and game development. These methods involve using techniques such as Minkowski sum to solve problems involving 3D reconstruction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8141759530098477
      },
      {
        "text": "Fractal Geometry and Its Applications: Fractal geometry is a mathematical concept that involves the study of geometric patterns that repeat at different scales, and it has many applications in computer graphics and game development.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8855289185503031
      }
    ]
  },
  {
    "representative_text": "Machine Learning and Neural Networks for Graphics Rendering: Machine learning and neural networks are used in computer graphics to simulate realistic physics and collisions, taking into account complex patterns and relationships in the data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Machine Learning and Neural Networks for Graphics Rendering: Machine learning and neural networks are used in computer graphics to simulate realistic physics and collisions, taking into account complex patterns and relationships in the data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Machine Learning and Deep Learning for Graphics Rendering: Machine learning and deep learning are used to solve problems in graphics rendering in computer graphics and game development. These techniques involve using neural networks to solve problems involving graphics rendering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8189283771180403
      }
    ]
  },
  {
    "representative_text": "Computational Physics: Computational physics involves the use of numerical methods and linear algebra to solve systems of partial differential equations and simulate the behavior of complex systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Physics: Computational physics involves the use of numerical methods and linear algebra to solve systems of partial differential equations and simulate the behavior of complex systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Data Compression and Reconstruction: Data compression and reconstruction techniques, such as SVD and eigenvalue decomposition, are used in computer graphics and game development to efficiently store and manipulate large amounts of data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Data Compression and Reconstruction: Data compression and reconstruction techniques, such as SVD and eigenvalue decomposition, are used in computer graphics and game development to efficiently store and manipulate large amounts of data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear System Solving and Least Squares: Linear system solving and least squares techniques, such as the use of linear algebra to solve systems of linear equations and perform least squares regression, are essential for tasks such as physics-based modeling and simulation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear System Solving and Least Squares: Linear system solving and least squares techniques, such as the use of linear algebra to solve systems of linear equations and perform least squares regression, are essential for tasks such as physics-based modeling and simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem of the Best Approximation: This theorem states that for a given linear transformation T and a vector v, there exists a vector u such that ||Tv - v|| is minimized.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem of the Best Approximation: This theorem states that for a given linear transformation T and a vector v, there exists a vector u such that ||Tv - v|| is minimized.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Cauchy-Schwarz Inequality: This inequality states that for all vectors u and v in a vector space, ||u · v||^2 ≤ ||u||^2 ||v||^2.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Cauchy-Schwarz Inequality: This inequality states that for all vectors u and v in a vector space, ||u · v||^2 ≤ ||u||^2 ||v||^2.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Cauchy-Schwarz Inequality for Complex Vector Spaces: This is a version of the Cauchy-Schwarz inequality for complex vector spaces, which states that the absolute value of the inner product of two vectors is less than or equal to the product of their norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8566364033808008
      }
    ]
  },
  {
    "representative_text": "The Computation of Matrix Multiplication: This is a fundamental problem in linear algebra, and various algorithms such as the Strassen's algorithm and the Coppersmith-Winograd algorithm can be used to compute matrix multiplication.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Computation of Matrix Multiplication: This is a fundamental problem in linear algebra, and various algorithms such as the Strassen's algorithm and the Coppersmith-Winograd algorithm can be used to compute matrix multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Linear Transformations: This is a fundamental problem in linear algebra, and various algorithms such as the Gram-Schmidt process and the QR algorithm can be used to compute linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8570283402459037
      },
      {
        "text": "The Computation of Non-Linear Transformations: This is a fundamental problem in linear algebra, and various algorithms such as the Gram-Schmidt process and the QR algorithm can be used to compute non-linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9125907633323005
      }
    ]
  },
  {
    "representative_text": "The Computation of Orthogonal Projections: This is a fundamental problem in linear algebra, and various algorithms such as the Gram-Schmidt process and the QR algorithm can be used to compute orthogonal projections.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Computation of Orthogonal Projections: This is a fundamental problem in linear algebra, and various algorithms such as the Gram-Schmidt process and the QR algorithm can be used to compute orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Computation of Linear Regression: This is a fundamental problem in machine learning, and various algorithms such as the ordinary least squares (OLS) method, the ridge regression method, and the Lasso regression method can be used to compute linear regression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Computation of Linear Regression: This is a fundamental problem in machine learning, and various algorithms such as the ordinary least squares (OLS) method, the ridge regression method, and the Lasso regression method can be used to compute linear regression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Non-Linear Regression: This is a fundamental problem in machine learning, and various algorithms such as the ordinary least squares (OLS) method, the ridge regression method, and the Lasso regression method can be used to compute non-linear regression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9085890370505456
      }
    ]
  },
  {
    "representative_text": "The Computation of Support Vector Machines: This is a fundamental problem in machine learning, and various algorithms such as the soft margin SVM and the hard margin SVM can be used to compute support vector machines.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Computation of Support Vector Machines: This is a fundamental problem in machine learning, and various algorithms such as the soft margin SVM and the hard margin SVM can be used to compute support vector machines.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Computation of Neural Networks: This is a fundamental problem in machine learning, and various algorithms such as the backpropagation algorithm and the stochastic gradient descent algorithm can be used to compute neural networks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "The Computation of Neural Networks: This is a fundamental problem in machine learning, and various algorithms such as the backpropagation algorithm and the stochastic gradient descent algorithm can be used to compute neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Convolutional Neural Networks: This is a fundamental problem in machine learning, and various algorithms such as the convolutional neural network (CNN) and the recurrent neural network (RNN) can be used to compute convolutional neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9106936565942803
      },
      {
        "text": "The Computation of Recurrent Neural Networks: This is a fundamental problem in machine learning, and various algorithms such as the backpropagation algorithm and the stochastic gradient descent algorithm can be used to compute recurrent neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9442423055617479
      },
      {
        "text": "The Computation of Non-Linear Neural Networks: This is a fundamental problem in machine learning, and various algorithms such as the backpropagation algorithm and the stochastic gradient descent algorithm can be used to compute non-linear neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9322207489035422
      },
      {
        "text": "The Computation of Non-Linear Convolutional Neural Networks: This is a fundamental problem in machine learning, and various algorithms such as the convolutional neural network (CNN) and the recurrent neural network (RNN) can be used to compute non-linear convolutional neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9438594977016523
      },
      {
        "text": "The Computation of Linear Algebra for Deep Learning: This is a fundamental problem in machine learning, and various algorithms such as the backpropagation algorithm and the stochastic gradient descent algorithm can be used to compute linear algebra for deep learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8989014549500332
      }
    ]
  },
  {
    "representative_text": "The Computation of Manifold Learning: This is a fundamental problem in machine learning, and various algorithms such as the manifold learning algorithm and the locally linear embedding (LLE) algorithm can be used to compute manifold learning.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Computation of Manifold Learning: This is a fundamental problem in machine learning, and various algorithms such as the manifold learning algorithm and the locally linear embedding (LLE) algorithm can be used to compute manifold learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Frame Theory: Frame theory is a mathematical framework for understanding the properties of frames, which are mathematical objects that can be used to represent signals or functions. Frames have applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Frame Theory: Frame theory is a mathematical framework for understanding the properties of frames, which are mathematical objects that can be used to represent signals or functions. Frames have applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Wavelet Frames: Wavelet frames are a specific type of frame that are used to represent signals or functions in the frequency domain. They have applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Wavelet Frames: Wavelet frames are a specific type of frame that are used to represent signals or functions in the frequency domain. They have applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Thresholding: Singular value thresholding is a mathematical technique that can be used to remove noise from a matrix by setting the singular values below a certain threshold to zero. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Thresholding: Singular value thresholding is a mathematical technique that can be used to remove noise from a matrix by setting the singular values below a certain threshold to zero. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Constrained Optimization: Constrained optimization is a mathematical technique that can be used to find the optimal solution to a problem subject to certain constraints. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Constrained Optimization: Constrained optimization is a mathematical technique that can be used to find the optimal solution to a problem subject to certain constraints. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Variational Methods: Variational methods are a class of mathematical techniques that can be used to find the optimal solution to a problem subject to certain constraints. They have applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8020928903190687
      },
      {
        "text": "Spectral Methods: Spectral methods are a class of mathematical techniques that can be used to solve optimization problems subject to certain constraints. They have applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8243852158020606
      },
      {
        "text": "Geometric Methods: Geometric methods are a class of mathematical techniques that can be used to solve optimization problems subject to certain constraints. They have applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8513995351345884
      },
      {
        "text": "Spectral Methods Variants: Spectral methods are a class of mathematical techniques that can be used to solve optimization problems subject to certain constraints. Variants like the generalized eigenvalue problem, the Sylvester equation, and the eigenvalue decomposition are essential in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8391063259116547
      },
      {
        "text": "Constrained Optimization Variants: Constrained optimization is a mathematical technique that can be used to find the optimal solution to a problem subject to certain constraints. Variants like the Karush-Kuhn-Tucker conditions, the Lagrange multiplier method, and the sequential quadratic programming algorithm are essential in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8570620788869734
      },
      {
        "text": "Variational Methods Variants: Variational methods are a class of mathematical techniques that can be used to find the optimal solution to a problem subject to certain constraints. Variants like the calculus of variations, the Lagrange multiplier method, and the sequential quadratic programming algorithm are essential in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8614564122624324
      },
      {
        "text": "Variational Methods: Variational methods, such as the method of Lagrange multipliers, are used to solve optimization problems in computer graphics and game development. These methods involve finding the minimum or maximum of a function subject to certain constraints.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.819918491970804
      },
      {
        "text": "Spectral Methods: Spectral methods are a class of mathematical techniques used in signal processing and image analysis to solve optimization problems subject to certain constraints. They are used for tasks such as image segmentation and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8836538964062444
      }
    ]
  },
  {
    "representative_text": "Karhunen-Loève Transform: The Karhunen-Loève transform is a mathematical technique that can be used to decompose a signal or function into its principal components. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Karhunen-Loève Transform: The Karhunen-Loève transform is a mathematical technique that can be used to decompose a signal or function into its principal components. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Semi-Definite Programming: Semi-definite programming is a mathematical technique that can be used to solve optimization problems subject to certain constraints. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Semi-Definite Programming: Semi-definite programming is a mathematical technique that can be used to solve optimization problems subject to certain constraints. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Graph Signal Processing: Graph signal processing is a branch of mathematics that deals with the analysis and processing of signals on graphs. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Graph Signal Processing: Graph signal processing is a branch of mathematics that deals with the analysis and processing of signals on graphs. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Graph Signal Processing Variants: Graph signal processing is a branch of mathematics that deals with the analysis and processing of signals on graphs. Variants like the graph Fourier transform, the graph wavelet transform, and the graph convolutional neural network are essential in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8867733747546799
      }
    ]
  },
  {
    "representative_text": "Sparse Representation: Sparse representation is a mathematical technique that can be used to represent a signal or function as a linear combination of a set of basis vectors. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sparse Representation: Sparse representation is a mathematical technique that can be used to represent a signal or function as a linear combination of a set of basis vectors. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dictionary Learning: Dictionary learning is a mathematical technique that can be used to learn the optimal dictionary for representing a signal or function. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dictionary Learning: Dictionary learning is a mathematical technique that can be used to learn the optimal dictionary for representing a signal or function. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-Stationary Signal Processing: Non-stationary signal processing is a branch of mathematics that deals with the analysis and processing of signals that are not stationary. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Stationary Signal Processing: Non-stationary signal processing is a branch of mathematics that deals with the analysis and processing of signals that are not stationary. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Time-Frequency Analysis: Time-frequency analysis is a branch of mathematics that deals with the analysis and processing of signals in both time and frequency domains. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Time-Frequency Analysis: Time-frequency analysis is a branch of mathematics that deals with the analysis and processing of signals in both time and frequency domains. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Time-Frequency Analysis Variants: Time-frequency analysis is a branch of mathematics that deals with the analysis and processing of signals in both time and frequency domains. Variants like the short-time Fourier transform, the continuous wavelet transform, and the time-frequency representation using the Short-Time Fourier Transform are essential in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9017825069668399
      }
    ]
  },
  {
    "representative_text": "Machine Learning with Linear Algebra: Machine learning with linear algebra is a field that deals with the application of linear algebra techniques to machine learning problems. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Machine Learning with Linear Algebra: Machine learning with linear algebra is a field that deals with the application of linear algebra techniques to machine learning problems. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Machine Learning with Linear Algebra Variants: Machine learning with linear algebra is a field that deals with the application of linear algebra techniques to machine learning problems. Variants like the kernel method, the support vector machine (SVM), and the neural network are essential in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9104416165205551
      }
    ]
  },
  {
    "representative_text": "Computational Geometry: Computational geometry is a branch of mathematics that deals with the analysis and processing of geometric objects. It has applications in signal processing, image analysis, and data compression.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Computational Geometry: Computational geometry is a branch of mathematics that deals with the analysis and processing of geometric objects. It has applications in signal processing, image analysis, and data compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Geometry: Computational geometry is a branch of mathematics that deals with the analysis and processing of geometric objects. It is used in signal processing and image analysis for tasks such as image segmentation and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9568887558896628
      }
    ]
  },
  {
    "representative_text": "Hadamard Matrix: A square matrix whose entries are either 1 or -1, used in various applications such as coding theory and optimization.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Hadamard Matrix: A square matrix whose entries are either 1 or -1, used in various applications such as coding theory and optimization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Hadamard Matrix: A square matrix with entries of 1's and -1's on the diagonal and 0's elsewhere, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8783081037495387
      }
    ]
  },
  {
    "representative_text": "Jacobi Iteration: An iterative method for solving systems of linear equations, used in various applications such as optimization and control theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jacobi Iteration: An iterative method for solving systems of linear equations, used in various applications such as optimization and control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Secant Method: An iterative method for solving systems of linear equations, used in various applications such as optimization and control theory.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Secant Method: An iterative method for solving systems of linear equations, used in various applications such as optimization and control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Newton's Method: An iterative method for solving systems of nonlinear equations, used in various applications such as optimization and control theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Newton's Method: An iterative method for solving systems of nonlinear equations, used in various applications such as optimization and control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Broyden-Fletcher-Goldfarb-Shanno (BFGS) Algorithm: An optimization algorithm used in various applications such as control theory and machine learning.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Broyden-Fletcher-Goldfarb-Shanno (BFGS) Algorithm: An optimization algorithm used in various applications such as control theory and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) of a Transfer Function Matrix: A technique used to analyze and design control systems, which involves decomposing a transfer function matrix into a product of three matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) of a Transfer Function Matrix: A technique used to analyze and design control systems, which involves decomposing a transfer function matrix into a product of three matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Phase Margin: A measure of the stability of a system, used in control theory to design and analyze control systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Phase Margin: A measure of the stability of a system, used in control theory to design and analyze control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gain Margin: A measure of the stability of a system, used in control theory to design and analyze control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9453101043094515
      }
    ]
  },
  {
    "representative_text": "Hash function analysis: A deeper analysis of hash functions, including their properties, vulnerabilities, and security guarantees, is essential for understanding their role in cryptography and cybersecurity.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hash function analysis: A deeper analysis of hash functions, including their properties, vulnerabilities, and security guarantees, is essential for understanding their role in cryptography and cybersecurity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cryptographic protocol analysis: A thorough analysis of cryptographic protocols, including their security properties, vulnerabilities, and potential attacks, is crucial for evaluating their effectiveness in ensuring secure communication.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cryptographic protocol analysis: A thorough analysis of cryptographic protocols, including their security properties, vulnerabilities, and potential attacks, is crucial for evaluating their effectiveness in ensuring secure communication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Post-quantum cryptography with lattice-based and code-based cryptographic schemes: Evaluating the security and performance of post-quantum cryptographic schemes, including lattice-based and code-based cryptographic schemes, is essential for ensuring the long-term security of cryptographic systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Post-quantum cryptography with lattice-based and code-based cryptographic schemes: Evaluating the security and performance of post-quantum cryptographic schemes, including lattice-based and code-based cryptographic schemes, is essential for ensuring the long-term security of cryptographic systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Post-quantum cryptographic schemes with lattice-based and code-based cryptography: Cryptographic schemes that use lattice-based and code-based cryptography to provide resistance to quantum computer attacks, ensuring long-term security.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8805268926122305
      },
      {
        "text": "Post-Quantum Cryptography with Lattice-Based and Code-Based Cryptographic Schemes: This includes evaluating the security and performance of post-quantum cryptographic schemes, including lattice-based and code-based cryptographic schemes, for ensuring long-term security.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9466077933090772
      },
      {
        "text": "Post-Quantum Cryptography with Multivariate Cryptographic Schemes: This includes evaluating the security and performance of post-quantum cryptographic schemes using multivariate cryptographic schemes, such as lattice-based and code-based cryptographic schemes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.877963200605044
      },
      {
        "text": "Post-quantum cryptography with multivariate cryptographic schemes: Evaluating the security and performance of post-quantum cryptographic schemes using multivariate cryptographic schemes, such as lattice-based and code-based cryptographic schemes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9114103435835926
      },
      {
        "text": "Post-quantum cryptography with multivariate cryptographic schemes: This includes the evaluation of post-quantum cryptography with multivariate cryptographic schemes, such as lattice-based and code-based cryptographic schemes, for ensuring long-term security.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9460777793756789
      }
    ]
  },
  {
    "representative_text": "Theorem on the Existence of an Orthonormal Basis: This theorem states that there exists an orthonormal basis for a vector space V if and only if the space is finite-dimensional.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem on the Existence of an Orthonormal Basis: This theorem states that there exists an orthonormal basis for a vector space V if and only if the space is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quaternions and Vector Spaces: Quaternions are a type of mathematical object that can be used to represent rotations and other transformations. Quaternion vector spaces are vector spaces with a scalar product that satisfy certain properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quaternions and Vector Spaces: Quaternions are a type of mathematical object that can be used to represent rotations and other transformations. Quaternion vector spaces are vector spaces with a scalar product that satisfy certain properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kaluza-Klein Theory and Vector Spaces: Kaluza-Klein theory is a theoretical framework for unifying general relativity and quantum mechanics. Vector spaces play a crucial role in this theory, particularly in the representation of gauge fields and other mathematical objects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kaluza-Klein Theory and Vector Spaces: Kaluza-Klein theory is a theoretical framework for unifying general relativity and quantum mechanics. Vector spaces play a crucial role in this theory, particularly in the representation of gauge fields and other mathematical objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Minkowski Sum and Minkowski Space: Minkowski space is a vector space with a scalar product that satisfies certain properties, such as linearity and positive definiteness. The Minkowski sum is a way of combining two vector spaces by adding corresponding components.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minkowski Sum and Minkowski Space: Minkowski space is a vector space with a scalar product that satisfies certain properties, such as linearity and positive definiteness. The Minkowski sum is a way of combining two vector spaces by adding corresponding components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Metric Tensor and Riemannian Geometry: The metric tensor is a mathematical object that can be used to represent the metric properties of a vector space. Riemannian geometry is a branch of mathematics that deals with the study of Riemannian metrics and other mathematical objects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Metric Tensor and Riemannian Geometry: The metric tensor is a mathematical object that can be used to represent the metric properties of a vector space. Riemannian geometry is a branch of mathematics that deals with the study of Riemannian metrics and other mathematical objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Connection and Lie Group: A connection is a mathematical object that can be used to represent the connection properties of a vector space. Lie groups are a type of mathematical object that can be used to represent the symmetries of a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Connection and Lie Group: A connection is a mathematical object that can be used to represent the connection properties of a vector space. Lie groups are a type of mathematical object that can be used to represent the symmetries of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Householder Transformations: Householder transformations are a type of orthogonal matrix that can be used to diagonalize matrices. They are named after the mathematician Alfred Haar, who introduced them in the 1930s.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Householder Transformations: Householder transformations are a type of orthogonal matrix that can be used to diagonalize matrices. They are named after the mathematician Alfred Haar, who introduced them in the 1930s.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Householder Transformations: Householder transformations are a type of orthogonal matrix that can be used to diagonalize matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9132918260305389
      },
      {
        "text": "The Householder Transformations: The Householder transformations are a set of linear transformations used to orthogonalize a matrix. These transformations are essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8639351356456821
      },
      {
        "text": "The Householder Reflections: These are a type of linear transformation used to orthogonalize a matrix, and are used in the Householder method for solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8861344259740569
      }
    ]
  },
  {
    "representative_text": "Fredholm Operators: Fredholm operators are a type of linear transformation that can be used to study the properties of linear operators. They are named after the mathematician Erik Fredholm, who introduced them in the early 20th century.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Fredholm Operators: Fredholm operators are a type of linear transformation that can be used to study the properties of linear operators. They are named after the mathematician Erik Fredholm, who introduced them in the early 20th century.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Fredholm Operators: Fredholm operators are a type of linear transformation that can be used to study the properties of linear operators.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9459209778551192
      }
    ]
  },
  {
    "representative_text": "Hilbert-Schmidt Operators: Hilbert-Schmidt operators are a type of linear transformation that can be used to study the properties of linear operators. They are named after the mathematicians David Hilbert and Erhard Schmidt, who introduced them in the early 20th century.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Hilbert-Schmidt Operators: Hilbert-Schmidt operators are a type of linear transformation that can be used to study the properties of linear operators. They are named after the mathematicians David Hilbert and Erhard Schmidt, who introduced them in the early 20th century.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Hilbert-Schmidt Operators: Hilbert-Schmidt operators are a type of linear transformation that can be used to study the properties of linear operators.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9373449420717632
      }
    ]
  },
  {
    "representative_text": "Spectral Theorem for Normal Matrices: The spectral theorem for normal matrices is a theorem that states that a normal matrix can be diagonalized using a unitary matrix. Normal matrices are matrices that commute with their adjoint.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Spectral Theorem for Normal Matrices: The spectral theorem for normal matrices is a theorem that states that a normal matrix can be diagonalized using a unitary matrix. Normal matrices are matrices that commute with their adjoint.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Spectral Theorem for Normal Matrices: The spectral theorem for normal matrices states that a normal matrix can be diagonalized using a unitary matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9413302922069215
      },
      {
        "text": "Spectral Theorem for Orthogonal Matrices: The spectral theorem for orthogonal matrices states that an orthogonal matrix can be orthogonally diagonalized if and only if it is a normal matrix, which is a matrix that commutes with its conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8642265079945475
      },
      {
        "text": "The Spectral Theorem for Normal Matrices: The theorem that states that a normal matrix can be diagonalized using a unitary matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9241583752053115
      },
      {
        "text": "The Spectral Theorem for Normal Operators: This theorem states that a normal operator on a Hilbert space can be diagonalized using an orthonormal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8454442783424007
      },
      {
        "text": "Orthogonal Matrix Spectral Theorem: A theorem that states that a square matrix can be orthogonally diagonalized if and only if it is a normal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9157239683582774
      }
    ]
  },
  {
    "representative_text": "Spectral Theorem for Hermitian Matrices: The spectral theorem for Hermitian matrices is a theorem that states that a Hermitian matrix can be diagonalized using an orthogonal matrix. Hermitian matrices are matrices that are equal to their own adjoint.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Spectral Theorem for Hermitian Matrices: The spectral theorem for Hermitian matrices is a theorem that states that a Hermitian matrix can be diagonalized using an orthogonal matrix. Hermitian matrices are matrices that are equal to their own adjoint.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Spectral Theorem for Hermitian Matrices: The spectral theorem for Hermitian matrices states that a Hermitian matrix can be diagonalized using an orthogonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9420771089194235
      }
    ]
  },
  {
    "representative_text": "Matrix Exponential: The matrix exponential is a function that can be used to compute the exponential of a matrix. It is defined as e^A = ∑[n=0 to ∞] (A^n / n!), where A is a square matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Matrix Exponential: The matrix exponential is a function that can be used to compute the exponential of a matrix. It is defined as e^A = ∑[n=0 to ∞] (A^n / n!), where A is a square matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Exponential: The matrix exponential is a function that can be used to compute the exponential of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9464809766175476
      }
    ]
  },
  {
    "representative_text": "Calculus of Variations: The calculus of variations is a branch of mathematics that deals with the optimization of functions subject to certain constraints. It is a fundamental tool in physics and engineering.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Calculus of Variations: The calculus of variations is a branch of mathematics that deals with the optimization of functions subject to certain constraints. It is a fundamental tool in physics and engineering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Calculus of Variations: The calculus of variations is a branch of mathematics that deals with the optimization of functions subject to certain constraints.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9550962458987602
      },
      {
        "text": "The Calculus of Variations: The optimization of functions subject to certain constraints, which is a fundamental tool in physics and engineering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9066594972735464
      }
    ]
  },
  {
    "representative_text": "Euler-Lagrange Equations: The Euler-Lagrange equations are a set of differential equations that can be used to solve optimization problems. They are named after the mathematician Leonhard Euler.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Euler-Lagrange Equations: The Euler-Lagrange equations are a set of differential equations that can be used to solve optimization problems. They are named after the mathematician Leonhard Euler.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Euler-Lagrange Equations: The Euler-Lagrange equations are a set of differential equations that can be used to solve optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9454869011155744
      },
      {
        "text": "The Euler-Lagrange Equations: The set of differential equations that can be used to solve optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9096645553657206
      }
    ]
  },
  {
    "representative_text": "Navier-Stokes Equations: The Navier-Stokes equations are a set of nonlinear partial differential equations that describe the motion of fluids. They are named after the mathematicians Claude-Louis Navier and George Gabriel Stokes.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Navier-Stokes Equations: The Navier-Stokes equations are a set of nonlinear partial differential equations that describe the motion of fluids. They are named after the mathematicians Claude-Louis Navier and George Gabriel Stokes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Navier-Stokes Equations: The set of nonlinear partial differential equations that describe the motion of fluids.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9166079600871027
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix square root and its relation to the orthogonal decomposition: The matrix square root of a matrix is a matrix that, when squared, gives the original matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix square root and its relation to the orthogonal decomposition: The matrix square root of a matrix is a matrix that, when squared, gives the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix logarithm and its relation to the orthogonal decomposition: The matrix logarithm of a matrix is a matrix that, when exponentiated, gives the original matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix logarithm and its relation to the orthogonal decomposition: The matrix logarithm of a matrix is a matrix that, when exponentiated, gives the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix exponential and its relation to the orthogonal decomposition: The matrix exponential of a matrix is a matrix that can be used to solve certain types of differential equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix exponential and its relation to the orthogonal decomposition: The matrix exponential of a matrix is a matrix that can be used to solve certain types of differential equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix singular value decomposition and its relation to the orthogonal decomposition: The matrix singular value decomposition of a matrix is a factorization of the matrix into its singular values and singular vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix singular value decomposition and its relation to the orthogonal decomposition: The matrix singular value decomposition of a matrix is a factorization of the matrix into its singular values and singular vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix Hankel matrix and its relation to the orthogonal decomposition: The matrix Hankel matrix is a matrix that has a specific structure that can be used to solve certain types of problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix Hankel matrix and its relation to the orthogonal decomposition: The matrix Hankel matrix is a matrix that has a specific structure that can be used to solve certain types of problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix Toeplitz matrix and its relation to the orthogonal decomposition: The matrix Toeplitz matrix is a matrix that has a specific structure that can be used to solve certain types of problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The concept of a matrix Toeplitz matrix and its relation to the orthogonal decomposition: The matrix Toeplitz matrix is a matrix that has a specific structure that can be used to solve certain types of problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a matrix Toeplitz-Hankel matrix and its relation to the orthogonal decomposition: The matrix Toeplitz-Hankel matrix is a matrix that has a specific structure that can be used to solve certain types of problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9587327656071629
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix circulant matrix and its relation to the orthogonal decomposition: The matrix circulant matrix is a matrix that has a specific structure that can be used to solve certain types of problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix circulant matrix and its relation to the orthogonal decomposition: The matrix circulant matrix is a matrix that has a specific structure that can be used to solve certain types of problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix banded matrix and its relation to the orthogonal decomposition: The matrix banded matrix is a matrix that has a specific structure that can be used to solve certain types of problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix banded matrix and its relation to the orthogonal decomposition: The matrix banded matrix is a matrix that has a specific structure that can be used to solve certain types of problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix Hermitian matrix and its relation to the orthogonal decomposition: The matrix Hermitian matrix is a matrix that satisfies a specific condition when multiplied by its conjugate transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The concept of a matrix Hermitian matrix and its relation to the orthogonal decomposition: The matrix Hermitian matrix is a matrix that satisfies a specific condition when multiplied by its conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a matrix skew-Hermitian matrix and its relation to the orthogonal decomposition: The matrix skew-Hermitian matrix is a matrix that satisfies a specific condition when multiplied by its conjugate transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9158544684165986
      },
      {
        "text": "The concept of a matrix skew-symmetric matrix and its relation to the orthogonal decomposition: The matrix skew-symmetric matrix is a matrix that satisfies a specific condition when multiplied by its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.901499832868633
      },
      {
        "text": "The concept of a matrix symmetric matrix and its relation to the orthogonal decomposition: The matrix symmetric matrix is a matrix that satisfies a specific condition when multiplied by its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9187717067680197
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix positive semi-definite matrix and its relation to the orthogonal decomposition: The matrix positive semi-definite matrix is a matrix that satisfies a specific condition when multiplied by its transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The concept of a matrix positive semi-definite matrix and its relation to the orthogonal decomposition: The matrix positive semi-definite matrix is a matrix that satisfies a specific condition when multiplied by its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a matrix negative semi-definite matrix and its relation to the orthogonal decomposition: The matrix negative semi-definite matrix is a matrix that satisfies a specific condition when multiplied by its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9241566960565962
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix eigenvalue perturbation and its relation to the orthogonal decomposition: The matrix eigenvalue perturbation is a method for approximating the eigenvalues of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The concept of a matrix eigenvalue perturbation and its relation to the orthogonal decomposition: The matrix eigenvalue perturbation is a method for approximating the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a matrix eigenvector perturbation and its relation to the orthogonal decomposition: The matrix eigenvector perturbation is a method for approximating the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9310854041895069
      },
      {
        "text": "The concept of a matrix eigenvalue decomposition with eigenvalue shifts and its relation to the orthogonal decomposition: The matrix eigenvalue decomposition with eigenvalue shifts is a method for approximating the eigenvalues of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8664430677153283
      },
      {
        "text": "The concept of a matrix eigenvector decomposition with eigenvector shifts and its relation to the orthogonal decomposition: The matrix eigenvector decomposition with eigenvector shifts is a method for approximating the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9114820190226726
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix orthogonal decomposition with orthogonal projection and its relation to the orthogonal decomposition: The matrix orthogonal decomposition with orthogonal projection is a method for approximating the orthogonal decomposition of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The concept of a matrix orthogonal decomposition with orthogonal projection and its relation to the orthogonal decomposition: The matrix orthogonal decomposition with orthogonal projection is a method for approximating the orthogonal decomposition of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a matrix singular value decomposition with singular value decomposition and its relation to the orthogonal decomposition: The matrix singular value decomposition with singular value decomposition is a method for approximating the singular value decomposition of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8729577032981353
      },
      {
        "text": "The Orthogonal Decomposition of a Matrix using Singular Value Decomposition: This is a method for decomposing a matrix into the product of three matrices: an orthogonal matrix, a diagonal matrix, and a diagonal matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8528456470365784
      }
    ]
  },
  {
    "representative_text": "Existence of Infinite Orthogonal Sets: Some vector spaces have infinite orthogonal sets, which can be used to construct orthonormal bases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Existence of Infinite Orthogonal Sets: Some vector spaces have infinite orthogonal sets, which can be used to construct orthonormal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of Infinite Orthogonal Sets on Infinite-Dimensional Vector Spaces: Infinite-dimensional vector spaces may have infinite orthogonal sets, which can be used to construct orthonormal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.948091580003761
      }
    ]
  },
  {
    "representative_text": "Closure under Linear Transformations for Infinite-Dimensional Vector Spaces: The closure under linear transformations for infinite-dimensional vector spaces is an important concept that is not explicitly mentioned in the provided knowledge points.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Closure under Linear Transformations for Infinite-Dimensional Vector Spaces: The closure under linear transformations for infinite-dimensional vector spaces is an important concept that is not explicitly mentioned in the provided knowledge points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Closure under Linear Transformations for Infinite-Dimensional Vector Spaces with Infinite-Dimensional Kernel: The closure under linear transformations for infinite-dimensional vector spaces with an infinite-dimensional kernel may not be a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8489129229809926
      }
    ]
  },
  {
    "representative_text": "The Identity Theorem: This theorem states that if two linear operators $T$ and $S$ between two vector spaces $V$ and $W$ are equal on a basis of $V$, then they are equal on all of $V$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Identity Theorem: This theorem states that if two linear operators $T$ and $S$ between two vector spaces $V$ and $W$ are equal on a basis of $V$, then they are equal on all of $V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Column Space of a Matrix: This concept is related to the rank-nullity theorem and states that the column space of a matrix is equal to the image of the linear transformation represented by the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Column Space of a Matrix: This concept is related to the rank-nullity theorem and states that the column space of a matrix is equal to the image of the linear transformation represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Singular Value Decomposition (SVD): This concept is related to the polar decomposition and states that the SVD is a factorization of a matrix into three matrices (U, Σ, V^T) that are used to decompose linear transformations into their rank-one components.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Singular Value Decomposition (SVD): This concept is related to the polar decomposition and states that the SVD is a factorization of a matrix into three matrices (U, Σ, V^T) that are used to decompose linear transformations into their rank-one components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Dual Space and Linear Functionals: This concept is related to the orthogonal projections and states that the dual space is the set of all linear functionals from a vector space to the field of scalars.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Dual Space and Linear Functionals: This concept is related to the orthogonal projections and states that the dual space is the set of all linear functionals from a vector space to the field of scalars.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Invertibility of Linear Transformations: This concept is related to the dual space and linear functionals and states that a linear transformation is invertible if and only if it is both injective and surjective.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "The Invertibility of Linear Transformations: This concept is related to the dual space and linear functionals and states that a linear transformation is invertible if and only if it is both injective and surjective.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Null Space and Column Space: This concept is related to the invertibility of linear transformations and states that the null space and column space of a matrix are related to the invertibility of the linear transformation represented by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8681611659572237
      },
      {
        "text": "The Determinant of a Linear Transformation: This concept is related to the invertibility of linear transformations and states that the determinant of a linear transformation can be used to determine its invertibility and the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8854247611208579
      },
      {
        "text": "Linear Transformation and Invertibility: The relationship between linear transformations and invertibility, including the concept of invertible linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8480589467899543
      },
      {
        "text": "The Linear Operator Determinant and Its Properties: This concept is related to the invertibility of linear operators and involves understanding the properties of the linear operator determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8378157182803874
      },
      {
        "text": "The Linear Operator Invertibility Criterion: This criterion is related to the invertibility of linear operators and involves understanding the properties of the linear operator.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8613646575500679
      },
      {
        "text": "Linear Transformations and the Inverse of a Linear Transformation: The inverse of a linear transformation is a linear transformation that \"reverses\" the original transformation. Understanding the properties of the inverse can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8182971120071483
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Separability: The relationship between linear transformations and separability, including the concept of separable linear transformations and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Separability: The relationship between linear transformations and separability, including the concept of separable linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Nilpotency: The relationship between linear transformations and nilpotency, including the concept of nilpotent linear transformations and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Nilpotency: The relationship between linear transformations and nilpotency, including the concept of nilpotent linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and the Nilpotency of a Linear Transformation: The nilpotency of a linear transformation is a measure of the transformation's \"infinite\" behavior. Understanding the properties of the nilpotency can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8759989812618805
      },
      {
        "text": "Linear Transformations and the Quasi-Nilpotency of a Linear Transformation: The quasi-nilpotency of a linear transformation is a measure of the transformation's \"almost infinite\" behavior. Understanding the properties of the quasi-nilpotency can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9028779223883083
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Quasi-Nilpotency: The relationship between linear transformations and quasi-nilpotency, including the concept of quasi-nilpotent linear transformations and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Quasi-Nilpotency: The relationship between linear transformations and quasi-nilpotency, including the concept of quasi-nilpotent linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Cyclic Decomposition: The relationship between linear transformations and cyclic decomposition, including the concept of cyclic decompositions and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Cyclic Decomposition: The relationship between linear transformations and cyclic decomposition, including the concept of cyclic decompositions and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Decomposition: The relationship between linear transformations and decomposition, including the concept of decompositions and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8787168239502774
      },
      {
        "text": "Linear Transformations and the Cyclic Decomposition of a Linear Transformation: The cyclic decomposition of a linear transformation is a factorization of the transformation into simpler components. Understanding the properties of the cyclic decomposition can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8688014422408245
      },
      {
        "text": "Linear Transformations and the Decomposition of a Linear Transformation: The decomposition of a linear transformation is a factorization of the transformation into simpler components. Understanding the properties of the decomposition can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8755029179015574
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Spectral Mapping Theorem (Alternative): Alternative representations of the spectral mapping theorem, such as the Jordan decomposition and its relationship to the spectral mapping theorem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Spectral Mapping Theorem (Alternative): Alternative representations of the spectral mapping theorem, such as the Jordan decomposition and its relationship to the spectral mapping theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Invariant Subspaces: The relationship between linear transformations and invariant subspaces, including the concept of invariant subspaces and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Invariant Subspaces: The relationship between linear transformations and invariant subspaces, including the concept of invariant subspaces and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and Invariant Subspaces: The study of linear transformations in the context of invariant subspaces, including the use of invariant subspaces to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9190744198295442
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Homomorphisms: The relationship between linear transformations and homomorphisms, including the concept of homomorphisms and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Homomorphisms: The relationship between linear transformations and homomorphisms, including the concept of homomorphisms and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Homotopies: The relationship between linear transformations and homotopies, including the concept of homotopies and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Homotopies: The relationship between linear transformations and homotopies, including the concept of homotopies and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Dimension Theory: The relationship between linear transformations and dimension theory, including the concept of dimension and its properties in the context of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Dimension Theory: The relationship between linear transformations and dimension theory, including the concept of dimension and its properties in the context of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and the Dimension Theory of a Linear Transformation: The dimension theory of a linear transformation is a study of the transformation's \"size\" and \"shape\". Understanding the properties of the dimension theory can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9023988176350504
      },
      {
        "text": "Linear Transformation and Dimension Theory: The dimension theory of a linear transformation is a study of the transformation's \"size\" and \"shape\". Understanding the properties of the dimension theory can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9303761038018626
      }
    ]
  },
  {
    "representative_text": "Rank and Nullity for Linear Transformations with Degenerate Image or Kernel in Fractal Spaces: The concept of analyzing the rank and nullity of linear transformations when the image or kernel is degenerate in fractal spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Rank and Nullity for Linear Transformations with Degenerate Image or Kernel in Fractal Spaces: The concept of analyzing the rank and nullity of linear transformations when the image or kernel is degenerate in fractal spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rank and Nullity for Linear Transformations with Degenerate Image or Kernel in Topological Spaces: The concept of analyzing the rank and nullity of linear transformations when the image or kernel is degenerate in topological spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.908558750953822
      },
      {
        "text": "Rank and Nullity of Linear Transformations on Fractal Spaces: The concept of analyzing the rank and nullity of linear transformations when the image or kernel is degenerate in fractal spaces, which is essential in understanding the properties of linear transformations in non-standard spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.899280412443251
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Quotient Space: The relationship between linear transformations and quotient spaces can be studied using determinants. Specifically, the determinant of the quotient space matrix can be used to determine the behavior of the linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Quotient Space: The relationship between linear transformations and quotient spaces can be studied using determinants. Specifically, the determinant of the quotient space matrix can be used to determine the behavior of the linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and the Quotient Space of a Linear Transformation: The quotient space of a linear transformation is a subspace obtained by identifying certain vectors as equivalent. Understanding the properties of the quotient space can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8029374981762695
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Null Space Rank: The rank of the null space of a linear transformation can be determined using determinants. Specifically, the determinant of the null space matrix can be used to determine the rank of the null space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Null Space Rank: The rank of the null space of a linear transformation can be determined using determinants. Specifically, the determinant of the null space matrix can be used to determine the rank of the null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Rank-Deficient Columns or Rows: The rank of the columns or rows of a matrix can be determined using determinants. Specifically, the determinant of the columns or rows matrix can be used to determine the rank of the columns or rows.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8409986141291923
      }
    ]
  },
  {
    "representative_text": "Matrix Inversion using the Neumann Series: This involves the study of the Neumann series, which is a method for inverting a matrix using a power series expansion.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Matrix Inversion using the Neumann Series: This involves the study of the Neumann series, which is a method for inverting a matrix using a power series expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inversion using the Neumann Series with Shift and Invert and Non-Real Entries: This involves the study of the Neumann series with shift and invert and non-real entries, which is a method for inverting a matrix using a power series expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.927511322311718
      },
      {
        "text": "Matrix Inversion using Non-Standard Methods: The study of non-standard methods for inverting a matrix, such as the Neumann series with shift and invert and non-real entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.897813809585353
      },
      {
        "text": "Matrix Inversion using Non-Standard Methods with Complex Entries: The study of non-standard methods for inverting a matrix with complex entries, such as the Neumann series with shift and invert and non-real entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9196562270505395
      },
      {
        "text": "Matrix Inversion using the Neumann Series with Non-Real Entries: This involves the study of the Neumann series for inverting a matrix with non-real entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9533593308688232
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix using the Pfaffian: This involves the study of the Pfaffian, which is a method for calculating the determinant of a skew-symmetric matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix using the Pfaffian: This involves the study of the Pfaffian, which is a method for calculating the determinant of a skew-symmetric matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix using the Pfaffian with Non-Symmetric Matrix: This involves the study of the Pfaffian with non-symmetric matrix, which is a generalization of the Pfaffian for skew-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9345797954929871
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) of a Matrix with Non-Positive Singular Values: This involves the study of the SVD of a matrix with non-positive singular values, which is a generalization of the SVD of a real matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix with Non-Positive Singular Values: This involves the study of the SVD of a matrix with non-positive singular values, which is a generalization of the SVD of a real matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix with Non-Orthogonal Singular Values and Complex Entries: This involves the study of the SVD of a matrix with non-orthogonal singular values and complex entries, which is a generalization of the SVD of a real matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9432921960817509
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix with Non-Orthogonal Singular Values and Complex Entries: The study of the SVD of a matrix with non-orthogonal singular values and complex entries, which is a generalization of the SVD of a real matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9059553695074232
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix with Complex Singular Values: The study of the SVD of a matrix with complex singular values, which is a generalization of the SVD of a real matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.915596982953707
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Matrix with Complex Eigenvalues and Orthogonal Projections: The SVD can be generalized to complex matrices, and its properties can be investigated in the context of orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9074786598253792
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Interpolation using Numerical Methods: This involves the study of numerical methods for interpolating eigenvalues, such as the power method and the Lanczos method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Interpolation using Numerical Methods: This involves the study of numerical methods for interpolating eigenvalues, such as the power method and the Lanczos method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix using the Coppersmith-Winograd Algorithm: This involves the study of the Coppersmith-Winograd algorithm, which is a method for calculating the determinant of a matrix using a divide-and-conquer approach.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix using the Coppersmith-Winograd Algorithm: This involves the study of the Coppersmith-Winograd algorithm, which is a method for calculating the determinant of a matrix using a divide-and-conquer approach.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix using the Strassen Algorithm: This involves the study of the Strassen algorithm, which is a method for calculating the determinant of a matrix using a divide-and-conquer approach.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9465861111250711
      },
      {
        "text": "Determinant of a Matrix using the Coppersmith-Winograd Algorithm with Non-Integer Powers: This involves the study of the Coppersmith-Winograd algorithm with non-integer powers, which is a generalization of the Coppersmith-Winograd algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8687354334954189
      },
      {
        "text": "Determinant of a Matrix using Non-Standard Algorithms: The study of non-standard algorithms for calculating the determinant of a matrix, such as the Strassen algorithm with non-integer powers and complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8966827027826536
      },
      {
        "text": "Determinant of a Matrix using Non-Standard Algorithms with Complex Entries: This involves the study of non-standard algorithms for calculating the determinant of a matrix with complex entries, such as the Strassen algorithm with non-integer powers and complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9071304346041151
      }
    ]
  },
  {
    "representative_text": "The Inverse of a Matrix with Non-Unitary Eigenvectors: Understanding the relationship between the inverse of a matrix and its eigenvalues, particularly when the eigenvectors are not unitary, is essential in understanding the behavior of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Inverse of a Matrix with Non-Unitary Eigenvectors: Understanding the relationship between the inverse of a matrix and its eigenvalues, particularly when the eigenvectors are not unitary, is essential in understanding the behavior of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inversion and Orthogonality: Understanding the relationship between matrix inversion and orthogonality, particularly in the context of eigenvectors and eigenvalues, is crucial in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.884905222586474
      },
      {
        "text": "The Inverse of a Matrix with Non-Unitary Eigenvectors: This topic deals with the properties of the inverse of a matrix with non-unitary eigenvectors, including the relationship between the eigenvalues and the inverse.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.889676377887167
      }
    ]
  },
  {
    "representative_text": "The Connection between Eigenvalues and the Minimal Polynomial: The relationship between eigenvalues and the minimal polynomial of a matrix is an important concept in understanding the properties of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Connection between Eigenvalues and the Minimal Polynomial: The relationship between eigenvalues and the minimal polynomial of a matrix is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Use of Givens Rotations in Numerical Computations: Givens rotations are a technique used to eliminate elements of a matrix in numerical computations, and understanding their application is essential in ensuring the accuracy of numerical computations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Use of Givens Rotations in Numerical Computations: Givens rotations are a technique used to eliminate elements of a matrix in numerical computations, and understanding their application is essential in ensuring the accuracy of numerical computations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Connection between Matrix Inversion and Transpose: The relationship between the inverse of a matrix and its transpose is an important concept in understanding the properties of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Connection between Matrix Inversion and Transpose: The relationship between the inverse of a matrix and its transpose is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Use of Orthogonality in Numerical Computations: Understanding the properties of orthogonal matrices and their application in numerical computations is essential in ensuring the accuracy of numerical computations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Use of Orthogonality in Numerical Computations: Understanding the properties of orthogonal matrices and their application in numerical computations is essential in ensuring the accuracy of numerical computations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Condition Number of Matrix Inversions: Explore the condition number of matrix inversions, which can be used to estimate the sensitivity of the solution to small changes in the input matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Condition Number of Matrix Inversions: Explore the condition number of matrix inversions, which can be used to estimate the sensitivity of the solution to small changes in the input matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-Standard Matrix Inversion Methods: Introduce alternative matrix inversion methods, such as the QR decomposition, the Cholesky decomposition, and the SVD.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Non-Standard Matrix Inversion Methods: Introduce alternative matrix inversion methods, such as the QR decomposition, the Cholesky decomposition, and the SVD.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Specialized Matrix Inversion Methods: Explore specialized matrix inversion methods, such as Cholesky decomposition, QR decomposition, and SVD, which are designed to handle specific types of matrices and provide efficient and accurate solutions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8800567258146328
      }
    ]
  },
  {
    "representative_text": "Computational Complexity of Matrix Inversion Methods: Analyze the computational complexity of different matrix inversion methods, including the time and space requirements of each method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Complexity of Matrix Inversion Methods: Analyze the computational complexity of different matrix inversion methods, including the time and space requirements of each method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Parallelization of Matrix Inversion Methods: Discuss the potential for parallelizing matrix inversion methods, including the use of multi-core processors and distributed computing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Parallelization of Matrix Inversion Methods: Discuss the potential for parallelizing matrix inversion methods, including the use of multi-core processors and distributed computing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Matrix Inversion Methods for Large-Scale Systems: Explore matrix inversion methods for large-scale systems, including the use of block methods, domain decomposition methods, and parallel algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8296571224350401
      },
      {
        "text": "Parallelization of Matrix Inversion Methods: This is an important topic in numerical linear algebra, as many matrix inversion methods can be parallelized to improve their performance on multi-core processors. Techniques such as block methods and domain decomposition methods can be used to parallelize matrix inversion methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8536894317942265
      }
    ]
  },
  {
    "representative_text": "Numerical Linear Algebra Software: Introduce popular numerical linear algebra software packages, such as ARPACK, MA57, and Singular Value Decomposition (SVD), which provide efficient and reliable matrix inversion methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Linear Algebra Software: Introduce popular numerical linear algebra software packages, such as ARPACK, MA57, and Singular Value Decomposition (SVD), which provide efficient and reliable matrix inversion methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Adaptive Matrix Inversion Methods: Explore adaptive matrix inversion methods, which adjust their strategy based on the input data to optimize performance and accuracy.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Adaptive Matrix Inversion Methods: Explore adaptive matrix inversion methods, which adjust their strategy based on the input data to optimize performance and accuracy.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Hybrid Matrix Inversion Methods: Explore hybrid matrix inversion methods, which combine multiple techniques, such as Gaussian elimination and QR decomposition, to achieve better performance and accuracy.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8418154741311947
      }
    ]
  },
  {
    "representative_text": "Semi-Supervised Matrix Inversion Methods: Discuss semi-supervised matrix inversion methods, which combine multiple datasets or sources of information to improve the accuracy and robustness of the solution.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Semi-Supervised Matrix Inversion Methods: Discuss semi-supervised matrix inversion methods, which combine multiple datasets or sources of information to improve the accuracy and robustness of the solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Semi-Supervised Matrix Inversion Methods: These are matrix inversion methods that combine multiple datasets or sources of information to improve the accuracy and robustness of the solution. Semi-supervised matrix inversion methods are important in many applications, such as machine learning and data analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8450878074151406
      }
    ]
  },
  {
    "representative_text": "Transfer Learning for Matrix Inversion Methods: Introduce transfer learning techniques, which leverage pre-trained models and knowledge to improve the performance of matrix inversion methods on new and unseen data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Transfer Learning for Matrix Inversion Methods: Introduce transfer learning techniques, which leverage pre-trained models and knowledge to improve the performance of matrix inversion methods on new and unseen data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Transfer Learning for Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8405113010262291
      },
      {
        "text": "Transfer Learning for Matrix Inversion Methods: This is a technique that leverages pre-trained models and knowledge to improve the performance of matrix inversion methods on new and unseen data. Transfer learning is an important topic in machine learning and data analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8025613243816638
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of Matrix Exponentiation: Explore the numerical stability of matrix exponentiation, including the effects of round-off errors and the impact of different numerical methods on stability.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Numerical Stability of Matrix Exponentiation: Explore the numerical stability of matrix exponentiation, including the effects of round-off errors and the impact of different numerical methods on stability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Stability of Matrix Exponentiation: A discussion of the numerical stability of matrix exponentiation and how to minimize the effects of rounding errors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9369970932814646
      }
    ]
  },
  {
    "representative_text": "Numerical Linear Algebra for High-Performance Computing: Discuss the application of numerical linear algebra techniques to high-performance computing, including the use of parallel algorithms and distributed computing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Linear Algebra for High-Performance Computing: Discuss the application of numerical linear algebra techniques to high-performance computing, including the use of parallel algorithms and distributed computing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Distributed Matrix Inversion Methods: Introduce distributed matrix inversion methods, which leverage distributed computing and parallelization to improve the performance and scalability of matrix inversion methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Distributed Matrix Inversion Methods: Introduce distributed matrix inversion methods, which leverage distributed computing and parallelization to improve the performance and scalability of matrix inversion methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Distributed Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8403499618488881
      }
    ]
  },
  {
    "representative_text": "Sparse Matrix Inversion Methods: Discuss sparse matrix inversion methods, which are designed to take advantage of the sparsity of the input matrix to improve performance and accuracy.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sparse Matrix Inversion Methods: Discuss sparse matrix inversion methods, which are designed to take advantage of the sparsity of the input matrix to improve performance and accuracy.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dense Matrix Inversion Methods: Introduce dense matrix inversion methods, which are designed to handle dense matrices and provide accurate and efficient solutions.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dense Matrix Inversion Methods: Introduce dense matrix inversion methods, which are designed to handle dense matrices and provide accurate and efficient solutions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Properties of Jordan Blocks: Jordan blocks are sub-matrices that appear in the Jordan canonical form of a matrix. They are used to diagonalize non-diagonalizable matrices. Understanding the properties of Jordan blocks, such as their size and structure, is crucial in analyzing the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Properties of Jordan Blocks: Jordan blocks are sub-matrices that appear in the Jordan canonical form of a matrix. They are used to diagonalize non-diagonalizable matrices. Understanding the properties of Jordan blocks, such as their size and structure, is crucial in analyzing the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix with a complex spectrum: A matrix has a complex spectrum if and only if it is not real symmetric.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix with a complex spectrum: A matrix has a complex spectrum if and only if it is not real symmetric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix with a characteristic polynomial: A matrix has a characteristic polynomial if and only if it is square.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix with a characteristic polynomial: A matrix has a characteristic polynomial if and only if it is square.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a matrix with an eigenvalue equation: A matrix has an eigenvalue equation if and only if it is a linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a matrix with an eigenvalue equation: A matrix has an eigenvalue equation if and only if it is a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rouché's Theorem: This theorem states that if A and B are square matrices, then the eigenvalues of A + εB lie between the eigenvalues of A and the eigenvalues of B, where ε is a small positive number.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Rouché's Theorem: This theorem states that if A and B are square matrices, then the eigenvalues of A + εB lie between the eigenvalues of A and the eigenvalues of B, where ε is a small positive number.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hadamard's Inequality: This inequality states that the sum of the squares of the absolute values of the elements of a matrix A is less than or equal to the sum of the squares of the absolute values of the elements of the transpose of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hadamard's Inequality: This inequality states that the sum of the squares of the absolute values of the elements of a matrix A is less than or equal to the sum of the squares of the absolute values of the elements of the transpose of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Mapping Theorem for Matrix Norms: This theorem states that the spectral mapping theorem for matrix norms, which states that the spectral radius of a matrix A is equal to the spectral radius of its diagonal matrix, can be extended to matrix norms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Spectral Mapping Theorem for Matrix Norms: This theorem states that the spectral mapping theorem for matrix norms, which states that the spectral radius of a matrix A is equal to the spectral radius of its diagonal matrix, can be extended to matrix norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gelfand's Theorem for Matrix Norms: This theorem states that the spectral radius of a matrix A is equal to the spectral radius of its diagonal matrix, and that the spectral radius of a matrix A is less than or equal to the spectral radius of its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.860796408873616
      },
      {
        "text": "Schur's Theorem for Matrix Norms: This theorem states that the spectral radius of a matrix A is equal to the spectral radius of its diagonal matrix, and that the spectral radius of a matrix A is less than or equal to the spectral radius of its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9250305330607052
      },
      {
        "text": "Rayleigh's Theorem for Matrix Norms: This theorem states that the spectral radius of a matrix A is equal to the spectral radius of its diagonal matrix, and that the spectral radius of a matrix A is less than or equal to the spectral radius of its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9441789920589252
      },
      {
        "text": "Lanczos' Theorem for Matrix Norms: This theorem states that the spectral radius of a matrix A is equal to the spectral radius of its diagonal matrix, and that the spectral radius of a matrix A is less than or equal to the spectral radius of its transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9469774308186941
      }
    ]
  },
  {
    "representative_text": "Lagrange's Interpolation Formula: This formula states that the eigenvalues of a matrix A can be expressed as a polynomial in the eigenvalues of a companion matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lagrange's Interpolation Formula: This formula states that the eigenvalues of a matrix A can be expressed as a polynomial in the eigenvalues of a companion matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Study of Orthogonal Matrices with Non-Standard Orthogonality: This involves studying the properties of orthogonal matrices with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Study of Orthogonal Matrices with Non-Standard Orthogonality: This involves studying the properties of orthogonal matrices with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Study of Orthogonal Differential Equations with Non-Standard Orthogonality: This involves studying the properties of orthogonal differential equations with non-standard orthogonality, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9071232943871811
      },
      {
        "text": "The Interplay between Orthogonality and Orthogonality in Non-Standard Inner Products: This involves studying the interplay between orthogonality and orthogonality in non-standard inner products, which can be used to study the properties of linear transformations and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8978253974491814
      }
    ]
  },
  {
    "representative_text": "The Use of Orthogonal Projection Matrices in Nonlinear Control Theory: This involves understanding how orthogonal projection matrices are used in nonlinear control theory to design controllers and solve control problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Use of Orthogonal Projection Matrices in Nonlinear Control Theory: This involves understanding how orthogonal projection matrices are used in nonlinear control theory to design controllers and solve control problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Pontryagin's Theorem: This theorem states that any linearly independent set of vectors in an inner product space can be orthonormalized using the Gram-Schmidt process.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pontryagin's Theorem: This theorem states that any linearly independent set of vectors in an inner product space can be orthonormalized using the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Householder and Givens Rotation Theorems: These theorems provide a more general framework for orthogonalizing vectors and matrices, which can be used to develop more efficient algorithms for the Gram-Schmidt process.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Householder and Givens Rotation Theorems: These theorems provide a more general framework for orthogonalizing vectors and matrices, which can be used to develop more efficient algorithms for the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Non-Orthogonal Basis and a Determinant of Zero: The determinant of a matrix with a non-orthogonal basis and a determinant of zero is zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Non-Orthogonal Basis and a Determinant of Zero: The determinant of a matrix with a non-orthogonal basis and a determinant of zero is zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-Symmetric Matrix Inversion: The process of finding the inverse of a non-symmetric matrix, which may require different methods than the symmetric case.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Symmetric Matrix Inversion: The process of finding the inverse of a non-symmetric matrix, which may require different methods than the symmetric case.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computation of Determinants for Non-Standard Matrices: Determinants for matrices with non-standard structures, such as block matrices or matrices with non-integer entries.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Computation of Determinants for Non-Standard Matrices: Determinants for matrices with non-standard structures, such as block matrices or matrices with non-integer entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with a Non-Standard Block Structure: The determinant of a matrix with a non-standard block structure, such as a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8815843335038164
      },
      {
        "text": "Determinant of a Matrix with a Non-Standard Block Structure: The properties of the determinant of a matrix with a non-standard block structure, such as a block diagonal matrix with non-square blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9175800218716885
      }
    ]
  },
  {
    "representative_text": "Regularization Techniques for Non-Symmetric Matrices: Regularization techniques for non-symmetric matrices, which may require modifications to existing algorithms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Regularization Techniques for Non-Symmetric Matrices: Regularization techniques for non-symmetric matrices, which may require modifications to existing algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Regularization Techniques for Non-Symmetric Matrices in High-Dimensional Spaces: Regularization techniques for non-symmetric matrices in high-dimensional spaces, which may require modifications to existing algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9541493476203162
      }
    ]
  },
  {
    "representative_text": "Computation of Determinants for Matrices with Complex Entries: Determinants for matrices with complex entries, which may require different methods than the real case.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computation of Determinants for Matrices with Complex Entries: Determinants for matrices with complex entries, which may require different methods than the real case.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Regularization Techniques for Inverse Matrix Computation in High-Dimensional Spaces: Regularization techniques for inverse matrix computation in high-dimensional spaces, which may require modifications to existing algorithms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Regularization Techniques for Inverse Matrix Computation in High-Dimensional Spaces: Regularization techniques for inverse matrix computation in high-dimensional spaces, which may require modifications to existing algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Sensitivity Analysis of Matrix Inversion Algorithms in High-Dimensional Spaces: Sensitivity analysis of matrix inversion algorithms in high-dimensional spaces, which may require modifications to existing methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8231469095228342
      }
    ]
  },
  {
    "representative_text": "Numerical Methods for Finding Inverses of Matrices with Non-Standard Structures: Numerical methods for finding inverses of matrices with non-standard structures, such as block matrices or matrices with non-integer entries.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Numerical Methods for Finding Inverses of Matrices with Non-Standard Structures: Numerical methods for finding inverses of matrices with non-standard structures, such as block matrices or matrices with non-integer entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Methods for Finding Inverses of Matrices with Non-Standard Bases: Numerical methods for finding inverses of matrices with non-standard bases, which may require modifications to existing algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8841167464888045
      },
      {
        "text": "Non-Standard Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572930459842432
      }
    ]
  },
  {
    "representative_text": "Computation of Determinants for Matrices with Non-Orthogonal Bases: Determinants for matrices with non-orthogonal bases, which may require different methods than the standard case.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computation of Determinants for Matrices with Non-Orthogonal Bases: Determinants for matrices with non-orthogonal bases, which may require different methods than the standard case.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Sylvester's Law of Inertia: A theorem that states that the inertia of a matrix (i.e., the number of positive, negative, and zero eigenvalues) is invariant under similarity transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Sylvester's Law of Inertia: A theorem that states that the inertia of a matrix (i.e., the number of positive, negative, and zero eigenvalues) is invariant under similarity transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Sylvester's Law of Inertia**: Sylvester's Law of Inertia states that the inertia of a matrix (the number of positive, negative, and zero eigenvalues) is preserved under similarity transformations. This law has implications for the linear independence of vectors in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8279214725261796
      },
      {
        "text": "Sylvester's Law of Inertia: Sylvester's Law of Inertia states that the inertia of a matrix (i.e., the number of positive, negative, and zero eigenvalues) is invariant under similarity transformations. This theorem can be useful in eigenvalue decomposition and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.939053475317587
      },
      {
        "text": "Sylvester's Law of Inertia for Linear Operators: This law generalizes Sylvester's law of inertia to linear operators and is essential in understanding the behavior of eigenvalues under linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8617188731442442
      },
      {
        "text": "Sylvester's Law of Inertia: Sylvester's law of inertia is a theorem that states that the rank and nullity of a matrix are preserved under elementary row and column operations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8956769949735124
      }
    ]
  },
  {
    "representative_text": "The Nullity-Multiplicity Theorem: A theorem that states that the nullity of a matrix is equal to the sum of the multiplicities of the zero eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Nullity-Multiplicity Theorem: A theorem that states that the nullity of a matrix is equal to the sum of the multiplicities of the zero eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Nullity-Multiplicity Theorem: The nullity-multiplicity theorem states that the nullity of a matrix is equal to the sum of the multiplicities of the zero eigenvalues. This theorem can be useful in eigenvalue decomposition and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8883710592146477
      }
    ]
  },
  {
    "representative_text": "The Determinant of a Matrix Product using the Block Triangularization Method: A method for computing the determinant of a matrix product using the block triangularization method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Determinant of a Matrix Product using the Block Triangularization Method: A method for computing the determinant of a matrix product using the block triangularization method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix Product with a Non-Standard Order and Non-Standard Block Structure: A method for computing the determinant of a matrix product where theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9098592455023173
      }
    ]
  },
  {
    "representative_text": "The Determinant of a Matrix Product using the Moore-Penrose Pseudoinverse Method: A method for computing the determinant of a matrix product using the Moore-Penrose pseudoinverse method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Determinant of a Matrix Product using the Moore-Penrose Pseudoinverse Method: A method for computing the determinant of a matrix product using the Moore-Penrose pseudoinverse method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computing Determinants using the Moore-Penrose Pseudoinverse with a Given Initial Guess: The Moore-Penrose pseudoinverse method can be used to compute the determinant of a matrix with a given initial guess, which can be useful in solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8111302700764584
      }
    ]
  },
  {
    "representative_text": "The Determinant of a Matrix Product using the Power Method: A method for computing the determinant of a matrix product using the power method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Determinant of a Matrix Product using the Power Method: A method for computing the determinant of a matrix product using the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Power Method: A method for computing the determinant of a matrix product using the inversion of a matrix using the power method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9562216177078477
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Power Method with a Given Initial Guess and a Non-Standard Rank: A method for computing the determinant of a matrix product using the inversion of a matrix using the power method with a given initial guess and a non-standard rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9430465152684092
      }
    ]
  },
  {
    "representative_text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the LU Decomposition with a Given Initial Guess and a Non-Standard Linear Transformation: A method for computing the determinant of a matrix product using the inversion of a matrix using the LU decomposition with a given initial guess and a non-standard linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the LU Decomposition with a Given Initial Guess and a Non-Standard Linear Transformation: A method for computing the determinant of a matrix product using the inversion of a matrix using the LU decomposition with a given initial guess and a non-standard linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the QR Decomposition with a Given Initial Guess and a Non-Standard Linear Transformation: A method for computing the determinant of a matrix product using the inversion of a matrix using the QR decomposition with a given initial guess and a non-standard linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9422021533173329
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Moore-Penrose Pseudoinverse with a Given Initial Guess and a Non-Standard Linear Transformation: A method for computing the determinant of a matrix product using the inversion of a matrix using the Moore-Penrose pseudoinverse with a given initial guess and a non-standard linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9227073006953691
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Power Method with a Given Initial Guess and a Non-Standard Linear Transformation: A method for computing the determinant of a matrix product using the inversion of a matrix using the power method with a given initial guess and a non-standard linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9491275760782945
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Schur Decomposition with a Given Initial Guess and a Non-Standard Jordan Canonical Form: A method for computing the determinant of a matrix product using the inversion of a matrix using the Schur decomposition with a given initial guess and a non-standard Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9135671336933509
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the LU Decomposition with a Given Initial Guess and a Non-Standard Jordan Canonical Form: A method for computing the determinant of a matrix product using the inversion of a matrix using the LU decomposition with a given initial guess and a non-standard Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9405121912072949
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the QR Decomposition with a Given Initial Guess and a Non-Standard Jordan Canonical Form: A method for computing the determinant of a matrix product using the inversion of a matrix using the QR decomposition with a given initial guess and a non-standard Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9460953531251519
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Moore-Penrose Pseudoinverse with a Given Initial Guess and a Non-Standard Jordan Canonical Form: A method for computing the determinant of a matrix product using the inversion of a matrix using the Moore-Penrose pseudoinverse with a given initial guess and a non-standard Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9378206443261292
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Power Method with a Given Initial Guess and a Non-Standard Jordan Canonical Form: A method for computing the determinant of a matrix product using the inversion of a matrix using the power method with a given initial guess and a non-standard Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9467717884481212
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Schur Decomposition with a Given Initial Guess and a Non-Standard Spectral Theorem: A method for computing the determinant of a matrix product using the inversion of a matrix using the Schur decomposition with a given initial guess and a non-standard spectral theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9281156260164526
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the LU Decomposition with a Given Initial Guess and a Non-Standard Spectral Theorem: A method for computing the determinant of a matrix product using the inversion of a matrix using the LU decomposition with a given initial guess and a non-standard spectral theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9372238528240713
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the QR Decomposition with a Given Initial Guess and a Non-Standard Spectral Theorem: A method for computing the determinant of a matrix product using the inversion of a matrix using the QR decomposition with a given initial guess and a non-standard spectral theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9427982656191989
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the Power Method with a Given Initial Guess and a Non-Standard Spectral Theorem: A method for computing the determinant of a matrix product using the inversion of a matrix using the power method with a given initial guess and a non-standard spectral theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9432749016607058
      },
      {
        "text": "The Determinant of a Matrix Product using the Inversion of a Matrix using the QR Decomposition with a Given Initial Guess and a Non-Standard Rank: A method for computing the determinant of a matrix product using the inversion of a matrix using the QR decomposition with a given initial guess and a non-standard rank.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9316037516141391
      }
    ]
  },
  {
    "representative_text": "Theoretical Background of Determinants and Inverse Matrices: This includes the study of the mathematical foundations of determinants and inverse matrices, such as the properties of determinants and the relationship between determinants and eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theoretical Background of Determinants and Inverse Matrices: This includes the study of the mathematical foundations of determinants and inverse matrices, such as the properties of determinants and the relationship between determinants and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theoretical Background of the Power Method: This includes the study of the mathematical foundations of the power method, such as the properties of the power method and the relationship between the power method and eigenvalue computation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Theoretical Background of the Power Method: This includes the study of the mathematical foundations of the power method, such as the properties of the power method and the relationship between the power method and eigenvalue computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theoretical Background of the Power Iteration for Large-Scale Matrices: This includes the study of the mathematical foundations of the power iteration for large-scale matrices, such as the properties of the power iteration and the relationship between the power iteration and eigenvalue computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8988801433168166
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of the Power Method: This includes the discussion of the numerical stability of the power method, including the concept of numerical stability and its applications in numerical linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Numerical Stability of the Power Method: This includes the discussion of the numerical stability of the power method, including the concept of numerical stability and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Stability of Power Iteration Computation for Large-Scale Matrices: The discussion of the numerical stability of the power iteration computation for large-scale matrices, including the concept of numerical stability and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8660426769361728
      }
    ]
  },
  {
    "representative_text": "Theoretical Background of the QR Algorithm: This includes the study of the mathematical foundations of the QR algorithm, such as the properties of the QR algorithm and the relationship between the QR algorithm and eigenvalue computation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theoretical Background of the QR Algorithm: This includes the study of the mathematical foundations of the QR algorithm, such as the properties of the QR algorithm and the relationship between the QR algorithm and eigenvalue computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of the QR Algorithm: This includes the discussion of the numerical stability of the QR algorithm, including the concept of numerical stability and its applications in numerical linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Stability of the QR Algorithm: This includes the discussion of the numerical stability of the QR algorithm, including the concept of numerical stability and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theoretical Background of the Jacobi-Davidson Algorithm: This includes the study of the mathematical foundations of the Jacobi-Davidson algorithm, such as the properties of the Jacobi-Davidson algorithm and the relationship between the Jacobi-Davidson algorithm and eigenvalue computation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Theoretical Background of the Jacobi-Davidson Algorithm: This includes the study of the mathematical foundations of the Jacobi-Davidson algorithm, such as the properties of the Jacobi-Davidson algorithm and the relationship between the Jacobi-Davidson algorithm and eigenvalue computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theoretical Background of the Jacobi-Davidson Algorithm for Large-Scale Matrices: This includes the study of the mathematical foundations of the Jacobi-Davidson algorithm for large-scale matrices, such as the properties of the Jacobi-Davidson algorithm and the relationship between the Jacobi-Davidson algorithm and eigenvalue computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9536976565205855
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of the Jacobi-Davidson Algorithm: This includes the discussion of the numerical stability of the Jacobi-Davidson algorithm, including the concept of numerical stability and its applications in numerical linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Numerical Stability of the Jacobi-Davidson Algorithm: This includes the discussion of the numerical stability of the Jacobi-Davidson algorithm, including the concept of numerical stability and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Stability of Jacobi-Davidson Algorithm Computation for Large-Scale Matrices: The discussion of the numerical stability of the Jacobi-Davidson algorithm computation for large-scale matrices, including the concept of numerical stability and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9223812300171453
      }
    ]
  },
  {
    "representative_text": "The Role of Linear Independence in Understanding the Properties of Linear Transformations: Linear independence plays a crucial role in understanding the properties of linear transformations, including the existence of inverse transformations, the rank of the transformation, and the nullity of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Role of Linear Independence in Understanding the Properties of Linear Transformations: Linear independence plays a crucial role in understanding the properties of linear transformations, including the existence of inverse transformations, the rank of the transformation, and the nullity of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Role of Linear Independence in Understanding the Structure of C-Algebras: Linear independence plays a crucial role in understanding the structure of C-algebras, including the existence of bases, the dimension of the algebra, and the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Role of Linear Independence in Understanding the Structure of C-Algebras: Linear independence plays a crucial role in understanding the structure of C-algebras, including the existence of bases, the dimension of the algebra, and the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Relationship Between Linear Independence and the Intersection of Subspaces: The intersection of subspaces and its relationship with linear independence is essential to understand in certain situations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Relationship Between Linear Independence and the Intersection of Subspaces: The intersection of subspaces and its relationship with linear independence is essential to understand in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Linear Independence and the Intersection of Subspaces: The relationship between linear independence and the intersection of subspaces, particularly in the context of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8725994548949423
      },
      {
        "text": "Intersection of Infinite-Dimensional Vector Spaces: The relationship between the intersection of infinite-dimensional vector spaces and the dimension of the resulting space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8355821338944469
      },
      {
        "text": "The Relationship Between Linear Independence and the Intersection of Subspaces in Infinite-Dimensional Spaces: The relationship between linear independence and the intersection of subspaces in infinite-dimensional spaces is more complex and nuanced than in finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9336204408521048
      },
      {
        "text": "Intersections of Subspaces: The intersection of two subspaces is equal to the subspace spanned by the intersection of the two subspaces. This theorem can be used to determine linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8638234667212131
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Dimension of the Row Space: The dimension of the row space of a matrix is related to linear independence. Specifically, the dimension of the row space is equal to the number of linearly independent rows in the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and Dimension of the Row Space: The dimension of the row space of a matrix is related to linear independence. Specifically, the dimension of the row space is equal to the number of linearly independent rows in the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Relationship between Linear Independence and the Trace of a Matrix: If a matrix has a non-zero trace, then its row space and column space are linearly independent.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Relationship between Linear Independence and the Trace of a Matrix: If a matrix has a non-zero trace, then its row space and column space are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis of the Orthogonal Complement: The basis of the orthogonal complement of a subspace is a set of vectors that span the orthogonal complement and are linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis of the Orthogonal Complement: The basis of the orthogonal complement of a subspace is a set of vectors that span the orthogonal complement and are linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Range Theorem: A theorem that states that the range of a linear transformation is equal to the orthogonal complement of the null space of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Range Theorem: A theorem that states that the range of a linear transformation is equal to the orthogonal complement of the null space of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Direct Sum Theorem: A theorem that states that the direct sum of two subspaces is equal to the subspace spanned by the union of the two subspaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Direct Sum Theorem: A theorem that states that the direct sum of two subspaces is equal to the subspace spanned by the union of the two subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvector and Eigenvalue for Infinite-Dimensional Matrices: A concept that generalizes the eigenvector and eigenvalue for finite-dimensional matrices to infinite-dimensional matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Eigenvector and Eigenvalue for Infinite-Dimensional Matrices: A concept that generalizes the eigenvector and eigenvalue for finite-dimensional matrices to infinite-dimensional matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Eigenvalue and Eigenvector Decomposition for Linear Operators on Infinite-Dimensional Spaces: This concept generalizes the eigenvalue and eigenvector decomposition to linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8323140021027672
      },
      {
        "text": "Eigenvectors and Eigenvalues for Infinite-Dimensional Matrices: The relationship between eigenvectors and eigenvalues in infinite-dimensional matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.849382509916583
      },
      {
        "text": "The concept of eigenvalue and eigenvector decomposition for linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8721649965086378
      }
    ]
  },
  {
    "representative_text": "Linear Independence of a Set of Functions with Non-Standard Linear Transformations: A theorem that generalizes the linear independence of a set of functions to non-standard linear transformations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Linear Transformations: A theorem that generalizes the linear independence of a set of functions to non-standard linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Scalar Multiplication: A theorem that generalizes the linear independence of a set of functions to non-standard scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9351010178410037
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Bases (Partially Ordered): This concept generalizes the linear independence of a set of functions to non-standard bases and partially ordered sets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8754455122239208
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Linear Transformations (Fixed Point Theorem): This theorem generalizes the linear independence of a set of functions to non-standard linear transformations, using the fixed point theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8703388263671121
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Scalar Multiplication (Non-Standard Analysis): This concept generalizes the linear independence of a set of functions to non-standard scalar multiplication, using non-standard analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8987494503349168
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Bases (Sorgenfrey Line): This concept generalizes the linear independence of a set of functions to non-standard bases and the Sorgenfrey line.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9073919116607834
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Linear Transformations (Kuratowski's Fixed Point Theorem): This theorem generalizes the linear independence of a set of functions to non-standard linear transformations, using Kuratowski's fixed point theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8868500738931544
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Bases (Product Space): This concept generalizes the linear independence of a set of functions to non-standard bases and product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9375139685589446
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Bases (Banach Spaces): This concept generalizes the linear independence of a set of functions to non-standard bases and Banach spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9470778598518119
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Bases (Banach-Tarski Extension): This concept generalizes the linear independence of a set of functions to non-standard bases and Banach-Tarski extension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.945869282120394
      },
      {
        "text": "Linear Independence of a Set of Functions with Non-Standard Linear Transformations (Schroeder-Whitney Theorem): This theorem generalizes the linear independence of a set of functions to non-standard linear transformations, using the Schroeder-Whitney theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.903526282849257
      },
      {
        "text": "Banach-Tarski Extension: The extension of linear independence to non-standard bases and Banach-Tarski extension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.805289992486279
      },
      {
        "text": "Linear Independence of a Set of Functions in a Vector Space with a Non-Standard Topology: This theorem generalizes the linear independence of a set of functions in a vector space with a non-standard topology.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9182450329685459
      }
    ]
  },
  {
    "representative_text": "Dependent and Free Variables in Non-Standard Bases: In a basis with non-standard basis vectors, the dependent variables are vectors that can be expressed as a linear combination of other vectors, while the free variables are vectors that cannot be expressed as a linear combination of other vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Dependent and Free Variables in Non-Standard Bases: In a basis with non-standard basis vectors, the dependent variables are vectors that can be expressed as a linear combination of other vectors, while the free variables are vectors that cannot be expressed as a linear combination of other vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dependent and Free Variables in Non-Standard Bases with Non-Standard Metric: In a basis with non-standard basis vectors, the dependent variables are vectors that can be expressed as a linear combination of other vectors, while the free variables are vectors that cannot be expressed as a linear combination of other vectors, when the vector space is equipped with a non-standard metric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9544236063744638
      },
      {
        "text": "Dependent and Free Variables in Non-Standard Bases: In non-standard bases, the concept of dependent and free variables needs to be re-examined to account for the non-standard basis vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8989656621390145
      }
    ]
  },
  {
    "representative_text": "Basis of a Vector Space with a Non-Standard Basis Element - Properties: This includes the study of properties of basis elements with non-integer or non-rational values.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Basis of a Vector Space with a Non-Standard Basis Element - Properties: This includes the study of properties of basis elements with non-integer or non-rational values.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Basis Element - Limitations: This concept explores the limitations of basis elements with non-integer or non-rational values.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8795381697909981
      }
    ]
  },
  {
    "representative_text": "Basis of a Vector Space with a Non-Standard Norm - Properties: This includes the study of properties of bases in normed vector spaces and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Basis of a Vector Space with a Non-Standard Norm - Properties: This includes the study of properties of bases in normed vector spaces and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Metric - Properties: This includes the study of properties of bases in Riemannian manifolds and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8735685083179305
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Topology - Properties: This includes the study of properties of bases in topological vector spaces and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9063630610610662
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Linear Transformation - Properties: This includes the study of properties of bases in vector spaces with non-standard linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9053560039320085
      },
      {
        "text": "Gloss on Basis Theorem for Non-Standard Vector Spaces - Implications: This includes the study of the implications of the Basis Theorem in non-standard vector spaces, including the properties and consequences of the Basis Theorem in such spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8516946238706141
      },
      {
        "text": "Gloss on Basis Theorem for Non-Standard Vector Spaces - Advanced Theorems: This includes the study of advanced theorems that relate the Basis Theorem in non-standard vector spaces, including the properties and consequences of the Basis Theorem in such spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8579774784060854
      }
    ]
  },
  {
    "representative_text": "Jordan Decomposition: Jordan decomposition is a method used to diagonalize a matrix. It can be used to understand the relationship between linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jordan Decomposition: Jordan decomposition is a method used to diagonalize a matrix. It can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matroid Theory: Matroid theory is a branch of mathematics that studies the properties of vector spaces and linear transformations. It can be used to understand the relationship between linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matroid Theory: Matroid theory is a branch of mathematics that studies the properties of vector spaces and linear transformations. It can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Combinatorial Geometry: Combinatorial geometry is a branch of mathematics that studies the properties of vector spaces and linear transformations. It can be used to understand the relationship between linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Combinatorial Geometry: Combinatorial geometry is a branch of mathematics that studies the properties of vector spaces and linear transformations. It can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Combinatorial Geometry and Linear Independence: Combinatorial geometry provides a framework for understanding the properties of geometric objects, such as polytopes and convex sets, and its techniques can be used to understand the relationship between linear independence and the dimension of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8703197784839016
      }
    ]
  },
  {
    "representative_text": "Numerical Linear Algebra: Numerical linear algebra is a branch of mathematics that studies the numerical solutions of linear algebra problems. It can be used to understand the relationship between linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Numerical Linear Algebra: Numerical linear algebra is a branch of mathematics that studies the numerical solutions of linear algebra problems. It can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Linear Algebra and Approximation: Numerical linear algebra is used to approximate solutions to linear algebra problems, and its techniques can be used to understand the relationship between linear independence and the dimension of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8863087378060075
      }
    ]
  },
  {
    "representative_text": "Tverberg's Theorem: Tverberg's theorem is a result in combinatorial geometry that states that a set of points in a vector space can be partitioned into a finite number of convex sets. It can be used to understand the relationship between linear independence and span.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Tverberg's Theorem: Tverberg's theorem is a result in combinatorial geometry that states that a set of points in a vector space can be partitioned into a finite number of convex sets. It can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Tverberg's Theorem: A result in combinatorial geometry that states that a set of points in a vector space can be partitioned into a finite number of convex sets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8085617004997501
      }
    ]
  },
  {
    "representative_text": "Borsuk-Ulam Theorem: The Borsuk-Ulam theorem is a result in topology that states that a continuous function from a vector space to itself must map a pair of antipodal points to antipodal points. It can be used to understand the relationship between linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Borsuk-Ulam Theorem: The Borsuk-Ulam theorem is a result in topology that states that a continuous function from a vector space to itself must map a pair of antipodal points to antipodal points. It can be used to understand the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Borsuk-Ulam Theorem: A result in topology that states that a continuous function from a vector space to itself must map a pair of antipodal points to antipodal points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8041168036364338
      }
    ]
  },
  {
    "representative_text": "The Lagrange Interpolation Formula**: The Lagrange interpolation formula is a method for constructing a polynomial that passes through a given set of points. This formula has implications for the linear independence of polynomials in a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Lagrange Interpolation Formula**: The Lagrange interpolation formula is a method for constructing a polynomial that passes through a given set of points. This formula has implications for the linear independence of polynomials in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Differential Geometry of Vector Spaces**: Differential geometry is a branch of mathematics that studies the properties of curves and surfaces in vector spaces. This branch has implications for the linear independence of forms in differential geometry.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Differential Geometry of Vector Spaces**: Differential geometry is a branch of mathematics that studies the properties of curves and surfaces in vector spaces. This branch has implications for the linear independence of forms in differential geometry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Nullity of a Linear Transformation: We need to consider the implications of the nullity of a linear transformation on linear independence.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Nullity of a Linear Transformation: We need to consider the implications of the nullity of a linear transformation on linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Nullity of a Linear Transformation: The implications of the nullity of a linear transformation on linear independence are not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8784455077120201
      },
      {
        "text": "Nullity of a Linear Transformation for Infinite-Dimensional Vector Spaces: The implications of the nullity of a linear transformation on linear independence are not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9225486128887657
      },
      {
        "text": "Nullity of a Linear Transformation for Infinite-Dimensional Vector Spaces: Investigate the implications of the nullity of a linear transformation on linear independence in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8962765422393917
      }
    ]
  },
  {
    "representative_text": "The Intersection of Span and Null Space: The intersection of the span of a set of vectors and the null space of a matrix is a subspace that contains all vectors that can be expressed as a linear combination of the original vectors and also map to the zero vector when multiplied by the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Intersection of Span and Null Space: The intersection of the span of a set of vectors and the null space of a matrix is a subspace that contains all vectors that can be expressed as a linear combination of the original vectors and also map to the zero vector when multiplied by the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Concept of Linear Dependence in the Context of Algebraic Geometry: This concept involves understanding the properties of linear dependence in the context of algebraic geometry, which is a more advanced application of linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Concept of Linear Dependence in the Context of Algebraic Geometry: This concept involves understanding the properties of linear dependence in the context of algebraic geometry, which is a more advanced application of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Concept of Linear Independence in the Context of Topology: This concept involves understanding the properties of linear independence in the context of topology, which is a more advanced application of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9346409145866603
      },
      {
        "text": "The Concept of Linear Dependence in the Context of Representation Theory: This concept involves understanding the properties of linear dependence in the context of representation theory, which is a more advanced application of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9441633428828613
      },
      {
        "text": "The Concept of Linear Independence in the Context of Computational Complexity: This concept involves understanding the properties of linear independence in the context of computational complexity, which is a more advanced application of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9206398131631999
      }
    ]
  },
  {
    "representative_text": "The Concept of Linear Independence in the Context of Machine Learning: This concept involves understanding the properties of linear independence in the context of machine learning, which is a more advanced application of linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Concept of Linear Independence in the Context of Machine Learning: This concept involves understanding the properties of linear independence in the context of machine learning, which is a more advanced application of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Manifold Learning: Manifold learning is a technique used in computer graphics and game development to learn the structure of complex data sets. Linear algebra is used to perform tasks such as dimensionality reduction and non-linear dimensionality reduction.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Manifold Learning: Manifold learning is a technique used in computer graphics and game development to learn the structure of complex data sets. Linear algebra is used to perform tasks such as dimensionality reduction and non-linear dimensionality reduction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computer Vision and Image Processing: Computer vision and image processing are used in computer graphics and game development to analyze and understand visual data. Linear algebra is used to perform tasks such as image filtering, image registration, and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Computer Vision and Image Processing: Computer vision and image processing are used in computer graphics and game development to analyze and understand visual data. Linear algebra is used to perform tasks such as image filtering, image registration, and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebraic Methods for Computer Vision: Linear algebraic methods for computer vision are used in computer graphics and game development to perform tasks such as object recognition and image registration. Linear algebra is used to optimize tasks such as image filtering and image registration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9165039009504667
      },
      {
        "text": "Linear Algebraic Methods for Robotics and Computer Vision: Linear algebraic methods for robotics and computer vision are used in computer graphics and game development to perform kinematic analysis and solve problems of motion planning and control. Linear algebra is used to optimize tasks such as object recognition and image registration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9202850046697739
      },
      {
        "text": "Linear Algebraic Methods for Computer Vision: Linear algebraic methods, such as eigenvalue decomposition, are used to solve problems in computer vision in computer graphics and game development. These techniques involve using linear algebra to solve problems involving computer vision.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9021010246755125
      },
      {
        "text": "Machine Learning for Computer Vision and Computer Vision-Based Physics-Based Optimization: Linear Regression and Geometry: Linear Regression and Graphics Rendering: Linear Algebraic Reduction: Machine Learning for Computer Vision: Linear Algebraic computing: Machine Learning for 3Distributed computing in Computer vision-based Image-based linear algebraiculatory methods like the (Scalgebra: Linear algebraic computing in Computer Vision: Linear algebraic equations are not only in computer vision for robustly in computer graphics rendering of linear algebraic principles are not listed are useful in 3Distributed computing the **(backup",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8104273112286029
      }
    ]
  },
  {
    "representative_text": "Differential Equations: Differential equations are used in computer graphics and game development to model complex systems and simulate realistic physics. Linear algebra is used to solve differential equations and perform tasks such as numerical integration and differentiation.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Differential Equations: Differential equations are used in computer graphics and game development to model complex systems and simulate realistic physics. Linear algebra is used to solve differential equations and perform tasks such as numerical integration and differentiation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Methods for Partial Differential Equations: Numerical methods for partial differential equations are used in computer graphics and game development to solve systems of partial differential equations and simulate realistic physics. Linear algebra is used to perform tasks such as numerical integration and differentiation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8975409078582551
      },
      {
        "text": "Computational Physics and Engineering: Computational physics and engineering involve the use of linear algebra to solve systems of partial differential equations and simulate realistic physics. Linear algebra is used to perform tasks such as numerical integration and differentiation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8217369978487883
      }
    ]
  },
  {
    "representative_text": "Computational Fluid Dynamics (CFD): CFD is a technique used in computer graphics and game development to simulate the behavior of fluids. Linear algebra is used to solve systems of partial differential equations and perform tasks such as fluid dynamics simulations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Fluid Dynamics (CFD): CFD is a technique used in computer graphics and game development to simulate the behavior of fluids. Linear algebra is used to solve systems of partial differential equations and perform tasks such as fluid dynamics simulations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Geometric Algebra: Geometric algebra is a mathematical framework that uses linear algebra to represent geometric objects and perform geometric operations. In computer graphics and game development, geometric algebra is used to perform tasks such as ray tracing and rendering.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Geometric Algebra: Geometric algebra is a mathematical framework that uses linear algebra to represent geometric objects and perform geometric operations. In computer graphics and game development, geometric algebra is used to perform tasks such as ray tracing and rendering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Geometric Algebra and Multivector Algebra: Geometric algebra is a mathematical framework that uses linear algebra to represent geometric objects and perform geometric operations. Multivector algebra is a generalization of geometric algebra that involves representing objects as combinations of vectors, scalars, and multivectors. This is useful in computer graphics for tasks such as ray tracing and rendering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9190203383205169
      },
      {
        "text": "Clifford Algebra: Clifford algebra is a mathematical framework that extends the properties of vector spaces to include the concept of geometric products. This is useful in computer graphics and game development for tasks such as ray tracing and rendering.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8869493217526943
      },
      {
        "text": "Geometric Algebra: Geometric algebra is a mathematical framework that combines the concepts of vectors, scalars, and multivectors to represent geometric objects and perform geometric operations. This is useful in computer graphics and game development for tasks such as object recognition and image processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9296815277075315
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Computation: Linear algebraic computation is a technique used in computer graphics and game development to perform tasks such as linear system solving and eigenvalue decomposition. Linear algebra is used to optimize tasks such as physics-based modeling and simulation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Computation: Linear algebraic computation is a technique used in computer graphics and game development to perform tasks such as linear system solving and eigenvalue decomposition. Linear algebra is used to optimize tasks such as physics-based modeling and simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Stability Analysis: Stability analysis is a technique used in computer graphics and game development to analyze the stability of complex systems. Linear algebra is used to perform tasks such as eigenvalue analysis and stability testing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8295594829754673
      }
    ]
  },
  {
    "representative_text": "Approximation Methods: Approximation methods are used in computer graphics and game development to approximate complex systems and models. Linear algebra is used to perform tasks such as numerical integration and differentiation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Approximation Methods: Approximation methods are used in computer graphics and game development to approximate complex systems and models. Linear algebra is used to perform tasks such as numerical integration and differentiation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Bounded Linear Systems: Bounded linear systems are used in computer graphics and game development to model complex systems and simulate realistic physics. Linear algebra is used to solve systems of linear equations and perform tasks such as numerical integration and differentiation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.841115260463118
      }
    ]
  },
  {
    "representative_text": "Spectral Methods: Spectral methods are used in computer graphics and game development to solve systems of linear equations and perform tasks such as eigenvalue decomposition. Linear algebra is used to optimize tasks such as physics-based modeling and simulation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spectral Methods: Spectral methods are used in computer graphics and game development to solve systems of linear equations and perform tasks such as eigenvalue decomposition. Linear algebra is used to optimize tasks such as physics-based modeling and simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kernel Methods for Non-Linear Regression with Gaussian Processes: This area of research involves using linear algebra techniques such as eigendecomposition and singular value decomposition to improve the performance of non-linear regression models using Gaussian processes.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Kernel Methods for Non-Linear Regression with Gaussian Processes: This area of research involves using linear algebra techniques such as eigendecomposition and singular value decomposition to improve the performance of non-linear regression models using Gaussian processes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Computation of Non-Linear Regression with Gaussian Processes: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to compute non-linear regression with Gaussian processes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8058080689915923
      },
      {
        "text": "The Computation of Non-Linear Support Vector Machines with Gaussian Processes: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to compute non-linear support vector machines with Gaussian processes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8649754129028506
      },
      {
        "text": "The Computation of Non-Linear Regression with Gaussian Processes and Orthogonal Projections: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to compute non-linear regression models using Gaussian processes and orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9266307982064415
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Reinforcement Learning: Policy Gradient Methods: This area of research involves using linear algebra techniques such as eigendecomposition and singular value decomposition to analyze and optimize reinforcement learning models for tasks such as policy gradient methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Reinforcement Learning: Policy Gradient Methods: This area of research involves using linear algebra techniques such as eigendecomposition and singular value decomposition to analyze and optimize reinforcement learning models for tasks such as policy gradient methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Reinforcement Learning: Deep Q-Networks: This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize reinforcement learning models for tasks such as deep Q-networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.948083469753353
      },
      {
        "text": "Linear Algebra for Reinforcement Learning: Policy Gradient Methods: Techniques for optimizing policy gradient methods, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8787423155419234
      },
      {
        "text": "Linear Algebra for Reinforcement Learning: Deep Q-Networks: Methods for optimizing deep Q-networks, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.917639547849865
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Deep Learning: Autoencoders and Generative Adversarial Networks (GANs): This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize deep learning models for tasks such as autoencoders and generative adversarial networks (GANs).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Deep Learning: Autoencoders and Generative Adversarial Networks (GANs): This area of research involves using linear algebra techniques such as matrix multiplication and eigendecomposition to analyze and optimize deep learning models for tasks such as autoencoders and generative adversarial networks (GANs).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Deep Learning: Autoencoders and Generative Adversarial Networks (GANs): Techniques for optimizing autoencoders and GANs, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9188057476826273
      },
      {
        "text": "Linear Algebra for Deep Learning: Graph Neural Networks: Methods for optimizing graph neural networks, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8702429777740334
      },
      {
        "text": "Linear Algebra for Deep Learning: Graph Attention Networks: Techniques for optimizing graph attention networks, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9062579860651692
      },
      {
        "text": "Linear Algebra for Deep Learning: Graph Neural Network Architectures: Architectures for building graph neural networks, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9081269197383046
      },
      {
        "text": "Linear Algebra for Deep Learning: Graph Neural Network Evaluation Metrics: Metrics for evaluating the performance of graph neural networks, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8736316655835139
      },
      {
        "text": "Linear Algebra for Deep Learning: Graph Convolutional Networks with Spectral Graph Convolution: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to optimize graph convolutional networks with spectral graph convolution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9032703729322782
      },
      {
        "text": "Linear Algebra for Deep Learning: Autoencoders with Orthogonal Projections: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to optimize autoencoders with orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8212004305999139
      },
      {
        "text": "Linear Algebra for Deep Learning: Graph Neural Networks with Orthogonal Projections: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to optimize graph neural networks with orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.901489732470397
      },
      {
        "text": "Linear Algebra for Deep Learning: Autoencoders with Orthogonal Projections and Spectral Graph Convolution: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to optimize autoencoders using orthogonal projections and spectral graph convolution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9219804965150533
      }
    ]
  },
  {
    "representative_text": "The Computation of Linear Algebra for Image Processing: This is a fundamental problem in image processing, and various algorithms such as the Gram-Schmidt process and the QR algorithm can be used to compute linear algebra for image processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Computation of Linear Algebra for Image Processing: This is a fundamental problem in image processing, and various algorithms such as the Gram-Schmidt process and the QR algorithm can be used to compute linear algebra for image processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Computation of Linear Algebra for Natural Language Processing: This is a fundamental problem in natural language processing, and various algorithms such as the Gram-Schmidt process and the QR algorithm can be used to compute linear algebra for natural language processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Computation of Linear Algebra for Natural Language Processing: This is a fundamental problem in natural language processing, and various algorithms such as the Gram-Schmidt process and the QR algorithm can be used to compute linear algebra for natural language processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Geometry: Computational geometry is a branch of mathematics that deals with the analysis and processing of geometric objects. It has various applications in signal processing, image analysis, and data compression. However, the relationship between computational geometry and linear algebra is not fully explored.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Geometry: Computational geometry is a branch of mathematics that deals with the analysis and processing of geometric objects. It has various applications in signal processing, image analysis, and data compression. However, the relationship between computational geometry and linear algebra is not fully explored.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Robust Stability: A measure of the stability of a system in the presence of uncertainty or noise, using linear algebra and optimization techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Robust Stability: A measure of the stability of a system in the presence of uncertainty or noise, using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Key recovery attacks: A type of attack that aims to recover the secret key used for encryption, often through exploiting weaknesses in the encryption algorithm or the implementation.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Key recovery attacks: A type of attack that aims to recover the secret key used for encryption, often through exploiting weaknesses in the encryption algorithm or the implementation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quantum key distribution (QKD) protocols: Protocols that use quantum mechanics to establish secure keys between parties, often used in quantum-resistant cryptography.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quantum key distribution (QKD) protocols: Protocols that use quantum mechanics to establish secure keys between parties, often used in quantum-resistant cryptography.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Zero-knowledge proof systems with multi-party computation: Protocols that enable secure multi-party computation using zero-knowledge proofs, allowing parties to verify the correctness of computations without revealing their individual inputs.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Zero-knowledge proof systems with multi-party computation: Protocols that enable secure multi-party computation using zero-knowledge proofs, allowing parties to verify the correctness of computations without revealing their individual inputs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Advanced Zero-Knowledge Proof Systems with Multi-Party Computation: This includes protocols that enable secure multi-party computation using zero-knowledge proofs, allowing parties to verify the correctness of computations without revealing their individual inputs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9043840840968493
      },
      {
        "text": "Advanced zero-knowledge proof systems with multi-party computation: Protocols that enable secure multi-party computation using zero-knowledge proofs, allowing parties to verify the correctness of computations without revealing their individual inputs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9818362746981328
      }
    ]
  },
  {
    "representative_text": "Cholesky decomposition",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Cholesky decomposition",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cholesky Decomposition Properties:",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8850362913810201
      }
    ]
  },
  {
    "representative_text": "Round-off errors",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Round-off errors",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Condition number",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Condition number",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Ill-conditioning",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8324549381116053
      }
    ]
  },
  {
    "representative_text": "Numerical instability",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical instability",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Definition",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Definition",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Estimation",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Estimation",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Analysis",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Analysis",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bounds",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bounds",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Error bounds",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Error bounds",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Parallel algorithms",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Parallel algorithms",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Distributed computing",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Distributed computing",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Load balancing",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Load balancing",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Synchronization",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Synchronization",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block methods",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block methods",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Domain decomposition methods",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Domain decomposition methods",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Domain Decomposition Methods: Explore domain decomposition methods, which use the domain decomposition of a matrix to solve a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8168288167162236
      }
    ]
  },
  {
    "representative_text": "Scalability",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalability",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Compressed sparse row (CSR)",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Compressed sparse row (CSR)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Compressed sparse column (CSC)",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9550882126512303
      },
      {
        "text": "Sparse matrix storage",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8860040771522861
      }
    ]
  },
  {
    "representative_text": "Efficient algorithms",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Efficient algorithms",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "LU decomposition",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "LU decomposition",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "LU Decomposition Properties:",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8841666807315194
      }
    ]
  },
  {
    "representative_text": "Pre-trained models",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pre-trained models",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Knowledge transfer",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Knowledge transfer",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Transfer learning",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8135149753890053
      }
    ]
  },
  {
    "representative_text": "Adaptation to new data",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Adaptation to new data",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Real-time adaptation",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.817930533156136
      }
    ]
  },
  {
    "representative_text": "Performance improvement",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Performance improvement",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Performance optimization",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9065133288385219
      }
    ]
  },
  {
    "representative_text": "Multiple datasets",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Multiple datasets",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Source combination",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Source combination",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Multiple techniques",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Multiple techniques",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Combination of methods",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Combination of methods",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Accuracy",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Accuracy",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "ARPACK",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "ARPACK",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Efficient and reliable methods",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Efficient and reliable methods",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Strategy adjustment",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Strategy adjustment",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Accuracy improvement",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Accuracy improvement",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Properties",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Properties",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Properties:",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8848835454397614
      }
    ]
  },
  {
    "representative_text": "Applications",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Applications",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Extension to non-invertible matrices",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Extension to non-invertible matrices",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A^T A = I, where I is the identity matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A^T A = I, where I is the identity matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A A^T = I.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A A^T = I.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "det(A) = ±1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "det(A) = ±1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Interior Point Methods (IPMs): a class of algorithms for solving linear and nonlinear optimization problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Interior Point Methods (IPMs): a class of algorithms for solving linear and nonlinear optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cutting Plane Methods: a class of algorithms for solving linear optimization problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cutting Plane Methods: a class of algorithms for solving linear optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dynamic Programming: a method for solving optimization problems by breaking them down into smaller subproblems.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dynamic Programming: a method for solving optimization problems by breaking them down into smaller subproblems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Model-Based Reinforcement Learning: a method for learning control policies using reinforcement learning.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Model-Based Reinforcement Learning: a method for learning control policies using reinforcement learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Deep Deterministic Policy Gradients (DDPG): a method for learning control policies using deep neural networks.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Deep Deterministic Policy Gradients (DDPG): a method for learning control policies using deep neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Second-Order Cone Programming (SOCP): a class of optimization problems that are used to solve problems involving second-order cone constraints.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Second-Order Cone Programming (SOCP): a class of optimization problems that are used to solve problems involving second-order cone constraints.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Semi-Definite Programming (SDP): a class of optimization problems that are used to solve problems involving semi-definite matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Semi-Definite Programming (SDP): a class of optimization problems that are used to solve problems involving semi-definite matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Semi-Definite Programming (SDP) Relaxation: A method for relaxing convex optimization problems to obtain a more tractable solution, which can be used in optimization and control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8061918361058444
      }
    ]
  },
  {
    "representative_text": "Linear Matrix Inequality (LMI) Solvers: a class of algorithms for solving linear matrix inequality problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Matrix Inequality (LMI) Solvers: a class of algorithms for solving linear matrix inequality problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Matrix Inequality (LMI) Solvers for Non-Convex Optimization: A class of algorithms for solving non-convex optimization problems using linear matrix inequalities.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8790529350557821
      }
    ]
  },
  {
    "representative_text": "Quadratic Programming (QP) Solvers: a class of algorithms for solving quadratic programming problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quadratic Programming (QP) Solvers: a class of algorithms for solving quadratic programming problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Methods: a class of algorithms for solving problems in machine learning using spectral analysis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spectral Methods: a class of algorithms for solving problems in machine learning using spectral analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Randomized Algorithms: a class of algorithms for solving problems in machine learning using random sampling.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Randomized Algorithms: a class of algorithms for solving problems in machine learning using random sampling.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Manifold Learning: a class of algorithms for learning complex structures in high-dimensional data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Manifold Learning: a class of algorithms for learning complex structures in high-dimensional data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Manifold Learning for Graph Data: Methods for reducing the dimensionality of graph data while preserving its underlying structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8292328397943771
      },
      {
        "text": "Manifold Learning for High-Dimensional Data: Geometric Methods: Methods for reducing the dimensionality of high-dimensional data while preserving its underlying structure, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8189211537752451
      },
      {
        "text": "Manifold Learning with Linear Algebra: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and manifold learning techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.885503371904161
      },
      {
        "text": "Manifold Learning for High-Dimensional Data: Isomap: A method that uses linear algebra operations such as matrix multiplication and eigendecomposition to reduce the dimensionality of high-dimensional data while preserving its underlying structure.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8753412796838513
      }
    ]
  },
  {
    "representative_text": "Deep Learning: a class of algorithms for learning complex structures in high-dimensional data using deep neural networks.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Deep Learning: a class of algorithms for learning complex structures in high-dimensional data using deep neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Deep Learning with Linear Algebra: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and deep neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8224232407530533
      }
    ]
  },
  {
    "representative_text": "Wavelet Analysis: a class of algorithms for analyzing signals using wavelets.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Wavelet Analysis: a class of algorithms for analyzing signals using wavelets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Wavelet Analysis with Linear Algebra: A class of algorithms for analyzing signals using wavelets and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8880819438736457
      }
    ]
  },
  {
    "representative_text": "Filter Design: a class of algorithms for designing filters for signal processing applications.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Filter Design: a class of algorithms for designing filters for signal processing applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Filter Design with Linear Algebra: A class of algorithms for designing filters for signal processing applications using linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8846232931524962
      }
    ]
  },
  {
    "representative_text": "Optimization in Signal Processing: a class of algorithms for optimizing signal processing systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Optimization in Signal Processing: a class of algorithms for optimizing signal processing systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Regression: a class of algorithms for predicting continuous outcomes from predictor variables.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Regression: a class of algorithms for predicting continuous outcomes from predictor variables.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Regression with Linear Algebra: A class of algorithms for predicting continuous outcomes from predictor variables using linear algebra and linear regression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8865675007314127
      }
    ]
  },
  {
    "representative_text": "Time Series Analysis: a class of algorithms for analyzing and forecasting time series data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Time Series Analysis: a class of algorithms for analyzing and forecasting time series data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Time Series Analysis with Linear Algebra: A class of algorithms for analyzing and forecasting time series data using linear algebra and time series analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8557216388185367
      }
    ]
  },
  {
    "representative_text": "Graph Analysis: a class of algorithms for analyzing and visualizing graph-structured data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Graph Analysis: a class of algorithms for analyzing and visualizing graph-structured data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Graph Analysis with Linear Algebra: A class of algorithms for analyzing and visualizing graph-structured data using linear algebra and graph analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.896224284663748
      }
    ]
  },
  {
    "representative_text": "Image Processing: a class of algorithms for processing and analyzing images.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Image Processing: a class of algorithms for processing and analyzing images.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Object Detection: a class of algorithms for detecting objects in images.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Object Detection: a class of algorithms for detecting objects in images.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Object Detection with Linear Algebra: A class of algorithms for detecting objects in images using linear algebra and object detection techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8537971937772213
      }
    ]
  },
  {
    "representative_text": "Image Segmentation: a class of algorithms for segmenting images into regions of interest.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Image Segmentation: a class of algorithms for segmenting images into regions of interest.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Image Segmentation with Linear Algebra: A class of algorithms for segmenting images into regions of interest using linear algebra and image segmentation techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.84248707236937
      }
    ]
  },
  {
    "representative_text": "Structure from Motion: a class of algorithms for estimating the structure of a scene from motion data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Structure from Motion: a class of algorithms for estimating the structure of a scene from motion data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Structure from Motion with Linear Algebra: A class of algorithms for estimating the structure of a scene from motion data using linear algebra and structure from motion techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.921702349264246
      }
    ]
  },
  {
    "representative_text": "Robot Arm Control: a class of algorithms for controlling robot arms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Robot Arm Control: a class of algorithms for controlling robot arms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Robot Arm Control with Linear Algebra: A class of algorithms for controlling robot arms using linear algebra and robot arm control techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8885287805123662
      }
    ]
  },
  {
    "representative_text": "Motion Planning: a class of algorithms for planning the motion of a robot.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Motion Planning: a class of algorithms for planning the motion of a robot.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Motion Planning with Linear Algebra: A class of algorithms for planning the motion of a robot using linear algebra and motion planning techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8688463092697163
      }
    ]
  },
  {
    "representative_text": "Object Manipulation: a class of algorithms for manipulating objects using robots.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Object Manipulation: a class of algorithms for manipulating objects using robots.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Object Manipulation with Linear Algebra: A class of algorithms for manipulating objects using robots and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9031887275446125
      }
    ]
  },
  {
    "representative_text": "Control of Autonomous Vehicles: a class of algorithms for controlling autonomous vehicles.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Control of Autonomous Vehicles: a class of algorithms for controlling autonomous vehicles.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Control of Autonomous Vehicles with Linear Algebra: A class of algorithms for controlling autonomous vehicles using linear algebra and control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8783881012584598
      }
    ]
  },
  {
    "representative_text": "Quadratic Forms: A quadratic form on a vector space V is a function that can be written as a sum of squares of linear combinations of vectors in V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Quadratic Forms: A quadratic form on a vector space V is a function that can be written as a sum of squares of linear combinations of vectors in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem on the Quadratic Form: This theorem states that a quadratic form on a vector space V is a function that can be written as a sum of squares of linear combinations of vectors in V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.886997932460875
      },
      {
        "text": "Definition: A quadratic form is a function that takes a vector as input and returns a scalar value.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8426720192556647
      }
    ]
  },
  {
    "representative_text": "Theorem on the Existence of a Linear Transformation: This theorem states that every vector space has a linear transformation from the vector space to itself.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Theorem on the Existence of a Linear Transformation: This theorem states that every vector space has a linear transformation from the vector space to itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorem on the Existence of a Non-Trivial Linear Transformation: This theorem states that there exists a non-trivial linear transformation from a vector space to itself.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9269764023584491
      }
    ]
  },
  {
    "representative_text": "Inner Product Spaces: In addition to the definition of inner product spaces, it's worth noting that inner product spaces are complete with respect to the norm induced by the inner product.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner Product Spaces: In addition to the definition of inner product spaces, it's worth noting that inner product spaces are complete with respect to the norm induced by the inner product.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Symmetric Bilinear Forms: A symmetric bilinear form is a bilinear form that is symmetric, meaning that <u, v> = <v, u> for all vectors u and v. Symmetric bilinear forms are used to define inner products and are essential in the study of vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Symmetric Bilinear Forms: A symmetric bilinear form is a bilinear form that is symmetric, meaning that <u, v> = <v, u> for all vectors u and v. Symmetric bilinear forms are used to define inner products and are essential in the study of vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Symmetric Bilinear Forms: A symmetric bilinear form is a bilinear form that is equal to its own transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8929033276297883
      },
      {
        "text": "Symmetric Quadratic Forms: A symmetric quadratic form is a quadratic form that is equal to the inner product of the input vector with its transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8325779977876778
      }
    ]
  },
  {
    "representative_text": "Projections onto a Subspace: Projections onto a subspace are linear transformations that map a vector to its closest point on the subspace. This concept is essential in the study of linear transformations and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Projections onto a Subspace: Projections onto a subspace are linear transformations that map a vector to its closest point on the subspace. This concept is essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvector Perturbation Methods: Eigenvector perturbation methods are numerical methods used to approximate the eigenvectors of a matrix. These methods are essential in the study of linear transformations and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvector Perturbation Methods: Eigenvector perturbation methods are numerical methods used to approximate the eigenvectors of a matrix. These methods are essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Perturbation Methods: Eigenvalue perturbation methods are numerical methods used to approximate the eigenvalues of a matrix. These methods are essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9238926301282269
      }
    ]
  },
  {
    "representative_text": "The Hilbert-Schmidt Theorem: The Hilbert-Schmidt theorem is a theorem that describes the properties of the inner product and the norm of a vector space. This theorem is essential in the study of vector spaces and their properties.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Hilbert-Schmidt Theorem: The Hilbert-Schmidt theorem is a theorem that describes the properties of the inner product and the norm of a vector space. This theorem is essential in the study of vector spaces and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Pontryagin Duality Theorem: The Pontryagin duality theorem is a theorem that describes the properties of the inner product and the norm of a vector space. This theorem is essential in the study of vector spaces and their properties.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Pontryagin Duality Theorem: The Pontryagin duality theorem is a theorem that describes the properties of the inner product and the norm of a vector space. This theorem is essential in the study of vector spaces and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Null Space of a Linear Transformation with Non-Injective: The null space of a linear transformation with non-injective is a subspace, but its dimension may not be the rank of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Null Space of a Linear Transformation with Non-Injective: The null space of a linear transformation with non-injective is a subspace, but its dimension may not be the rank of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Subspace Invariant with Non-Trivial Null Space: A subspace invariant with a non-trivial null space may not be a subspace, as the null space may not be closed under the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Subspace Invariant with Non-Trivial Null Space: A subspace invariant with a non-trivial null space may not be a subspace, as the null space may not be closed under the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Isomorphism between Vector Spaces with Different Dimensions: Two vector spaces with different dimensions may not be isomorphic, as the dimension of the vector space is a fundamental property of the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Isomorphism between Vector Spaces with Different Dimensions: Two vector spaces with different dimensions may not be isomorphic, as the dimension of the vector space is a fundamental property of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Decomposition of a Linear Transformation with Non-Trivial Null Space: The tensor decomposition of a linear transformation with a non-trivial null space may not be unique, as the null space may not be closed under the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Tensor Decomposition of a Linear Transformation with Non-Trivial Null Space: The tensor decomposition of a linear transformation with a non-trivial null space may not be unique, as the null space may not be closed under the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Tensor Decomposition for Infinite-Dimensional Vector Spaces: The tensor decomposition for infinite-dimensional vector spaces may not be unique, as the tensor product may not be well-defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.814138731378935
      }
    ]
  },
  {
    "representative_text": "Spectral Theorem for Infinite-Dimensional Vector Spaces: The spectral theorem for infinite-dimensional vector spaces may not be applicable, as the eigenvalues and eigenvectors may not be well-defined for infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Spectral Theorem for Infinite-Dimensional Vector Spaces: The spectral theorem for infinite-dimensional vector spaces may not be applicable, as the eigenvalues and eigenvectors may not be well-defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Decomposition for Infinite-Dimensional Vector Spaces: The eigenvalue decomposition for infinite-dimensional vector spaces may not be unique, as the eigenvalues and eigenvectors may not be well-defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.822931316862533
      },
      {
        "text": "The Spectral Theorem for Infinite-Dimensional Vector Spaces: The spectral theorem for infinite-dimensional vector spaces states that a matrix can be diagonalized, but the eigenvalues and eigenvectors may not be well-defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8906548832047427
      },
      {
        "text": "The Eigenvalue Decomposition for Infinite-Dimensional Vector Spaces: The eigenvalue decomposition for infinite-dimensional vector spaces states that a matrix can be decomposed into a sum of eigenvalue matrices, but the eigenvalues and eigenvectors may not be well-defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8917005650931451
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) for Infinite-Dimensional Vector Spaces: The SVD for infinite-dimensional vector spaces may not be applicable, as the singular values may not be well-defined for infinite-dimensional vector spaces.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) for Infinite-Dimensional Vector Spaces: The SVD for infinite-dimensional vector spaces may not be applicable, as the singular values may not be well-defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Singular Value Decomposition (SVD) for Infinite-Dimensional Vector Spaces: The SVD for infinite-dimensional vector spaces states that a matrix can be decomposed into a sum of singular value matrices, but the singular values may not be well-defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9251978464698418
      }
    ]
  },
  {
    "representative_text": "Linear Operator Identities: There are several identities related to linear operators, such as the identity $(I - A)(I + A) = I^2 - A^2$, which can be used to simplify expressions involving linear operators.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Operator Identities: There are several identities related to linear operators, such as the identity $(I - A)(I + A) = I^2 - A^2$, which can be used to simplify expressions involving linear operators.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Schur's Theorem for Invariant Subspaces: This theorem states that if a linear transformation $T$ has an invariant subspace $W$, then the matrix representation of $T$ can be diagonalized with respect to a basis of $W$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Schur's Theorem for Invariant Subspaces: This theorem states that if a linear transformation $T$ has an invariant subspace $W$, then the matrix representation of $T$ can be diagonalized with respect to a basis of $W$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Linear Operator Spectrum: This concept is related to the eigenvalues and states that the linear operator spectrum of a linear transformation $T$ is the set of all eigenvalues of $T$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Linear Operator Spectrum: This concept is related to the eigenvalues and states that the linear operator spectrum of a linear transformation $T$ is the set of all eigenvalues of $T$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Operator Spectrum and Its Properties: This concept is related to the eigenvalues of linear operators and involves understanding the properties of the linear operator spectrum.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8979366713012321
      }
    ]
  },
  {
    "representative_text": "Commutativity of Matrix Multiplication: The property that the order of matrix multiplication does not affect the result, i.e., AB = BA.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Commutativity of Matrix Multiplication: The property that the order of matrix multiplication does not affect the result, i.e., AB = BA.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Commutativity of matrix multiplication: The property that matrix multiplication is commutative, i.e., AB = BA, is a fundamental property of matrix multiplication.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9112173740041893
      }
    ]
  },
  {
    "representative_text": "Cyclic Vectors: A vector v that satisfies Av = λv and Av^T = μv^T, where A is a matrix, λ and μ are scalars, and v is a non-zero vector.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cyclic Vectors: A vector v that satisfies Av = λv and Av^T = μv^T, where A is a matrix, λ and μ are scalars, and v is a non-zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block Matrix Determinants with Non-Orthogonal Bases: A generalization of block matrix determinants that involves using non-orthogonal bases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Block Matrix Determinants with Non-Orthogonal Bases: A generalization of block matrix determinants that involves using non-orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Laplace Expansion for Block Matrices with Non-Orthogonal Bases: This is an extension of Laplace expansion to block matrices with non-orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8812736774133787
      },
      {
        "text": "Block Matrix Determinants with Non-Orthogonal Bases: This is a generalization of block matrix determinants that involves using non-orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8957674441242116
      },
      {
        "text": "Cofactor Expansion with Non-Orthogonal Bases: A method for expanding the determinant of a matrix using non-orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.861061390488342
      }
    ]
  },
  {
    "representative_text": "Sylvester's Law of Inertia for Block Matrices: An extension of Sylvester's law of inertia to block matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Sylvester's Law of Inertia for Block Matrices: An extension of Sylvester's law of inertia to block matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Sylvester's Law of Inertia for Non-Square Matrices: An extension of Sylvester's law of inertia to non-square matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9274773937008403
      },
      {
        "text": "Sylvester's Law of Inertia Properties:",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8808659764252023
      }
    ]
  },
  {
    "representative_text": "Block Matrix Inversion with Non-Orthogonal Bases: A method for inverting a block matrix using non-orthogonal bases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block Matrix Inversion with Non-Orthogonal Bases: A method for inverting a block matrix using non-orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Poincaré Duality for Block Matrices: An extension of Poincaré duality to block matrices, which involves using non-orthogonal bases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Poincaré Duality for Block Matrices: An extension of Poincaré duality to block matrices, which involves using non-orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Multiplicativity with Non-Unitary Eigenvectors: Understanding the properties of eigenvalue multiplicativity with non-unitary eigenvectors is crucial in many applications.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Multiplicativity with Non-Unitary Eigenvectors: Understanding the properties of eigenvalue multiplicativity with non-unitary eigenvectors is crucial in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Connection between Eigenvalues and the Resolvent Matrix: The relationship between eigenvalues and the resolvent matrix is an important concept in understanding the properties of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Connection between Eigenvalues and the Resolvent Matrix: The relationship between eigenvalues and the resolvent matrix is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Connection between Eigenvalues and the Controllability Gramian: The relationship between eigenvalues and the controllability Gramian is an important concept in understanding the properties of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Connection between Eigenvalues and the Controllability Gramian: The relationship between eigenvalues and the controllability Gramian is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Use of the Controllability Gramian in Eigenvalue Decomposition: Understanding the properties of the controllability Gramian and its application in eigenvalue decomposition is essential in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8676158557947133
      },
      {
        "text": "The Connection between Eigenvalues and the Controllability Gramian: This connection is an important concept in understanding the properties of matrices and their eigenvalues, particularly in control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9336547800992003
      },
      {
        "text": "The Controllability Gramian: The controllability Gramian is a matrix that is used to estimate the eigenvalues of a matrix. It is a useful concept in understanding the properties of matrices and their eigenvalues, particularly in control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8591889216262292
      }
    ]
  },
  {
    "representative_text": "The Connection between Eigenvalues and the Moore-Penrose Inverse: The relationship between eigenvalues and the Moore-Penrose inverse is an important concept in understanding the properties of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Connection between Eigenvalues and the Moore-Penrose Inverse: The relationship between eigenvalues and the Moore-Penrose inverse is an important concept in understanding the properties of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Use of the Moore-Penrose Inverse in Matrix Inversion: Understanding the properties of the Moore-Penrose inverse and its application in matrix inversion is essential in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8456292104760285
      },
      {
        "text": "The Moore-Penrose Inverse: The Moore-Penrose inverse is a matrix that is the inverse of a matrix in the sense that it satisfies certain properties. It is a useful concept in understanding the properties of matrices and their eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8793629427096158
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of Matrix Inversion Methods:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Numerical Stability of Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Condition Number of Matrix Inversions:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8815324472475417
      },
      {
        "text": "Approximation and Error Analysis of Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8828139482867878
      }
    ]
  },
  {
    "representative_text": "Matrix Inversion Methods for Large-Scale Systems:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Matrix Inversion Methods for Large-Scale Systems:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Sparse Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8867142864570652
      },
      {
        "text": "Dense Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8948564592613147
      },
      {
        "text": "Specialized Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9002394049418421
      },
      {
        "text": "Hybrid Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8803115349482489
      },
      {
        "text": "Adaptive Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8900752626186308
      }
    ]
  },
  {
    "representative_text": "Semi-Supervised Matrix Inversion Methods:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Semi-Supervised Matrix Inversion Methods:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Conjugate Transpose: The conjugate transpose of a matrix A is a matrix obtained by taking the transpose of A and then taking the complex conjugate of each entry.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Conjugate Transpose: The conjugate transpose of a matrix A is a matrix obtained by taking the transpose of A and then taking the complex conjugate of each entry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Deflation in SVD Computation: Deflation is a technique used in SVD computation to reduce the rank of the matrix by subtracting the range of the first singular value from the original matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Deflation in SVD Computation: Deflation is a technique used in SVD computation to reduce the rank of the matrix by subtracting the range of the first singular value from the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Ill-Conditioned Matrices: Ill-conditioned matrices are matrices that have a large condition number, meaning that small changes in the input can result in large changes in the output.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Ill-Conditioned Matrices: Ill-conditioned matrices are matrices that have a large condition number, meaning that small changes in the input can result in large changes in the output.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "SVD of a Matrix with Non-Uniformly Distributed Singular Values: The SVD of a matrix with non-uniformly distributed singular values can be computed using a different approach than the standard SVD algorithm.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "SVD of a Matrix with Non-Uniformly Distributed Singular Values: The SVD of a matrix with non-uniformly distributed singular values can be computed using a different approach than the standard SVD algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Mathematical Notation for Eigenvalues and Eigenvectors: The notation used to denote eigenvalues and eigenvectors, such as λ and v, is not explicitly mentioned in the provided list.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Mathematical Notation for Eigenvalues and Eigenvectors: The notation used to denote eigenvalues and eigenvectors, such as λ and v, is not explicitly mentioned in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Role of Eigenvalues in Stability Analysis: Eigenvalues play a crucial role in stability analysis of linear systems. However, this is not explicitly mentioned in the provided list.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Role of Eigenvalues in Stability Analysis: Eigenvalues play a crucial role in stability analysis of linear systems. However, this is not explicitly mentioned in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Stability Analysis: Eigenvalues play a crucial role in stability analysis of linear systems. The stability of a linear system can be determined by the signs of the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8500893900465661
      }
    ]
  },
  {
    "representative_text": "The Interlacing Theorem: This theorem states that the eigenvalues of a submatrix of a larger matrix are interlaced with the eigenvalues of the larger matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Interlacing Theorem: This theorem states that the eigenvalues of a submatrix of a larger matrix are interlaced with the eigenvalues of the larger matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Interlacing Property of Eigenvalues: The eigenvalues of a submatrix of a larger matrix are interlaced with the eigenvalues of the larger matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9101393903299397
      },
      {
        "text": "The Interlacing Property of Eigenvalues (Real Eigenvalues): The eigenvalues of a submatrix of a larger matrix are interlaced with the eigenvalues of the larger matrix, even if the larger matrix has real eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9363794073581259
      },
      {
        "text": "The Interlacing Property of Eigenvalues (General Case): The eigenvalues of a submatrix of a larger matrix are interlaced with the eigenvalues of the larger matrix, even if the larger matrix has complex eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.953707788230685
      },
      {
        "text": "Eigenvalue Interlacing Properties:",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8234886583078931
      }
    ]
  },
  {
    "representative_text": "The Spectral Mapping Theorem (Complex Eigenvalues): The spectral mapping theorem states that if λ is an eigenvalue of a matrix A, then the eigenvalues of the matrix e^(A) are e^(λ).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Spectral Mapping Theorem (Complex Eigenvalues): The spectral mapping theorem states that if λ is an eigenvalue of a matrix A, then the eigenvalues of the matrix e^(A) are e^(λ).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Diagonalization of a Matrix using the Spectral Mapping Theorem: The spectral mapping theorem states that the eigenvalues of a matrix A are the same as the eigenvalues of the matrix e^(A), where e^(A) is the matrix exponential.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8593782338780335
      },
      {
        "text": "Spectral Mapping Theorem for Complex Eigenvalues: The spectral mapping theorem states that if λ is an eigenvalue of a matrix A, then the eigenvalues of the matrix e^(A) are e^(λ). This theorem is related to the concept of matrix exponentials.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9057818183843418
      }
    ]
  },
  {
    "representative_text": "The Diagonalization of a Matrix using the Smith Normal Form: The Smith normal form is a diagonal matrix that represents the Smith normal form of a matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Diagonalization of a Matrix using the Smith Normal Form: The Smith normal form is a diagonal matrix that represents the Smith normal form of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Smith Normal Form of a Matrix: The Smith normal form of a matrix is a diagonal matrix that represents the Smith normal form of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9523728819559365
      },
      {
        "text": "The Diagonalization of a Matrix using the Smith Normal Form: The Smith normal form of a matrix can be used to diagonalize the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8990382333344409
      }
    ]
  },
  {
    "representative_text": "The Gelfand's Theorem (Alternative Method): Gelfand's theorem can be used to prove that a matrix can be diagonalized if and only if its characteristic polynomial can be factored into distinct linear factors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Gelfand's Theorem (Alternative Method): Gelfand's theorem can be used to prove that a matrix can be diagonalized if and only if its characteristic polynomial can be factored into distinct linear factors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Gelfand's Theorem (Alternative Method): Gelfand's theorem can be used to prove that a matrix can be diagonalized if and only if its characteristic polynomial can be factored into distinct linear factors. An alternative method is not mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8099144411298236
      }
    ]
  },
  {
    "representative_text": "The Study of Orthogonal Complement of a Subspace in Non-Standard Spaces: This involves studying the orthogonal complement of a subspace in non-standard spaces, such as non-standard metric spaces or non-standard vector spaces.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Study of Orthogonal Complement of a Subspace in Non-Standard Spaces: This involves studying the orthogonal complement of a subspace in non-standard spaces, such as non-standard metric spaces or non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Complement of a Subspace in Non-Standard Spaces: This involves studying the orthogonal complement of a subspace in non-standard spaces, such as non-standard metric spaces or non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9458580707832935
      },
      {
        "text": "Non-standard orthogonal complements: This involves studying the properties of orthogonal complements in non-standard inner product spaces, such as non-standard metric spaces or non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9142341798444963
      },
      {
        "text": "Orthogonal Complement of a Subspace with Respect to a Non-Standard Connection: A study of a generalization of a study of a study of a study of a subspace Theory: This involves studying the study of a subspace theory that involves studying the study of a subspace**: This involves studying the study of a subspace of a subspace of a Subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8209195865890793
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Rank: The rank of an orthogonal matrix is equal to the number of non-zero rows or columns, which is also equal to the number of linearly independent rows or columns.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Rank: The rank of an orthogonal matrix is equal to the number of non-zero rows or columns, which is also equal to the number of linearly independent rows or columns.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Connection between Orthogonal Diagonalization and Orthogonal Complement: Orthogonal diagonalization is closely related to the orthogonal complement, and provides a way to find the orthogonal complement of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Connection between Orthogonal Diagonalization and Orthogonal Complement: Orthogonal diagonalization is closely related to the orthogonal complement, and provides a way to find the orthogonal complement of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Use of Orthogonal Projection Matrices in Quantum Information Processing: This includes studying the use of orthogonal projection matrices in quantum information processing, such as quantum teleportation and quantum error correction.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Use of Orthogonal Projection Matrices in Quantum Information Processing: This includes studying the use of orthogonal projection matrices in quantum information processing, such as quantum teleportation and quantum error correction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Projection Matrices and the Connection to Quantum Information Processing: Using orthogonal projection matrices in quantum information processing, such as quantum teleportation and quantum error correction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9219211835094921
      }
    ]
  },
  {
    "representative_text": "The Use of Orthogonal Projection Matrices in Nonlinear Partial Differential Equations: This includes studying the use of orthogonal projection matrices in nonlinear partial differential equations, such as the Navier-Stokes equations and the Schrödinger equation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Use of Orthogonal Projection Matrices in Nonlinear Partial Differential Equations: This includes studying the use of orthogonal projection matrices in nonlinear partial differential equations, such as the Navier-Stokes equations and the Schrödinger equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal projection matrices and the solution of nonlinear partial differential equations: This involves understanding how orthogonal projection matrices are used to solve nonlinear partial differential equations, such as the Navier-Stokes equations and the Schrödinger equation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.931617125858111
      }
    ]
  },
  {
    "representative_text": "The Relationship between Orthogonal Projection Matrices and Orthogonal Series Expansions in Functional Analysis on Banach Spaces: This involves studying the relationship between orthogonal projection matrices and orthogonal series expansions in functional analysis on Banach spaces, which is important in the study of functional analysis and partial differential equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Relationship between Orthogonal Projection Matrices and Orthogonal Series Expansions in Functional Analysis on Banach Spaces: This involves studying the relationship between orthogonal projection matrices and orthogonal series expansions in functional analysis on Banach spaces, which is important in the study of functional analysis and partial differential equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The relationship between orthogonal projection matrices and orthogonal series expansions in functional analysis on locally compact spaces: This involves studying the relationship between orthogonal projection matrices and orthogonal series expansions in functional analysis on locally compact spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8904424379698396
      },
      {
        "text": "The relationship between orthogonal projection matrices and orthogonal series expansions in operator theory: This involves studying the relationship between orthogonal projection matrices and orthogonal series expansions in operator theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8884495900973143
      },
      {
        "text": "The Relationship between Orthogonal Projection Matrices and Orthogonal Series Expansions in Functional Analysis on Banach Spaces: This involves studying the relationship between orthogonal projection matrices and orthogonal series expansions in functional analysis on Banach spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9616481485711699
      }
    ]
  },
  {
    "representative_text": "Pontryagin's Inequality: This states that for any orthogonal projection matrix P, the spectral radius of P is less than or equal to 1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pontryagin's Inequality: This states that for any orthogonal projection matrix P, the spectral radius of P is less than or equal to 1.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) of Orthogonal Matrices: An orthogonal matrix can also be decomposed into a product of an orthogonal matrix Q and a diagonal matrix D, where D contains the singular values of the original matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) of Orthogonal Matrices: An orthogonal matrix can also be decomposed into a product of an orthogonal matrix Q and a diagonal matrix D, where D contains the singular values of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Matrix Decomposition using SVD: The Singular value decomposition of a decomposition of a decomposition of a matrix SVD is not only the singular value decomposition of Orthogonal Matrix**: The QR Decomposition of orthogonal matrix, which is a matrix, which is a process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8395275095513914
      }
    ]
  },
  {
    "representative_text": "The Connection between the Gram-Schmidt Process and the Fourier Transform: The Gram-Schmidt process is closely related to the Fourier transform, and can be used to compute the Fourier transform of a signal.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Connection between the Gram-Schmidt Process and the Fourier Transform: The Gram-Schmidt process is closely related to the Fourier transform, and can be used to compute the Fourier transform of a signal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Connection between the Gram-Schmidt Process and the Wavelet Transform: The Gram-Schmidt process is closely related to the wavelet transform, and can be used to compute the wavelet transform of a signal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9027975267886572
      }
    ]
  },
  {
    "representative_text": "Relationship between Determinants and the Inverse of a Matrix for Non-Square Matrices: There are some theorems that establish a relationship between the determinant of a matrix and the inverse of the matrix for non-square matrices.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relationship between Determinants and the Inverse of a Matrix for Non-Square Matrices: There are some theorems that establish a relationship between the determinant of a matrix and the inverse of the matrix for non-square matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Non-Diagonal Dominant Matrix and a Non-Orthogonal Basis: There are some properties and relationships that can be derived for determinants of matrices with non-diagonal dominant matrices and non-orthogonal bases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Non-Diagonal Dominant Matrix and a Non-Orthogonal Basis: There are some properties and relationships that can be derived for determinants of matrices with non-diagonal dominant matrices and non-orthogonal bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Jacobi's Formula: This formula is used to calculate the determinant of a matrix by expanding along a diagonal element.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jacobi's Formula: This formula is used to calculate the determinant of a matrix by expanding along a diagonal element.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Perron-Frobenius Theorem: This theorem states that if a matrix has a non-zero determinant, then it has a non-zero eigenvalue with a non-negative real part.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Perron-Frobenius Theorem: This theorem states that if a matrix has a non-zero determinant, then it has a non-zero eigenvalue with a non-negative real part.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Gelfand's Theorem: This theorem states that if a matrix has a non-zero determinant, then it has a non-zero eigenvalue with a non-negative real part.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9220872330788907
      }
    ]
  },
  {
    "representative_text": "Parallel Computing for Matrix Inversion Algorithms: Parallel computing for matrix inversion algorithms is a technique used to speed up the computation of matrix inverses by using multiple processors or cores.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Parallel Computing for Matrix Inversion Algorithms: Parallel computing for matrix inversion algorithms is a technique used to speed up the computation of matrix inverses by using multiple processors or cores.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Parallel Computing for Matrix Inversion Algorithms: Parallel computing for matrix inversion algorithms can be done using techniques such as multi-threading and distributed computing to speed up the computation of matrix inverses.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9358012672437123
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix Product using the Schur Decomposition with a Non- Inversion: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem of a non-standard Eigenvalue of a matrix: Theorem: Theorem for Non-standard Determinant matrix:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix Product using the Schur Decomposition with a Non- Inversion: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem of a non-standard Eigenvalue of a matrix: Theorem: Theorem for Non-standard Determinant matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of Schur Decomposition Computation: The discussion of the numerical stability of Schur decomposition computation, including the concept of numerical stability and its applications in numerical linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Stability of Schur Decomposition Computation: The discussion of the numerical stability of Schur decomposition computation, including the concept of numerical stability and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Advanced Theorem: The Interlacing Theorem for Eigenvalues of Triangular Matrices: This theorem states that the eigenvalues of a triangular matrix are interlaced, i.e., the eigenvalues of the upper triangular part of a matrix are greater than or equal to the eigenvalues of the lower triangular part of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Advanced Theorem: The Interlacing Theorem for Eigenvalues of Triangular Matrices: This theorem states that the eigenvalues of a triangular matrix are interlaced, i.e., the eigenvalues of the upper triangular part of a matrix are greater than or equal to the eigenvalues of the lower triangular part of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of Non-Symmetric Matrix Eigenvalue Computation: The discussion of the numerical stability of eigenvalue computation for non-symmetric matrices, including the concept of numerical stability and its applications in numerical linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Stability of Non-Symmetric Matrix Eigenvalue Computation: The discussion of the numerical stability of eigenvalue computation for non-symmetric matrices, including the concept of numerical stability and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Advanced Theorem: The Sylvester's Theorem for Determinants of Triangular Matrices: This theorem states that the determinant of a triangular matrix can be computed using the eigenvalues of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Advanced Theorem: The Sylvester's Theorem for Determinants of Triangular Matrices: This theorem states that the determinant of a triangular matrix can be computed using the eigenvalues of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Complexity of Determinant Computation for Triangular Matrices: The analysis of the computational complexity of algorithms for computing the determinant of triangular matrices, including the time and space complexity of various algorithms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Complexity of Determinant Computation for Triangular Matrices: The analysis of the computational complexity of algorithms for computing the determinant of triangular matrices, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of Determinant Computation for Triangular Matrices: The discussion of the numerical stability of determinant computation for triangular matrices, including the concept of numerical stability and its applications in numerical linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Stability of Determinant Computation for Triangular Matrices: The discussion of the numerical stability of determinant computation for triangular matrices, including the concept of numerical stability and its applications in numerical linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Complexity of Determinant Computation for Large-Scale Matrices using the Jacobi-Davidson Algorithm: The analysis of the computational complexity of algorithms for computing the determinant of large-scale matrices using the Jacobi-Davidson algorithm, including the time and space complexity of various algorithms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Complexity of Determinant Computation for Large-Scale Matrices using the Jacobi-Davidson Algorithm: The analysis of the computational complexity of algorithms for computing the determinant of large-scale matrices using the Jacobi-Davidson algorithm, including the time and space complexity of various algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Advanced Theorem: Theorem of Vandermondecomposition of theore theore: This theorem for Determinant of theorel: This includes theore theore of theore: This theorem for Determinant eigenvalue computation**: This is not only for Determinantibedral Decomposition of Eigenv",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Advanced Theorem: Theorem of Vandermondecomposition of theore theore: This theorem for Determinant of theorel: This includes theore theore of theore: This theorem for Determinant eigenvalue computation**: This is not only for Determinantibedral Decomposition of Eigenv",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence of the Sum of Vectors: A set of vectors is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution. However, the sum of two linearly independent vectors is not necessarily linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence of the Sum of Vectors: A set of vectors is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution. However, the sum of two linearly independent vectors is not necessarily linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and Boundedness: A set of vectors is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution. However, the boundedness of the vectors does not affect their linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8587826884661248
      },
      {
        "text": "Linear Independence and Symmetric Polynomials: A set of vectors is said to be linearly independent if the only linear combination of the vectors that equals the zero vector is the trivial solution. However, the symmetric polynomials of the vectors can be used to determine linear independence.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8324162430731658
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Complement of a Subspace: The relationship between linear independence and the complement of a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Complement of a Subspace: The relationship between linear independence and the complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Linear Independence and the Orthogonal Complement: The relationship between linear independence and the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9509806159126102
      },
      {
        "text": "Internal Hom: The relationship between linear independence and the complement theorem in infinite-dimensional vector spaces using internal hom.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8243928737611992
      }
    ]
  },
  {
    "representative_text": "The Linear Independence Criterion using the Rank-Nullity Theorem: A criterion for determining whether a set of vectors is linearly independent using the rank-nullity theorem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Linear Independence Criterion using the Rank-Nullity Theorem: A criterion for determining whether a set of vectors is linearly independent using the rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Independence Criterion using the Fundamental Theorem of Algebra: A criterion for determining whether a set of vectors is linearly independent using the fundamental theorem of algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8991326432034545
      },
      {
        "text": "The Linear Independence Criterion using the Schur Complement: A criterion for determining whether a set of vectors is linearly independent using the Schur complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8823907928510029
      },
      {
        "text": "The Linear Independence Criterion using the Eigenvectors: A criterion for determining whether a set of vectors is linearly independent using eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.934113302402185
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Separating Hyperplane Theorem: The relationship between linear independence and the separating hyperplane theorem, particularly in the context of linear programming.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Separating Hyperplane Theorem: The relationship between linear independence and the separating hyperplane theorem, particularly in the context of linear programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Separating Hyperplane Theorem: The relationship between linear independence and the separating hyperplane theorem is a crucial concept to understand in the context of linear programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8517249345238322
      }
    ]
  },
  {
    "representative_text": "Span Theorem Implies Additivity: The Span Theorem states that the span of a set of vectors is equal to the smallest subspace that contains the set. This theorem implies that the span of a union of sets is equal to the union of the spans of the individual sets.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span Theorem Implies Additivity: The Span Theorem states that the span of a set of vectors is equal to the smallest subspace that contains the set. This theorem implies that the span of a union of sets is equal to the union of the spans of the individual sets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Orthogonal Complement Implies Dimension: A set of vectors is linearly independent if and only if the orthogonal complement of the span of the set is equal to the zero vector, which is also equal to the subspace with a basis of the same number of vectors as the original vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Orthogonal Complement Implies Dimension: A set of vectors is linearly independent if and only if the orthogonal complement of the span of the set is equal to the zero vector, which is also equal to the subspace with a basis of the same number of vectors as the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cramer's Rule with Pivoting and Orthogonal Complement: Cramer's rule with pivoting is a method for finding the solution to a system of linear equations by using determinants, using pivoting to avoid division by zero. This method can also be used to find the orthogonal complement of a subspace.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Cramer's Rule with Pivoting and Orthogonal Complement: Cramer's rule with pivoting is a method for finding the solution to a system of linear equations by using determinants, using pivoting to avoid division by zero. This method can also be used to find the orthogonal complement of a subspace.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cramer's Rule with Pivoting and Dimension: Cramer's rule with pivoting is a method for finding the solution to a system of linear equations by using determinants, using pivoting to avoid division by zero. This method can also be used to find the dimension of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9156926680227995
      }
    ]
  },
  {
    "representative_text": "Interpretation of Span as a Subspace: Understanding that the span of a set of vectors is a subspace is crucial, but it's essential to delve deeper into the implications of this concept, such as the existence of orthogonal complements and the dimension of subspaces within the span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Interpretation of Span as a Subspace: Understanding that the span of a set of vectors is a subspace is crucial, but it's essential to delve deeper into the implications of this concept, such as the existence of orthogonal complements and the dimension of subspaces within the span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dimension of the Span of a Linear Combination of Vectors: Developing a deeper understanding of how the dimension of the span of a linear combination of vectors relates to the dimension of the original vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Dimension of the Span of a Linear Combination of Vectors: Developing a deeper understanding of how the dimension of the span of a linear combination of vectors relates to the dimension of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Relationship between the Span of a Set of Vectors and the Span of its Linear Combinations: Investigating the relationship between the span of a set of vectors and the span of its linear combinations, including the implications for understanding the dimension of the span and the existence of a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8887573904361863
      },
      {
        "text": "Dimension of the Span of a Linear Combination of Vectors using Linear Independence: Develop a deeper understanding of how the dimension of the span of a linear combination of vectors relates to the linear independence of the vectors and the dimension of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9264704662665407
      }
    ]
  },
  {
    "representative_text": "Existence of a Basis for a Vector Space with a Non-Trivial Null Space: Exploring the conditions under which a basis for a vector space with a non-trivial null space can be constructed, including the role of linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Existence of a Basis for a Vector Space with a Non-Trivial Null Space: Exploring the conditions under which a basis for a vector space with a non-trivial null space can be constructed, including the role of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of a Basis for a Vector Space with a Trivial Null Space: Investigate the conditions under which a basis for a vector space with a trivial null space can be constructed, including the role of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9524400465984879
      },
      {
        "text": "Existence of a Basis for a Vector Space with a Non-Trivial Null Space: Explore the conditions under which a basis for a vector space with a non-trivial null space can be constructed, including the existence of a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9520065354884464
      }
    ]
  },
  {
    "representative_text": "Subspace Basis Theorem and its Implications: Investigating the implications of the Subspace Basis Theorem, including its use in understanding the dimension of subspaces and the relationship between linear transformations and subspaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Subspace Basis Theorem and its Implications: Investigating the implications of the Subspace Basis Theorem, including its use in understanding the dimension of subspaces and the relationship between linear transformations and subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace Basis Theorem and its Implications for Vector Spaces with Non-Trivial Null Spaces: Investigate the applications and implications of the Subspace Basis Theorem, including its use in understanding the dimension of vector spaces with non-trivial null spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8782509880041008
      }
    ]
  },
  {
    "representative_text": "Weak Basis in Infinite-Dimensional Vector Spaces: This concept generalizes the weak basis to infinite-dimensional vector spaces, which can be useful in understanding the relationship between linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Weak Basis in Infinite-Dimensional Vector Spaces: This concept generalizes the weak basis to infinite-dimensional vector spaces, which can be useful in understanding the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Weak Basis in Infinite-Dimensional Vector Spaces (Internal Hom): This concept generalizes the weak basis to infinite-dimensional vector spaces, which can be useful in understanding the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9473540286177715
      }
    ]
  },
  {
    "representative_text": "Spanning Set for a Vector Space with a Non-Standard Basis: A spanning set for a vector space with a non-standard basis is a set of vectors that spans the vector space, but may not be linearly independent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spanning Set for a Vector Space with a Non-Standard Basis: A spanning set for a vector space with a non-standard basis is a set of vectors that spans the vector space, but may not be linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis of a Vector Space with a Non-Standard Inner Product - Basis Theorem: This theorem states that if a set of vectors is a basis for a complex vector space with a non-standard inner product, then the set is also a basis for the space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Basis of a Vector Space with a Non-Standard Inner Product - Basis Theorem: This theorem states that if a set of vectors is a basis for a complex vector space with a non-standard inner product, then the set is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Norm - Basis Theorem: This theorem states that if a set of vectors is a basis for a normed vector space with a non-standard norm, then the set is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8820302690424309
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Metric - Basis Theorem: This theorem states that if a set of vectors is a basis for a Riemannian manifold with a non-standard metric, then the set is also a basis for the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8973719095200574
      }
    ]
  },
  {
    "representative_text": "Zorn's Lemma: This theorem states that if every chain in a partially ordered set has an upper bound, then the set has a maximal element. This can be applied to show that a vector space has a basis if and only if every linearly independent subset has an upper bound.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Zorn's Lemma: This theorem states that if every chain in a partially ordered set has an upper bound, then the set has a maximal element. This can be applied to show that a vector space has a basis if and only if every linearly independent subset has an upper bound.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Zorn's Lemma: This lemma states that every partially ordered set has a maximal element.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8281661953988093
      }
    ]
  },
  {
    "representative_text": "The Krein-Milman Theorem: This theorem states that a convex subset of a Banach space is the closed convex hull of its extreme points. This can be applied to show that a vector space has a basis if and only if every convex subset of the space is the closed convex hull of its extreme points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Krein-Milman Theorem: This theorem states that a convex subset of a Banach space is the closed convex hull of its extreme points. This can be applied to show that a vector space has a basis if and only if every convex subset of the space is the closed convex hull of its extreme points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Krein-Milman Theorem for Banach Spaces: This theorem states that a convex subset of a Banach space is the closed convex hull of its extreme points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8216135046288711
      }
    ]
  },
  {
    "representative_text": "The Uniform Spanning Theorem: This theorem states that if V is a vector space and S is a set of linearly independent vectors, then S spans V if and only if S is a spanning set for V.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Uniform Spanning Theorem: This theorem states that if V is a vector space and S is a set of linearly independent vectors, then S spans V if and only if S is a spanning set for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Hahn-Banach Extension Theorem: This theorem states that if V is a Banach space and U is a subspace of V, then every linear functional on U can be extended to a linear functional on V.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Hahn-Banach Extension Theorem: This theorem states that if V is a Banach space and U is a subspace of V, then every linear functional on U can be extended to a linear functional on V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Hahn-Banach Separation Theorem: This theorem states that if V is a Banach space and S is a non-empty subset of V, then there exists a non-zero linear functional f on V such that f(x) = 0 for all x in S.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8592802146459877
      },
      {
        "text": "The Separation Theorem for Banach Spaces: This theorem states that if V is a Banach space and S is a non-empty subset of V, then there exists a non-zero linear functional f on V such that f(x) = 0 for all x in S.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9119579976584298
      },
      {
        "text": "The Extension Theorem for Linear Operators: This theorem states that if T: V → W is a linear operator between Banach spaces V and W, and U is a subspace of V, then every linear functional on U can be extended to a linear functional on V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8457621643691164
      },
      {
        "text": "Hahn-Banach Theorem: This theorem states that is a basis",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8064224021061414
      }
    ]
  },
  {
    "representative_text": "The concept of dual basis: A basis of the dual space, which is related to the concept of linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of dual basis: A basis of the dual space, which is related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of Hausdorff dimension: A measure of the complexity of a set in a vector space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of Hausdorff dimension: A measure of the complexity of a set in a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matroid theory: A branch of mathematics that studies the properties of vector spaces and linear transformations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matroid theory: A branch of mathematics that studies the properties of vector spaces and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Combinatorial geometry: A branch of mathematics that studies the properties of vector spaces and linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Combinatorial geometry: A branch of mathematics that studies the properties of vector spaces and linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical linear algebra: A branch of mathematics that studies the numerical solutions of linear algebra problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Numerical linear algebra: A branch of mathematics that studies the numerical solutions of linear algebra problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Linear Algebra: A branch of linear algebra that deals with numerical methods for solving systems of linear equations, which can be used to improve the accuracy of solutions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8537103483998305
      },
      {
        "text": "Computational Linear Algebra: A branch of linear algebra that deals with computational methods for solving systems of linear equations, which can be used to improve the accuracy of solutions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.891694724664272
      },
      {
        "text": "Robust Linear Algebra: A branch of linear algebra that deals with robust methods for solving systems of linear equations, which can be used to improve the accuracy of solutions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8587158958819714
      },
      {
        "text": "Computational Linear Algebra with GPUs: A branch of linear algebra that deals with the use of Graphics Processing Units (GPUs) for solving systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8048192836421688
      },
      {
        "text": "Numerical Linear Algebra with Adaptive Algorithms: A branch of linear algebra that deals with adaptive algorithms for solving systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8807841834700187
      },
      {
        "text": "Numerical Linear Algebra with Adaptive Algorithms: A branch of linear algebra that deals with adaptive algorithms for solving systems of linear equations, such as the adaptive linearization method.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8627126606350297
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Properties of a Fuzzy Vector Space: Fuzzy vector spaces are a type of mathematical structure that combines the concepts of vector spaces and fuzzy logic, and the concept of linear independence in this context is worth exploring.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Properties of a Fuzzy Vector Space: Fuzzy vector spaces are a type of mathematical structure that combines the concepts of vector spaces and fuzzy logic, and the concept of linear independence in this context is worth exploring.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Existence of a Basis in a Fuzzy Vector Space: Exploring the concept of linear independence in fuzzy vector spaces, which are a type of mathematical structure that combines the concepts of vector spaces and fuzzy logic, and the existence of a basis in this context.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9087977830816358
      },
      {
        "text": "The existence of a basis in fuzzy vector spaces: Investigate the concept of a basis in fuzzy vector spaces, which are a type of mathematical structure that combines the concepts of vector spaces and fuzzy logic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8911902718722373
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Existence of a Basis in a p-Adic Space: p-Adic spaces are a type of topological vector space, and the concept of linear independence in this context is worth exploring.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Existence of a Basis in a p-Adic Space: p-Adic spaces are a type of topological vector space, and the concept of linear independence in this context is worth exploring.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Dimension of a Seminormed Space: Seminormed spaces are a type of topological vector space, and the concept of linear independence in this context is worth exploring.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8438782872926602
      },
      {
        "text": "Linear Independence and the Dimension of a p-Adic Vector Space: Exploring the concept of linear independence in p-adic vector spaces, which are a type of topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9275445401317617
      },
      {
        "text": "Linear Independence and the Existence of a Basis in a Seminormed Space: Exploring the concept of linear independence in seminormed spaces, which are a type of topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9289117895773706
      },
      {
        "text": "Linear Independence and the Existence of a Basis in a p-Adic Banach Space: Exploring the concept of linear independence in p-adic Banach spaces, which are a type of infinite-dimensional Banach space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8960845145818315
      },
      {
        "text": "Linear Independence and the Properties of a Topological Vector Space with Non-Standard Topology: Exploring the properties of a topological vector space with non-standard topology and the concept of linear independence in this context.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8795775775022622
      },
      {
        "text": "Linear Independence and the Dimension of a p-Adic Hilbert Space: Exploring the concept of linear independence in p-adic Hilbert spaces, which are a type of infinite-dimensional Hilbert space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8922876187756518
      },
      {
        "text": "Linear independence in Fréchet spaces: Explore the properties of linear independence in Fréchet spaces, which are a type of topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.904031382100517
      },
      {
        "text": "Linear independence in locally convex spaces: Investigate the concept of linear independence in locally convex spaces, which are a type of topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9045870866610067
      },
      {
        "text": "Linear independence in p-adic vector spaces: Investigate the properties of linear independence in p-adic vector spaces, which are a type of topological vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9327511496452228
      },
      {
        "text": "Linear Independence in Locally Convex Spaces with Non-Standard Bases: Investigating the properties of linear independence in locally convex spaces with non-standard bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8788788526785987
      },
      {
        "text": "Linear Independence in p-Adic Vector Spaces with Non-Standard Bases: Investigating the properties of linear independence in p-adic vector spaces with non-standard bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8311408146325221
      },
      {
        "text": "Linear Independence in Fréchet Spaces: Fréchet spaces are topological vector spaces with a topology defined by a family of seminorms. The concept of linear independence in Fréchet spaces is more complex than in finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8225205302192309
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Existence of a Basis in a Riesz Space: Riesz spaces are a type of topological vector space, and the concept of linear independence in this context is worth exploring.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Existence of a Basis in a Riesz Space: Riesz spaces are a type of topological vector space, and the concept of linear independence in this context is worth exploring.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence and the Properties of a Riesz Space: Exploring the properties of a Riesz space, which is a type of topological vector space, and the concept of linear independence in this context.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9280630744952876
      },
      {
        "text": "Linear Independence in Riesz Spaces with Non-Standard Bases: Investigating the properties of linear independence in Riesz spaces with non-standard bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8707582277875356
      }
    ]
  },
  {
    "representative_text": "Span Implies Linear Independence for Vector Spaces with a Non-Trivial Span: The relationship between the span of a non-trivial subspace and linear independence is not explicitly addressed in the provided list.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span Implies Linear Independence for Vector Spaces with a Non-Trivial Span: The relationship between the span of a non-trivial subspace and linear independence is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rank of a Matrix and Linear Independence: The relationship between the rank of a matrix and linear independence is not explicitly addressed in the provided list.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Rank of a Matrix and Linear Independence: The relationship between the rank of a matrix and linear independence is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of a Linear Transformation and Linear Independence for Infinite-Dimensional Vector Spaces: The relationship between the span of a linear transformation and linear independence in infinite-dimensional vector spaces is not explicitly addressed in the provided list.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span of a Linear Transformation and Linear Independence for Infinite-Dimensional Vector Spaces: The relationship between the span of a linear transformation and linear independence in infinite-dimensional vector spaces is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dimension of the Image of a Linear Transformation for Infinite-Dimensional Vector Spaces: The relationship between the dimension of the image of a linear transformation and the original vector space in infinite-dimensional vector spaces is not explicitly addressed in the provided list.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Dimension of the Image of a Linear Transformation for Infinite-Dimensional Vector Spaces: The relationship between the dimension of the image of a linear transformation and the original vector space in infinite-dimensional vector spaces is not explicitly addressed in the provided list.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of the Image of a Linear Transformation for Infinite-Dimensional Vector Spaces: Explore the relationship between the dimension of the image of a linear transformation and the original vector space in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.833475881801768
      }
    ]
  },
  {
    "representative_text": "Spherical Coordinates: Spherical coordinates are a way of representing points in 3D space using latitude, longitude, and radius. This is useful in computer graphics for tasks such as terrain rendering and atmosphere simulation.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spherical Coordinates: Spherical coordinates are a way of representing points in 3D space using latitude, longitude, and radius. This is useful in computer graphics for tasks such as terrain rendering and atmosphere simulation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Polar Coordinates: Polar coordinates are a way of representing points in 2D space using radius and angle. This is useful in computer graphics for tasks such as image filtering and image registration.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Polar Coordinates: Polar coordinates are a way of representing points in 2D space using radius and angle. This is useful in computer graphics for tasks such as image filtering and image registration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Optimization Techniques for Non-Convex Optimization: Non-convex optimization is a type of optimization problem that involves finding the maximum or minimum of a function that does not have a single global maximum or minimum. This is useful in computer graphics for tasks such as lighting, shading, and animation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Optimization Techniques for Non-Convex Optimization: Non-convex optimization is a type of optimization problem that involves finding the maximum or minimum of a function that does not have a single global maximum or minimum. This is useful in computer graphics for tasks such as lighting, shading, and animation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Topology and Persistent Homology: Computational topology is a field that uses linear algebra to study the properties of geometric shapes. Persistent homology is a technique used in computational topology to study the shape of a set of points over time. This is useful in computer graphics for tasks such as object recognition and image registration.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Computational Topology and Persistent Homology: Computational topology is a field that uses linear algebra to study the properties of geometric shapes. Persistent homology is a technique used in computational topology to study the shape of a set of points over time. This is useful in computer graphics for tasks such as object recognition and image registration.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Topology and Persistent Homology for 3D Reconstruction: Computational topology and persistent homology are used to solve problems involving 3D reconstruction in computer graphics and game development. These methods involve using techniques such as persistent homology to solve problems involving the reconstruction of 3D objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8335398390259239
      },
      {
        "text": "Persistent Homology: Persistent homology is a mathematical concept that involves the study of the shape of a set of points over time. This is useful in computer graphics and game development for tasks such as object recognition and image processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8581266494210226
      }
    ]
  },
  {
    "representative_text": "Computational Fluid Dynamics and Navier-Stokes Equations: Computational fluid dynamics is a technique used in computer graphics and game development to simulate the behavior of fluids. Navier-Stokes equations are a type of differential equation that govern the motion of fluids. This is useful in computer graphics for tasks such as fluid dynamics simulations and object recognition.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Computational Fluid Dynamics and Navier-Stokes Equations: Computational fluid dynamics is a technique used in computer graphics and game development to simulate the behavior of fluids. Navier-Stokes equations are a type of differential equation that govern the motion of fluids. This is useful in computer graphics for tasks such as fluid dynamics simulations and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Fluid Dynamics and Navier-Stokes Equations for Fluid Dynamics Simulations: Computational fluid dynamics and Navier-Stokes equations are used to solve problems involving fluid dynamics simulations in computer graphics and game development. These methods involve using techniques such as Navier-Stokes equations to solve problems involving fluid dynamics simulations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8835685994748532
      },
      {
        "text": "Computational Fluid Dynamics and Its Applications: Computational fluid dynamics is a technique used in computer graphics and game development to simulate the behavior of fluids and gases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8419100480447193
      }
    ]
  },
  {
    "representative_text": "Neural Tangent Function (NTF): A mathematical function that relates the neural network to a linear function, useful for understanding the behavior of neural networks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Neural Tangent Function (NTF): A mathematical function that relates the neural network to a linear function, useful for understanding the behavior of neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Deep Learning: Neural Tangent Function (NTF): A mathematical function that relates the neural network to a linear function, useful for understanding the behavior of neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8817138870468358
      }
    ]
  },
  {
    "representative_text": "Softmax Activation Function: A non-linear activation function commonly used in neural networks for multi-class classification problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Softmax Activation Function: A non-linear activation function commonly used in neural networks for multi-class classification problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Deep Learning: Softmax Activation Function: A non-linear activation function commonly used in neural networks for multi-class classification problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8865833517011982
      }
    ]
  },
  {
    "representative_text": "ReLU (Rectified Linear Unit) Activation Function: A non-linear activation function commonly used in neural networks for feature extraction and classification tasks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "ReLU (Rectified Linear Unit) Activation Function: A non-linear activation function commonly used in neural networks for feature extraction and classification tasks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Deep Learning: ReLU (Rectified Linear Unit) Activation Function: A non-linear activation function commonly used in neural networks for feature extraction and classification tasks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8993302864309434
      }
    ]
  },
  {
    "representative_text": "Dropout Techniques: Methods for randomly dropping out neurons during training to prevent overfitting.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dropout Techniques: Methods for randomly dropping out neurons during training to prevent overfitting.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Regularization Techniques: Methods for adding a penalty term to the loss function to prevent overfitting.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Regularization Techniques: Methods for adding a penalty term to the loss function to prevent overfitting.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gradient Descent Variants: Variants of gradient descent, such as Adam, RMSProp, and SGD, that improve convergence rates and stability.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gradient Descent Variants: Variants of gradient descent, such as Adam, RMSProp, and SGD, that improve convergence rates and stability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Batch Normalization Variants: Variants of batch normalization, such as layer normalization and instance normalization, that improve stability and performance.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Batch Normalization Variants: Variants of batch normalization, such as layer normalization and instance normalization, that improve stability and performance.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Attention Mechanisms: Mechanisms that allow neural networks to focus on specific parts of the input data when making predictions.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Attention Mechanisms: Mechanisms that allow neural networks to focus on specific parts of the input data when making predictions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Graph Attention Networks: Neural networks that use attention mechanisms to focus on specific parts of the input data when making predictions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8317410512047163
      }
    ]
  },
  {
    "representative_text": "Graph Neural Networks: Neural networks that operate on graph-structured data, where each node in the graph represents a data point and edges represent relationships between data points.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Graph Neural Networks: Neural networks that operate on graph-structured data, where each node in the graph represents a data point and edges represent relationships between data points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Graph Convolutional Networks: Neural networks that use graph convolution to learn features from graph-structured data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8304616796239559
      }
    ]
  },
  {
    "representative_text": "Spectral Graph Convolution: A type of graph convolution that uses spectral methods to compute the convolution of graph nodes.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spectral Graph Convolution: A type of graph convolution that uses spectral methods to compute the convolution of graph nodes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Chebyshev Polynomials: A type of polynomial used in spectral graph convolution to compute the convolution of graph nodes.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Chebyshev Polynomials: A type of polynomial used in spectral graph convolution to compute the convolution of graph nodes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Graph Autoencoders: Neural networks that learn to represent graph data in a lower-dimensional space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Graph Autoencoders: Neural networks that learn to represent graph data in a lower-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Graph Neural Network Architectures: Architectures for building graph neural networks, such as Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs).",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Graph Neural Network Architectures: Architectures for building graph neural networks, such as Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs).",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Graph Neural Network Optimization: Methods for optimizing graph neural networks, such as gradient descent and stochastic gradient descent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Graph Neural Network Optimization: Methods for optimizing graph neural networks, such as gradient descent and stochastic gradient descent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Graph Neural Network Evaluation Metrics: Metrics for evaluating the performance of graph neural networks, such as accuracy and precision.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Graph Neural Network Evaluation Metrics: Metrics for evaluating the performance of graph neural networks, such as accuracy and precision.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Graph Neural Network Applications: Applications of graph neural networks in various fields, such as computer vision, natural language processing, and recommender systems.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Graph Neural Network Applications: Applications of graph neural networks in various fields, such as computer vision, natural language processing, and recommender systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Deep Learning: Graph Neural Network Applications: Applications of graph neural networks in various fields, such as computer vision, natural language processing, and recommender systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9057205239216355
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Physics: Linear algebra is used in physics tasks such as calculating eigenvalues and eigenvectors, solving systems of linear equations, and representing physical systems using matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 10,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Physics: Linear algebra is used in physics tasks such as calculating eigenvalues and eigenvectors, solving systems of linear equations, and representing physical systems using matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Engineering: Linear algebra is used in engineering tasks such as designing electronic circuits, analyzing mechanical systems, and solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8257196199280676
      },
      {
        "text": "The use of linear algebra in physics: Linear algebra is used in a wide range of applications in physics, including quantum mechanics, electromagnetism, and general relativity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.861802035891388
      },
      {
        "text": "Multibody Systems: Linear algebra is used to model and simulate the motion of complex systems, such as robotic arms or vehicles.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8149157336023309
      },
      {
        "text": "Physics: Linear algebra is used in physics to describe the behavior of physical systems, including the motion of objects and the behavior of fields.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9208050454879135
      },
      {
        "text": "Biology: Linear algebra is used in biology to describe the behavior of biological systems, including the structure of molecules and the behavior of populations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8064839614922958
      },
      {
        "text": "Economics: Linear algebra is used in economics to describe the behavior of economic systems, including the behavior of markets and the behavior of firms.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8318302382376572
      },
      {
        "text": "Engineering: Linear algebra is used in engineering for tasks such as design optimization, structural analysis, and control systems.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8953038192927398
      },
      {
        "text": "Mathematics: Linear algebra is used in mathematics to solve systems of linear equations and to study the properties of matrices.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8351300347620088
      },
      {
        "text": "Physics with Linear Systems: Linear systems are a type of system that can be modeled using linear equations. Linear algebra is used in physics to describe the behavior of physical systems, including the motion of objects and the behavior of fields.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8347785133843093
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Machine Learning with Non-Stationary Signals: Linear algebra is used in machine learning tasks with non-stationary signals such as time-series classification, wavelet-based classification, and spectral-based classification.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Machine Learning with Non-Stationary Signals: Linear algebra is used in machine learning tasks with non-stationary signals such as time-series classification, wavelet-based classification, and spectral-based classification.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Optimization Techniques:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Optimization Techniques:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Advanced Control Theory:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Advanced Control Theory:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Optimization:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Optimization:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Machine Learning:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Machine Learning:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra and Computer Vision:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8181850374797008
      },
      {
        "text": "Linear Algebra and Robotics:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8234942878418727
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Signal Processing:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Signal Processing:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Data Analysis:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Data Analysis:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Secure Communication Protocols with Quantum-Resistant Cryptography: This includes protocols that use quantum-resistant cryptography to provide secure communication over high-speed and high-throughput networks.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Secure Communication Protocols with Quantum-Resistant Cryptography: This includes protocols that use quantum-resistant cryptography to provide secure communication over high-speed and high-throughput networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Linear Independence and Span of Complex Vectors: The properties and behavior of linear independence and span for complex vectors, including their relationships with the matrix exponential and the Jordan canonical form.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Independence and Span of Complex Vectors: The properties and behavior of linear independence and span for complex vectors, including their relationships with the matrix exponential and the Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Basis and Dimension of Complex Vector Spaces: The properties and behavior of basis and dimension for complex vector spaces, including their relationships with the matrix exponential and the Jordan canonical form.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Basis and Dimension of Complex Vector Spaces: The properties and behavior of basis and dimension for complex vector spaces, including their relationships with the matrix exponential and the Jordan canonical form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product Spaces: An inner product space is a vector space equipped with an inner product. Inner product spaces are essential in the study of linear transformations and their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner Product Spaces: An inner product space is a vector space equipped with an inner product. Inner product spaces are essential in the study of linear transformations and their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue and Eigenvector Decomposition: This concept is related to the diagonalization of linear operators and involves decomposing a linear operator into a sum of diagonal operators.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvalue and Eigenvector Decomposition: This concept is related to the diagonalization of linear operators and involves decomposing a linear operator into a sum of diagonal operators.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Subspace Decomposition: This concept is related to the eigenvectors and eigenvalues of a matrix. It involves decomposing a vector space into a direct sum of subspaces, where each subspace is associated with an eigenvector of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8209553875311959
      }
    ]
  },
  {
    "representative_text": "Theorem of Invariance of Bilinear Forms: This theorem states that the bilinear form of a linear transformation is preserved under the transformation, which is essential in understanding the properties of bilinear forms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem of Invariance of Bilinear Forms: This theorem states that the bilinear form of a linear transformation is preserved under the transformation, which is essential in understanding the properties of bilinear forms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Invariant Subspace Theorem for Linear Operators: This theorem generalizes the invariant subspace theorem to linear operators and is essential in understanding the behavior of invariant subspaces under linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Invariant Subspace Theorem for Linear Operators: This theorem generalizes the invariant subspace theorem to linear operators and is essential in understanding the behavior of invariant subspaces under linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Invariant Subspace Theorem for Linear Operators on Infinite-Dimensional Spaces: This theorem generalizes the invariant subspace theorem to linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8765640011431877
      },
      {
        "text": "The Linear Operator Invariant Subspace Theorem for Infinite-Dimensional Spaces: The theorem generalizing the invariant subspace theorem to linear operators on infinite-dimensional spaces is an extension of the theorem on finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8843480072511977
      }
    ]
  },
  {
    "representative_text": "The Schur's Theorem for Linear Operators: This theorem generalizes Schur's theorem to linear operators and is essential in understanding the behavior of diagonalizable linear operators.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Schur's Theorem for Linear Operators: This theorem generalizes Schur's theorem to linear operators and is essential in understanding the behavior of diagonalizable linear operators.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Linear Operator Range and Its Properties: This concept is related to the image of linear operators and involves understanding the properties of the linear operator range.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Operator Range and Its Properties: This concept is related to the image of linear operators and involves understanding the properties of the linear operator range.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Linear Operator Nullity and Its Properties: This concept is related to the kernel of linear operators and involves understanding the properties of the linear operator nullity.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Operator Nullity and Its Properties: This concept is related to the kernel of linear operators and involves understanding the properties of the linear operator nullity.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Linear Operator Rank-Nullity Consequence: This consequence is related to the rank-nullity theorem and involves understanding the properties of the linear operator rank-nullity theorem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Linear Operator Rank-Nullity Consequence: This consequence is related to the rank-nullity theorem and involves understanding the properties of the linear operator rank-nullity theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Operator Rank-Nullity Consequence for Infinite-Dimensional Spaces: The consequence of the rank-nullity theorem for linear operators on infinite-dimensional spaces is an extension of the consequence on finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8343307742659924
      }
    ]
  },
  {
    "representative_text": "The Linear Operator Theorem of Invariance of the Determinant: This theorem states that the determinant of a linear transformation is preserved under the transformation, which is essential in understanding the properties of the determinant.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Operator Theorem of Invariance of the Determinant: This theorem states that the determinant of a linear transformation is preserved under the transformation, which is essential in understanding the properties of the determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Infinite-Dimensional Linear Transformations: A study of linear transformations between infinite-dimensional vector spaces, which can be used to study the properties of infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Infinite-Dimensional Linear Transformations: A study of linear transformations between infinite-dimensional vector spaces, which can be used to study the properties of infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Vector Space Homomorphisms: The study of linear transformations between vector spaces, which can be used to represent linear transformations and other mathematical objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8291981945763904
      },
      {
        "text": "Linear Transformations and Operator Algebras: The study of linear transformations in the context of operator algebras, including the use of operator algebras to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8333602082883313
      },
      {
        "text": "Linear Transformations and Functional Analysis: The application of functional analysis to linear transformations, including the use of functional analysis to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8658252209750649
      },
      {
        "text": "Linear Transformations and Topological Vector Spaces: The study of linear transformations in the context of topological vector spaces, including the use of topological vector spaces to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9132815306484398
      },
      {
        "text": "Linear Transformations and Banach Spaces: The application of Banach spaces to linear transformations, including the use of Banach spaces to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8826648773068935
      },
      {
        "text": "Linear Transformations and Complete Metrics: The study of linear transformations in the context of complete metrics, including the use of complete metrics to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8199479551783815
      },
      {
        "text": "Linear Transformations on Banach Spaces: A study of linear transformations on Banach spaces, which are complete normed vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9015218112170558
      }
    ]
  },
  {
    "representative_text": "Functional Analysis: A branch of mathematics that studies the properties of vector spaces and linear transformations in a functional analysis framework, which can be used to study the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Functional Analysis: A branch of mathematics that studies the properties of vector spaces and linear transformations in a functional analysis framework, which can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Measure Theory: A branch of mathematics that studies the properties of measure spaces and linear transformations, which can be used to study the properties of linear transformations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Measure Theory: A branch of mathematics that studies the properties of measure spaces and linear transformations, which can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hilbert Spaces: A branch of mathematics that studies the properties of infinite-dimensional vector spaces, which can be used to study the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hilbert Spaces: A branch of mathematics that studies the properties of infinite-dimensional vector spaces, which can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Graph Theory: A branch of mathematics that studies the properties of graphs and linear transformations, which can be used to study the properties of linear transformations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spectral Graph Theory: A branch of mathematics that studies the properties of graphs and linear transformations, which can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Trace of a Linear Transformation: The trace of a linear transformation is the sum of the diagonal elements of its matrix representation. Understanding the properties of the trace can provide additional insights into the behavior of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Trace of a Linear Transformation: The trace of a linear transformation is the sum of the diagonal elements of its matrix representation. Understanding the properties of the trace can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and the Trace: The trace of a matrix can be used to analyze the properties of linear transformations, including the rank and nullity of a transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8705776353455792
      },
      {
        "text": "Orthogonal Projection Matrices and the Trace: The trace of an orthogonal projection matrix is related to the dimension of the projected subspace. This property can be used to derive various results and theorems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8061703566757475
      },
      {
        "text": "Linear Transformation and Trace: Exploring the trace of a linear transformation, which provides a way to determine the sum of the diagonal elements of the transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8876010313862144
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Polarization Identity: The polarization identity is a relationship between the kernel and image of a linear transformation, which can be used to derive other important results.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Polarization Identity: The polarization identity is a relationship between the kernel and image of a linear transformation, which can be used to derive other important results.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Polarization Identity: A fundamental identity in linear algebra that provides a way to relate the kernel and image of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.922486612203749
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Isomorphism between V and W: The isomorphism between V and W is a relationship between two vector spaces that preserves their linear structure. Understanding the properties of the isomorphism can provide additional insights into the behavior of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Isomorphism between V and W: The isomorphism between V and W is a relationship between two vector spaces that preserves their linear structure. Understanding the properties of the isomorphism can provide additional insights into the behavior of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Commutativity and Associativity of Matrix Multiplication in Higher Dimensions: This involves the study of the commutativity and associativity of matrix multiplication in higher dimensions, which is not explicitly covered in the existing points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Commutativity and Associativity of Matrix Multiplication in Higher Dimensions: This involves the study of the commutativity and associativity of matrix multiplication in higher dimensions, which is not explicitly covered in the existing points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Norms and their Properties in Higher Dimensions: This involves the study of matrix norms in higher dimensions, which includes the study of matrix norms, their properties, and theorems related to matrix norms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Norms and their Properties in Higher Dimensions: This involves the study of matrix norms in higher dimensions, which includes the study of matrix norms, their properties, and theorems related to matrix norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Decomposition and its Applications: This involves the study of tensor decomposition and its applications in various fields, such as machine learning, computer vision, and signal processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tensor Decomposition and its Applications: This involves the study of tensor decomposition and its applications in various fields, such as machine learning, computer vision, and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix using the Strassen Algorithm with Non-Integer Powers and Complex Entries: This involves the study of the Strassen algorithm with non-integer powers and complex entries, which is a generalization of the Strassen algorithm.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix using the Strassen Algorithm with Non-Integer Powers and Complex Entries: This involves the study of the Strassen algorithm with non-integer powers and complex entries, which is a generalization of the Strassen algorithm.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination for Block Matrices: An extension of Gaussian elimination to block matrices, which involves using elementary row operations to transform the block matrix into upper triangular form.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination for Block Matrices: An extension of Gaussian elimination to block matrices, which involves using elementary row operations to transform the block matrix into upper triangular form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cholesky Decomposition for Non-Symmetric Matrices: An extension of Cholesky decomposition to non-symmetric matrices, which involves using the properties of the matrix and the eigenvalues.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cholesky Decomposition for Non-Symmetric Matrices: An extension of Cholesky decomposition to non-symmetric matrices, which involves using the properties of the matrix and the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Spectral Theorem for Block Matrices: A theorem that states that every block matrix can be diagonalized using a similarity transformation.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 11,
    "detailed_sources": [
      {
        "text": "The Spectral Theorem for Block Matrices: A theorem that states that every block matrix can be diagonalized using a similarity transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Theorem of Eigenvalues for Block Matrices: A theorem that states that every block matrix has a set of eigenvalues that are the eigenvalues of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8845522381274732
      },
      {
        "text": "The Theorem of Eigenvectors for Block Matrices: A theorem that states that every block matrix has a set of eigenvectors that are the eigenvectors of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9370829594735489
      },
      {
        "text": "The Theorem of Eigenvalues for Block Diagonal Matrices: A theorem that states that every block diagonal matrix has a set of eigenvalues that are the eigenvalues of the corresponding diagonal blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9294640604884313
      },
      {
        "text": "The Theorem of Eigenvectors for Block Diagonal Matrices: A theorem that states that every block diagonal matrix has a set of eigenvectors that are the eigenvectors of the corresponding diagonal blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9312066057534385
      },
      {
        "text": "The Spectral Theorem for Block Matrices using the Schur Complement: This is a theorem that states that every block matrix can be diagonalized using a similarity transformation and the Schur complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8411597793837019
      },
      {
        "text": "The Theorem of Eigenvalues for Block Diagonal Matrices using the Schur Complement: This is a theorem that states that every block diagonal matrix has a set of eigenvalues that are the eigenvalues of the corresponding diagonal blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9287020756709514
      },
      {
        "text": "The Theorem of Eigenvectors for Block Diagonal Matrices using the Schur Complement: This is a theorem that states that every block diagonal matrix has a set of eigenvectors that are the eigenvectors of the corresponding diagonal blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9506514115462972
      },
      {
        "text": "The Theorem of Eigenvalues for Block Matrices using the Schur Complement: This is a theorem that states that every block matrix has a set of eigenvalues that are the eigenvalues of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9385927748268423
      },
      {
        "text": "The Theorem of Eigenvectors for Block Matrices using the Schur Complement: This is a theorem that states that every block matrix has a set of eigenvectors that are the eigenvectors of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9525841500693946
      },
      {
        "text": "The Spectral Theorem for Block Matrices using the Schur Complement (Generalized): A theorem that states that every block matrix can be diagonalized using a similarity transformation and the Schur complement.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8747240336796045
      }
    ]
  },
  {
    "representative_text": "The Theorem of Inverses for Block Matrices: A theorem that states that every block matrix has an inverse if and only if its Schur complement is invertible.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Theorem of Inverses for Block Matrices: A theorem that states that every block matrix has an inverse if and only if its Schur complement is invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Theorem of Inverses for Non-Symmetric Matrices: A theorem that states that every non-symmetric matrix has an inverse if and only if its Schur complement is invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8887698459131073
      },
      {
        "text": "The Theorem of Inverses for Block Matrices using the Schur Complement: This is a theorem that states that every block matrix has an inverse if and only if its Schur complement is invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9373750080714283
      }
    ]
  },
  {
    "representative_text": "The Theorem of Determinants for Block Matrices: A theorem that states that every block matrix has a determinant that is the product of the determinants of the corresponding submatrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Theorem of Determinants for Block Matrices: A theorem that states that every block matrix has a determinant that is the product of the determinants of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Theorem of Determinants for Block Diagonal Matrices: A theorem that states that every block diagonal matrix has a determinant that is the product of the determinants of the corresponding diagonal blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.927885892657553
      },
      {
        "text": "The Theorem of Determinants for Block Diagonal Matrices using the Schur Complement: This is a theorem that states that every block diagonal matrix has a determinant that is the product of the determinants of the corresponding diagonal blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9278051836797696
      },
      {
        "text": "The Theorem of Determinants for Block Matrices using the Schur Complement: This is a theorem that states that every block matrix has a determinant that is the product of the determinants of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9449111736571871
      }
    ]
  },
  {
    "representative_text": "The Theorem of Cofactors for Block Matrices: A theorem that states that every block matrix has a set of cofactors that are the cofactors of the corresponding submatrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Theorem of Cofactors for Block Matrices: A theorem that states that every block matrix has a set of cofactors that are the cofactors of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Theorem of Cofactors for Non-Symmetric Matrices: A theorem that states that every non-symmetric matrix has a set of cofactors that are the cofactors of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8931398188908024
      }
    ]
  },
  {
    "representative_text": "The Theorem of Eigenvalues for Non-Symmetric Matrices: A theorem that states that every non-symmetric matrix has a set of eigenvalues that are the eigenvalues of the corresponding submatrices.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Theorem of Eigenvalues for Non-Symmetric Matrices: A theorem that states that every non-symmetric matrix has a set of eigenvalues that are the eigenvalues of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Theorem of Eigenvectors for Non-Symmetric Matrices: A theorem that states that every non-symmetric matrix has a set of eigenvectors that are the eigenvectors of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9542322302684025
      },
      {
        "text": "The Theorem of Eigenvalues for Non-Symmetric Matrices using the Schur Complement: This is a theorem that states that every non-symmetric matrix has a set of eigenvalues that are the eigenvalues of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9225320055658057
      },
      {
        "text": "The Theorem of Eigenvectors for Non-Symmetric Matrices using the Schur Complement: This is a theorem that states that every non-symmetric matrix has a set of eigenvectors that are the eigenvectors of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9562444707032349
      }
    ]
  },
  {
    "representative_text": "The Theorem of Determinants for Non-Symmetric Matrices: A theorem that states that every non-symmetric matrix has a determinant that is the product of the determinants of the corresponding submatrices.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Theorem of Determinants for Non-Symmetric Matrices: A theorem that states that every non-symmetric matrix has a determinant that is the product of the determinants of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Theorem of Determinants for Non-Symmetric Matrices using the Schur Complement: This is a theorem that states that every non-symmetric matrix has a determinant that is the product of the determinants of the corresponding submatrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9191081420566991
      }
    ]
  },
  {
    "representative_text": "The Theorem of Inverses for Block Diagonal Matrices: A theorem that states that every block diagonal matrix has an inverse if and only if its diagonal blocks are invertible.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Theorem of Inverses for Block Diagonal Matrices: A theorem that states that every block diagonal matrix has an inverse if and only if its diagonal blocks are invertible.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Interlacing Theorem: This theorem states that if A and B are square matrices of the same size, and λ is an eigenvalue of A, then λ is an eigenvalue of B if and only if λ is an eigenvalue of A.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Interlacing Theorem: This theorem states that if A and B are square matrices of the same size, and λ is an eigenvalue of A, then λ is an eigenvalue of B if and only if λ is an eigenvalue of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Interpolation: This topic deals with methods for interpolating eigenvalues of a matrix, such as the Lagrange interpolation formula.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Interpolation: This topic deals with methods for interpolating eigenvalues of a matrix, such as the Lagrange interpolation formula.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Nakamura's Theorem: This theorem states that a matrix is invertible if and only if its determinant is non-zero and its characteristic polynomial has no repeated roots.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Nakamura's Theorem: This theorem states that a matrix is invertible if and only if its determinant is non-zero and its characteristic polynomial has no repeated roots.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Nakamura's Theorem: Discuss Nakamura's theorem, which states that a matrix is invertible if and only if its determinant is non-zero and its characteristic polynomial has no repeated roots.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.949639736480983
      }
    ]
  },
  {
    "representative_text": "Computational Complexity of Matrix Inversion Methods: This is an important topic in numerical linear algebra, as many matrix inversion methods have different computational complexities. Understanding the computational complexity of a matrix inversion method is crucial in determining its performance and scalability.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Complexity of Matrix Inversion Methods: This is an important topic in numerical linear algebra, as many matrix inversion methods have different computational complexities. Understanding the computational complexity of a matrix inversion method is crucial in determining its performance and scalability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hybrid Matrix Inversion Methods: These are matrix inversion methods that combine multiple techniques, such as Gaussian elimination and QR decomposition, to achieve better performance and accuracy. Hybrid matrix inversion methods are important in many applications, such as numerical linear algebra and control theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hybrid Matrix Inversion Methods: These are matrix inversion methods that combine multiple techniques, such as Gaussian elimination and QR decomposition, to achieve better performance and accuracy. Hybrid matrix inversion methods are important in many applications, such as numerical linear algebra and control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Methods for SVD Computation: There are several computational methods for computing the SVD of a matrix, including the QR algorithm, Householder transformations, Givens rotations, and the Lanczos algorithm. Each method has its own strengths and weaknesses, and the choice of method depends on the specific problem and the characteristics of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Methods for SVD Computation: There are several computational methods for computing the SVD of a matrix, including the QR algorithm, Householder transformations, Givens rotations, and the Lanczos algorithm. Each method has its own strengths and weaknesses, and the choice of method depends on the specific problem and the characteristics of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Role of Eigenvectors in Stability Analysis: Eigenvectors play a crucial role in stability analysis of linear systems. The stability of a linear system can be determined by the signs of the eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Role of Eigenvectors in Stability Analysis: Eigenvectors play a crucial role in stability analysis of linear systems. The stability of a linear system can be determined by the signs of the eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Role of Eigenvectors in Linear Transformations: Eigenvectors play a crucial role in understanding the linear transformations represented by a matrix. They can be used to analyze the behavior of linear systems and determine the stability of a system.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8925817743737919
      }
    ]
  },
  {
    "representative_text": "The Relationship between Eigenvalues and the Matrix Exponential: The matrix exponential e^(A) is a fundamental concept in linear algebra, and its relationship with eigenvalues is not explicitly mentioned in the provided points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Relationship between Eigenvalues and the Matrix Exponential: The matrix exponential e^(A) is a fundamental concept in linear algebra, and its relationship with eigenvalues is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Concept of a Matrix having a Complex Conjugate Pair of Eigenvalues: A matrix has a complex conjugate pair of eigenvalues if and only if it is real symmetric. This property is not explicitly mentioned in the provided points.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Concept of a Matrix having a Complex Conjugate Pair of Eigenvalues: A matrix has a complex conjugate pair of eigenvalues if and only if it is real symmetric. This property is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Diagonalization of a Matrix using the Smith Normal Form (Alternative Method): The Smith normal form of a matrix can be used to diagonalize the matrix, but an alternative method is not mentioned in the provided points.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Diagonalization of a Matrix using the Smith Normal Form (Alternative Method): The Smith normal form of a matrix can be used to diagonalize the matrix, but an alternative method is not mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Diagonalization of a Matrix using the Jordan Decomposition (Alternative Method): The Jordan decomposition of a matrix can be used to diagonalize the matrix, but an alternative method is not mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.861014941198674
      },
      {
        "text": "The Diagonalization of a Matrix using the Block Diagonalization (Alternative Method): The block diagonalization of a matrix can be used to diagonalize the matrix, but an alternative method is not mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.918017799372747
      },
      {
        "text": "The Diagonalization of a Matrix using the QR Algorithm (Alternative Method): The QR algorithm can be used to diagonalize a matrix, even if the matrix is not symmetric. This is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8399275672426554
      }
    ]
  },
  {
    "representative_text": "The Power Method (Alternative Method): The power method can be used to find the dominant eigenvalue and eigenvector of a matrix, even if the matrix is not symmetric. This is not explicitly mentioned in the provided points.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Power Method (Alternative Method): The power method can be used to find the dominant eigenvalue and eigenvector of a matrix, even if the matrix is not symmetric. This is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Interlacing Property of Eigenvalues (General Case): The eigenvalues of a submatrix of a larger matrix are interlaced with the eigenvalues of the larger matrix, even if the larger matrix has complex eigenvalues. This property is not explicitly mentioned in the provided points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Interlacing Property of Eigenvalues (General Case): The eigenvalues of a submatrix of a larger matrix are interlaced with the eigenvalues of the larger matrix, even if the larger matrix has complex eigenvalues. This property is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Relationship between Eigenvalues and the Matrix Determinant: The determinant of a matrix can be expressed in terms of its eigenvalues, but the relationship between eigenvalues and the determinant is not explicitly mentioned in the provided points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Relationship between Eigenvalues and the Matrix Determinant: The determinant of a matrix can be expressed in terms of its eigenvalues, but the relationship between eigenvalues and the determinant is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Relationship between Eigenvalues and the Matrix Inverse: The inverse of a matrix can be expressed in terms of its eigenvalues and eigenvectors, but the relationship between eigenvalues and the inverse is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9005146914313278
      }
    ]
  },
  {
    "representative_text": "The Relationship between Eigenvalues and the Matrix Trace: The trace of a matrix is equal to the sum of its eigenvalues, but the relationship between eigenvalues and the trace is not explicitly mentioned in the provided points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Relationship between Eigenvalues and the Matrix Trace: The trace of a matrix is equal to the sum of its eigenvalues, but the relationship between eigenvalues and the trace is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Diagonalization of a Matrix using the Spectral Mapping Theorem: The spectral mapping theorem states that the eigenvalues of a matrix A are the same as the eigenvalues of the matrix e^(A), where e^(A) is the matrix exponential. This theorem can be used to diagonalize a matrix, but it is not explicitly mentioned in the provided points.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Diagonalization of a Matrix using the Spectral Mapping Theorem: The spectral mapping theorem states that the eigenvalues of a matrix A are the same as the eigenvalues of the matrix e^(A), where e^(A) is the matrix exponential. This theorem can be used to diagonalize a matrix, but it is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Diagonalization of a Matrix using the Inverse of a Matrix: The inverse of a matrix can be expressed in terms of its eigenvalues and eigenvectors, and this can be used to diagonalize a matrix. However, this is not explicitly mentioned in the provided points.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Diagonalization of a Matrix using the Inverse of a Matrix: The inverse of a matrix can be expressed in terms of its eigenvalues and eigenvectors, and this can be used to diagonalize a matrix. However, this is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Diagonalization Theorem for Non-Symmetric Matrices: While the Diagonalization Theorem is typically stated for symmetric matrices, there are also versions of the theorem that can be applied to non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Diagonalization Theorem for Non-Symmetric Matrices: While the Diagonalization Theorem is typically stated for symmetric matrices, there are also versions of the theorem that can be applied to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvector-Pseudoinverse: This concept is related to the pseudoinverse of a matrix, which is a generalization of the inverse of a matrix. It is used to extend the concept of eigenvectors to non-invertible matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvector-Pseudoinverse: This concept is related to the pseudoinverse of a matrix, which is a generalization of the inverse of a matrix. It is used to extend the concept of eigenvectors to non-invertible matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Pseudoinverse of a Matrix: The pseudoinverse of a matrix is a generalization of the inverse of a matrix. It is used to extend the concept of eigenvectors to non-invertible matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8942709224875314
      }
    ]
  },
  {
    "representative_text": "Generalized Eigenvalues: These are eigenvalues that are associated with a generalized eigenvector of a matrix. They are used to extend the concept of eigenvectors to matrices that are not diagonalizable.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Generalized Eigenvalues: These are eigenvalues that are associated with a generalized eigenvector of a matrix. They are used to extend the concept of eigenvectors to matrices that are not diagonalizable.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Deflation for Non-Symmetric Matrices: This is an extension of the deflation technique used in the QR algorithm for symmetric matrices to non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Deflation for Non-Symmetric Matrices: This is an extension of the deflation technique used in the QR algorithm for symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lanczos Method for Non-Symmetric Matrices: This is an extension of the Lanczos method for symmetric matrices to non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lanczos Method for Non-Symmetric Matrices: This is an extension of the Lanczos method for symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Min-Max Theorem for Non-Symmetric Matrices: This is an extension of the min-max theorem for symmetric matrices to non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Min-Max Theorem for Non-Symmetric Matrices: This is an extension of the min-max theorem for symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rayleigh's Theorem for Non-Symmetric Matrices: This is an extension of Rayleigh's theorem to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8705253614280104
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) for Non-Symmetric Matrices: While SVD is typically applied to symmetric matrices, there are also versions of the decomposition that can be applied to non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) for Non-Symmetric Matrices: While SVD is typically applied to symmetric matrices, there are also versions of the decomposition that can be applied to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Product of Matrices: This is a fundamental concept in linear algebra that is used to extend the concept of eigenvalues and eigenvectors to non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Tensor Product of Matrices: This is a fundamental concept in linear algebra that is used to extend the concept of eigenvalues and eigenvectors to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Kronecker Product of Matrices for Non-Symmetric Matrices: This is a fundamental concept in linear algebra that is used to extend the concept of eigenvalues and eigenvectors to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9489416977603389
      }
    ]
  },
  {
    "representative_text": "Algebraic and Geometric Multiplicity: These are concepts that are closely related to eigenvalues and eigenvectors. They are used to extend the concept of eigenvectors to non-diagonalizable matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Algebraic and Geometric Multiplicity: These are concepts that are closely related to eigenvalues and eigenvectors. They are used to extend the concept of eigenvectors to non-diagonalizable matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Algebraic and Geometric Multiplicity for Non-Symmetric Matrices: These are concepts that are closely related to eigenvalues and eigenvectors. They are used to extend the concept of eigenvectors to non-diagonalizable matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9591974131106187
      }
    ]
  },
  {
    "representative_text": "Jordan Canonical Form for Non-Symmetric Matrices: This is an extension of the Jordan canonical form technique used for symmetric matrices to non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Jordan Canonical Form for Non-Symmetric Matrices: This is an extension of the Jordan canonical form technique used for symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Householder Reflection: The Householder reflection is a linear transformation that reflects a vector across a hyperplane. It is a fundamental tool in linear algebra and is used in various algorithms for orthogonalization and diagonalization.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Householder Reflection: The Householder reflection is a linear transformation that reflects a vector across a hyperplane. It is a fundamental tool in linear algebra and is used in various algorithms for orthogonalization and diagonalization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Relating Determinants to the Fundamental Theorem of Algebra: The fundamental theorem of algebra states that a polynomial equation of degree n has exactly n complex roots. The determinant of a matrix can be related to the roots of a polynomial equation using the characteristic polynomial.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relating Determinants to the Fundamental Theorem of Algebra: The fundamental theorem of algebra states that a polynomial equation of degree n has exactly n complex roots. The determinant of a matrix can be related to the roots of a polynomial equation using the characteristic polynomial.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Non-Integrable Transformation Matrix: The determinant of a matrix with a non-integrable transformation matrix can be calculated using the formula for the determinant of a matrix with a non-integrable transformation matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 13,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Non-Integrable Transformation Matrix: The determinant of a matrix with a non-integrable transformation matrix can be calculated using the formula for the determinant of a matrix with a non-integrable transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with a Non-Symmetric Transformation Matrix: The determinant of a matrix with a non-symmetric transformation matrix can be calculated using the formula for the determinant of a matrix with a non-symmetric transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9176225954205913
      },
      {
        "text": "Determinant of a Matrix with a Non-Linear Transformation Matrix: The determinant of a matrix with a non-linear transformation matrix can be calculated using the formula for the determinant of a matrix with a non-linear transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.94752736527313
      },
      {
        "text": "Determinant of a matrix with a non-constant coefficient matrix in a non-linear transformation: The determinant of a matrix with a non-constant coefficient matrix in a non-linear transformation can be calculated using the formula for the determinant of a matrix with a non-constant coefficient matrix in a non-linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9209347926484217
      },
      {
        "text": "Determinant of a matrix with a non-zero determinant in a non-linear transformation: The determinant of a matrix with a non-zero determinant in a non-linear transformation can be calculated using the formula for the determinant of a matrix with a non-zero determinant in a non-linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9369243160673454
      },
      {
        "text": "Determinant of a matrix with a non-zero determinant in a non-constant coefficient matrix: The determinant of a matrix with a non-zero determinant in a non-constant coefficient matrix can be calculated using the formula for the determinant of a matrix with a non-zero determinant in a non-constant coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9026033601862793
      },
      {
        "text": "Determinant of a matrix with a non-zero determinant in a non-integrable transformation: The determinant of a matrix with a non-zero determinant in a non-integrable transformation can be calculated using the formula for the determinant of a matrix with a non-zero determinant in a non-integrable transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9475810705528647
      },
      {
        "text": "Determinant of a matrix with a non-zero determinant in a non-orthogonal basis: The determinant of a matrix with a non-zero determinant in a non-orthogonal basis can be calculated using the formula for the determinant of a matrix with a non-zero determinant in a non-orthogonal basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9097134321565719
      },
      {
        "text": "Determinant of a matrix with a non-zero determinant in a non-symmetric transformation matrix: The determinant of a matrix with a non-zero determinant in a non-symmetric transformation matrix can be calculated using the formula for the determinant of a matrix with a non-zero determinant in a non-symmetric transformation matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9573796086252901
      },
      {
        "text": "Determinant of a matrix with a non-zero determinant in a non-constant coefficient matrix in a non-linear transformation: The determinant of a matrix with a non-zero determinant in a non-constant coefficient matrix in a non-linear transformation can be calculated using the formula for the determinant of a matrix with a non-zero determinant in a non-constant coefficient matrix in a non-linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9590474952706317
      },
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Transformation Matrix and Non-Constant Coefficient Matrix: The determinant of a matrix with a non-orthogonal transformation matrix and a non-constant coefficient matrix can be calculated using the formula for the determinant of a matrix with a non-orthogonal transformation matrix and a non-constant coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9191481683639458
      },
      {
        "text": "Determinant of a Matrix with Non-Symmetric Transformation Matrix and Non-Constant Coefficient Matrix: The determinant of a matrix with a non-symmetric transformation matrix and a non-constant coefficient matrix can be calculated using the formula for the determinant of a matrix with a non-symmetric transformation matrix and a non-constant coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9309211564797684
      },
      {
        "text": "Determinant of a Matrix with a Non-Constant Coefficient Matrix in a Non-Integrable Transformation: The determinant of a matrix with a non-constant coefficient matrix in a non-integrable transformation can be calculated using the formula for the determinant of a matrix with a non-constant coefficient matrix in a non-integrable transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.943331551111944
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Non-Linear Jacobian Matrix: The determinant of a matrix with a non-linear Jacobian matrix can be calculated using the formula for the determinant of a matrix with a non-linear Jacobian matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Non-Linear Jacobian Matrix: The determinant of a matrix with a non-linear Jacobian matrix can be calculated using the formula for the determinant of a matrix with a non-linear Jacobian matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a matrix with a non-zero determinant in a non-linear Jacobian matrix: The determinant of a matrix with a non-zero determinant in a non-linear Jacobian matrix can be calculated using the formula for the determinant of a matrix with a non-zero determinant in a non-linear Jacobian matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9537854907572165
      },
      {
        "text": "Determinant of a Matrix with Non-Linear Jacobian Matrix and Non-Constant Coefficient Matrix: The determinant of a matrix with a non-linear Jacobian matrix and a non-constant coefficient matrix can be calculated using the formula for the determinant of a matrix with a non-linear Jacobian matrix and a non-constant coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9481494167151121
      },
      {
        "text": "Determinant of a Matrix with Non-Integrable Transformation Matrix and Non-Linear Jacobian Matrix: The determinant of a matrix with a non-integrable transformation matrix and a non-linear Jacobian matrix can be calculated using the formula for the determinant of a matrix with a non-integrable transformation matrix and a non-linear Jacobian matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9257348265770249
      },
      {
        "text": "Determinant of a Matrix with Non-Symmetric Transformation Matrix and Non-Linear Jacobian Matrix: The determinant of a matrix with a non-symmetric transformation matrix and a non-linear Jacobian matrix can be calculated using the formula for the determinant of a matrix with a non-symmetric transformation matrix and a non-linear Jacobian matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9404888167771306
      },
      {
        "text": "Determinant of a Matrix with a Non-Constant Jacobian Matrix: The determinant of a matrix with a non-constant Jacobian matrix can be calculated using theorelated matrix:'",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8682053156371099
      }
    ]
  },
  {
    "representative_text": "Relating Determinants to the Cauchy Integral Formula: The Cauchy integral formula can be used to calculate the determinant of a matrix using a contour integral.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relating Determinants to the Cauchy Integral Formula: The Cauchy integral formula can be used to calculate the determinant of a matrix using a contour integral.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Relating Determinants to the Gelfand's Theorem: Gelfand's theorem states that if a matrix has a non-zero determinant, then it has a non-zero eigenvalue with a non-negative real part. The determinant of a matrix can be related to the eigenvalues using Gelfand's theorem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relating Determinants to the Gelfand's Theorem: Gelfand's theorem states that if a matrix has a non-zero determinant, then it has a non-zero eigenvalue with a non-negative real part. The determinant of a matrix can be related to the eigenvalues using Gelfand's theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inverse of a Matrix with Complex Eigenvalues: The inverse of a matrix with complex eigenvalues can be calculated using the formula A^-1 = (1/λ) ∑(λi A^i), where λi are the eigenvalues of the matrix and A^i are the corresponding eigenvectors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inverse of a Matrix with Complex Eigenvalues: The inverse of a matrix with complex eigenvalues can be calculated using the formula A^-1 = (1/λ) ∑(λi A^i), where λi are the eigenvalues of the matrix and A^i are the corresponding eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Condition Number of a Matrix with Complex Entries: The condition number of a matrix with complex entries can be calculated using the formula cond(A) = |det(A)| / |det(A^)|, where det(A) is the determinant of the matrix and A^ is the conjugate transpose of A.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Condition Number of a Matrix with Complex Entries: The condition number of a matrix with complex entries can be calculated using the formula cond(A) = |det(A)| / |det(A^)|, where det(A) is the determinant of the matrix and A^ is the conjugate transpose of A.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inverse of a Matrix with Non-Standard Basis: The inverse of a matrix with non-standard basis can be calculated using the Gram-Schmidt process.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inverse of a Matrix with Non-Standard Basis: The inverse of a matrix with non-standard basis can be calculated using the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Lanczos is a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Lanczos is a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "It is not only one.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "It is not only one.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Linear Independence and Span: This sub-category examines the concepts of linear independence, span, and basis, which are fundamental in understanding vector spaces and linear transformations. -> Linear Independence: This concept deals with the idea that a set of vectors can be combined in various ways to produce only the zero vector, indicating that the vectors are not dependent on each other. Key aspects include::",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Linear Independence and Span: This sub-category examines the concepts of linear independence, span, and basis, which are fundamental in understanding vector spaces and linear transformations. -> Linear Independence: This concept deals with the idea that a set of vectors can be combined in various ways to produce only the zero vector, indicating that the vectors are not dependent on each other. Key aspects include::",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Additional knowledge for Linear Independence and Span: This sub-category examines the concepts of linear independence, span, and basis, which are fundamental in understanding vector spaces and linear transformations. -> Basis: A set of vectors that spans a vector space and is linearly independent.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9471110031478498
      },
      {
        "text": "Additional knowledge for Vector Spaces: This sub-category deals with the study of vector spaces, including definitions, properties, and operations such as addition, scalar multiplication, and basis vectors. -> Basis and Dimension: This sub-field focuses on the concept of a basis for a vector space, which is a set of linearly independent vectors that span the entire space. The dimension of a vector space is the number of vectors in a basis for the space.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8246915267052606
      },
      {
        "text": "Additional knowledge for Linear Independence and Span: This sub-category examines the concepts of linear independence, span, and basis, which are fundamental in understanding vector spaces and linear transformations. -> Linear Independence Theorem: A set of vectors is linearly independent if and only if the equation $a1v1 + a2v2 + \\ldots + anvn = 0$ implies that $a1 = a2 = \\ldots = a_n = 0$.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9014712987690847
      },
      {
        "text": "Additional knowledge for Linear Independence and Span: This sub-category examines the concepts of linear independence, span, and basis, which are fundamental in understanding vector spaces and linear transformations. -> Linear Dependence Test: A method to determine if a set of vectors is linearly dependent, often using the process of row reduction.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9196310409531019
      },
      {
        "text": "Additional knowledge for Linear Independence and Span: This sub-category examines the concepts of linear independence, span, and basis, which are fundamental in understanding vector spaces and linear transformations. -> Free basis: A set of vectors that is linearly independent but may not span the entire vector space.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9143038027725932
      }
    ]
  },
  {
    "representative_text": "Completeness of the Basis Theorem (with generalized dependent vectors): A basis for a vector space with generalized dependent vectors is said to be complete if it spans the entire space and is linearly dependent.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Completeness of the Basis Theorem (with generalized dependent vectors): A basis for a vector space with generalized dependent vectors is said to be complete if it spans the entire space and is linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces: This theorem states that if V is an infinite-dimensional vector space and S is a set of linearly independent vectors, then S spans V if and only if the rank of the linear transformation T: V → V induced by S is equal to the dimension of V.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces: This theorem states that if V is an infinite-dimensional vector space and S is a set of linearly independent vectors, then S spans V if and only if the rank of the linear transformation T: V → V induced by S is equal to the dimension of V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Uniform Continuity of Linear Operators: This theorem states that if T: V → W is a linear operator between Banach spaces V and W, then T is uniformly continuous if and only if T is bounded.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Uniform Continuity of Linear Operators: This theorem states that if T: V → W is a linear operator between Banach spaces V and W, then T is uniformly continuous if and only if T is bounded.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Closed Graph Theorem: This theorem states that if T: V → W is a linear operator between Banach spaces V and W, and U is a closed subspace of V, then T is continuous if and only if T(U) is closed in W.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Closed Graph Theorem: This theorem states that if T: V → W is a linear operator between Banach spaces V and W, and U is a closed subspace of V, then T is continuous if and only if T(U) is closed in W.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Minkowski's Theorem: A theorem that states that a set of vectors in a vector space can be partitioned into a finite number of convex sets if and only if the sum of the norms of any subset of the vectors is bounded.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minkowski's Theorem: A theorem that states that a set of vectors in a vector space can be partitioned into a finite number of convex sets if and only if the sum of the norms of any subset of the vectors is bounded.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Birkhoff's Theorem: A theorem that states that a matrix can be decomposed into a product of orthogonal matrices if and only if the matrix is symmetric.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Birkhoff's Theorem: A theorem that states that a matrix can be decomposed into a product of orthogonal matrices if and only if the matrix is symmetric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Birkhoff's Theorem and Orthogonal Decomposition: Birkhoff's Theorem can be used to decompose a matrix into a product of orthogonal matrices, and its variations can be used to solve optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8301524839455791
      }
    ]
  },
  {
    "representative_text": "Theorems of Frobenius: A set of theorems that provide a connection between linear algebra and other areas of mathematics, such as number theory and algebraic geometry.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Theorems of Frobenius: A set of theorems that provide a connection between linear algebra and other areas of mathematics, such as number theory and algebraic geometry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Theorems of Frobenius and Linear Algebra: Theorems of Frobenius provide a connection between linear algebra and other areas of mathematics, such as number theory and algebraic geometry, and their variations can be used to solve optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8539182387625965
      }
    ]
  },
  {
    "representative_text": "The concept of a \"free\" vector space: A vector space that is not necessarily spanned by a basis, but is still a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The concept of a \"free\" vector space: A vector space that is not necessarily spanned by a basis, but is still a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The relationship between linear independence and the concept of \"independence\" in other mathematical structures: The concept of linear independence is not unique to vector spaces, and can be applied to other mathematical structures, such as modules and R-modules.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The relationship between linear independence and the concept of \"independence\" in other mathematical structures: The concept of linear independence is not unique to vector spaces, and can be applied to other mathematical structures, such as modules and R-modules.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of \"linear dependence\" in other mathematical structures: The concept of linear dependence is not unique to vector spaces, and can be applied to other mathematical structures, such as modules and R-modules.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.903889645534019
      }
    ]
  },
  {
    "representative_text": "The connection between linear algebra and combinatorial geometry: Linear algebra provides a framework for understanding the properties of geometric objects, such as polytopes and convex sets.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The connection between linear algebra and combinatorial geometry: Linear algebra provides a framework for understanding the properties of geometric objects, such as polytopes and convex sets.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The relationship between linear algebra and algebraic geometry: Linear algebra provides a framework for understanding the properties of algebraic objects, such as varieties and algebraic curves.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The relationship between linear algebra and algebraic geometry: Linear algebra provides a framework for understanding the properties of algebraic objects, such as varieties and algebraic curves.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The relationship between linear algebra and differential geometry: Linear algebra provides a framework for understanding the properties of geometric objects, such as curves and surfaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9047979653419306
      }
    ]
  },
  {
    "representative_text": "The connection between linear algebra and differential equations: Linear algebra provides a framework for solving differential equations, including linear differential equations and systems of differential equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The connection between linear algebra and differential equations: Linear algebra provides a framework for solving differential equations, including linear differential equations and systems of differential equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The relationship between linear algebra and number theory: Linear algebra provides a framework for understanding the properties of number theoretic objects, such as prime numbers and modular forms.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The relationship between linear algebra and number theory: Linear algebra provides a framework for understanding the properties of number theoretic objects, such as prime numbers and modular forms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The connection between linear algebra and algebraic topology: Linear algebra provides a framework for understanding the properties of topological objects, such as manifolds and homotopy types.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The connection between linear algebra and algebraic topology: Linear algebra provides a framework for understanding the properties of topological objects, such as manifolds and homotopy types.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The use of linear algebra in coding theory: Linear algebra is used in a wide range of applications in coding theory, including error-correcting codes and cryptography.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The use of linear algebra in coding theory: Linear algebra is used in a wide range of applications in coding theory, including error-correcting codes and cryptography.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The use of topology in linear independence and span: Topological concepts, such as the Hausdorff dimension or the concept of convergence in a topological vector space, can be applied to the context of linear independence and span. This approach could provide additional insights into the relationships between these concepts.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The use of topology in linear independence and span: Topological concepts, such as the Hausdorff dimension or the concept of convergence in a topological vector space, can be applied to the context of linear independence and span. This approach could provide additional insights into the relationships between these concepts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Impact of Non-Standard Topologies on Linear Independence in Infinite-Dimensional Spaces: Investigating the impact of non-standard topologies on linear independence in infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Impact of Non-Standard Topologies on Linear Independence in Infinite-Dimensional Spaces: Investigating the impact of non-standard topologies on linear independence in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Wedderburn-Artin Theorem: This theorem states that every finite-dimensional algebra over a field is isomorphic to a matrix algebra over a division algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Wedderburn-Artin Theorem: This theorem states that every finite-dimensional algebra over a field is isomorphic to a matrix algebra over a division algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Homotopy Methods: Homotopy methods are used to solve nonlinear optimization problems in computer graphics and game development. These methods involve finding a homotopy between the original problem and a simpler one, allowing for the solution of the original problem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Homotopy Methods: Homotopy methods are used to solve nonlinear optimization problems in computer graphics and game development. These methods involve finding a homotopy between the original problem and a simpler one, allowing for the solution of the original problem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Approximation Techniques for Eigenvalue Decomposition: Approximation techniques, such as the Lanczos algorithm, are used to solve eigenvalue decomposition problems in computer graphics and game development. These techniques provide a fast and efficient way to compute eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Approximation Techniques for Eigenvalue Decomposition: Approximation techniques, such as the Lanczos algorithm, are used to solve eigenvalue decomposition problems in computer graphics and game development. These techniques provide a fast and efficient way to compute eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Decomposition and Its Applications: Eigenvalue decomposition is a technique used in computer graphics and game development to solve systems of linear equations and to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8800132692574052
      }
    ]
  },
  {
    "representative_text": "Numerical Methods for Nonlinear Systems: Numerical methods, such as the Newton-Raphson method, are used to solve nonlinear systems of equations in computer graphics and game development. These methods involve finding the roots of a nonlinear function.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Methods for Nonlinear Systems: Numerical methods, such as the Newton-Raphson method, are used to solve nonlinear systems of equations in computer graphics and game development. These methods involve finding the roots of a nonlinear function.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Harmonic Analysis: Computational harmonic analysis is used to solve problems involving harmonic analysis in computer graphics and game development. This involves using techniques such as Fourier analysis to solve problems involving periodic functions.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Computational Harmonic Analysis: Computational harmonic analysis is used to solve problems involving harmonic analysis in computer graphics and game development. This involves using techniques such as Fourier analysis to solve problems involving periodic functions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Harmonic Analysis and Its Applications: Computational harmonic analysis is a technique used in computer graphics and game development to solve problems involving harmonic analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9438045254745041
      }
    ]
  },
  {
    "representative_text": "Spectral Methods for Solving Partial Differential Equations: Spectral methods are used to solve partial differential equations in computer graphics and game development. These methods involve using techniques such as spectral decomposition to solve problems involving partial differential equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Spectral Methods for Solving Partial Differential Equations: Spectral methods are used to solve partial differential equations in computer graphics and game development. These methods involve using techniques such as spectral decomposition to solve problems involving partial differential equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue-Based Methods for Solving Partial Differential Equations: Eigenvalue-based methods are used in computer graphics and game development to solve problems involving partial differential equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8628367726258541
      }
    ]
  },
  {
    "representative_text": "Computational Geometry and Voronoi Diagrams for Object Recognition: Computational geometry and Voronoi diagrams are used to solve problems involving object recognition in computer graphics and game development. These methods involve using techniques such as Voronoi diagrams to solve problems involving object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Geometry and Voronoi Diagrams for Object Recognition: Computational geometry and Voronoi diagrams are used to solve problems involving object recognition in computer graphics and game development. These methods involve using techniques such as Voronoi diagrams to solve problems involving object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spherical Coordinates and Polar Coordinates for 3D Reconstruction: Spherical coordinates and polar coordinates are used to solve problems involving 3D reconstruction in computer graphics and game development. These methods involve using techniques such as spherical coordinates to solve problems involving 3D reconstruction.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spherical Coordinates and Polar Coordinates for 3D Reconstruction: Spherical coordinates and polar coordinates are used to solve problems involving 3D reconstruction in computer graphics and game development. These methods involve using techniques such as spherical coordinates to solve problems involving 3D reconstruction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Krylov Subspace Methods for Non-Linear Systems: An extension of Krylov subspace methods to non-linear systems, which can be used for tasks such as non-linear regression and non-linear classification.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Krylov Subspace Methods for Non-Linear Systems: An extension of Krylov subspace methods to non-linear systems, which can be used for tasks such as non-linear regression and non-linear classification.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Time Series Analysis: Non-Stationary Time Series: Techniques for analyzing and modeling non-stationary time series data, which involve linear algebra operations such as eigendecomposition and singular value decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Time Series Analysis: Non-Stationary Time Series: Techniques for analyzing and modeling non-stationary time series data, which involve linear algebra operations such as eigendecomposition and singular value decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Time Series Analysis: Stationarity Testing: Techniques for testing the stationarity of time series data using linear algebra operations such as eigenvalue decomposition and singular value decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9066508005166589
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Natural Language Processing: Text Classification: Methods for classifying text, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Natural Language Processing: Text Classification: Methods for classifying text, which involve linear algebra operations such as matrix multiplication and eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Modal Analysis: Modal analysis is a technique used in signal processing and image analysis to analyze the modes of vibration or oscillation of a system. It is based on the use of eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Modal Analysis: Modal analysis is a technique used in signal processing and image analysis to analyze the modes of vibration or oscillation of a system. It is based on the use of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Iteration: Singular value iteration is an algorithm used in signal processing and image analysis to compute the singular values of a matrix. It is used for tasks such as image compression and signal denoising.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Iteration: Singular value iteration is an algorithm used in signal processing and image analysis to compute the singular values of a matrix. It is used for tasks such as image compression and signal denoising.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Wavelet Transform Properties: The wavelet transform has several properties, such as locality, stability, and completeness, which are used in signal processing and image analysis for tasks such as image compression and signal denoising.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Wavelet Transform Properties: The wavelet transform has several properties, such as locality, stability, and completeness, which are used in signal processing and image analysis for tasks such as image compression and signal denoising.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Stability Analysis: Stability analysis is a technique used in signal processing and image analysis to analyze the stability of a system. It is based on the use of eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Stability Analysis: Stability analysis is a technique used in signal processing and image analysis to analyze the stability of a system. It is based on the use of eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Controllability and Observability: Controllability and observability are properties of a system that determine its ability to be controlled and observed. They are used in signal processing and image analysis for tasks such as image segmentation and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Controllability and Observability: Controllability and observability are properties of a system that determine its ability to be controlled and observed. They are used in signal processing and image analysis for tasks such as image segmentation and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kalman Filter: The Kalman filter is a recursive algorithm used in signal processing and image analysis to estimate the state of a system from noisy measurements. It is used for tasks such as image tracking and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kalman Filter: The Kalman filter is a recursive algorithm used in signal processing and image analysis to estimate the state of a system from noisy measurements. It is used for tasks such as image tracking and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Model-Based Predictive Control (MBPC) with Model Order Reduction: A method for reducing the order of a model while maintaining its accuracy, which can be used in optimization and control systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Model-Based Predictive Control (MBPC) with Model Order Reduction: A method for reducing the order of a model while maintaining its accuracy, which can be used in optimization and control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Optimization of Non-Convex Functions: A technique for optimizing non-convex functions using linear algebra and optimization techniques, such as the use of trust regions and approximation methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "Optimization of Non-Convex Functions: A technique for optimizing non-convex functions using linear algebra and optimization techniques, such as the use of trust regions and approximation methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Optimization of Non-Linear Systems using Linear Algebra: A technique for optimizing non-linear systems using linear algebra and optimization techniques, such as the use of gradient-based methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8149037195824669
      },
      {
        "text": "Optimization of Non-Convex Functions using Linear Algebra and Optimization Techniques: A technique for optimizing non-convex functions using linear algebra and optimization techniques, such as the use of trust regions and approximation methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9266134959583019
      },
      {
        "text": "Linear Algebra and Optimization Techniques for Non-Convex Optimization: A class of algorithms for solving non-convex optimization problems using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8776493122638417
      },
      {
        "text": "Optimization of Non-Convex Functions using Linear Algebra and Deep Learning: A technique for optimizing non-convex functions using linear algebra and deep learning techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9155918845999714
      },
      {
        "text": "Linear Algebra and Optimization Techniques for Non-Convex Optimization with Deep Learning: A class of algorithms for solving non-convex optimization problems using linear algebra and deep learning techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9016022875599606
      },
      {
        "text": "Optimization of Non-Convex Functions using Linear Algebra and Gaussian Processes: A technique for optimizing non-convex functions using linear algebra and Gaussian processes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9237504430627995
      },
      {
        "text": "Linear Algebra and Optimization Techniques for Non-Convex Optimization with Gaussian Processes: A class of algorithms for solving non-convex optimization problems using linear algebra and Gaussian processes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9189552872063025
      },
      {
        "text": "Optimization of Non-Convex Functions using Linear Algebra and Model-Based Predictive Control: A technique for optimizing non-convex functions using linear algebra and model-based predictive control.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9246593700259331
      },
      {
        "text": "Linear Algebra and Optimization Techniques for Non-Convex Optimization with Model-Based Predictive Control: A class of algorithms for solving non-convex optimization problems using linear algebra and model-based predictive control.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8850237132481796
      },
      {
        "text": "Optimization of Non-Convex Functions using Linear Algebra and Quadratic Programming: A technique for optimizing non-convex functions using linear algebra and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9513350912158562
      },
      {
        "text": "Linear Algebra and Optimization Techniques for Non-Convex Optimization with Quadratic Programming: A class of algorithms for solving non-convex optimization problems using linear algebra and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9184413590414547
      },
      {
        "text": "Linear Algebra and Optimization Techniques for Non-Convex Optimization with Gaussian Processes and Optimization Techniques and Quadratic Programming: A class of algorithms for solving non-convex optimization problems using linear algebra and Gaussian processes and optimization techniques and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9172405775378976
      },
      {
        "text": "Linear Algebraic Methods for Non-Linear Optimization: Techniques such as gradient descent, Newton's method, and quasi-Newton methods for solving non-linear optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8372239329639741
      }
    ]
  },
  {
    "representative_text": "Bilinear System Theory with Non-Convex Optimization: A framework for analyzing and designing control systems using bilinear systems and non-convex optimization techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bilinear System Theory with Non-Convex Optimization: A framework for analyzing and designing control systems using bilinear systems and non-convex optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Robust Control Theory with Model-Based Predictive Control: A framework for designing control systems that can handle uncertainty and noise using model-based predictive control and robust control theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Robust Control Theory with Model-Based Predictive Control: A framework for designing control systems that can handle uncertainty and noise using model-based predictive control and robust control theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Machine Learning with Gaussian Processes: A class of algorithms for modeling and analyzing complex systems using Gaussian processes and linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Machine Learning with Gaussian Processes: A class of algorithms for modeling and analyzing complex systems using Gaussian processes and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Methods for Non-Convex Optimization: A class of algorithms for solving non-convex optimization problems using spectral analysis and linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spectral Methods for Non-Convex Optimization: A class of algorithms for solving non-convex optimization problems using spectral analysis and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Randomized Algorithms for Non-Convex Optimization: A class of algorithms for solving non-convex optimization problems using random sampling and linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Randomized Algorithms for Non-Convex Optimization: A class of algorithms for solving non-convex optimization problems using random sampling and linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Robust Optimization with Linear Algebra: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and robust optimization techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 14,
    "detailed_sources": [
      {
        "text": "Robust Optimization with Linear Algebra: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and robust optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Stochastic Optimization with Linear Algebra: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and stochastic optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9175054555884212
      },
      {
        "text": "Robust Optimization with Linear Algebra and Deep Learning: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and deep learning techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9403424722517972
      },
      {
        "text": "Stochastic Optimization with Linear Algebra and Deep Learning: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and deep learning techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9328562763291545
      },
      {
        "text": "Robust Optimization with Linear Algebra and Gaussian Processes: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and Gaussian processes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9198523632150768
      },
      {
        "text": "Stochastic Optimization with Linear Algebra and Gaussian Processes: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and Gaussian processes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9304013040854497
      },
      {
        "text": "Robust Optimization with Linear Algebra and Model-Based Predictive Control: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and model-based predictive control.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8828879268394801
      },
      {
        "text": "Stochastic Optimization with Linear Algebra and Model-Based Predictive Control: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and model-based predictive control.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9167670455077841
      },
      {
        "text": "Robust Optimization with Linear Algebra and Quadratic Programming: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9259643501908967
      },
      {
        "text": "Stochastic Optimization with Linear Algebra and Quadratic Programming: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9444592068615102
      },
      {
        "text": "Robust Optimization with Linear Algebra and Gaussian Processes and Optimization Techniques and Quadratic Programming: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and Gaussian processes and optimization techniques and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9154796049173322
      },
      {
        "text": "Stochastic Optimization with Linear Algebra and Gaussian Processes and Optimization Techniques and Quadratic Programming: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and Gaussian processes and optimization techniques and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.931637436167309
      },
      {
        "text": "Robust Optimization with Linear Algebra and Model-Based Predictive Control and Optimization Techniques and Quadratic Programming: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and model-based predictive control and optimization techniques and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8975290116395571
      },
      {
        "text": "Stochastic Optimization with Linear Algebra and Model-Based Predictive Control and Optimization Techniques and Quadratic Programming: A class of algorithms for solving optimization problems with uncertainty and noise using linear algebra and model-based predictive control and optimization techniques and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9297144740156718
      }
    ]
  },
  {
    "representative_text": "Machine Learning with Linear Algebra and Optimization Techniques: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and optimization techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Machine Learning with Linear Algebra and Optimization Techniques: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Deep Learning with Linear Algebra and Optimization Techniques: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and optimization techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.951531190377197
      },
      {
        "text": "Machine Learning with Linear Algebra and Deep Learning: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and deep learning techniques.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.957098365214953
      },
      {
        "text": "Machine Learning with Linear Algebra and Gaussian Processes: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and Gaussian processes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9239455353566859
      },
      {
        "text": "Machine Learning with Linear Algebra and Model-Based Predictive Control: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and model-based predictive control.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8720687530428033
      },
      {
        "text": "Machine Learning with Linear Algebra and Quadratic Programming: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9269328854495196
      },
      {
        "text": "Machine Learning with Linear Algebra and Gaussian Processes and Optimization Techniques and Quadratic Programming: A class of algorithms for learning complex structures in high-dimensional data using linear algebra and Gaussian processes and optimization techniques and quadratic programming.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9328998799923569
      }
    ]
  },
  {
    "representative_text": "Machine Learning with Linear Algebra and Model-Based Predictive Control and Optimization Techniques and Quadratic Quadratic optimization techniques: A class of Linear Algebra**: A class of Non-",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Machine Learning with Linear Algebra and Model-Based Predictive Control and Optimization Techniques and Quadratic Quadratic optimization techniques: A class of Linear Algebra**: A class of Non-",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quantum-resistant cryptographic techniques for secure digital signatures: Methods for exploring alternative cryptographic techniques that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, to ensure long-term security in secure digital signatures.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Quantum-resistant cryptographic techniques for secure digital signatures: Methods for exploring alternative cryptographic techniques that are resistant to quantum computer attacks, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, to ensure long-term security in secure digital signatures.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Quantum-resistant cryptographic techniques for secure digital signatures: This includes the analysis of quantum-resistant cryptographic techniques, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, for secure digital signatures.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8641634470284372
      }
    ]
  },
  {
    "representative_text": "Dual Basis: Given a basis {v1, v2, ..., vn} for a vector space V, there exists a dual basis {f1, f2, ..., fn} in the dual space V* such that f(vi) = δij, where δij is the Kronecker delta.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dual Basis: Given a basis {v1, v2, ..., vn} for a vector space V, there exists a dual basis {f1, f2, ..., fn} in the dual space V* such that f(vi) = δij, where δij is the Kronecker delta.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Change of Basis Formula: Given a change of basis matrix P from a basis {v1, v2, ..., vn} to a basis {w1, w2, ..., wn}, the change of basis formula states that the coordinates of a vector u in the basis {w1, w2, ..., wn} are given by Pu, where P is the change of basis matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Change of Basis Formula: Given a change of basis matrix P from a basis {v1, v2, ..., vn} to a basis {w1, w2, ..., wn}, the change of basis formula states that the coordinates of a vector u in the basis {w1, w2, ..., wn} are given by Pu, where P is the change of basis matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence and Span: A set of vectors {v1, v2, ..., vn} is linearly independent if and only if the only linear combination a1v1 + a2v2 + ... + anvn = 0 is when all coefficients a1, a2, ..., an = 0. The span of the set {v1, v2, ..., vn} is the set of all linear combinations of the form a1v1 + a2v2 + ... + anvn.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and Span: A set of vectors {v1, v2, ..., vn} is linearly independent if and only if the only linear combination a1v1 + a2v2 + ... + anvn = 0 is when all coefficients a1, a2, ..., an = 0. The span of the set {v1, v2, ..., vn} is the set of all linear combinations of the form a1v1 + a2v2 + ... + anvn.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis Theorem for Infinite-Dimensional Vector Spaces: Every infinite-dimensional vector space has a basis, which can be constructed using the Gram-Schmidt process.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis Theorem for Infinite-Dimensional Vector Spaces: Every infinite-dimensional vector space has a basis, which can be constructed using the Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hilbert-Schmidt Theorem for Infinite-Dimensional Vector Spaces: Every linear transformation T from an infinite-dimensional vector space V to itself is invertible if and only if the sum of the squares of the singular values of T is finite.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hilbert-Schmidt Theorem for Infinite-Dimensional Vector Spaces: Every linear transformation T from an infinite-dimensional vector space V to itself is invertible if and only if the sum of the squares of the singular values of T is finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dual Basis Theorem: A theorem that states that the dual basis of a vector space is unique up to scalar multiplication.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dual Basis Theorem: A theorem that states that the dual basis of a vector space is unique up to scalar multiplication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hilbert Space Dimension: The concept of the dimension of a Hilbert space, which is a complete inner product space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hilbert Space Dimension: The concept of the dimension of a Hilbert space, which is a complete inner product space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Topological Vector Spaces: The study of vector spaces equipped with a topology, which can affect the properties of linear transformations and vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Topological Vector Spaces: The study of vector spaces equipped with a topology, which can affect the properties of linear transformations and vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fuzzy Vector Spaces: The study of vector spaces with fuzzy scalar multiplication and addition operations, which can affect the closure under scalar multiplication and addition.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Fuzzy Vector Spaces: The study of vector spaces with fuzzy scalar multiplication and addition operations, which can affect the closure under scalar multiplication and addition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Banach Spaces: The study of complete normed vector spaces, which are vector spaces equipped with a norm that satisfies certain properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Banach Spaces: The study of complete normed vector spaces, which are vector spaces equipped with a norm that satisfies certain properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Operator Algebras: The study of vector spaces equipped with a norm and a multiplication operation, which can be used to represent linear transformations and other mathematical objects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Operator Algebras: The study of vector spaces equipped with a norm and a multiplication operation, which can be used to represent linear transformations and other mathematical objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Representation Theory: The study of vector spaces equipped with a representation of a group or ring, which can be used to represent linear transformations and other mathematical objects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Representation Theory: The study of vector spaces equipped with a representation of a group or ring, which can be used to represent linear transformations and other mathematical objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "K-theory: The study of vector spaces equipped with a K-theory, which is a way of measuring the \"size\" of a vector space.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "K-theory: The study of vector spaces equipped with a K-theory, which is a way of measuring the \"size\" of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector Space Automorphisms: The study of bijective linear transformations between vector spaces, which can be used to represent linear transformations and other mathematical objects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector Space Automorphisms: The study of bijective linear transformations between vector spaces, which can be used to represent linear transformations and other mathematical objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector Space Decompositions: The study of vector spaces that can be decomposed into a direct sum of subspaces, which can be used to represent linear transformations and other mathematical objects.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector Space Decompositions: The study of vector spaces that can be decomposed into a direct sum of subspaces, which can be used to represent linear transformations and other mathematical objects.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Compact and Infinite Dimensional Vector Spaces: Understanding the differences between compact and infinite dimensional vector spaces, and how they affect the properties of linear transformations and matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Compact and Infinite Dimensional Vector Spaces: Understanding the differences between compact and infinite dimensional vector spaces, and how they affect the properties of linear transformations and matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-Standard Vector Spaces: Studying non-standard vector spaces, such as vector spaces with non-standard scalar multiplication or addition, and how these affect the properties of linear transformations and matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Non-Standard Vector Spaces: Studying non-standard vector spaces, such as vector spaces with non-standard scalar multiplication or addition, and how these affect the properties of linear transformations and matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations on Non-Standard Vector Spaces: Extending the concept of linear transformations to non-standard vector spaces, and studying the properties and behavior of these transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.894174697932092
      }
    ]
  },
  {
    "representative_text": "Symmetric and Skew-Symmetric Matrices: Understanding the properties and behavior of symmetric and skew-symmetric matrices, and how they relate to linear transformations and matrix representations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Symmetric and Skew-Symmetric Matrices: Understanding the properties and behavior of symmetric and skew-symmetric matrices, and how they relate to linear transformations and matrix representations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Completeness of the Inner Product Space: This concept states that an inner product space is complete if every Cauchy sequence of vectors in the space converges to a vector in the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Completeness of the Inner Product Space: This concept states that an inner product space is complete if every Cauchy sequence of vectors in the space converges to a vector in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces: The rank-nullity theorem for infinite-dimensional vector spaces states that the rank of a linear transformation plus the nullity of the transformation equals the dimension of the domain.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces: The rank-nullity theorem for infinite-dimensional vector spaces states that the rank of a linear transformation plus the nullity of the transformation equals the dimension of the domain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Rank-Nullity Theorem for Infinite-Dimensional Vector Spaces: The relationship between the rank and nullity of a linear transformation in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9232142481409404
      },
      {
        "text": "Interpretation of the Rank-Nullity Theorem in Infinite-Dimensional Spaces: The rank-nullity theorem states that the sum of the rank and nullity of a linear transformation is equal to the dimension of the domain vector space. In infinite-dimensional spaces, this theorem can be used to relate the rank and nullity of a linear transformation to the dimension of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.873655118404612
      },
      {
        "text": "Interpretation of the Rank-Nullity Theorem in Finite-Dimensional Spaces: This theorem states that the sum of the rank and nullity of a linear transformation is equal to the dimension of the domain vector space. In finite-dimensional spaces, this theorem can be used to relate the rank and nullity of a linear transformation to the dimension of the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8886180538127713
      }
    ]
  },
  {
    "representative_text": "The Tensor Decomposition for Infinite-Dimensional Vector Spaces: The tensor decomposition for infinite-dimensional vector spaces states that a tensor can be decomposed into a sum of tensor products, but the tensor product may not be well-defined for infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Tensor Decomposition for Infinite-Dimensional Vector Spaces: The tensor decomposition for infinite-dimensional vector spaces states that a tensor can be decomposed into a sum of tensor products, but the tensor product may not be well-defined for infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Riesz Representation Theorem: This theorem states that every linear functional on a Hilbert space can be represented as an inner product with a fixed vector in the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Riesz Representation Theorem: This theorem states that every linear functional on a Hilbert space can be represented as an inner product with a fixed vector in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Riesz representation theorem states that every linear functional on a Hilbert space can be represented as an inner product with a fixed vector in the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9279691449818237
      },
      {
        "text": "Riesz Representation Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8156252121351143
      }
    ]
  },
  {
    "representative_text": "Dual Basis: A dual basis is a set of vectors in the dual space that corresponds to a given basis in the original space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dual Basis: A dual basis is a set of vectors in the dual space that corresponds to a given basis in the original space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Pinching Theorem: This theorem states that if a linear operator T has a finite-dimensional image, then its kernel is finite-dimensional.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Pinching Theorem: This theorem states that if a linear operator T has a finite-dimensional image, then its kernel is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The pinching theorem states that if a linear operator T has a finite-dimensional image, then its kernel is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9402977444343552
      }
    ]
  },
  {
    "representative_text": "The Fundamental Theorem of Linear Transformations for Infinite-Dimensional Spaces: This theorem generalizes the fundamental theorem of linear transformations to infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Fundamental Theorem of Linear Transformations for Infinite-Dimensional Spaces: This theorem generalizes the fundamental theorem of linear transformations to infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Spectral Theorem for Linear Operators on Infinite-Dimensional Spaces: This theorem generalizes the spectral theorem to linear operators on infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Spectral Theorem for Linear Operators on Infinite-Dimensional Spaces: This theorem generalizes the spectral theorem to linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Operator Spectral Mapping Theorem for Infinite-Dimensional Spaces: The theorem generalizing the spectral mapping theorem to linear operators on infinite-dimensional spaces is an extension of the theorem on finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8332060834736892
      }
    ]
  },
  {
    "representative_text": "The Inverse of a Linear Operator on an Infinite-Dimensional Space: The concept of the inverse of a linear operator on an infinite-dimensional space is an extension of the concept on finite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Inverse of a Linear Operator on an Infinite-Dimensional Space: The concept of the inverse of a linear operator on an infinite-dimensional space is an extension of the concept on finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Schur's Theorem for Bilinear Transformations: This theorem generalizes Schur's theorem to bilinear transformations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Schur's Theorem for Bilinear Transformations: This theorem generalizes Schur's theorem to bilinear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Jordan Canonical Form for Linear Operators on Infinite-Dimensional Spaces: This concept generalizes the Jordan canonical form to linear operators on infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Jordan Canonical Form for Linear Operators on Infinite-Dimensional Spaces: This concept generalizes the Jordan canonical form to linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Jordan canonical form for linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8389270495863959
      },
      {
        "text": "Jordan Canonical Form:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.816723356200648
      }
    ]
  },
  {
    "representative_text": "The Linear Operator Adjoint Operator for Infinite-Dimensional Spaces: The concept of the adjoint operator for linear operators on infinite-dimensional spaces is an extension of the concept on finite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Operator Adjoint Operator for Infinite-Dimensional Spaces: The concept of the adjoint operator for linear operators on infinite-dimensional spaces is an extension of the concept on finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Cofactor Expansion for Linear Operators on Infinite-Dimensional Spaces: The cofactor expansion for linear operators on infinite-dimensional spaces is an extension of the cofactor expansion for linear operators on finite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Cofactor Expansion for Linear Operators on Infinite-Dimensional Spaces: The cofactor expansion for linear operators on infinite-dimensional spaces is an extension of the cofactor expansion for linear operators on finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Cofactor Expansion for Infinite-Dimensional Matrices: The method for calculating the determinant of an infinite-dimensional matrix by expanding along a row or column.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8504146763928822
      }
    ]
  },
  {
    "representative_text": "The Linear Operator Determinant for Infinite-Dimensional Spaces: The concept of the determinant of a linear operator on an infinite-dimensional space is an extension of the concept on finite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Linear Operator Determinant for Infinite-Dimensional Spaces: The concept of the determinant of a linear operator on an infinite-dimensional space is an extension of the concept on finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Operator Theorem of Invariance of the Determinant for Infinite-Dimensional Spaces: The theorem generalizing the theorem of invariance of the determinant to linear operators on infinite-dimensional spaces is an extension of the theorem on finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8687716036483248
      }
    ]
  },
  {
    "representative_text": "The Linear Operator Theorem of Invariance of Bilinear Forms for Infinite-Dimensional Spaces: The theorem generalizing the theorem of invariance of bilinear forms to linear operators on infinite-dimensional spaces is an extension of the theorem on finite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Operator Theorem of Invariance of Bilinear Forms for Infinite-Dimensional Spaces: The theorem generalizing the theorem of invariance of bilinear forms to linear operators on infinite-dimensional spaces is an extension of the theorem on finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvector Centroid: The eigenvector centroid of a matrix is the average of its eigenvectors. It can be used to compute the centroid of the eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvector Centroid: The eigenvector centroid of a matrix is the average of its eigenvectors. It can be used to compute the centroid of the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Exponentiation by Diagonalization: A method for computing the matrix exponential using diagonalization, which can be used to solve systems of differential equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Exponentiation by Diagonalization: A method for computing the matrix exponential using diagonalization, which can be used to solve systems of differential equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Algebraic Topology: A branch of mathematics that studies the study of mathematics that studies the study of Vector spaces, which studies theore",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Algebraic Topology: A branch of mathematics that studies the study of mathematics that studies the study of Vector spaces, which studies theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Schur Complement: The Schur complement is a concept in linear algebra that provides a way to decompose a matrix into a product of two matrices. Understanding the Schur complement can provide additional insights into the behavior of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Schur Complement: The Schur complement is a concept in linear algebra that provides a way to decompose a matrix into a product of two matrices. Understanding the Schur complement can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Pfaffian: The Pfaffian is a concept in linear algebra that provides a way to compute the determinant of a skew-symmetric matrix. Understanding the Pfaffian can provide additional insights into the behavior of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Pfaffian: The Pfaffian is a concept in linear algebra that provides a way to compute the determinant of a skew-symmetric matrix. Understanding the Pfaffian can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Spectral Projector: The spectral projector is a concept in linear algebra that provides a way to project a vector onto the image of a linear transformation. Understanding the spectral projector can provide additional insights into the behavior of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Spectral Projector: The spectral projector is a concept in linear algebra that provides a way to project a vector onto the image of a linear transformation. Understanding the spectral projector can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Unitary Invariant Measure: The unitary invariant measure is a concept in linear algebra that provides a way to measure the size of a linear transformation. Understanding the unitary invariant measure can provide additional insights into the behavior of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Unitary Invariant Measure: The unitary invariant measure is a concept in linear algebra that provides a way to measure the size of a linear transformation. Understanding the unitary invariant measure can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Bounded Linear Operator: The bounded linear operator is a concept in linear algebra that provides a way to measure the \"size\" of a linear transformation. Understanding the bounded linear operator can provide additional insights into the behavior of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Bounded Linear Operator: The bounded linear operator is a concept in linear algebra that provides a way to measure the \"size\" of a linear transformation. Understanding the bounded linear operator can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Approximation Theorem: The approximation theorem is a concept in linear algebra that provides a way to approximate a linear transformation using a finite number of basis vectors. Understanding the approximation theorem can provide additional insights into the behavior of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Approximation Theorem: The approximation theorem is a concept in linear algebra that provides a way to approximate a linear transformation using a finite number of basis vectors. Understanding the approximation theorem can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Orthogonal Decomposition: The orthogonal decomposition is a concept in linear algebra that provides a way to decompose a vector into a sum of orthogonal vectors. Understanding the orthogonal decomposition can provide additional insights into the behavior of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Orthogonal Decomposition: The orthogonal decomposition is a concept in linear algebra that provides a way to decompose a vector into a sum of orthogonal vectors. Understanding the orthogonal decomposition can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and the Spectral Theorem for Normal Operators: The spectral theorem for normal operators is a concept in linear algebra that provides a way to diagonalize a normal operator. Understanding the spectral theorem for normal operators can provide additional insights into the behavior of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Transformations and the Spectral Theorem for Normal Operators: The spectral theorem for normal operators is a concept in linear algebra that provides a way to diagonalize a normal operator. Understanding the spectral theorem for normal operators can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and the Spectral Theorem for Hermitian Operators: The spectral theorem for Hermitian operators is a concept in linear algebra that provides a way to diagonalize a Hermitian operator. Understanding the spectral theorem for Hermitian operators can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9101800311353536
      },
      {
        "text": "Linear Transformations and the Spectral Theorem for Unitary Operators: The spectral theorem for unitary operators is a concept in linear algebra that provides a way to diagonalize a unitary operator. Understanding the spectral theorem for unitary operators can provide additional insights into the behavior of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9311783895538114
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) of a Block Matrix: This is an extension of SVD to block matrices, which involves using the properties of the matrix and the eigenvalues to decompose the matrix into the product of three matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) of a Block Matrix: This is an extension of SVD to block matrices, which involves using the properties of the matrix and the eigenvalues to decompose the matrix into the product of three matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Decomposition (SVD) of a Block Matrix with Complex Eigenvalues (Generalized): An extension of SVD to block matrices with complex eigenvalues, which involves using the properties of the matrix and the eigenvalues to decompose the matrix into the product of three matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8936035999982064
      }
    ]
  },
  {
    "representative_text": "The Orthogonality of Eigenvectors in Complex Eigenvalue Problems: Understanding the properties of eigenvector orthogonality in complex eigenvalue problems is essential in many applications.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Orthogonality of Eigenvectors in Complex Eigenvalue Problems: Understanding the properties of eigenvector orthogonality in complex eigenvalue problems is essential in many applications.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Brouwer-Lipschitz Theorem: This theorem states that if a matrix is invertible and its inverse is continuous, then the matrix is continuous.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Brouwer-Lipschitz Theorem: This theorem states that if a matrix is invertible and its inverse is continuous, then the matrix is continuous.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvector Equations: These are equations that describe the eigenvectors of a matrix. The eigenvectors of a matrix are the non-zero vectors that, when multiplied by the matrix, result in a scaled version of themselves.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvector Equations: These are equations that describe the eigenvectors of a matrix. The eigenvectors of a matrix are the non-zero vectors that, when multiplied by the matrix, result in a scaled version of themselves.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvector Equations: Discuss eigenvector equations, which describe the eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8922638740561843
      }
    ]
  },
  {
    "representative_text": "Numerical Stability: This is the ability of a numerical method to produce accurate results even when the input matrix is ill-conditioned. Numerical stability is an important consideration when solving systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Numerical Stability: This is the ability of a numerical method to produce accurate results even when the input matrix is ill-conditioned. Numerical stability is an important consideration when solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Stability: Numerical stability refers to the sensitivity of numerical methods to small changes in the input data.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8615765252267563
      }
    ]
  },
  {
    "representative_text": "Iterative Methods: These are methods that use repeated applications of a matrix to solve a system of linear equations. Iterative methods are useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Iterative Methods: These are methods that use repeated applications of a matrix to solve a system of linear equations. Iterative methods are useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Iterative Methods: Explore iterative methods, which use repeated applications of a matrix to solve a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8378828034299894
      },
      {
        "text": "Iterative Methods for Solving Systems of Linear Equations: A class of algorithms for solving systems of linear equations, which are used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8002465697972947
      },
      {
        "text": "Krylov Subspace Methods: A class of algorithms for solving systems of linear equations, which are used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8331782660722065
      },
      {
        "text": "Krylov Subspace Methods with Applications: A class of algorithms for solving systems of linear equations using the Krylov subspace, which is a subspace of the vector space spanned by the vectors obtained by repeatedly multiplying the initial vector by the matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.840964481683034
      }
    ]
  },
  {
    "representative_text": "Block Methods: These are methods that use the block structure of a matrix to solve a system of linear equations. Block methods are useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Block Methods: These are methods that use the block structure of a matrix to solve a system of linear equations. Block methods are useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Block Methods: Discuss block methods, which use the block structure of a matrix to solve a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.800523078019245
      }
    ]
  },
  {
    "representative_text": "Domain Decomposition Methods: These are methods that use the domain decomposition of a matrix to solve a system of linear equations. Domain decomposition methods are useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Domain Decomposition Methods: These are methods that use the domain decomposition of a matrix to solve a system of linear equations. Domain decomposition methods are useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Parallelization: This is the process of using multiple processors or cores to solve a system of linear equations. Parallelization is useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Parallelization: This is the process of using multiple processors or cores to solve a system of linear equations. Parallelization is useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Distributed Computing: This is the process of using a network of computers to solve a system of linear equations. Distributed computing is useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8485736146629601
      },
      {
        "text": "Parallelization: Discuss parallelization, which is the process of using multiple processors or cores to solve a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.807598719817354
      },
      {
        "text": "Distributed Computing: Explore distributed computing, which is the process of using a network of computers to solve a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8768423368923459
      }
    ]
  },
  {
    "representative_text": "Machine Learning: This is the process of using machine learning algorithms to solve a system of linear equations. Machine learning is useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Machine Learning: This is the process of using machine learning algorithms to solve a system of linear equations. Machine learning is useful for solving large systems of linear equations and for analyzing the stability of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Machine Learning: Discuss machine learning, which is the process of using machine learning algorithms to solve a system of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8753127839113863
      },
      {
        "text": "Linear Algebra with Machine Learning: A connection between linear algebra and machine learning, which can be used to solve systems of linear equations using machine learning techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8792860978971784
      }
    ]
  },
  {
    "representative_text": "Ill-Conditioned Matrices and Regularization: Ill-conditioned matrices are matrices that have a large condition number, meaning that small changes in the input can result in large changes in the output. Regularization techniques can be used to improve the stability of the SVD algorithm by adding a small penalty term to the objective function.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Ill-Conditioned Matrices and Regularization: Ill-conditioned matrices are matrices that have a large condition number, meaning that small changes in the input can result in large changes in the output. Regularization techniques can be used to improve the stability of the SVD algorithm by adding a small penalty term to the objective function.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lanczos' Theorem for Non-Symmetric Matrices: This theorem states that if A is a symmetric matrix, then A can be diagonalized using a similarity transformation. However, this theorem does not have a direct extension to non-symmetric matrices.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lanczos' Theorem for Non-Symmetric Matrices: This theorem states that if A is a symmetric matrix, then A can be diagonalized using a similarity transformation. However, this theorem does not have a direct extension to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block Triangularization for Non-Symmetric Matrices: This is an extension of the block triangularization technique used for symmetric matrices to non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block Triangularization for Non-Symmetric Matrices: This is an extension of the block triangularization technique used for symmetric matrices to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gelfand's Theorem for Non-Symmetric Matrices: This theorem states that if A is a square matrix with distinct eigenvalues, then A can be diagonalized using a similarity transformation. However, this theorem does not have a direct extension to non-symmetric matrices.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gelfand's Theorem for Non-Symmetric Matrices: This theorem states that if A is a square matrix with distinct eigenvalues, then A can be diagonalized using a similarity transformation. However, this theorem does not have a direct extension to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lagrange's Interpolation Formula for Non-Symmetric Matrices: This formula states that the eigenvalues of a matrix A can be expressed as a polynomial in the eigenvalues of a companion matrix. However, this formula does not have a direct extension to non-symmetric matrices.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lagrange's Interpolation Formula for Non-Symmetric Matrices: This formula states that the eigenvalues of a matrix A can be expressed as a polynomial in the eigenvalues of a companion matrix. However, this formula does not have a direct extension to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spectral Mapping Theorem for Matrix Norms: This theorem states that the spectral mapping theorem for matrix norms, which states that the spectral radius of a matrix A is equal to the spectral radius of its diagonal matrix, can be extended to matrix norms. However, this theorem does not have a direct extension to non-symmetric matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spectral Mapping Theorem for Matrix Norms: This theorem states that the spectral mapping theorem for matrix norms, which states that the spectral radius of a matrix A is equal to the spectral radius of its diagonal matrix, can be extended to matrix norms. However, this theorem does not have a direct extension to non-symmetric matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Rank: The concept of the rank of an orthogonal matrix, which is the number of non-zero rows or columns of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Rank: The concept of the rank of an orthogonal matrix, which is the number of non-zero rows or columns of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Orthogonality Conditions for Quadratic Forms: The conditions under which a quadratic form is positive definite, positive semi-definite, or negative definite in terms of orthogonal matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Orthogonality Conditions for Quadratic Forms: The conditions under which a quadratic form is positive definite, positive semi-definite, or negative definite in terms of orthogonal matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Decomposition Error Analysis: Analysis of the errors that can occur in orthogonal matrix decomposition, including the effects of round-off errors and numerical instability.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Decomposition Error Analysis: Analysis of the errors that can occur in orthogonal matrix decomposition, including the effects of round-off errors and numerical instability.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Decomposition Optimization: Optimization techniques for orthogonal matrix decomposition, including the use of linear programming and convex optimization.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Decomposition Optimization: Optimization techniques for orthogonal matrix decomposition, including the use of linear programming and convex optimization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Decomposition Connections to Other Linear Algebra Concepts: Connections between orthogonal matrix decomposition and other linear algebra concepts, including eigenvalue decomposition, singular value decomposition, and matrix factorization.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Decomposition Connections to Other Linear Algebra Concepts: Connections between orthogonal matrix decomposition and other linear algebra concepts, including eigenvalue decomposition, singular value decomposition, and matrix factorization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Matrix Decomposition Connections to Other Mathematical Fields: Connections between orthogonal matrix decomposition and other mathematical fields, including calculus, differential equations, and statistics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9004009502996284
      },
      {
        "text": "The Connection between Orthogonal Diagonalization and Orthogonal Matrix Decomposition: The connection between orthogonal diagonalization and orthogonal matrix decomposition, including the use of orthogonal projections and orthogonal complements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8453983892147816
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Decomposition Numerical Methods: Numerical methods for solving systems of linear equations using orthogonal matrix decomposition, including the use of orthogonal projections and orthogonal complements.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Decomposition Numerical Methods: Numerical methods for solving systems of linear equations using orthogonal matrix decomposition, including the use of orthogonal projections and orthogonal complements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Numerical Methods for Orthogonal Matrix Decomposition: Numerical methods for solving systems of linear equations using orthogonal matrix decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8910330556794597
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Decomposition Computational Complexity: Computational complexity analysis of orthogonal matrix decomposition algorithms, including the use of Big O notation and complexity theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Decomposition Computational Complexity: Computational complexity analysis of orthogonal matrix decomposition algorithms, including the use of Big O notation and complexity theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Complexity of Orthogonal Matrix Decomposition: Computational complexity analysis of orthogonal matrix decomposition algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.931133057367125
      }
    ]
  },
  {
    "representative_text": "Orthogonal Matrix Decomposition Advanced Theorems: Advanced theorems related to orthogonal matrix decomposition, including the use of orthogonal projections and orthogonal complements.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Orthogonal Matrix Decomposition Advanced Theorems: Advanced theorems related to orthogonal matrix decomposition, including the use of orthogonal projections and orthogonal complements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Matrix Decomposition Applications in Machine Learning: Applications of orthogonal matrix decomposition in machine learning, including the use of orthogonal projections and orthogonal complements for dimensionality reduction and feature extraction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.837740163697718
      },
      {
        "text": "Orthogonal Matrix Decomposition Applications in Statistics: Applications of orthogonal matrix decomposition in statistics, including the use of orthogonal projections and orthogonal complements for hypothesis testing and regression analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8662008046112414
      },
      {
        "text": "Orthogonal Matrix Decomposition Applications in Signal Processing: Applications of orthogonal matrix decomposition in signal processing, including the use of orthogonal projections and orthogonal complements for filtering and noise reduction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8866037405247595
      },
      {
        "text": "Advanced Orthogonal Matrix Decomposition Techniques: Advanced techniques for orthogonal matrix decomposition, including the use of orthogonal projections and orthogonal complements.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8913533909930333
      }
    ]
  },
  {
    "representative_text": "Orthogonal Decomposition of a Matrix using the Schmidt Process: The Schmidt process can be used to orthogonal decomposition can be used to orthogonal decomposition of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Orthogonal Decomposition of a Matrix using the Schmidt Process: The Schmidt process can be used to orthogonal decomposition can be used to orthogonal decomposition of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Decomposition of a Matrix using the Schmidt Process and Gram-Schmidt Process and QR Decomposition: The combination of the Schmidt process, the Gram-Schmidt process, and the QR decomposition can be used to orthogonalize a matrix, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9105384052851395
      },
      {
        "text": "The Gram-Schmidt Process for Orthogonal Matrix Decomposition: The Gram-Schmidt process is an algorithm for orthogonalizing a set of vectors, which can be used for orthogonal matrix decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921905033800323
      },
      {
        "text": "Orthogonalization of a Matrix using the Gram-Schmidt Process and Householder Transformation: The Gram-Schmidt process can be used to orthogonalize a matrix, and the Householder transformation can be used to further orthogonalize the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.88438088900897
      },
      {
        "text": "Properties of the Schmidt Matrix for Matrices using the Schmidt Process and Gram-Schmidt Process and QR Decomposition: The Schmidt matrix can be extended to matrices using the Schmidt process, the Gram-Schmidt process, and the QR decomposition, and its properties can be investigated.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8805831999474973
      },
      {
        "text": "Orthogonalization of a Matrix using the Gram-Schmidt Process and Householder Transformation and Spectral Theorem: The Gram-Schmidt process can be used to orthogonalize a matrix, the Householder transformation can be used to further orthogonalize the matrix, and the spectral theorem can be used to investigate the properties of the orthogonalized matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8860446225299325
      },
      {
        "text": "Orthogonal Decomposition using the Schmidt Process: Decomposing a vector space into an orthogonal basis using the Schmidt process, with applications in signal processing and image analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8340357870929824
      }
    ]
  },
  {
    "representative_text": "Neumann Series: A mathematical technique used to calculate the determinant of a matrix using a power series expansion.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Neumann Series: A mathematical technique used to calculate the determinant of a matrix using a power series expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Neumann Series for determinants: The Neumann series can be used to calculate the determinant of a matrix using a power series expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8793795573788945
      }
    ]
  },
  {
    "representative_text": "The use of determinants in control theory: Determinants are used in control theory to analyze and design control systems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The use of determinants in control theory: Determinants are used in control theory to analyze and design control systems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The use of determinants in numerical analysis: Determinants are used in numerical analysis to analyze and solve numerical problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The use of determinants in numerical analysis: Determinants are used in numerical analysis to analyze and solve numerical problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The relationship between determinants and matrix norms: Determinants are related to matrix norms, which are used to analyze and design numerical algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8161695895976995
      }
    ]
  },
  {
    "representative_text": "The concept of a \"matrix with a non-zero determinant\": A matrix that has a non-zero determinant, which is related to the existence and uniqueness of the inverse of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The concept of a \"matrix with a non-zero determinant\": A matrix that has a non-zero determinant, which is related to the existence and uniqueness of the inverse of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"matrix with a non-integer determinant\": A matrix that has a non-integer determinant, which is related to the existence and uniqueness of the inverse of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9073798175170344
      },
      {
        "text": "Matrix with a non-zero determinant and determinants: A matrix with a non-zero determinant can be used to determine the existence and uniqueness of the inverse of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8662051143648778
      },
      {
        "text": "Matrix with a non-integer determinant and determinants: A matrix with a non-integer determinant can be used to determine the existence and uniqueness of the inverse of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9028027263004085
      }
    ]
  },
  {
    "representative_text": "The use of determinants in statistical analysis: Determinants are used in statistical analysis to analyze and design statistical models.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The use of determinants in statistical analysis: Determinants are used in statistical analysis to analyze and design statistical models.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The application of determinants in cryptography: Determinants are used in cryptography to analyze and design cryptographic algorithms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The application of determinants in cryptography: Determinants are used in cryptography to analyze and design cryptographic algorithms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant and Inverse Matrices in Cryptography: Determinants and inverse matrices are used in cryptography to analyze the behavior of linear transformations. For example, the determinant of a matrix is used to determine the invertibility of a matrix, which is essential in cryptographic protocols.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8582487419705721
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix using the Adjoint and Determinant: A method for computing the determinant of a square matrix using the adjoint and determinant.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix using the Adjoint and Determinant: A method for computing the determinant of a square matrix using the adjoint and determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Properties of a non-standard Determinant a standard Eigenvalue.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Properties of a non-standard Determinant a standard Eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The provided are not theore eigenvalue eigenvalue decomposition.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The provided are not theore eigenvalue eigenvalue decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Role of Schur's in Understanding theore in Topological spaces: Linear Independence of Linear Independence: Linear Independence of the Mazuradifferent spaces are some of Linear Independence: Linear Independence: Linear Independence of Linear Independence of linear independence: A set of Linear Independence of Linear Independence**:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Role of Schur's in Understanding theore in Topological spaces: Linear Independence of Linear Independence: Linear Independence of the Mazuradifferent spaces are some of Linear Independence: Linear Independence: Linear Independence of Linear Independence of linear independence: A set of Linear Independence of Linear Independence**:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Span in Finite-Dimensional Vector Spaces: This statement is not necessarily true for finite-dimensional vector spaces, as the dimension of the vector space is a critical factor.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Span in Finite-Dimensional Vector Spaces: This statement is not necessarily true for finite-dimensional vector spaces, as the dimension of the vector space is a critical factor.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Linear Independence Criterion using the Fundamental Theorem of Algebra: This criterion provides a more general approach to determining linear independence, but it's essential to understand the limitations and nuances of this method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The Linear Independence Criterion using the Fundamental Theorem of Algebra: This criterion provides a more general approach to determining linear independence, but it's essential to understand the limitations and nuances of this method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Independence Criterion using the Schur Complement: This criterion provides a more general approach to determining linear independence, but it's essential to understand the limitations and nuances of this method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9202405293205422
      },
      {
        "text": "The Linear Independence Criterion using the Rank-Nullity Theorem: This criterion provides a more general approach to determining linear independence, but it's essential to understand the limitations and nuances of this method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9450091053964371
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Eigenvectors: The use of eigenvectors as a criterion for determining linear independence is a more advanced concept that requires a deeper understanding of linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Eigenvectors: The use of eigenvectors as a criterion for determining linear independence is a more advanced concept that requires a deeper understanding of linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Span in Projective Spaces: The relationship between linear independence and span in projective spaces is a crucial concept to understand in certain situations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Span in Projective Spaces: The relationship between linear independence and span in projective spaces is a crucial concept to understand in certain situations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Relationship between Linear Independence and the Eigenvalues of a Matrix with a Non-Empty Kernel and Non-Empty Row Space: This point is not explicitly mentioned but is related to the concept of linear independence and eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Relationship between Linear Independence and the Eigenvalues of a Matrix with a Non-Empty Kernel and Non-Empty Row Space: This point is not explicitly mentioned but is related to the concept of linear independence and eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Independence of a Set of Vectors under a Linear Transformation with a Non-Empty Kernel and Non-Empty Row Space: This point is not explicitly mentioned but is related to the concept of linear independence under a linear transformation with a non-empty kernel and row space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8709746157311963
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space of a Matrix with a Non-Empty Kernel: This point is not explicitly mentioned but is related to the concept of linear independence and the null space of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9406212472961881
      },
      {
        "text": "The Relationship between Linear Independence and the Eigenvalues of a Matrix with a Non-Empty Kernel and Non-Empty Row Space (with Non-Real Eigenvalues): This point, as mentioned, is not explicitly listed but is related to the concept of linear independence and eigenvalues with non-real eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9202769356101386
      },
      {
        "text": "The Relationship between Linear Independence and the Null Space of a Matrix with a Non-Empty Kernel and Non-Empty Row Space (with Non-Linear Transformations): This point, as mentioned, is not explicitly listed but is related to the concept of linear independence and the null space of a matrix with a non-empty kernel and row space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9547687407874359
      }
    ]
  },
  {
    "representative_text": "The Dimension of the Intersection of Two Subspaces with Different Dimensions and Non-Empty Null Spaces: This point is not explicitly mentioned but is related to the concept of dimension and intersection of subspaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Dimension of the Intersection of Two Subspaces with Different Dimensions and Non-Empty Null Spaces: This point is not explicitly mentioned but is related to the concept of dimension and intersection of subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Dimension of the Intersection of Two Subspaces with Different Dimensions and Non-Empty Null Spaces (with Non-Linear Transformations): This point, as mentioned, is not explicitly listed but is related to the concept of dimension and intersection of subspaces with different dimensions and non-empty null spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9153902323178447
      }
    ]
  },
  {
    "representative_text": "The Dimension of the Span of a Vector Space with a Non-Empty Kernel and Non-Empty Row Space with Different Dimensions: This point is not explicitly mentioned but is related to the concept of dimension and span of a vector space with a non-empty kernel and row space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Dimension of the Span of a Vector Space with a Non-Empty Kernel and Non-Empty Row Space with Different Dimensions: This point is not explicitly mentioned but is related to the concept of dimension and span of a vector space with a non-empty kernel and row space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Dimension of the Span of a Vector Space with a Non-Empty Kernel and Non-Empty Row Space with Different Dimensions and Non-Empty Column Space (with Non-Linear Transformations): This point, as mentioned, is not explicitly listed but is related to the concept of dimension and span of a vector space with a non-empty kernel, row space, and column space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9459624237997835
      }
    ]
  },
  {
    "representative_text": "The Linear Independence of the Image of a Linear Transformation under a Composition of Linear Transformations with a Non-Empty Row Space: This point is not explicitly mentioned but is related to the concept of linear independence under a composition of linear transformations with a non-empty row space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "The Linear Independence of the Image of a Linear Transformation under a Composition of Linear Transformations with a Non-Empty Row Space: This point is not explicitly mentioned but is related to the concept of linear independence under a composition of linear transformations with a non-empty row space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Linear Independence of the Image of a Linear Transformation under a Change of Basis with a Non-Empty Kernel and Non-Empty Row Space: This point, as mentioned, is not explicitly listed but is related to the concept of linear independence under a change of basis with a non-empty kernel and row space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9009095291824335
      },
      {
        "text": "The Linear Independence of a Set of Vectors under a Linear Transformation with a Non-Empty Column Space (with Non-Linear Transformations): This point, as mentioned, is not explicitly listed but is related to the concept of linear independence under a linear transformation with a non-empty column space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9207923156764488
      },
      {
        "text": "The Linear Independence of a Set of Vectors under a Linear Transformation with a Non-Empty Kernel and Non-Empty Row Space (with Non-Real Eigenvalues): This point, as mentioned, is not explicitly listed but is related to the concept of linear independence under a linear transformation with a non-empty kernel, row space, and non-real eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9389596021505494
      },
      {
        "text": "The Linear Independence of a Set of Vectors under a Linear Transformation with a Non-Empty Column Space and Non-Real Eigenvalues: This point is not explicitly mentioned but is related to the concept of linear independence under a linear transformation with a non-empty column space and non-real eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9347432328755967
      },
      {
        "text": "Linear Independence of a Set of Vectors under a Linear Transformation with Non-Real Eigenvalues: This point is not explicitly listed but is related to the concept of linear independence under a linear transformation with non-real eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9197251826643903
      },
      {
        "text": "Linear Independence of a Set of Vectors under a Linear Transformation with Non-Linear Transformations: This point is not explicitly listed but is related to the concept of linear independence under a linear transformation with non-linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9285142847215551
      }
    ]
  },
  {
    "representative_text": "Dimension Implies Orthogonal Complement Implies Span Implies Basis Implies Linear Independence: The dimension of a vector space is equal to the number of vectors in a basis for the space if and only if the orthogonal complement of the subspace has a basis with the same number of vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Dimension Implies Orthogonal Complement Implies Span Implies Basis Implies Linear Independence: The dimension of a vector space is equal to the number of vectors in a basis for the space if and only if the orthogonal complement of the subspace has a basis with the same number of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of the Orthogonal Complement Implies Span: The dimension of the orthogonal complement of a subspace is equal to the subtopics that spans the other orthogonal complement of basis Implies Basis Implies Basis Theorem and Basis Implies Dimension Theorem, and theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8626104227455629
      }
    ]
  },
  {
    "representative_text": "Non-Standard Analysis: The extension of linear independence to non-standard scalar multiplication using non-standard analysis.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Standard Analysis: The extension of linear independence to non-standard scalar multiplication using non-standard analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Product Space: The relationship between linear independence and non-standard bases in product spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Product Space: The relationship between linear independence and non-standard bases in product spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kuratowski's Fixed Point Theorem: The relationship between linear independence and the existence of a basis in infinite-dimensional vector spaces using Kuratowski's fixed point theorem.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kuratowski's Fixed Point Theorem: The relationship between linear independence and the existence of a basis in infinite-dimensional vector spaces using Kuratowski's fixed point theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Sorgenfrey Line: The extension of linear independence to non-standard bases and the Sorgenfrey line.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sorgenfrey Line: The extension of linear independence to non-standard bases and the Sorgenfrey line.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projections in Infinite-Dimensional Vector Spaces: The relationship between orthogonal projections and linear independence in infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projections in Infinite-Dimensional Vector Spaces: The relationship between orthogonal projections and linear independence in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination for Infinite-Dimensional Matrices: The method for transforming an infinite-dimensional matrix into row echelon form.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination for Infinite-Dimensional Matrices: The method for transforming an infinite-dimensional matrix into row echelon form.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of an Infinite-Dimensional Matrix: The concept of a determinant for an infinite-dimensional matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Determinant of an Infinite-Dimensional Matrix: The concept of a determinant for an infinite-dimensional matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of the determinant of a linear operator on an infinite-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9001885577634634
      },
      {
        "text": "Linear Operator Determinant for Infinite-Dimensional Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9310904398249785
      }
    ]
  },
  {
    "representative_text": "Linear Independence of a Set of Rational Functions: The theorem that states a set of rational functions is linearly independent if and only if the only linear combination that equals the zero function is the trivial solution.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Linear Independence of a Set of Rational Functions: The theorem that states a set of rational functions is linearly independent if and only if the only linear combination that equals the zero function is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of a Set of Trigonometric Series: The theorem that states a set of trigonometric series is linearly independent if and only if the only linear combination that equals the zero function is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8167419657736783
      },
      {
        "text": "Linear Independence of a Set of Polynomials: The theorem that states a set of polynomials is linearly independent if and only if the only linear combination that equals the zero function is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9093290264677265
      },
      {
        "text": "Linear Independence of a Set of Matrices: The theorem that states a set of matrices is linearly independent if and only if the only linear combination of the matrices that equals the zero matrix is the trivial solution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8563519364183603
      }
    ]
  },
  {
    "representative_text": "Cyclic Vectors: A vector that can be expressed as a linear combination of other vectors in a basis. This concept is closely related to the concept of linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cyclic Vectors: A vector that can be expressed as a linear combination of other vectors in a basis. This concept is closely related to the concept of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis Vectors with Zero Norm: In some vector spaces, the basis vectors may have zero norm. This requires a more careful analysis of linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis Vectors with Zero Norm: In some vector spaces, the basis vectors may have zero norm. This requires a more careful analysis of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Convergence of Sequences of Vectors: In infinite-dimensional spaces, we need to consider the concept of convergence of sequences of vectors. This is a more nuanced concept than in finite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Convergence of Sequences of Vectors: In infinite-dimensional spaces, we need to consider the concept of convergence of sequences of vectors. This is a more nuanced concept than in finite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Convergence of Sequences of Vectors in Infinite-Dimensional Spaces: In infinite-dimensional spaces, we need to consider the concept of convergence of sequences of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9170589315629044
      },
      {
        "text": "Convergence of Sequences of Vectors in Infinite-Dimensional Spaces with Non-Standard Metric: In infinite-dimensional spaces, we need to consider the concept of convergence of sequences of vectors when the vector space is equipped with a non-standard metric.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9054649862480688
      }
    ]
  },
  {
    "representative_text": "Span of a Set of Vectors with Non-Standard Properties: This concept studies the span of a set of vectors with non-standard properties, such as non-standard norms or inner products.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span of a Set of Vectors with Non-Standard Properties: This concept studies the span of a set of vectors with non-standard properties, such as non-standard norms or inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis of a Vector Space with a Non-Standard Dimension - Constructive Methods: This concept provides constructive methods for finding a basis of a vector space with a non-standard dimension.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis of a Vector Space with a Non-Standard Dimension - Constructive Methods: This concept provides constructive methods for finding a basis of a vector space with a non-standard dimension.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Existence of a Basis in Non-Standard Vector Spaces with Non-Standard Dimensions - Geometric Implications: This concept explores the geometric implications of the existence of a basis in non-standard vector spaces with non-standard dimensions.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Existence of a Basis in Non-Standard Vector Spaces with Non-Standard Dimensions - Geometric Implications: This concept explores the geometric implications of the existence of a basis in non-standard vector spaces with non-standard dimensions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of a Basis in Non-Standard Vector Spaces with Non-Standard Dimensions - Algebraic Implications: This concept explores the algebraic implications of the existence of a basis in non-standard vector spaces with non-standard dimensions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9151300441320679
      },
      {
        "text": "Existence of a Basis in Non-Standard Vector Spaces with Non-Standard Dimensions - Topological Implications: This concept examines the topological implications of the existence of a basis in non-standard vector spaces with non-standard dimensions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9370614439232946
      },
      {
        "text": "Basis of a Vector Space with a Non-Standard Linear Transformation - Algebraic Implications: This concept studies the algebraic implications of the basis of a vector space with a non-standard linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8442688923913964
      }
    ]
  },
  {
    "representative_text": "Basis of a Vector Space with a Non-Standard Topology - Analytic Implications: This concept examines the implications of the basis of a vector space with a non-standard topology on its analytic properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis of a Vector Space with a Non-Standard Topology - Analytic Implications: This concept examines the implications of the basis of a vector space with a non-standard topology on its analytic properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The concept of a \"Linearly Independent System of Bases Extension with Respect to a Non-Standard Scalar Multiplication\": A set of linearly independent systems of bases for a vector space with non-standard scalar multiplication that can be extended to a larger set of linearly independent systems of bases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The concept of a \"Linearly Independent System of Bases Extension with Respect to a Non-Standard Scalar Multiplication\": A set of linearly independent systems of bases for a vector space with non-standard scalar multiplication that can be extended to a larger set of linearly independent systems of bases.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The concept of a \"Linearly Independent Basis Extension with Respect to a Non-Standard Scalar Multiplication\": A basis for a vector space with non-standard scalar multiplication that can be extended to a larger basis by adding vectors from the original basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9388148497487631
      },
      {
        "text": "The concept of a \"Linearly Independent System of Bases with Respect to a Non-Standard Scalar Multiplication\": A set of bases for a vector space with non-standard scalar multiplication that is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9323521634672362
      },
      {
        "text": "The concept of a \"Linearly Independent System of Bases with Respect to a Non-Standard Vector Addition\": A set of bases for a vector space with non-standard vector addition that is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9293139592026016
      },
      {
        "text": "The concept of a \"Linearly Dependent System of Bases with Respect to a Non-Standard Vector Addition\": A set of bases for a vector space with non-standard vector addition that is linearly dependent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9293026294908848
      }
    ]
  },
  {
    "representative_text": "Dimension of the Span of a Linearly Independent Set and its Orthogonal Complement: The dimension of the span of a linearly independent set and its orthogonal complement is equal to the dimension of the entire vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension of the Span of a Linearly Independent Set and its Orthogonal Complement: The dimension of the span of a linearly independent set and its orthogonal complement is equal to the dimension of the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of the Orthogonal Decomposition: The span of the orthogonal decomposition of a vector space is the entire vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Span of the Orthogonal Decomposition: The span of the orthogonal decomposition of a vector space is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Dimension of the Span of the Orthogonal Decomposition: The dimension of the span of the orthogonal decomposition of a vector space is equal to the number of subspaces in the decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8396750988688312
      },
      {
        "text": "Span of the Orthogonal Decomposition of a Free Basis: The span of the orthogonal decomposition of a free basis is the entire vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8791219692062762
      },
      {
        "text": "Dimension of the Span of the Orthogonal Decomposition of a Free Basis: The dimension of the span of the orthogonal decomposition of a free basis is equal to the number of subspaces in the decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8973249926717657
      }
    ]
  },
  {
    "representative_text": "Free Basis and Orthogonal Decomposition: A free basis of a vector space can be used to decompose the space into a direct sum of subspaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Free Basis and Orthogonal Decomposition: A free basis of a vector space can be used to decompose the space into a direct sum of subspaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Direct Sum Decomposition: The direct sum decomposition of a vector space into a direct sum of subspaces, which can be used to show that can be used in the study of a basis for Linear Independence of vectors in this theorem has implications of a generalization of a set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8008093334126256
      },
      {
        "text": "Free Basis and Orthogonal Decomposition: The free basis can be used to decompose a vector space into a direct sum of subspaces, and its variations can be used to solve optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8780205385123889
      }
    ]
  },
  {
    "representative_text": "The Impact of Non-Standard Fuzzy Linear Operators on Linear Independence: Investigating the impact of non-standard fuzzy linear operators on linear independence in infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Impact of Non-Standard Fuzzy Linear Operators on Linear Independence: Investigating the impact of non-standard fuzzy linear operators on linear independence in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Fuzzy linear operators and linear independence: Explore the properties of fuzzy linear operators and their relationship to linear independence in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9033085012143824
      }
    ]
  },
  {
    "representative_text": "Linear Independence and the Properties of a Fuzzy Banach Space: Exploring the properties of a fuzzy Banach space, which is a type of infinite-dimensional Banach space, and the concept of linear independence in this context.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence and the Properties of a Fuzzy Banach Space: Exploring the properties of a fuzzy Banach space, which is a type of infinite-dimensional Banach space, and the concept of linear independence in this context.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of a Linear Combination of Vectors and Linear Independence for Infinite-Dimensional Vector Spaces: Examine the relationship between the span of a linear combination of vectors and linear independence in infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span of a Linear Combination of Vectors and Linear Independence for Infinite-Dimensional Vector Spaces: Examine the relationship between the span of a linear combination of vectors and linear independence in infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Clustering and Dimensionality Reduction: Linear algebra is used in clustering algorithms like k-means and hierarchical clustering to reduce the dimensionality of data and group similar objects together.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Clustering and Dimensionality Reduction: Linear algebra is used in clustering algorithms like k-means and hierarchical clustering to reduce the dimensionality of data and group similar objects together.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Independent Component Analysis (ICA): ICA is a technique used in computer graphics and game development to separate mixed signals into independent components.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Independent Component Analysis (ICA): ICA is a technique used in computer graphics and game development to separate mixed signals into independent components.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Factorization: Matrix factorization is a technique used in computer graphics and game development to decompose matrices into smaller matrices that capture the most important features.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Factorization: Matrix factorization is a technique used in computer graphics and game development to decompose matrices into smaller matrices that capture the most important features.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quadratic Forms and Their Applications: Quadratic forms are used in computer graphics and game development to solve systems of linear equations and to find the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quadratic Forms and Their Applications: Quadratic forms are used in computer graphics and game development to solve systems of linear equations and to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Methods for Machine Learning: Linear algebraic methods like linear regression and curve fitting are used in computer graphics and game development to train machine learning models.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Methods for Machine Learning: Linear algebraic methods like linear regression and curve fitting are used in computer graphics and game development to train machine learning models.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-Linear Algebraic Methods for Computer Vision: Non-linear algebraic methods like optimization techniques and homotopy methods are used in computer graphics and game development to solve problems in computer vision.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Linear Algebraic Methods for Computer Vision: Non-linear algebraic methods like optimization techniques and homotopy methods are used in computer graphics and game development to solve problems in computer vision.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Topology and Its Applications: Computational topology is a field that uses linear algebra to study the properties of geometric shapes, and it has many applications in computer graphics and game development.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Topology and Its Applications: Computational topology is a field that uses linear algebra to study the properties of geometric shapes, and it has many applications in computer graphics and game development.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Methods for Graphics Rendering: Linear algebraic methods like linear regression and curve fitting are used in graphics rendering to optimize tasks like lighting, shading, and animation.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Methods for Graphics Rendering: Linear algebraic methods like linear regression and curve fitting are used in graphics rendering to optimize tasks like lighting, shading, and animation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Clustering: A technique that uses eigenvalue decomposition to cluster data points based on their eigenvectors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Clustering: A technique that uses eigenvalue decomposition to cluster data points based on their eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Computer Vision: Optical Flow Estimation: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to estimate the motion between frames in video sequences.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Computer Vision: Optical Flow Estimation: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to estimate the motion between frames in video sequences.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kernel Methods: Polynomial Kernels: A type of kernel function that uses polynomial transformations to map data into a higher-dimensional space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kernel Methods: Polynomial Kernels: A type of kernel function that uses polynomial transformations to map data into a higher-dimensional space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Deep Learning: Attention Mechanisms for Graph Neural Networks: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to optimize attention mechanisms for graph neural networks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Deep Learning: Attention Mechanisms for Graph Neural Networks: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to optimize attention mechanisms for graph neural networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Deep Learning: Graph Attention Networks with Spectral Graph Attention: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to optimize graph attention networks with spectral graph attention.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9384088841604679
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Reinforcement Learning: Value Function Decomposition: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to decompose value functions in reinforcement learning.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Reinforcement Learning: Value Function Decomposition: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to decompose value functions in reinforcement learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra for Reinforcement Learning: Value Function Decomposition with Temporal Difference Methods and Orthogonal Projections: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to decompose value functions using temporal difference methods and orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9529073958389395
      },
      {
        "text": "Linear Algebra for Reinforcement Learning: Value Function Decomposition with Temporal Difference Methods and Spectral Graph Convolution: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to decompose value functions using temporal difference methods and spectral graph convolution.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9575850016730171
      }
    ]
  },
  {
    "representative_text": "Higher-Order Singular Value Decomposition (HSVD): An extension of Singular Value Decomposition (SVD) that can handle higher-order tensors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Higher-Order Singular Value Decomposition (HSVD): An extension of Singular Value Decomposition (SVD) that can handle higher-order tensors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Train Decomposition: A method for factorizing tensors into a sum of lower-order tensors, useful for applications in control theory and machine learning.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Tensor Train Decomposition: A method for factorizing tensors into a sum of lower-order tensors, useful for applications in control theory and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Tensor Train Decomposition: A method for factorizing tensors into a sum of",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8453461457496199
      }
    ]
  },
  {
    "representative_text": "Kronecker Product of Linear Transformations: A way to combine linear transformations using the Kronecker product, useful for applications in control theory and signal processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kronecker Product of Linear Transformations: A way to combine linear transformations using the Kronecker product, useful for applications in control theory and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Model-Based Predictive Control (MPC) with Non-Convex Optimization: A control strategy that uses linear algebra and optimization techniques to predict the behavior of a finite-hyperop",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Model-Based Predictive Control (MPC) with Non-Convex Optimization: A control strategy that uses linear algebra and optimization techniques to predict the behavior of a finite-hyperop",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quantum Key Distribution (QKD) protocols and their security guarantees: This includes the analysis of QKD protocols, such as BB84, Ekert91, and others, and their security guarantees against various attacks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quantum Key Distribution (QKD) protocols and their security guarantees: This includes the analysis of QKD protocols, such as BB84, Ekert91, and others, and their security guarantees against various attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Side-channel attack resistance techniques for quantum computers: This includes the analysis of side-channel attack resistance techniques, such as quantum-resistant masking and quantum-resistant key recovery, for protecting quantum computers against side-channel attacks.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Side-channel attack resistance techniques for quantum computers: This includes the analysis of side-channel attack resistance techniques, such as quantum-resistant masking and quantum-resistant key recovery, for protecting quantum computers against side-channel attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quantum-resistant cryptographic techniques for secure authentication and authorization: This includes the analysis of quantum-resistant cryptographic techniques, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, for secure authentication and authorization.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quantum-resistant cryptographic techniques for secure authentication and authorization: This includes the analysis of quantum-resistant cryptographic techniques, such as lattice-based cryptography, code-based cryptography, and multivariate cryptography, for secure authentication and authorization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The fundamental theorem of linear transformations for infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "The fundamental theorem of linear transformations for infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The invariant subspace theorem for linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.915458756656857
      },
      {
        "text": "The consequence of the rank-nullity theorem for linear operators on linear operators on linear operators on infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8890366829822084
      },
      {
        "text": "Fundamental Theorem of Linear Transformations for Infinite-Dimensional Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.889436692269451
      },
      {
        "text": "Invariant Subspace Theorem for Linear Operators on Infinite-Dimensional Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9182584751499268
      },
      {
        "text": "Linear Operator Rank-Nullity Consequence for Infinite-Dimensional Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8867608692269892
      }
    ]
  },
  {
    "representative_text": "Although theorel.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Although theorel.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Spanning Set Theorem for Vector Spaces: This theorem states that a set of vectors {v1, v2, ..., vn} spans V if and only if every vector w ∈ V can be expressed as a linear combination of these vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Spanning Set Theorem for Vector Spaces: This theorem states that a set of vectors {v1, v2, ..., vn} spans V if and only if every vector w ∈ V can be expressed as a linear combination of these vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Cofactor Expansion and Determinant Theorem for Vector Spaces: This theorem states that the determinant of a matrix A can be computed using the cofactors of each element, and it can be used to study the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Cofactor Expansion and Determinant Theorem for Vector Spaces: This theorem states that the determinant of a matrix A can be computed using the cofactors of each element, and it can be used to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Symmetric and Skew-Symmetric Matrices: A matrix that is equal to its own transpose, and a matrix that is not equal to its own transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Symmetric and Skew-Symmetric Matrices: A matrix that is equal to its own transpose, and a matrix that is not equal to its own transpose.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Trace and Determinant: The sum of the diagonal elements of a matrix, and the product of the diagonal elements of a matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Trace and Determinant: The sum of the diagonal elements of a matrix, and the product of the diagonal elements of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cramer's Rule and Variance: A method for solving systems of linear equations by using determinants, and the concept of variance in probability theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cramer's Rule and Variance: A method for solving systems of linear equations by using determinants, and the concept of variance in probability theory.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement and Orthogonal Projection: This involves studying the properties of orthogonal complements and orthogonal projections, including the definition of orthogonal complements, orthogonal projection properties, and the characterization of orthogonal projection spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 7,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement and Orthogonal Projection: This involves studying the properties of orthogonal complements and orthogonal projections, including the definition of orthogonal complements, orthogonal projection properties, and the characterization of orthogonal projection spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Complement and Orthogonal Projection Theorems: This involves studying the theorems related to orthogonal complements and orthogonal projections, including the definition of orthogonal complement theorems, orthogonal projection theorems properties, and the characterization of orthogonal complement and orthogonal projection theorem spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9060174771473883
      },
      {
        "text": "Additional knowledge for Orthogonality and Orthogonal Projections: This sub-category studies the properties of orthogonal vectors, orthogonal projections, and orthogonal matrices. -> Orthogonal Complement: This involves finding the orthogonal complement of a subspace, which is a set of vectors that are orthogonal to every vector in the given subspace. The orthogonal complement is also known as the annihilator or the orthogonalizer.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8299909887818694
      },
      {
        "text": "Additional knowledge for Orthogonality and Orthogonal Projections: This sub-category studies the properties of orthogonal vectors, orthogonal projections, and orthogonal matrices. -> Orthogonal Diagonalization: This is a process of diagonalizing a matrix using orthogonal matrices, which results in a diagonal matrix containing the eigenvalues of the original matrix. This technique is useful in solving systems of linear equations and in linear algebra applications.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8087210074882056
      },
      {
        "text": "Additional knowledge for Orthogonality and Orthogonal Projections: This sub-category studies the properties of orthogonal vectors, orthogonal projections, and orthogonal matrices. -> Orthogonal Decomposition: This is a method of decomposing a vector or a matrix into its orthogonal components, which are orthogonal to each other. Orthogonal decomposition is commonly used in signal processing, image compression, and other applications where data can be decomposed into its orthogonal components.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8749929277212061
      },
      {
        "text": "Additional knowledge for Orthogonality and Orthogonal Projections: This sub-category studies the properties of orthogonal vectors, orthogonal projections, and orthogonal matrices. -> Orthogonal Projection Matrices: This involves finding the orthogonal projection matrix onto a subspace, which is a matrix that projects any vector onto a subspace while preserving the orthogonality between the projected vector and the orthogonal complement of the subspace.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9245593568469533
      },
      {
        "text": "Additional knowledge for Orthogonality and Orthogonal Projections: This sub-category studies the properties of orthogonal vectors, orthogonal projections, and orthogonal matrices. -> Gram-Schmidt Process: This is a method for orthonormalizing a set of vectors, which involves orthogonalizing the vectors and then normalizing them to have a length of 1. The Gram-Schmidt process is a fundamental technique in linear algebra and is used to find orthonormal bases for vector spaces.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9016585448085996
      }
    ]
  },
  {
    "representative_text": "Orthogonalization and Orthogonalization Methods: This involves studying the properties of orthogonalization and orthogonalization methods, including the definition of orthogonalization, orthogonalization method properties, and the characterization of orthogonalization spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonalization and Orthogonalization Methods: This involves studying the properties of orthogonalization and orthogonalization methods, including the definition of orthogonalization, orthogonalization method properties, and the characterization of orthogonalization spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Methods and Linear Algebra: This involves studying the properties of numerical methods and linear algebra, including the definition of numerical methods, numerical method properties, and the characterization of numerical method spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Methods and Linear Algebra: This involves studying the properties of numerical methods and linear algebra, including the definition of numerical methods, numerical method properties, and the characterization of numerical method spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Stability and Convergence of Numerical Methods: This involves studying the properties of stability and convergence of numerical methods, including the definition of stability and convergence, stability and convergence method properties, and the characterization of stability and convergence spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Stability and Convergence of Numerical Methods: This involves studying the properties of stability and convergence of numerical methods, including the definition of stability and convergence, stability and convergence method properties, and the characterization of stability and convergence spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Error Analysis and Error Bound: This involves studying the properties of error analysis and error bound, including the definition of error analysis, error bound properties, and the characterization of error analysis and error bound spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Error Analysis and Error Bound: This involves studying the properties of error analysis and error bound, including the definition of error analysis, error bound properties, and the characterization of error analysis and error bound spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cauchy-Schwarz Inequality and Bessel's Inequality: This involves studying the properties of Cauchy-Schwarz inequality and Bessel's inequality, including the definition of Cauchy-Schwarz inequality and Bessel's inequality properties, and the characterization of Cauchy-Schwarz inequality and Bessel's inequality theorem spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cauchy-Schwarz Inequality and Bessel's Inequality: This involves studying the properties of Cauchy-Schwarz inequality and Bessel's inequality, including the definition of Cauchy-Schwarz inequality and Bessel's inequality properties, and the characterization of Cauchy-Schwarz inequality and Bessel's inequality theorem spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Plancherel's Theorem and Fourier Transform and Fourier Transform and Fourier Transform: This involves: This involves studying theorevergence of Orthogarithmorphism: This involves studying theore",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Plancherel's Theorem and Fourier Transform and Fourier Transform and Fourier Transform: This involves: This involves studying theorevergence of Orthogarithmorphism: This involves studying theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Existence of Infinite Orthogonal Sets of Orthogonal Sets of Infinite Orthogonal Decomposition of Linear Algebraic coexistence of a  Orthogonal Decomposition of a 0",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Existence of Infinite Orthogonal Sets of Orthogonal Sets of Infinite Orthogonal Decomposition of Linear Algebraic coexistence of a  Orthogonal Decomposition of a 0",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Linear Transformations: This sub-category explores the concept of linear transformations, including linear operators, matrices, and their relationships with vector spaces. -> Linear Operators: This sub-field focuses on the properties and behavior of linear operators, which are functions between vector spaces that preserve the operations of vector addition and scalar multiplication. Linear operators can be represented by matrices, and their properties can be used to determine their behavior on specific vector spaces.:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Linear Transformations: This sub-category explores the concept of linear transformations, including linear operators, matrices, and their relationships with vector spaces. -> Linear Operators: This sub-field focuses on the properties and behavior of linear operators, which are functions between vector spaces that preserve the operations of vector addition and scalar multiplication. Linear operators can be represented by matrices, and their properties can be used to determine their behavior on specific vector spaces.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Additional knowledge for Linear Transformations: This sub-category explores the concept of linear transformations, including linear operators, matrices, and their relationships with vector spaces. -> Matrix Representations of Linear Transformations: This sub-field explores the relationship between linear transformations and their matrix representations. It discusses the concept of matrix multiplication and how it can be used to represent the action of a linear transformation on a vector. The sub-field also delves into the properties of matrix representations, such as the fact that the matrix of a linear transformation can be used to determine its eigenvalues and eigenvectors.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9334886008657179
      },
      {
        "text": "Additional knowledge for Vector Spaces: This sub-category deals with the study of vector spaces, including definitions, properties, and operations such as addition, scalar multiplication, and basis vectors. -> Linear Transformations and Matrix Representations: This sub-field explores the relationship between linear transformations and matrix representations. It involves studying how linear transformations can be represented as matrices and how these matrices can be used to solve systems of linear equations and find eigenvalues and eigenvectors.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8931564784670458
      }
    ]
  },
  {
    "representative_text": "Bilinear Transformation Theorem:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bilinear Transformation Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dual Basis:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Dual Basis:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "**Dual Basis Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8732102621539033
      }
    ]
  },
  {
    "representative_text": "Pinching Theorem:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pinching Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Interlacing Theorem: A theorem that states that the eigenvalues of a matrix are interlaced, meaning that the eigenvalues of a matrix are either in increasing or decreasing order.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Eigenvalue Interlacing Theorem: A theorem that states that the eigenvalues of a matrix are interlaced, meaning that the eigenvalues of a matrix are either in increasing or decreasing order.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Interlacing Theorem: A theorem that states that the eigenvalues of a matrix and its submatrices interlace, meaning that the eigenvalues of the submatrix are between the eigenvalues of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.942837164898781
      },
      {
        "text": "Eigenvalue Interlacing: Eigenvalue interlacing is a theorem that states that the eigenvalues of a matrix are interlaced with the eigenvalues of a related matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9360847075690848
      },
      {
        "text": "Definition: Eigenvalue interlacing is a theorem that states that the eigenvalues of a matrix are interlaced with the eigenvalues of a related matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9506046357686213
      },
      {
        "text": "Interlacing Theorem: The interlacing theorem states that the eigenvalues of a matrix are interlaced with the eigenvalues of a related matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9490015228710997
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix using Cyclic Matrices: A^T: A Alternating Linear Algebraic Matrix: A theorem: A Rank- Matrix: A generalization of a mathematical structure of a differentiable matrices: A, which is not only theore",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix using Cyclic Matrices: A^T: A Alternating Linear Algebraic Matrix: A theorem: A Rank- Matrix: A generalization of a mathematical structure of a differentiable matrices: A, which is not only theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix using the Cauchy-Binet Formula with Complex Entries: This involves the study of the Cauchy-Binet formula for calculating the determinant of a matrix with complex entries.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix using the Cauchy-Binet Formula with Complex Entries: This involves the study of the Cauchy-Binet formula for calculating the determinant of a matrix with complex entries.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Invertibility Criterion using the Inverse of the Adjugate Matrix for Block Matrices: This is a criterion for determining the invertibility of a block matrix using the inverse of the adjugate matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Invertibility Criterion using the Inverse of the Adjugate Matrix for Block Matrices: This is a criterion for determining the invertibility of a block matrix using the inverse of the adjugate matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Arnoldi Iteration: The Arnoldi iteration is an algorithm used to find the eigenvalues and eigenvectors of a matrix. It involves iteratively applying a QR decomposition and using the resulting matrices to estimate the eigenvalues.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Arnoldi Iteration: The Arnoldi iteration is an algorithm used to find the eigenvalues and eigenvectors of a matrix. It involves iteratively applying a QR decomposition and using the resulting matrices to estimate the eigenvalues.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Rayleigh Interpolation Formula: The Rayleigh interpolation formula is a formula used to estimate the eigenvalues of a matrix. It is defined as the average of the eigenvalues of a set of matrices that are similar to the original matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Rayleigh Interpolation Formula: The Rayleigh interpolation formula is a formula used to estimate the eigenvalues of a matrix. It is defined as the average of the eigenvalues of a set of matrices that are similar to the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The QR Interpolation Formula: The QR interpolation formula is a formula used to estimate the eigenvalues of a matrix. It is defined as the average of the eigenvalues of a set of matrices that are similar to the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8902629509745692
      },
      {
        "text": "The Rayleigh Interpolation Formula: This formula is used to estimate the eigenvalues of a matrix, defined as the average of the eigenvalues of a set of matrices that are similar to the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9405380222430186
      },
      {
        "text": "The QR Interpolation Formula: This formula is used to estimate the eigenvalues of a matrix, defined as the average of the eigenvalues of a set of matrices that are similar to the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9446998569269217
      }
    ]
  },
  {
    "representative_text": "The Lanczos Interpolation Formula: The Lanczos interpolation formula is a formula used to estimate the eigenvalues of a matrix. It is defined as the average of the eigenvalues of a set of matrices that are similar to the original matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Lanczos Interpolation Formula: The Lanczos interpolation formula is a formula used to estimate the eigenvalues of a matrix. It is defined as the average of the eigenvalues of a set of matrices that are similar to the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Lanczos Interpolation Formula: This formula is used to estimate the eigenvalues of a matrix, defined as the average of the eigenvalues of a set of matrices that are similar to the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9468409051814449
      }
    ]
  },
  {
    "representative_text": "Load Balancing and Synchronization: Discuss the importance of load balancing and synchronization in parallelizing matrix inversion methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Load Balancing and Synchronization: Discuss the importance of load balancing and synchronization in parallelizing matrix inversion methods.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block Triangular Matrices: Discuss block triangular matrices, which are square matrices that can be partitioned into a block upper triangular matrix and a block lower triangular matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block Triangular Matrices: Discuss block triangular matrices, which are square matrices that can be partitioned into a block upper triangular matrix and a block lower triangular matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computation of Eigenvalues using the Levenberg-Marquardt Method: The Levenberg-Marquardt method is an iterative method for computing eigenvalues of a matrix. It involves minimizing the Frobenius norm of the residual matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computation of Eigenvalues using the Levenberg-Marquardt Method: The Levenberg-Marquardt method is an iterative method for computing eigenvalues of a matrix. It involves minimizing the Frobenius norm of the residual matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Algebraicarithmetic and Geometric Properties of Linear Independence of Eigenvalue Decomposition of a Matrix: Theorem**: Theorem for a Basis for eigenvalue decomposition of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Algebraicarithmetic and Geometric Properties of Linear Independence of Eigenvalue Decomposition of a Matrix: Theorem**: Theorem for a Basis for eigenvalue decomposition of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bessel's Inequality for Non-Symmetric Inequality for Non-Spectral Mapping Theorem: This is a generalized eigenvalue decomposition: This is not have been in a matrix: This is not mentioned.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bessel's Inequality for Non-Symmetric Inequality for Non-Spectral Mapping Theorem: This is a generalized eigenvalue decomposition: This is not have been in a matrix: This is not mentioned.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Decomposition using Singular Value Decomposition (SVD): While the SVD is mentioned, the specific application of orthogonal decomposition using SVD for orthogonal projection matrices is not explicitly listed.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Decomposition using Singular Value Decomposition (SVD): While the SVD is mentioned, the specific application of orthogonal decomposition using SVD for orthogonal projection matrices is not explicitly listed.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Schur Complement: The Schur complement is a way to compute the determinant of a block matrix. It has applications in orthogonal projection matrices, particularly when dealing with block matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Schur Complement: The Schur complement is a way to compute the determinant of a block matrix. It has applications in orthogonal projection matrices, particularly when dealing with block matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Boundedness of the Gram-Schmidt Process: Investigate the boundedness of the Gram-Schmidt process, which refers to the existence of a constant that bounds the norm of the resulting orthogonal vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Boundedness of the Gram-Schmidt Process: Investigate the boundedness of the Gram-Schmidt process, which refers to the existence of a constant that bounds the norm of the resulting orthogonal vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a matrix with a non-integer determinant: The determinant of a matrix with a non-integer determinant can be calculated using the formula for the determinant of a matrix with a non-integer determinant.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Determinant of a matrix with a non-integer determinant: The determinant of a matrix with a non-integer determinant can be calculated using the formula for the determinant of a matrix with a non-integer determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a matrix with a non-zero determinant in a non-integer determinant: The determinant of a matrix with a non-zero determinant in a non-integer determinant can be calculated using the formula for the determinant of a matrix with a non-zero determinant in a non-integer determinant.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9460021150978299
      }
    ]
  },
  {
    "representative_text": "This theorem that is a matrix can be a matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "This theorem that is a matrix can be a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "This theorem states that is not only if A has been improved version of the original",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "This theorem states that is not only if A has been improved version of the original",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "It seems to be:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "It seems to be:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Some additional knowledge:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Some additional knowledge:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Some additional knowledge points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9405264482594371
      }
    ]
  },
  {
    "representative_text": "Here are not also includes theorel:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are not also includes theorel:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Relationship between Span and Basis using the Orthogonal Complement: The span of a set of vectors is equal to the orthogonal complement is equal to the spanned up of the relationship between two basis Implies Dimension Implies the spanned to the spanned vectors is not only if and Basis Extension Theorem: Theorem: Theorem: Theorem: Theorem: Theorem**: A set of a basis Extension of the spanned Implies Basis Extension Theorem.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relationship between Span and Basis using the Orthogonal Complement: The span of a set of vectors is equal to the orthogonal complement is equal to the spanned up of the relationship between two basis Implies Dimension Implies the spanned to the spanned vectors is not only if and Basis Extension Theorem: Theorem: Theorem: Theorem: Theorem: Theorem**: A set of a basis Extension of the spanned Implies Basis Extension Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem of Finite-Dimensional Vector Spaces with Bounded Bases: This theorem states that if a vector space has a bounded basis, then the space is finite-dimensional.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Theorem of Finite-Dimensional Vector Spaces with Bounded Bases: This theorem states that if a vector space has a bounded basis, then the space is finite-dimensional.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kolmogorov's Theorem: This theorem states that if a vector space has a basis, then the space is finite-dimensional if and only if the dimension of the space is finite.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kolmogorov's Theorem: This theorem states that if a vector space has a basis, then the space is finite-dimensional if and only if the dimension of the space is finite.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Theorem of Linear Independence in Infinite-Dimensional Vector Spaces: This theorem states that if a set of vectors is linearly independent in an infinite-dimensional vector space, then the set spans the space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Theorem of Linear Independence in Infinite-Dimensional Vector Spaces: This theorem states that if a set of vectors is linearly independent in an infinite-dimensional vector space, then the set spans the space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence Implies Span for Infinite-Dimensional Vector Spaces: This theorem states that if a set of vectors is linearly independent, then the span of the set is a subspace of the original vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9060560336385854
      },
      {
        "text": "Span Implies Linear Independence for Infinite-Dimensional Vector Spaces: This theorem states that if the span of a set of vectors is a subspace, then the set is linearly independent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9503202720537751
      }
    ]
  },
  {
    "representative_text": "Non-Standard Basis and Spanning Property in Infinite-Dimensional Spaces: In infinite-dimensional spaces, we need to consider the concept of convergence and how it affects the spanning property when the vector space is equipped with a non-standard basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Standard Basis and Spanning Property in Infinite-Dimensional Spaces: In infinite-dimensional spaces, we need to consider the concept of convergence and how it affects the spanning property when the vector space is equipped with a non-standard basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis Vectors with Non-Standard Basis and Non-Standard Metric: In some vector spaces, the basis vectors may have non-standard basis vectors and non-standard metric. This requires a more careful analysis of linear independence and span.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis Vectors with Non-Standard Basis and Non-Standard Metric: In some vector spaces, the basis vectors may have non-standard basis vectors and non-standard metric. This requires a more careful analysis of linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "This theorem for V.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "This theorem for V.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "This theorem for Infinite-Dimply basis for Infinite-Dimensional vector space is missing point-wise Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies basis for Infinite-Dimensional vector space",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "This theorem for Infinite-Dimply basis for Infinite-Dimensional vector space is missing point-wise Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Linear Independence Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Linear Independence Implies Basis Implies Basis Implies Basis Implies Basis Implies Basis Implies basis for Infinite-Dimensional vector space",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spanning Set Theorem Variations: While the Spanning Set Theorem states that if a set of vectors spans a vector space, then every vector in the space can be expressed as a linear combination of the spanning set vectors, variations of this theorem for infinite dimensional spaces or for specific types of vector spaces could be explored further.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spanning Set Theorem Variations: While the Spanning Set Theorem states that if a set of vectors spans a vector space, then every vector in the space can be expressed as a linear combination of the spanning set vectors, variations of this theorem for infinite dimensional spaces or for specific types of vector spaces could be explored further.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-standard seminorms and linear independence: Investigate the impact of non-standard seminorms on linear independence in infinite-dimensional spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-standard seminorms and linear independence: Investigate the impact of non-standard seminorms on linear independence in infinite-dimensional spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The impact of non-standard inner products on linear independence in infinite-dimensional Hilbert spaces: Investigate the impact of non-standard inner products on linear independence in infinite-dimensional Hilbert spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The impact of non-standard inner products on linear independence in infinite-dimensional Hilbert spaces: Investigate the impact of non-standard inner products on linear independence in infinite-dimensional Hilbert spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Finite-Dimensional Span Theorem: This theorem states that the span of a set of vectors is equal to the image of the linear transformation that maps each vector to the zero vector.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Finite-Dimensional Span Theorem: This theorem states that the span of a set of vectors is equal to the image of the linear transformation that maps each vector to the zero vector.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dimension of the Intersection of Span and Null Space: The dimension of the intersection of the span of a set of vectors and the null space of a matrix is equal to the number of linearly independent vectors in the span of the set.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension of the Intersection of Span and Null Space: The dimension of the intersection of the span of a set of vectors and the null space of a matrix is equal to the number of linearly independent vectors in the span of the set.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Row Echelon Form of a Matrix with Rank 1: A matrix with rank 1 has only one linearly independent row (or column), which is the row (or column) containing a single non-zero entry.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Row Echelon Form of a Matrix with Rank 1: A matrix with rank 1 has only one linearly independent row (or column), which is the row (or column) containing a single non-zero entry.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of the Column Space: The span of the column space of a matrix is equal to the entire codomain.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span of the Column Space: The span of the column space of a matrix is equal to the entire codomain.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Weyl's Unitary Trick: This trick involves using the concept of orthogonal projections to find a unitary matrix that diagonalizes a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Weyl's Unitary Trick: This trick involves using the concept of orthogonal projections to find a unitary matrix that diagonalizes a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear System Solving Techniques: Techniques such as Gaussian elimination, LU decomposition, and Cramer's rule for solving systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear System Solving Techniques: Techniques such as Gaussian elimination, LU decomposition, and Cramer's rule for solving systems of linear equations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear system of equations: A system of equations that can be solved using linear algebra techniques, such as Gaussian elimination and LU decomposition.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8194477044232182
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) Techniques: Techniques such as SVD factorization, singular value thresholding, and SVD-based image denoising.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) Techniques: Techniques such as SVD factorization, singular value thresholding, and SVD-based image denoising.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Factorization Techniques: Techniques such as matrix factorization using singular values, matrix factorization using eigenvalues, and matrix factorization using polynomial eigendecomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Factorization Techniques: Techniques such as matrix factorization using singular values, matrix factorization using eigenvalues, and matrix factorization using polynomial eigendecomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quadratic Form Techniques: Techniques such as quadratic form decomposition, quadratic form factorization, and quadratic form approximation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quadratic Form Techniques: Techniques such as quadratic form decomposition, quadratic form factorization, and quadratic form approximation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Geometry and Its Applications: Techniques such as convex hull computation, Delaunay triangulation, and Voronoi diagram computation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Geometry and Its Applications: Techniques such as convex hull computation, Delaunay triangulation, and Voronoi diagram computation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fractal Geometry and Its Applications: Techniques such as fractal dimension computation, fractal geometry factorization, and fractal-based image compression.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Fractal Geometry and Its Applications: Techniques such as fractal dimension computation, fractal geometry factorization, and fractal-based image compression.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-Linear Algebraic Methods for Computer Vision: Techniques such as optimization techniques, homotopy methods, and non-linear dimensionality reduction.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Linear Algebraic Methods for Computer Vision: Techniques such as optimization techniques, homotopy methods, and non-linear dimensionality reduction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Fluid Dynamics and Its Applications: Techniques such as Navier-Stokes equation solving, finite element method, and computational fluid dynamics-based image processing.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Fluid Dynamics and Its Applications: Techniques such as Navier-Stokes equation solving, finite element method, and computational fluid dynamics-based image processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Topology and Its Applications: Techniques such as persistent homology computation, topological data analysis, and topological data visualization.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Computational Topology and Its Applications: Techniques such as persistent homology computation, topological data analysis, and topological data visualization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Computational Topology and Its Applications in 3D Reconstruction: Techniques such as topological data analysis, topological data visualization, and topological data processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.877638423415247
      }
    ]
  },
  {
    "representative_text": "Spherical Harmonics and Their Applications: Techniques such as spherical harmonic decomposition, spherical harmonic expansion, and spherical harmonic-based image processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Spherical Harmonics and Their Applications: Techniques such as spherical harmonic decomposition, spherical harmonic expansion, and spherical harmonic-based image processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Spherical Harmonics and Their Applications in 3D Reconstruction: Techniques such as spherical harmonic-based surface reconstruction, spherical harmonic-based point cloud processing, and spherical harmonic-based object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8572221851270068
      }
    ]
  },
  {
    "representative_text": "Computational Harmonic Analysis and Its Applications: Techniques such as Fourier analysis, discrete Fourier transform, and wavelet analysis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Harmonic Analysis and Its Applications: Techniques such as Fourier analysis, discrete Fourier transform, and wavelet analysis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue-Based Methods for Solving Partial Differential Equations: Techniques such as eigenvalue-based finite element method, eigenvalue-based boundary element method, and eigenvalue-based spectral method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue-Based Methods for Solving Partial Differential Equations: Techniques such as eigenvalue-based finite element method, eigenvalue-based boundary element method, and eigenvalue-based spectral method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Geometry and Its Applications in 3D Reconstruction: Techniques such as point cloud processing, surface reconstruction, and 3D reconstruction from images.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Geometry and Its Applications in 3D Reconstruction: Techniques such as point cloud processing, surface reconstruction, and 3D reconstruction from images.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Fluid Dynamics and Its Applications in 3D Reconstruction: Techniques such as fluid dynamics-based image processing, fluid dynamics-based surface reconstruction, and fluid dynamics-based object recognition.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Fluid Dynamics and Its Applications in 3D Reconstruction: Techniques such as fluid dynamics-based image processing, fluid dynamics-based surface reconstruction, and fluid dynamics-based object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kernel Methods for Non-Linear Regression with Polynomial Kernels: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to compute non-linear regression models using polynomial kernels.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kernel Methods for Non-Linear Regression with Polynomial Kernels: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to compute non-linear regression models using polynomial kernels.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kernel Methods for Non-Linear Regression with RBF Kernels: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to compute non-linear regression models using RBF kernels.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kernel Methods for Non-Linear Regression with RBF Kernels: Techniques that use linear algebra operations such as matrix multiplication and eigendecomposition to compute non-linear regression models using RBF kernels.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra for Reinforcement Learning: Policy Gradient Methods with Temporal Difference Methods and Orthogonal Projections: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to optimize policy gradient methods using temporal difference methods and orthogonal projections.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra for Reinforcement Learning: Policy Gradient Methods with Temporal Difference Methods and Orthogonal Projections: Methods that use linear algebra operations such as matrix multiplication and eigendecomposition to optimize policy gradient methods using temporal difference methods and orthogonal projections.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are not covered':",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Here are not covered':",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Here are not covered:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9202086549980444
      },
      {
        "text": "1. Some of the areas that are not covered' :",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.89829681147762
      }
    ]
  },
  {
    "representative_text": "Here are not provided:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are not provided:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are not included are theore",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are not included are theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Linear Algebra Applications: This sub-category explores the practical applications of linear algebra in various fields such as physics, engineering, computer science, and data analysis. -> Cryptography and Cybersecurity: Linear algebra is used in cryptography and cybersecurity for tasks such as encryption and decryption, digital signatures, and cryptographic protocols. It's also used in secure communication systems, such as secure multi-party computation and homomorphic encryption.:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Linear Algebra Applications: This sub-category explores the practical applications of linear algebra in various fields such as physics, engineering, computer science, and data analysis. -> Cryptography and Cybersecurity: Linear algebra is used in cryptography and cybersecurity for tasks such as encryption and decryption, digital signatures, and cryptographic protocols. It's also used in secure communication systems, such as secure multi-party computation and homomorphic encryption.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "*    is a set theory of a generalized the Gram-Singular value decomposition:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "*    is a set theory of a generalized the Gram-Singular value decomposition:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "linear algebraic.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "linear algebraic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "- linear algebraic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9104173119633139
      }
    ]
  },
  {
    "representative_text": "Lie Algebra and Vector Spaces: The study of Lie algebras and their relationship to vector spaces, which is a fundamental concept in mathematics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lie Algebra and Vector Spaces: The study of Lie algebras and their relationship to vector spaces, which is a fundamental concept in mathematics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "K-theory and Vector Spaces: The study of K-theory and its relationship to vector spaces, which is a fundamental concept in mathematics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "K-theory and Vector Spaces: The study of K-theory and its relationship to vector spaces, which is a fundamental concept in mathematics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Operator Algebras and Vector Spaces: The study of operator algebras and their relationship to vector spaces, which is a fundamental concept in mathematics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Operator Algebras and Vector Spaces: The study of operator algebras and their relationship to vector spaces, which is a fundamental concept in mathematics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of a Vector Space and its Basis: The study of the span of a vector space and its basis, which is a fundamental concept in linear algebra.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Span of a Vector Space and its Basis: The study of the span of a vector space and its basis, which is a fundamental concept in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of a Set of Vectors and its Dimension: The study of linear independence of a set of vectors and its dimension, which is a fundamental concept in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.864571660286197
      },
      {
        "text": "Linear Dependence of a Set of Vectors and its Dimension: The study of linear dependence of a set of vectors and its dimension, which is a fundamental concept in linear algebra.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9280540313407666
      }
    ]
  },
  {
    "representative_text": "Vector Space Decompositions and their Applications: The study of vector space decompositions and their applications in linear algebra, which is a fundamental concept in mathematics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Vector Space Decompositions and their Applications: The study of vector space decompositions and their applications in linear algebra, which is a fundamental concept in mathematics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Decomposition and its Applications: The study of orthogonal decomposition and its applications in linear algebra, which is a fundamental concept in mathematics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9384910207127222
      },
      {
        "text": "Gram-Schmidt Process and its Applications: The study of the Gram-Schmidt process and its applications in linear algebra, which is a fundamental concept in mathematics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8921265776471823
      }
    ]
  },
  {
    "representative_text": "**Lancient orthonormsized Inner Product of Vector Space of theorethethe concept of a Linear Independence of a generalization of a generalization:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Lancient orthonormsized Inner Product of Vector Space of theorethethe concept of a Linear Independence of a generalization of a generalization:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Operator Trace: The trace of a linear operator is the sum of its diagonal entries in its matrix representation. It has various applications, including the calculation of eigenvalues and the study of the properties of the operator.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Operator Trace: The trace of a linear operator is the sum of its diagonal entries in its matrix representation. It has various applications, including the calculation of eigenvalues and the study of the properties of the operator.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Operator Conjugate Transpose: The conjugate transpose of a linear operator is a linear operator that, when composed with the original operator, results in the identity operator on the domain space. This concept is essential in understanding the properties of linear operators on complex vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Operator Conjugate Transpose: The conjugate transpose of a linear operator is a linear operator that, when composed with the original operator, results in the identity operator on the domain space. This concept is essential in understanding the properties of linear operators on complex vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Operator Polar Decomposition with Complex Vector Spaces: The polar decomposition of a linear operator on a complex vector space involves decomposing the operator into a product of an orthogonal operator and a positive semi-definite operator. This concept is crucial in understanding the properties of linear operators on complex vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Operator Polar Decomposition with Complex Vector Spaces: The polar decomposition of a linear operator on a complex vector space involves decomposing the operator into a product of an orthogonal operator and a positive semi-definite operator. This concept is crucial in understanding the properties of linear operators on complex vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Finite Dimensionality: A fundamental concept in linear algebra, discussing the properties of linear transformations on finite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Finite Dimensionality: A fundamental concept in linear algebra, discussing the properties of linear transformations on finite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Projection Theorems: Discussing the projection theorems, which provide a way to project a vector onto the image of a linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Projection Theorems: Discussing the projection theorems, which provide a way to project a vector onto the image of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Jordan Decomposition: Discussing the Jordan decomposition of a linear transformation, which provides a way to diagonalize the transformation using Jordan blocks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Jordan Decomposition: Discussing the Jordan decomposition of a linear transformation, which provides a way to diagonalize the transformation using Jordan blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Jordan Decomposition: The Jordan decomposition of a linear transformation, which is a way to diagonalize the transformation using Jordan blocks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9286067272314157
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Minimal Polynomial: A fundamental concept in understanding the behavior of linear transformations, including the minimal polynomial of a linear transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Minimal Polynomial: A fundamental concept in understanding the behavior of linear transformations, including the minimal polynomial of a linear transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformation and Minimal Polynomial: The minimal polynomial of a linear transformation, which is the monic polynomial of least degree that annihilates the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8558945414551005
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Conjugate Transpose: Discussing the conjugate transpose of a linear transformation, which provides a way to determine the transpose of the transformation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Conjugate Transpose: Discussing the conjugate transpose of a linear transformation, which provides a way to determine the transpose of the transformation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations on Fuzzy Vector Spaces: This is a new area of study that requires an extension of the rank-nullity theorem to fuzzy vector spaces.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations on Fuzzy Vector Spaces: This is a new area of study that requires an extension of the rank-nullity theorem to fuzzy vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Advanced Theorem: The Minimax Theorem for Linear Transformations: This is an advanced theorem that relates the rank and nullity of a linear transformation to its eigenvalues and eigenvectors, which is essential in understanding the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Advanced Theorem: The Minimax Theorem for Linear Transformations: This is an advanced theorem that relates the rank and nullity of a linear transformation to its eigenvalues and eigenvectors, which is essential in understanding the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Advanced Theorem: The Rank-Nullity Theorem for Linear Transformations with Non-Standard Nilpotency: This is an advanced theorem that requires an extension of the rank-nullity theorem to linear transformations with non-standard nilpotency.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Advanced Theorem: The Rank-Nullity Theorem for Linear Transformations with Non-Standard Nilpotency: This is an advanced theorem that requires an extension of the rank-nullity theorem to linear transformations with non-standard nilpotency.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Advanced Theorem: The Rank-Nullity Theorem for Linear Transformations with Non-Standard Scaling and Boundedness: This is an advanced theorem that requires an extension of the rank-nullity theorem to linear transformations with non-standard scaling and boundedness.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9430533348068375
      },
      {
        "text": "Advanced Theorem: The Rank-Nullity Theorem for Linear Transformations with Non-Standard Completeness and Non-Standard Norms: This is an advanced theorem that requires an extension of the rank-nullity theorem to linear transformations with non-standard completeness and non-standard norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9686748357848538
      },
      {
        "text": "Advanced Theorem: The Rank-Nullity Theorem for Linear Transformations with Non- This is not applicable to Linear Transformations with Non- Theorem: This is not applicable to determine theore advanced theorem**: This is a more",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.862353871030149
      }
    ]
  },
  {
    "representative_text": "Continued Fractions and Determinants: A method for calculating the determinant of a matrix using continued fractions.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Continued Fractions and Determinants: A method for calculating the determinant of a matrix using continued fractions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Mixed-Sign Pattern: A method for calculating the determinant of a matrix with a mixed-sign pattern.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Mixed-Sign Pattern: A method for calculating the determinant of a matrix with a mixed-sign pattern.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Non-Linear Eigenvalue (Special Case): A method for calculating the determinant of a matrix with a non-linear eigenvalue.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Non-Linear Eigenvalue (Special Case): A method for calculating the determinant of a matrix with a non-linear eigenvalue.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Invertibility of a Matrix with a Zero Eigenvalue: This topic deals with the relationship between the eigenvalues and invertibility of a matrix, including the existence of a zero eigenvalue and the implications for the invertibility of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Invertibility of a Matrix with a Zero Eigenvalue: This topic deals with the relationship between the eigenvalues and invertibility of a matrix, including the existence of a zero eigenvalue and the implications for the invertibility of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "The Invertibility of a Matrix with a Non-Positive Eigenvalue: This topic deals with the relationship between the eigenvalues and invertibility of a matrix, including the existence of a non-positive eigenvalue and the implications for the invertibility of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9316506391925623
      }
    ]
  },
  {
    "representative_text": "The Orthogonormality of Eigenvectoriality of a Matrix: This is a matrix: This is a Matrix Norms: This is a matrix**: This is not only applies to find theore",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Orthogonormality of Eigenvectoriality of a Matrix: This is a matrix: This is a Matrix Norms: This is a matrix**: This is not only applies to find theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix Factorization: This is a technique used to factorize a matrix into a product of two matrices. It is a fundamental concept in linear algebra and has numerous applications in data analysis and machine learning.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline",
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Matrix Factorization: This is a technique used to factorize a matrix into a product of two matrices. It is a fundamental concept in linear algebra and has numerous applications in data analysis and machine learning.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Structured Matrix Decomposition: A factorization of a matrix into its structured factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8142707691730238
      },
      {
        "text": "Structured Matrix Decomposition with Applications: A factorization of a matrix into its structured factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8831210237360054
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) for Non-Square Matrices: While SVD is typically applied to square matrices, there are extensions to the method that allow us to apply it to non-square matrices in certain contexts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) for Non-Square Matrices: While SVD is typically applied to square matrices, there are extensions to the method that allow us to apply it to non-square matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Numerical Stability of Matrix Exponentiation for Non-Integer Determinant: Numerical stability is an important consideration when solving systems of linear equations using matrix exponentiation. The numerical stability of matrix exponentiation can be affected by the determinant of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Numerical Stability of Matrix Exponentiation for Non-Integer Determinant: Numerical stability is an important consideration when solving systems of linear equations using matrix exponentiation. The numerical stability of matrix exponentiation can be affected by the determinant of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination for Non-Symmetric Matrices: Gaussian elimination is a method for solving systems of linear equations by transforming the matrix into upper triangular form. While it is typically applied to symmetric matrices, there are extensions to the method that allow us to apply it to non-symmetric matrices in certain contexts.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination for Non-Symmetric Matrices: Gaussian elimination is a method for solving systems of linear equations by transforming the matrix into upper triangular form. While it is typically applied to symmetric matrices, there are extensions to the method that allow us to apply it to non-symmetric matrices in certain contexts.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Iteration (SVI): This is an iterative method for computing the SVD of a matrix. It is an alternative to the QR algorithm and can be more efficient for certain types of matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Iteration (SVI): This is an iterative method for computing the SVD of a matrix. It is an alternative to the QR algorithm and can be more efficient for certain types of matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Eigenvalues and Eigenvectors: This sub-category introduces the concepts of eigenvalues, eigenvectors, and diagonalization, which are essential in solving systems of linear equations. -> Eigenvector Equations: This component involves finding the eigenvectors of a matrix by solving the equation (A - λI)v = 0, where A is the matrix, λ is the eigenvalue, I is the identity matrix, and v is the eigenvector.:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Eigenvalues and Eigenvectors: This sub-category introduces the concepts of eigenvalues, eigenvectors, and diagonalization, which are essential in solving systems of linear equations. -> Eigenvector Equations: This component involves finding the eigenvectors of a matrix by solving the equation (A - λI)v = 0, where A is the matrix, λ is the eigenvalue, I is the identity matrix, and v is the eigenvector.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Additional knowledge for Eigenvalues and Eigenvectors: This sub-category introduces the concepts of eigenvalues, eigenvectors, and diagonalization, which are essential in solving systems of linear equations. -> Diagonalization of Matrices: This component focuses on the process of diagonalizing a matrix, which involves finding its eigenvalues and eigenvectors, and then using them to construct a diagonal matrix that represents the original matrix in a new basis.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9519538576187265
      },
      {
        "text": "Additional knowledge for Eigenvalues and Eigenvectors: This sub-category introduces the concepts of eigenvalues, eigenvectors, and diagonalization, which are essential in solving systems of linear equations. -> Characteristics and Properties of Eigenvalues: This sub-field discusses the properties and characteristics of eigenvalues, such as their multiplicity, whether they are real or complex, and their relationship with the matrix's determinant and rank.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9299818321539832
      },
      {
        "text": "Additional knowledge for Matrices and Linear Equations: This sub-category focuses on the use of matrices to represent linear equations, including matrix operations, determinants, and inverses. -> Inverse Matrices and Eigenvalues: Inverse matrices are crucial in solving systems of linear equations, as they allow for the isolation of variables. Eigenvalues, on the other hand, are scalar values that represent how much a matrix stretches or compresses a vector. Understanding the properties of eigenvalues and their relationship to the inverse matrix is essential in many applications.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8281845902428797
      }
    ]
  },
  {
    "representative_text": "3. This theorem: Theorem: This theorem: Theorem: Eigenvector of matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "3. This theorem: Theorem: This theorem: Theorem: Eigenvector of matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Linear Independence of a matrix: This is not explicitly mentioned in the provided points.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Independence of a matrix: This is not explicitly mentioned in the provided points.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix- Cyclic matrices with respect to Non-Schips' Theorem of a : :",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix- Cyclic matrices with respect to Non-Schips' Theorem of a : :",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Properties of the Schmidt Matrix of a Matrix using the Schmidt Matrix using the Schmidt Matrix: The Schmidt Matrix using the Schmidt Matrix: Theorem: Theorem for Matrices for Matrices of a matrix: Theorem for Matrices: Theorem: Theorem for Matrices: The QR decomposition**: Theorem.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Properties of the Schmidt Matrix of a Matrix using the Schmidt Matrix using the Schmidt Matrix: The Schmidt Matrix using the Schmidt Matrix: Theorem: Theorem for Matrices for Matrices of a matrix: Theorem for Matrices: Theorem: Theorem for Matrices: The QR decomposition**: Theorem.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Busemann Pseudometric: This involves using the Busemann pseudometric to define a metric on a vector space, which can lead to new applications of orthogonal projection matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Busemann Pseudometric: This involves using the Busemann pseudometric to define a metric on a vector space, which can lead to new applications of orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Projection Matrices and the Busemann Pseudometric: Using the Busemann pseudometric to define a metric on a vector space, leading to new applications of orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8485159228590535
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Rellich-Kondrachov Theorem: This theorem states that a compact operator can be approximated by a finite-rank operator. It has applications in orthogonal projection matrices, particularly when dealing with compact operators.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Rellich-Kondrachov Theorem: This theorem states that a compact operator can be approximated by a finite-rank operator. It has applications in orthogonal projection matrices, particularly when dealing with compact operators.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Hodge Decomposition: The Hodge decomposition is a way to decompose a differential form into its harmonic and exact components. It has applications in orthogonal projection matrices, particularly when dealing with differential forms and tensors.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Hodge Decomposition: The Hodge decomposition is a way to decompose a differential form into its harmonic and exact components. It has applications in orthogonal projection matrices, particularly when dealing with differential forms and tensors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Determinants and Inverse Matrices: This sub-category delves into the calculation of determinants and the concept of inverse matrices, including their properties and applications. -> Matrix Inverse and Determinant Relationships: This sub-field explores the relationship between the determinant of a matrix and the existence and uniqueness of its inverse. It discusses how the determinant is used to determine the invertibility of a matrix, and how the inverse of a matrix can be calculated using the determinant.:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Determinants and Inverse Matrices: This sub-category delves into the calculation of determinants and the concept of inverse matrices, including their properties and applications. -> Matrix Inverse and Determinant Relationships: This sub-field explores the relationship between the determinant of a matrix and the existence and uniqueness of its inverse. It discusses how the determinant is used to determine the invertibility of a matrix, and how the inverse of a matrix can be calculated using the determinant.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Additional knowledge for Determinants and Inverse Matrices: This sub-category delves into the calculation of determinants and the concept of inverse matrices, including their properties and applications. -> Cofactor Expansion and Determinant Properties: This sub-field focuses on the method of cofactor expansion, which is used to calculate the determinant of a matrix. It also covers properties of determinants, such as the relationship between determinants and matrix invertibility, and the use of determinants in solving systems of linear equations.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9180857797714777
      },
      {
        "text": "Additional knowledge for Determinants and Inverse Matrices: This sub-category delves into the calculation of determinants and the concept of inverse matrices, including their properties and applications. -> Inverse Matrix Calculation and Applications: This sub-field delves into the various methods for calculating the inverse of a matrix, including the Gauss-Jordan method, LU decomposition, and the adjugate method. It also covers the applications of inverse matrices in solving systems of linear equations, finding eigenvalues and eigenvectors, and representing linear transformations.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9460251631289859
      },
      {
        "text": "Additional knowledge for Determinants and Inverse Matrices: This sub-category delves into the calculation of determinants and the concept of inverse matrices, including their properties and applications. -> Determinants and Matrix Rank: This sub-field examines the relationship between the determinant of a matrix and its rank. It discusses how the determinant can be used to determine the rank of a matrix, and how the rank can be used to determine the invertibility of a matrix.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9409888867797079
      },
      {
        "text": "Additional knowledge for Determinants and Inverse Matrices: This sub-category delves into the calculation of determinants and the concept of inverse matrices, including their properties and applications. -> Eigendecomposition and Determinants: This sub-field explores the relationship between determinants and eigendecomposition, which is a method for decomposing a matrix into its eigenvalues and eigenvectors. It discusses how the determinant can be used to determine the eigenvalues of a matrix, and how eigendecomposition can be used to solve systems of linear equations and find the inverse of a matrix.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9586265659424722
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Complex Eigenvalues and Linear Transformations: The determinant of a matrix with complex eigenvalues can be calculated using the same formulas as for real matrices, but with additional considerations for the complex arithmetic. However, the relationship between determinants and linear transformations is not fully explored.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues and Linear Transformations: The determinant of a matrix with complex eigenvalues can be calculated using the same formulas as for real matrices, but with additional considerations for the complex arithmetic. However, the relationship between determinants and linear transformations is not fully explored.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Non-Orthogonal Basis and Non-Constant Coefficient Matrix: The determinant of a matrix with a non-orthogonal basis and a non-constant coefficient matrix can be calculated using the formula for the determinant of a matrix with a non-orthogonal basis and a non-constant coefficient matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Basis and Non-Constant Coefficient Matrix: The determinant of a matrix with a non-orthogonal basis and a non-constant coefficient matrix can be calculated using the formula for the determinant of a matrix with a non-orthogonal basis and a non-constant coefficient matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a Matrix with Non-Orthogonal Basis and Non-Linear Jacobian Matrix: The determinant of a matrix with a non-orthogonal basis and a non-linear Jacobian matrix can be calculated using the formula for the determinant of a matrix with a non-orthogonal basis and a non-linear Jacobian matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9237575257643884
      },
      {
        "text": "Determinant of a Matrix with a Non-Orthogonal Basis and a Non-Linear Jacobian Matrix: The determinant of a matrix with a matrix with a matrix with a matrix and a non-linear Jacobian Basis and a non- theorelated matrix can be calculated using theorelinalg matrix can be calculated using theorelative matrix can be calculated using theorelated matrix: Theorem for a matrix and Non- Determinant's theorem on a non- Determinant basis and a matrix can be calculated using theorelated matrix can be calculated using theorelated matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8596642501387906
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Bounds: Upper and lower bounds for the eigenvalues of a matrix, which can be used to estimate the determinant of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Eigenvalue Bounds: Upper and lower bounds for the eigenvalues of a matrix, which can be used to estimate the determinant of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Eigenvalue Bounds for Non-Standard Matrices: Upper and lower bounds for the eigenvalues of a non-standard matrix, which can be used to estimate the determinant of a non-standard matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8642167237661875
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with Complex Eigenvalues: The calculation of theore theorel",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with Complex Eigenvalues: The calculation of theore theorel",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Determinant of a matrix with a complex eigenvalue using the resolvent: A method for calculating the determinant of a matrix with a complex eigenvalue using the resolvent.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8696860328191787
      }
    ]
  },
  {
    "representative_text": "The Linear Independence Criterion using the Rank-Nullity Theorem in Infinite-Dimensional Vector Spaces: This criterion provides a more general approach to determining linear independence in infinite-dimensional vector spaces, but it's essential to understand the limitations and nuances of this method.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Linear Independence Criterion using the Rank-Nullity Theorem in Infinite-Dimensional Vector Spaces: This criterion provides a more general approach to determining linear independence in infinite-dimensional vector spaces, but it's essential to understand the limitations and nuances of this method.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Existence of a Basis for a Vector Space with a Linearly Dependent Set of Vectors: This concept is essential in understanding the existence of a basis for a vector space with a linearly dependent set of vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Existence of a Basis for a Vector Space with a Linearly Dependent Set of Vectors: This concept is essential in understanding the existence of a basis for a vector space with a linearly dependent set of vectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Existence of a Basis for a Vector Space using Linear Transformations: This concept is essential in understanding the existence of a basis for a vector space using linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8823975585906235
      }
    ]
  },
  {
    "representative_text": "Relationship between Span and Orthogonal Complement: This relationship is essential in understanding the dimension of the span and the existence of a basis for a vector space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relationship between Span and Orthogonal Complement: This relationship is essential in understanding the dimension of the span and the existence of a basis for a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Existence of a Linear Transformation with a Trivial Null Space: This concept is essential in understanding the existence of a linear transformation with a trivial null space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Existence of a Linear Transformation with a Trivial Null Space: This concept is essential in understanding the existence of a linear transformation with a trivial null space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Existence of a Basis in a Vector Space with a Non-Standard Bounded Basis: This concept generalizes the existence of a basis in a vector space with a bounded basis.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Existence of a Basis in a Vector Space with a Non-Standard Bounded Basis: This concept generalizes the existence of a basis in a vector space with a bounded basis.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Category-theoretic implications of basis in non-standard vector spaces: The concept of basis is fundamental in understanding vector spaces and linear transformations. However, the existing points do not explicitly address the category-theoretic implications of basis in non-standard vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Category-theoretic implications of basis in non-standard vector spaces: The concept of basis is fundamental in understanding vector spaces and linear transformations. However, the existing points do not explicitly address the category-theoretic implications of basis in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Algebraic implications of basis in non-standard linear transformation spaces: The concept of basis is crucial in defining a linear transformation. However, the existing points do not explicitly address the algebraic implications of basis in non-standard linear transformation spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8854253024612853
      },
      {
        "text": "Category-Theoretic Implications of Basis in Non-Standard Vector Spaces: The concept of basis is fundamental in understanding vector spaces and linear transformations, and the category-theoretic implications of basis in non-standard vector spaces are crucial for understanding the properties of bases in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8646643149777498
      },
      {
        "text": "Algebraic Implications of Basis in Non-Standard Linear Transformation Spaces: The concept of basis is crucial in defining a linear transformation, and the algebraic implications of basis in non-standard linear transformation spaces are essential for understanding the properties of bases in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8923004940070846
      }
    ]
  },
  {
    "representative_text": "Topological implications of basis in non-standard vector spaces: The concept of basis is closely related to topological properties. However, the existing points do not explicitly address the topological implications of basis in non-standard vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Topological implications of basis in non-standard vector spaces: The concept of basis is closely related to topological properties. However, the existing points do not explicitly address the topological implications of basis in non-standard vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Infinite-Dimensional Linear Independence: A concept that extends the notion of linear independence to infinite-dimensional vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Infinite-Dimensional Linear Independence: A concept that extends the notion of linear independence to infinite-dimensional vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Connection between Linear Independence and the Spectral Theorem: The spectral theorem states that a Hermitian matrix can be diagonalized by a unitary matrix. This theorem has implications for linear independence, as it shows that a Hermitian matrix can be expressed as a linear combination of its eigenvalues and eigenvectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Connection between Linear Independence and the Spectral Theorem: The spectral theorem states that a Hermitian matrix can be diagonalized by a unitary matrix. This theorem has implications for linear independence, as it shows that a Hermitian matrix can be expressed as a linear combination of its eigenvalues and eigenvectors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Role of Linear Independence in the Study of Symmetric Polynomials: Symmetric polynomials are polynomials that remain unchanged under any permutation of the variables. Linear independence plays a crucial role in the study of symmetric polynomials, as it allows us to count the number of distinct monomials.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Role of Linear Independence in the Study of Symmetric Polynomials: Symmetric polynomials are polynomials that remain unchanged under any permutation of the variables. Linear independence plays a crucial role in the study of symmetric polynomials, as it allows us to count the number of distinct monomials.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Independence of Symmetric Polynomials: The symmetric polynomials of vectors can be used to determine linear independence, providing an alternative approach to the traditional method of using determinants or inner products.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8015154784066293
      }
    ]
  },
  {
    "representative_text": "Linear Independence in the Context of Topology: Topology is the study of properties of shapes that are preserved under continuous deformations. Linear independence has implications for topology, as it allows us to study the connectivity of spaces and the properties of continuous functions.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence in the Context of Topology: Topology is the study of properties of shapes that are preserved under continuous deformations. Linear independence has implications for topology, as it allows us to study the connectivity of spaces and the properties of continuous functions.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Connection between Linear Independence and the Theory of Automorphic Forms: Automorphic forms are functions on a group that satisfy certain transformation properties. Linear independence plays a crucial role in the study of automorphic forms, as it allows us to count the number of distinct forms and study their properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Connection between Linear Independence and the Theory of Automorphic Forms: Automorphic forms are functions on a group that satisfy certain transformation properties. Linear independence plays a crucial role in the study of automorphic forms, as it allows us to count the number of distinct forms and study their properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Dependence in the Context of Representation Theory: Representation theory is the study of linear representations of groups. Linear dependence has implications for representation theory, as it allows us to study the properties of representations and the symmetries of spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Dependence in the Context of Representation Theory: Representation theory is the study of linear representations of groups. Linear dependence has implications for representation theory, as it allows us to study the properties of representations and the symmetries of spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Role of Linear Independence in the Study of Quantum Mechanics: Quantum mechanics is the study of the behavior of particles at the quantum systems at the fundamental particles at the mathematical modelled by the study of a linear independence of Linear Independence of a linear independence of a vector space is a  is a basis",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Role of Linear Independence in the Study of Quantum Mechanics: Quantum mechanics is the study of the behavior of particles at the quantum systems at the fundamental particles at the mathematical modelled by the study of a linear independence of Linear Independence of a linear independence of a vector space is a  is a basis",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Minkowski Sum: Minkowski sum is a mathematical operation that combines two sets of points by summing their coordinates. This is useful in computer graphics and game development for tasks such as terrain rendering and object recognition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minkowski Sum: Minkowski sum is a mathematical operation that combines two sets of points by summing their coordinates. This is useful in computer graphics and game development for tasks such as terrain rendering and object recognition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Linear Algebra Applications: This sub-category explores the practical applications of linear algebra in various fields such as physics, engineering, computer science, and data analysis. -> Machine Learning and Data Science: Linear algebra is used extensively in machine learning and data science for tasks such as dimensionality reduction (PCA, t-SNE), data visualization, and feature extraction. It's also used in neural networks, where linear algebra operations like matrix multiplication and vector addition are essential.:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Linear Algebra Applications: This sub-category explores the practical applications of linear algebra in various fields such as physics, engineering, computer science, and data analysis. -> Machine Learning and Data Science: Linear algebra is used extensively in machine learning and data science for tasks such as dimensionality reduction (PCA, t-SNE), data visualization, and feature extraction. It's also used in neural networks, where linear algebra operations like matrix multiplication and vector addition are essential.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Additional knowledge for Linear Algebra Applications: This sub-category explores the practical applications of linear algebra in various fields such as physics, engineering, computer science, and data analysis. -> Signal Processing and Image Analysis: Linear algebra is used in signal processing and image analysis for tasks such as filtering, convolution, and Fourier analysis. It's also used in image compression, de-noising, and object recognition.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9243763392354212
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Public-Key Cryptography: There is a strong connection between linear algebra and public-key cryptography. Specifically, linear algebra is used in the construction of public-key cryptosystems, such as RSA and elliptic curve cryptography. Understanding the linear algebra behind these cryptosystems is essential for secure communication.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 8,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Public-Key Cryptography: There is a strong connection between linear algebra and public-key cryptography. Specifically, linear algebra is used in the construction of public-key cryptosystems, such as RSA and elliptic curve cryptography. Understanding the linear algebra behind these cryptosystems is essential for secure communication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Quantum-Resistant Cryptography and Linear Algebra: Quantum-resistant cryptography, such as lattice-based and code-based cryptography, relies heavily on linear algebra. Understanding the linear algebra behind these cryptographic schemes is essential for secure communication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8083315095493764
      },
      {
        "text": "Linear Algebra and Digital Signatures: Digital signatures rely heavily on linear algebra. Understanding the linear algebra behind digital signatures is essential for secure authentication and integrity of data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8171605763971621
      },
      {
        "text": "Lattice-Based Cryptography and Linear Algebra: Lattice-based cryptography relies heavily on linear algebra. Understanding the linear algebra behind lattice-based cryptography is essential for secure communication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8812579994639682
      },
      {
        "text": "Code-Based Cryptography and Linear Algebra: Code-based cryptography relies heavily on linear algebra. Understanding the linear algebra behind code-based cryptography is essential for secure communication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.901440785325867
      },
      {
        "text": "Multivariate Cryptography and Linear Algebra: Multivariate cryptography relies heavily on linear algebra. Understanding the linear algebra behind multivariate cryptography is essential for secure communication.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8828954654837051
      },
      {
        "text": "Linear Algebra and Secure Communication Protocols: Linear algebra is used in secure communication protocols, such as the Transport Layer Security (TLS) protocol. Understanding the linear algebra behind these protocols is essential for secure communication over high-speed and high-throughput networks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.859834302218127
      },
      {
        "text": "Quantum-Resistant Cryptographic Hash Functions and Linear Algebra: Quantum-resistant cryptographic hash functions, such as quantum-resistant hash functions based on lattices or codes, rely heavily on linear algebra. Understanding the linear algebra behind these hash functions is essential for secure authentication and integrity of data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8184347498401945
      }
    ]
  },
  {
    "representative_text": "Matrix Representations of Linear Transformations in Cryptography: Linear transformations can be represented using matrices, which are essential in cryptography. Matrix representations of linear transformations are used in various cryptographic protocols, such as secure multi-party computation and homomorphic encryption.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix Representations of Linear Transformations in Cryptography: Linear transformations can be represented using matrices, which are essential in cryptography. Matrix representations of linear transformations are used in various cryptographic protocols, such as secure multi-party computation and homomorphic encryption.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalues and Eigenvectors in Cryptography: Eigenvalues and eigenvectors are used in cryptography to analyze the behavior of linear transformations. For example, eigenvalues and eigenvectors are used to analyze the security of public-key cryptosystems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalues and Eigenvectors in Cryptography: Eigenvalues and eigenvectors are used in cryptography to analyze the behavior of linear transformations. For example, eigenvalues and eigenvectors are used to analyze the security of public-key cryptosystems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Secure Multi-Party Computation: Linear algebra is used in secure multi-party computation protocols, such as the Bulletproofs protocol. Understanding the linear algebra behind these protocols is essential for secure computation on private data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Secure Multi-Party Computation: Linear algebra is used in secure multi-party computation protocols, such as the Bulletproofs protocol. Understanding the linear algebra behind these protocols is essential for secure computation on private data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Advanced Zero-Knowledge Proof Systems and Linear Algebra: Advanced zero-knowledge proof systems, such as the Bulletproofs protocol, rely heavily on linear algebra. Understanding the linear algebra behind these protocols is essential for enabling secure zero-knowledge proofs.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8320223633494015
      }
    ]
  },
  {
    "representative_text": "Homomorphic Encryption and Linear Algebra: Homomorphic encryption relies heavily on linear algebra. Understanding the linear algebra behind homomorphic encryption is essential for secure computation on encrypted data.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Homomorphic Encryption and Linear Algebra: Homomorphic encryption relies heavily on linear algebra. Understanding the linear algebra behind homomorphic encryption is essential for secure computation on encrypted data.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Side-Channel Attack Resistance and Linear Algebra: Side-channel attack resistance techniques, such as masking and masking-based key recovery, rely heavily on linear algebra. Understanding the linear algebra behind these techniques is essential for protecting cryptographic hardware against side-channel attacks.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Side-Channel Attack Resistance and Linear Algebra: Side-channel attack resistance techniques, such as masking and masking-based key recovery, rely heavily on linear algebra. Understanding the linear algebra behind these techniques is essential for protecting cryptographic hardware against side-channel attacks.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are not mentioned in Matrices of a matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are not mentioned in Matrices of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Some additional missing points that is not mentioned that relates to be associated with respect to",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Some additional missing points that is not mentioned that relates to be associated with respect to",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Inner Product of a set of these are some scalar (not be used in a set of these points that are some scalars.':",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Inner Product of a set of these are some scalar (not be used in a set of these points that are some scalars.':",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additionally, and its a basis for vector spaces are theorelated to be space",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Additionally, and its a basis for vector spaces are theorelated to be space",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "-> Matrix Representations' can be found are identified:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "-> Matrix Representations' can be found are identified:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Non-Linear Transformations: The distinction between linear and non-linear transformations, and how non-linear transformations can be analyzed using techniques such as Taylor series expansion.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Non-Linear Transformations: The distinction between linear and non-linear transformations, and how non-linear transformations can be analyzed using techniques such as Taylor series expansion.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Polynomial Transformations: The relationship between linear transformations and polynomial transformations, including the use of polynomial transformations to approximate linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Polynomial Transformations: The relationship between linear transformations and polynomial transformations, including the use of polynomial transformations to approximate linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Differential Operators: The connection between linear transformations and differential operators, including the use of differential operators to analyze linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Differential Operators: The connection between linear transformations and differential operators, including the use of differential operators to analyze linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Integral Operators: The relationship between linear transformations and integral operators, including the use of integral operators to analyze linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Integral Operators: The relationship between linear transformations and integral operators, including the use of integral operators to analyze linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Measure Theory: The application of measure theory to linear transformations, including the use of measures to analyze the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Measure Theory: The application of measure theory to linear transformations, including the use of measures to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Category Theory: The use of category theory to analyze linear transformations, including the application of category theory to study the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Category Theory: The use of category theory to analyze linear transformations, including the application of category theory to study the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Homological Algebra: The application of homological algebra to linear transformations, including the use of homological algebra to analyze the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Homological Algebra: The application of homological algebra to linear transformations, including the use of homological algebra to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Hausdorff Spaces: The application of Hausdorff spaces to linear transformations, including the use of Hausdorff spaces to analyze the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Hausdorff Spaces: The application of Hausdorff spaces to linear transformations, including the use of Hausdorff spaces to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Symmetric Operators: The application of symmetric operators to linear transformations, including the use of symmetric operators to analyze the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Symmetric Operators: The application of symmetric operators to linear transformations, including the use of symmetric operators to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Linear Transformations and Hermitian Operators: The study of linear transformations in the context of Hermitian operators, including the use of Hermitian operators to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8770368131350518
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Unitary Operators: The application of unitary operators to linear transformations, including the use of unitary operators to analyze the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Unitary Operators: The application of unitary operators to linear transformations, including the use of unitary operators to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Linear Folds: The study of linear transformations in the context of linear folds, including the use of linear folds to analyze the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Linear Folds: The study of linear transformations in the context of linear folds, including the use of linear folds to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Fiber Bundles: The application of fiber bundles to linear transformations, including the use of fiber bundles to analyze the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Fiber Bundles: The application of fiber bundles to linear transformations, including the use of fiber bundles to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations and Higher-Dimensional Transformations: The study of linear transformations in the context of higher-dimensional transformations, including the use of higher-dimensional transformations to analyze the properties of linear transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations and Higher-Dimensional Transformations: The study of linear transformations in the context of higher-dimensional transformations, including the use of higher-dimensional transformations to analyze the properties of linear transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a matrix with complex eigenvalues and a specific type of matrix: The determinant of a matrix with complex eigenvalues and a specific type of matrix, such as a Jordan block, is not necessarily equal to the determinant of the matrix with constant coefficients.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a matrix with complex eigenvalues and a specific type of matrix: The determinant of a matrix with complex eigenvalues and a specific type of matrix, such as a Jordan block, is not necessarily equal to the determinant of the matrix with constant coefficients.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a matrix with a non-symmetric linear transformation and a specific type of decomposition: The determinant of a matrix representing a non-symmetric linear transformation with a specific type of decomposition, such as SVD, is not necessarily equal to the determinant of the transpose of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a matrix with a non-symmetric linear transformation and a specific type of decomposition: The determinant of a matrix representing a non-symmetric linear transformation with a specific type of decomposition, such as SVD, is not necessarily equal to the determinant of the transpose of the matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The existing points are:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The existing points are:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Adjugate matrix properties: The adjugate matrix has various properties, including being the transpose of the cofactor matrix, and is used to find the inverse of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Adjugate matrix properties: The adjugate matrix has various properties, including being the transpose of the cofactor matrix, and is used to find the inverse of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are not mentioned in theore",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are not mentioned in theore",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Krythinnalgebraic Complex Orthogonal Diagonal Matrix Orthogonal Orthogonal Matrix-normality of Orthogon theoreth of Orthogonal Diagonalization of Orthogonal matrix orthogonormality of Orthogonal matrices' Orthogonalization of Orthogonal Decomposition of an orthogon a process of Orthogonalization of Orthogonal Matrix Orthogonal Matrix Norms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Krythinnalgebraic Complex Orthogonal Diagonal Matrix Orthogonal Orthogonal Matrix-normality of Orthogon theoreth of Orthogonal Diagonalization of Orthogonal matrix orthogonormality of Orthogonal matrices' Orthogonalization of Orthogonal Decomposition of an orthogon a process of Orthogonalization of Orthogonal Matrix Orthogonal Matrix Norms.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Kronecker Product of Orthogonal Projection Matrices: Combining two orthogonal projection matrices into a larger matrix using the Kronecker product, with applications in block matrices and signal processing.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Kronecker Product of Orthogonal Projection Matrices: Combining two orthogonal projection matrices into a larger matrix using the Kronecker product, with applications in block matrices and signal processing.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Rellich-Kondrachov Theorem: Approximating compact operators using finite-rank operators, with applications in orthogonal projection matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Rellich-Kondrachov Theorem: Approximating compact operators using finite-rank operators, with applications in orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Fourier Transform: Decomposing functions into their frequency components using the Fourier transform, with applications in orthogonal projection matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Fourier Transform: Decomposing functions into their frequency components using the Fourier transform, with applications in orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Hodge Decomposition: Decomposing differential forms into their harmonic and exact components using the Hodge decomposition, with applications in orthogonal projection matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Hodge Decomposition: Decomposing differential forms into their harmonic and exact components using the Hodge decomposition, with applications in orthogonal projection matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Orthogonal Decomposition of Operator Algebras: Decomposing operator algebras into their orthogonal components, with applications in quantum mechanics and mathematical physics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Orthogonal Decomposition of Operator Algebras: Decomposing operator algebras into their orthogonal components, with applications in quantum mechanics and mathematical physics.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Connection to Machine Learning: Using orthogonal projection matrices in machine learning, such as dimensionality reduction and feature extraction.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Connection to Machine Learning: Using orthogonal projection matrices in machine learning, such as dimensionality reduction and feature extraction.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quasi-Orthonormality: A set of vectors is quasi-orthogonal if the inner product of any two distinct vectors is zero, but the inner product of any vector with itself is not zero.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quasi-Orthonormality: A set of vectors is quasi-orthogonal if the inner product of any two distinct vectors is zero, but the inner product of any vector with itself is not zero.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt Process Variants: There are several variants of the Gram-Schmidt process, including the Householder Gram-Schmidt process, the Givens Gram-Schmidt process, and the QR Gram-Schmidt process.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt Process Variants: There are several variants of the Gram-Schmidt process, including the Householder Gram-Schmidt process, the Givens Gram-Schmidt process, and the QR Gram-Schmidt process.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence Implies Span with Respect to a Given Basis in Infinite-Dimensional Vector Spaces with Respect to a Given Subspace: A Basis Extension Theorem: A set of a basis extension theorem: A set of a set of a Basis Extension Theorem: A set of Linear Independence of Linear Independence: A set of Linear Independence of a set of Linear Independence: A set of Linear Independence: A set of a crucial in Infinite-Dimensional vector spaces: A set is not necessarily true for Linear Independence: A set of a set of linear independence of a more closely related to determine",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence Implies Span with Respect to a Given Basis in Infinite-Dimensional Vector Spaces with Respect to a Given Subspace: A Basis Extension Theorem: A set of a basis extension theorem: A set of a set of a Basis Extension Theorem: A set of Linear Independence of Linear Independence: A set of Linear Independence of a set of Linear Independence: A set of Linear Independence: A set of a crucial in Infinite-Dimensional vector spaces: A set is not necessarily true for Linear Independence: A set of a set of linear independence of a more closely related to determine",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**The Linear Independence of the Linear Independence of a Generalized Linear Independence of a set of a Linear Independence of a Set of a set of a Set of a Set of a set of a Set of a set of a Set of a set of a Set of a set of a set of a set of a Set of a Set of a set is not explicitly listed is not explicitly listed as a linear Independence of a Set of a set of a set of a Set of a set of a non- Linear Independence of a Set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a vector space: This point is not explicitly listed",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**The Linear Independence of the Linear Independence of a Generalized Linear Independence of a set of a Linear Independence of a Set of a set of a Set of a Set of a set of a Set of a set of a Set of a set of a Set of a set of a set of a set of a Set of a Set of a set is not explicitly listed is not explicitly listed as a linear Independence of a Set of a set of a set of a Set of a set of a non- Linear Independence of a Set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a vector space: This point is not explicitly listed",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Span of the sum of two subspaces: Given two subspaces $U$ and $V$ of a vector space $W$, the span of $U + V$ is equal to the span of $U$ plus the span of $V$.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Span of the sum of two subspaces: Given two subspaces $U$ and $V$ of a vector space $W$, the span of $U + V$ is equal to the span of $U$ plus the span of $V$.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Span of the sum of two subspaces: Given two subspaces $U$ and $V$",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9186985108677139
      },
      {
        "text": "Span of the sum of two subspaces: Given two subspaces $U$ and $V$ of a vector space $W$, the span of $U + V$ is equal to the span of $U$ plus theore rigorous proof of a set of a vector space.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9460425675861284
      }
    ]
  },
  {
    "representative_text": "Span of the intersection of two subspaces: Given two subspaces $U$ and $V$ of a vector space $W$, the span of $U \\cap V$ is equal to the intersection of the spans of $U$ and $V$:$",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Span of the intersection of two subspaces: Given two subspaces $U$ and $V$ of a vector space $W$, the span of $U \\cap V$ is equal to the intersection of the spans of $U$ and $V$:$",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Basis Extension and Reduction in Non-Standard Vector Spaces with Non-Standard Metric: The process of Non- Basis Extension of Basis of a vector spaces,",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Basis Extension and Reduction in Non-Standard Vector Spaces with Non-Standard Metric: The process of Non- Basis Extension of Basis of a vector spaces,",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are some other mathematical concept:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are some other mathematical concept:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "- Theorelated to be used in a few more detailed explanation of Linear Independence of a basis for Infinite-Dimensional Spaces:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "- Theorelated to be used in a few more detailed explanation of Linear Independence of a basis for Infinite-Dimensional Spaces:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Minkowski's Theorem and Convex Sets: Minkowski's Theorem provides a framework for understanding the properties of convex sets, and its variations can be used to solve optimization problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Minkowski's Theorem and Convex Sets: Minkowski's Theorem provides a framework for understanding the properties of convex sets, and its variations can be used to solve optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Carathéodory's Theorem and Point Representation: Carathéodory's Theorem provides a framework for understanding the properties of point representation in convex sets, and its variations can be used to solve optimization problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Carathéodory's Theorem and Point Representation: Carathéodory's Theorem provides a framework for understanding the properties of point representation in convex sets, and its variations can be used to solve optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Dimension of the Vector Space using Orthogonal Decomposition: The dimension of a vector space can be found using the orthogonal decomposition of the space, and its variations can be used to solve optimization problems.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Dimension of the Vector Space using Orthogonal Decomposition: The dimension of a vector space can be found using the orthogonal decomposition of the space, and its variations can be used to solve optimization problems.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Geomorphology: This field uses linear algebra to study the geometry of natural landscapes and simulate geological processes.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Geomorphology: This field uses linear algebra to study the geometry of natural landscapes and simulate geological processes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Homomorphic Encryption and Linear Algebra: Homomorphism: Homomorphism: Homomorphic Encryption SVDeterminants: Homomorphic encryption: Homomorphism: Homomorphic Encryption: Homomorphic encryption schemes: Homomorphic encryption schemes.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Homomorphic Encryption and Linear Algebra: Homomorphism: Homomorphism: Homomorphic Encryption SVDeterminants: Homomorphic encryption: Homomorphism: Homomorphic Encryption: Homomorphic encryption schemes: Homomorphic encryption schemes.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Vector Spaces: This sub-category deals with the study of vector spaces, including definitions, properties, and operations such as addition, scalar multiplication, and basis vectors. -> Subspaces and Subspaces of Subspaces: This sub-field explores the concept of subspaces, which are vector spaces that are subsets of a larger vector space. It involves studying the properties of subspaces, such as closure under linear transformations, and the concept of subspaces of subspaces, which is a more advanced topic in linear algebra.:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Vector Spaces: This sub-category deals with the study of vector spaces, including definitions, properties, and operations such as addition, scalar multiplication, and basis vectors. -> Subspaces and Subspaces of Subspaces: This sub-field explores the concept of subspaces, which are vector spaces that are subsets of a larger vector space. It involves studying the properties of subspaces, such as closure under linear transformations, and the concept of subspaces of subspaces, which is a more advanced topic in linear algebra.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Relatively Linear Operators: A relatively linear operator is a linear operator that is linear with respect to a subspace of linearlyinearly:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Relatively Linear Operators: A relatively linear operator is a linear operator that is linear with respect to a subspace of linearlyinearly:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Some additional missing:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Some additional missing:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "Additional missing points:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.869776653746195
      }
    ]
  },
  {
    "representative_text": "Here are not mentioned in theoremarkovector Analysis', specifically in linear transformations, and related to be extended list is a",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are not mentioned in theoremarkovector Analysis', specifically in linear transformations, and related to be extended list is a",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and Spectral Mapping Theorem: The spectral mapping theorem, which states that the eigenvalues of a linear transformation are the same as the eigenvalues of its matrix representation.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and Spectral Mapping Theorem: The spectral mapping theorem, which states that the eigenvalues of a linear transformation are the same as the eigenvalues of its matrix representation.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformation and the Rank- Linear Transformation and Characteristic Theorem: Theorem of a Linear Transformation: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformation and the Rank- Linear Transformation and Characteristic Theorem: Theorem of a Linear Transformation: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem: Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations on Banach Spaces with Non-Standard Norms and Boundedness.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations on Banach Spaces with Non-Standard Norms and Boundedness.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Rank and Nullity for Linear Transformations with Non-Standard Inner Products in Banach Spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Rank and Nullity for Linear Transformations with Non-Standard Inner Products in Banach Spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations on Fractal Spaces with Non-Standard Dimensions and Fractal Properties.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations on Fractal Spaces with Non-Standard Dimensions and Fractal Properties.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations on Group Theory with Non-Standard Group Actions and Representation Spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations on Group Theory with Non-Standard Group Actions and Representation Spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Transformations on Representation Theory with Non-Standard Representation Spaces and Operator Algebras and Operator Algeometric Spaces and Operator Algeometries of Linear Transformations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Transformations on Representation Theory with Non-Standard Representation Spaces and Operator Algebras and Operator Algeometric Spaces and Operator Algeometries of Linear Transformations.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Linear Transformations: This sub-category explores the concept of linear transformations, including linear operators, matrices, and their relationships with vector spaces. -> Determinants and Linear Transformations: This sub-field explores the relationship between determinants and linear transformations. The determinant of a matrix can be used to determine the behavior of a linear transformation, and it has numerous applications in linear algebra and other fields. The sub-field also delves into the properties of determinants, such as the fact that the determinant of a product of matrices is equal to the product of their determinants.:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Linear Transformations: This sub-category explores the concept of linear transformations, including linear operators, matrices, and their relationships with vector spaces. -> Determinants and Linear Transformations: This sub-field explores the relationship between determinants and linear transformations. The determinant of a matrix can be used to determine the behavior of a linear transformation, and it has numerous applications in linear algebra and other fields. The sub-field also delves into the properties of determinants, such as the fact that the determinant of a product of matrices is equal to the product of their determinants.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a matrix with a non-symmetric linear transformation with a non- A and a non- Determinant of a non- The final combination of a matrix: Theorem  (This is not equal to be used in a matrix of a non- Determinant matrix**: Theorem  is not necessarily equal to determine the determinant of a matrix of a non-zero Determinant of a non-",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a matrix with a non-symmetric linear transformation with a non- A and a non- Determinant of a non- The final combination of a matrix: Theorem  (This is not equal to be used in a matrix of a non- Determinant matrix**: Theorem  is not necessarily equal to determine the determinant of a matrix of a non-zero Determinant of a non-",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "This involves.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "This involves.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Complement of a Subspace with Respect to a Non-Standard Basis: This involves various applications: A more...",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Complement of a Subspace with Respect to a Non-Standard Basis: This involves various applications: A more...",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are not listed:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Here are not listed:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "1. Here are not listed below:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.9362653472792191
      }
    ]
  },
  {
    "representative_text": "Orthogonal Projection Matrices and the Rellichthe Rellichstein: This theorem: This involves theorectal.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Projection Matrices and the Rellichthe Rellichstein: This theorem: This involves theorectal.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bessel's Inequality: Theorem: Theorem: Theorem: Theorem: Bessel'sch that theore theory of the Bessel's Gram-Sch as a theorem**: Bessel'stangent Decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bessel's Inequality: Theorem: Theorem: Theorem: Theorem: Bessel'sch that theore theory of the Bessel's Gram-Sch as a theorem**: Bessel'stangent Decomposition.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "in Hilberts'",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "in Hilberts'",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Determinant of a Matrix with a Non-Integrable Singularity and a Nilpotent factors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Determinant of a Matrix with a Non-Integrable Singularity and a Nilpotent factors.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Determinant of a Matrix with a Non-Symmetric Transformation Matrix and a Non-Integrable Transformation Matrix: The determinant of a matrix with a non-symmetric transformation matrix and a non-integrable transformation matrix can be calculated using the formula for this is a matrix can be calculated using theorelated matrix and has been calculated using theorelated matrix:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Determinant of a Matrix with a Non-Symmetric Transformation Matrix and a Non-Integrable Transformation Matrix: The determinant of a matrix with a non-symmetric transformation matrix and a non-integrable transformation matrix can be calculated using the formula for this is a matrix can be calculated using theorelated matrix and has been calculated using theorelated matrix:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Independence of Generalized Vector Spaces: Generalized vector spaces, such as generalized inner product spaces, provide a way to generalize theore theore theore theore theoreto theore theore the zero vector space of Linear Independence of theore of theore theore theore: A set of linear independence of theore theore of linear Independence of Linear Independence:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Independence of Generalized Vector Spaces: Generalized vector spaces, such as generalized inner product spaces, provide a way to generalize theore theore theore theore theoreto theore theore the zero vector space of Linear Independence of theore of theore theore theore: A set of linear independence of theore theore of linear Independence of Linear Independence:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Schmidt Decomposition: A decomposition of a vector into its orthogonal components, which can be useful in understanding the relationship between linear independence and span.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Schmidt Decomposition: A decomposition of a vector into its orthogonal components, which can be useful in understanding the relationship between linear independence and span.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Systems with Non-Square Matrices:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Systems with Non-Square Matrices:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Linear Transformations: This sub-category explores the concept of linear transformations, including linear operators, matrices, and their relationships with vector spaces. -> Kernel and Image of Linear Transformations: This sub-field examines the kernel and image of a linear transformation, which are essential concepts in understanding the behavior of linear transformations. The kernel of a linear transformation is the set of vectors that are mapped to the zero vector, while the image is the set of vectors that are mapped from the domain to the codomain. Understanding the kernel and image of a linear transformation can provide valuable insights into its properties and behavior.:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Linear Transformations: This sub-category explores the concept of linear transformations, including linear operators, matrices, and their relationships with vector spaces. -> Kernel and Image of Linear Transformations: This sub-field examines the kernel and image of a linear transformation, which are essential concepts in understanding the behavior of linear transformations. The kernel of a linear transformation is the set of vectors that are mapped to the zero vector, while the image is the set of vectors that are mapped from the domain to the codomain. Understanding the kernel and image of a linear transformation can provide valuable insights into its properties and behavior.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Pontryagin Duality: A theorem that establishes a duality between the space of linear functionals on a vector spaces**:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pontryagin Duality: A theorem that establishes a duality between the space of linear functionals on a vector spaces**:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Kronecker",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Kronecker",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Relatively:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Relatively:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Complex Linear Transformations: A generalization of linear transformations to complex vector spaces.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Complex Linear Transformations: A generalization of linear transformations to complex vector spaces.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Approximation of Linear Transformations: A study of approximating linear transformations using finite-dimensional approximations, such as follows theore is obtained in linear algebraic and specifically, which can be applied to be extensively",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Approximation of Linear Transformations: A study of approximating linear transformations using finite-dimensional approximations, such as follows theore is obtained in linear algebraic and specifically, which can be applied to be extensively",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are not equal to determine",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are not equal to determine",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Additional knowledge for Matrices and Linear Equations: This sub-category focuses on the use of matrices to represent linear equations, including matrix operations, determinants, and inverses. -> Matrix Inversion Methods: There are several methods to find the inverse of a matrix, including the Gauss-Jordan elimination method, LU decomposition, and the adjoint method. Each method has its own strengths and weaknesses, and choosing the right method depends on the specific problem and the properties of the matrix.:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Additional knowledge for Matrices and Linear Equations: This sub-category focuses on the use of matrices to represent linear equations, including matrix operations, determinants, and inverses. -> Matrix Inversion Methods: There are several methods to find the inverse of a matrix, including the Gauss-Jordan elimination method, LU decomposition, and the adjoint method. Each method has its own strengths and weaknesses, and choosing the right method depends on the specific problem and the properties of the matrix.:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Orthogonal matrix, orthogon orthogonal decomposition of an a process of an orthogonal diagonalization of an orthogonal diagonalization of Orthogonal decomposition of anorthogonal Decomposition of Orthogonal Decomposition of Orthogonal diagonalization.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Orthogonal matrix, orthogon orthogonal decomposition of an a process of an orthogonal diagonalization of an orthogonal diagonalization of Orthogonal decomposition of anorthogonal Decomposition of Orthogonal Decomposition of Orthogonal diagonalization.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "' could include:",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "' could include:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      },
      {
        "text": "' could be:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 0.8987453326967041
      }
    ]
  },
  {
    "representative_text": "The provided. Theoremsspace, are not only the properties of orthogonal decomposition of the Subspace: Some of Orthogonal Projection of Orthogonal decomposition of Orthogonal Decomposition matrices.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The provided. Theoremsspace, are not only the properties of orthogonal decomposition of the Subspace: Some of Orthogonal Projection of Orthogonal decomposition of Orthogonal Decomposition matrices.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Mixed Determinants: A mixed determinant is a determinant of a matrix with both real and complex entries. The calculation of mixed determinants involves complex arithmetic.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Mixed Determinants: A mixed determinant is a determinant of a matrix with both real and complex entries. The calculation of mixed determinants involves complex arithmetic.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Conjugate that are very interesting to be done using a",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Conjugate that are very interesting to be done using a",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Interlacing Theorem for Non-Square Matrices: A theorem that states that the eigenvalues of a non-square matrix and its submatrices interlace, meaning that the eigenvalues of the submatrix are between the eigenvalues of the original matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Interlacing Theorem for Non-Square Matrices: A theorem that states that the eigenvalues of a non-square matrix and its submatrices interlace, meaning that the eigenvalues of the submatrix are between the eigenvalues of the original matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block Triangularization: This is a method for decomposing a matrix into a product of a block triangular matrix and a unitary matrix. It can be used to compute the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block Triangularization: This is a method for decomposing a matrix into a product of a block triangular matrix and a unitary matrix. It can be used to compute the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Computing the Dimension of a Vector Space using the Linear Independence of a Basis with Respect to a Non-Standard Vector Addition to be used in a basis for Infinite-Dimensional Spaces'",
    "is_in_domain": false,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Computing the Dimension of a Vector Space using the Linear Independence of a Basis with Respect to a Non-Standard Vector Addition to be used in a basis for Infinite-Dimensional Spaces'",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "**Rank-",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "**Rank-",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Direct Sum Decomposition: The direct sum decomposition of a vector space into a direct sum of linear independence of a linear independence of linear dimension: Theore theore theore the dimension, and Theorem in Theorem: There are in the linear Independence of a finite dimensional spaces**: Theorem:",
    "is_in_domain": true,
    "pipelines_covered": [
      "RecursiveTaxonomyExplorer"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Direct Sum Decomposition: The direct sum decomposition of a vector space into a direct sum of linear independence of a linear independence of linear dimension: Theore theore theore the dimension, and Theorem in Theorem: There are in the linear Independence of a finite dimensional spaces**: Theorem:",
        "pipeline": "RecursiveTaxonomyExplorer",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Invertibility of matrices: A condition for a matrix to be invertible, which can be used to determine whether a matrix has a solution to a system of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Invertibility of matrices: A condition for a matrix to be invertible, which can be used to determine whether a matrix has a solution to a system of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Stability of linear systems: A measure of how well a linear system can be approximated by a smaller system, which can be used to determine the stability of a system.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Stability of linear systems: A measure of how well a linear system can be approximated by a smaller system, which can be used to determine the stability of a system.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Buchberger's Algorithm: An algorithm for computing the Gröbner basis of a polynomial ideal, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Buchberger's Algorithm: An algorithm for computing the Gröbner basis of a polynomial ideal, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Row Echelon Form (REF): A canonical form for matrices, which is similar to RREF but may have non-zero rows.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Row Echelon Form (REF): A canonical form for matrices, which is similar to RREF but may have non-zero rows.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Column Echelon Form (CEF): A canonical form for matrices, which is similar to RREF but with rows and columns interchanged.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Column Echelon Form (CEF): A canonical form for matrices, which is similar to RREF but with rows and columns interchanged.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Stochastic Matrix: A square matrix with non-negative entries, which is used in linear algebra to model random processes and to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Stochastic Matrix: A square matrix with non-negative entries, which is used in linear algebra to model random processes and to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Markov Chain: A mathematical system that undergoes transitions from one state to another, which is used in linear algebra to model random processes and to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Markov Chain: A mathematical system that undergoes transitions from one state to another, which is used in linear algebra to model random processes and to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fourier Series: A mathematical representation of a function as an infinite sum of sinusoidal functions, which is used in linear algebra to solve systems of linear equations and to approximate functions.",
    "is_in_domain": false,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Fourier Series: A mathematical representation of a function as an infinite sum of sinusoidal functions, which is used in linear algebra to solve systems of linear equations and to approximate functions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Pseudospectral Methods: A class of algorithms for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Pseudospectral Methods: A class of algorithms for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Pseudospectral Quadrature: A method for approximating the solution of a system of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9285112491213763
      }
    ]
  },
  {
    "representative_text": "Time-Domain Methods: A class of algorithms for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Time-Domain Methods: A class of algorithms for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Frequency-Domain Methods: A class of algorithms for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9198064561112708
      },
      {
        "text": "Time-Domain Methods for Solving Systems of Linear Equations: A class of algorithms for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9260516489614694
      },
      {
        "text": "Frequency-Domain Methods for Solving Systems of Linear Equations: A class of algorithms for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9260820306923456
      }
    ]
  },
  {
    "representative_text": "Block Triangular Factorization: A factorization of a matrix into its block triangular factors, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Block Triangular Factorization: A factorization of a matrix into its block triangular factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Block LU Decomposition: A factorization of a matrix into its block lower and upper triangular factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8728775734066223
      },
      {
        "text": "Block Cholesky Decomposition: A factorization of a matrix into its block lower triangular factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9339812439852304
      },
      {
        "text": "Block Schur Decomposition: A factorization of a matrix into its block Schur factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8925399686191582
      },
      {
        "text": "Block LU Factorization: A factorization of a matrix into its block lower and upper triangular factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9363792042429541
      },
      {
        "text": "Block Cholesky Decomposition with Applications: A factorization of a matrix into its block lower triangular factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9245884924241594
      },
      {
        "text": "Block Schur Decomposition with Applications: A factorization of a matrix into its block Schur factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8993272056171322
      },
      {
        "text": "Block Triangular Factorization with Applications: A factorization of a matrix into its block triangular factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9205212809444102
      },
      {
        "text": "Block Triangular Decomposition: A factorization of a matrix into its block triangular factors, which is used to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9456292915097412
      }
    ]
  },
  {
    "representative_text": "Block Diagonal Factorization: A factorization of a matrix into its block diagonal factors, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Block Diagonal Factorization: A factorization of a matrix into its block diagonal factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Block Diagonal Factorization with Applications: A factorization of a matrix into its block diagonal factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9575468875577071
      }
    ]
  },
  {
    "representative_text": "Toeplitz Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Toeplitz Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Block Toeplitz Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9259781202268653
      },
      {
        "text": "Toeplitz-Like Matrix with Applications: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9424333652656344
      }
    ]
  },
  {
    "representative_text": "Circulant Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Circulant Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Block Circulant Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9588117922094054
      }
    ]
  },
  {
    "representative_text": "Hankel Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Hankel Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Block Hankel Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9419666887689426
      }
    ]
  },
  {
    "representative_text": "Block Schur Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block Schur Matrix: A matrix with constant entries along diagonals, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block Givens Rotation: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Block Givens Rotation: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Givens Rotation: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9540535423900287
      }
    ]
  },
  {
    "representative_text": "Block Householder Reflection: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Block Householder Reflection: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Householder Transformation: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9239934123068201
      },
      {
        "text": "Block Householder Transformation with Applications: A method for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9454981779187983
      }
    ]
  },
  {
    "representative_text": "Block QR Algorithm: An algorithm for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Block QR Algorithm: An algorithm for solving systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Block QR Factorization: A factorization of a matrix into its block QR factors, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Block QR Factorization: A factorization of a matrix into its block QR factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Block QR Factorization with Applications: A factorization of a matrix into its block QR factors, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9497200744322232
      }
    ]
  },
  {
    "representative_text": "Pseudoinverse with Applications: A mathematical concept that is used in linear algebra to solve systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Pseudoinverse with Applications: A mathematical concept that is used in linear algebra to solve systems of linear equations, which is used in linear algebra to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Residuals and Error Analysis: A method for analyzing the residuals and errors in a system of linear equations, which can be used to improve the accuracy of solutions.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Residuals and Error Analysis: A method for analyzing the residuals and errors in a system of linear equations, which can be used to improve the accuracy of solutions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Complex Numbers: A branch of linear algebra that deals with complex numbers, which can be used to solve systems of linear equations with complex coefficients.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Complex Numbers: A branch of linear algebra that deals with complex numbers, which can be used to solve systems of linear equations with complex coefficients.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Non-Standard Metrics: A branch of linear algebra that deals with non-standard metrics, which can be used to solve systems of linear equations with non-standard metrics.",
    "is_in_domain": false,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Non-Standard Metrics: A branch of linear algebra that deals with non-standard metrics, which can be used to solve systems of linear equations with non-standard metrics.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra with Non-Standard Metrics: A branch of linear algebra that deals with non-standard metrics, such as the Euclidean metric, the inner product, and the Frobenius norm.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8946781833056012
      }
    ]
  },
  {
    "representative_text": "Lie Algebras and Linear Algebra: A connection between Lie algebras and linear algebra, which can be used to solve systems of linear equations with Lie algebraic structures.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lie Algebras and Linear Algebra: A connection between Lie algebras and linear algebra, which can be used to solve systems of linear equations with Lie algebraic structures.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Homotopy Methods for Solving Systems of Linear Equations: A method for solving systems of linear equations using homotopy techniques, which can be used to improve the accuracy of solutions.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Homotopy Methods for Solving Systems of Linear Equations: A method for solving systems of linear equations using homotopy techniques, which can be used to improve the accuracy of solutions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Non-Linear Equations: A branch of linear algebra that deals with non-linear equations, which can be used to solve systems of linear equations with non-linear constraints.",
    "is_in_domain": false,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Non-Linear Equations: A branch of linear algebra that deals with non-linear equations, which can be used to solve systems of linear equations with non-linear constraints.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Optimization Techniques for Solving Systems of Linear Equations: A method for solving systems of linear equations using optimization techniques, which can be used to improve the accuracy of solutions.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Optimization Techniques for Solving Systems of Linear Equations: A method for solving systems of linear equations using optimization techniques, which can be used to improve the accuracy of solutions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Multi-Objective Optimization for Solving Systems of Linear Equations: A method for solving systems of linear equations using multi-objective optimization techniques, which can be used to improve the accuracy of solutions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8827508391656095
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Graph Theory: A connection between linear algebra and graph theory, which can be used to solve systems of linear equations using graph-theoretic methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Graph Theory: A connection between linear algebra and graph theory, which can be used to solve systems of linear equations using graph-theoretic methods.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Signal Processing: A connection between linear algebra and signal processing, which can be used to solve systems of linear equations using signal processing techniques.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Signal Processing: A connection between linear algebra and signal processing, which can be used to solve systems of linear equations using signal processing techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra with Image Processing: A connection between linear algebra and image processing, which can be used to solve systems of linear equations using image processing techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9158181151317424
      },
      {
        "text": "Linear Algebra with Time Series Analysis: A connection between linear algebra and time series analysis, which can be used to solve systems of linear equations using time series analysis techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8612604927728492
      },
      {
        "text": "Linear algebra with image processing: A connection between linear algebra and image processing, which can be used to solve systems of linear equations using image processing techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8982015868941482
      },
      {
        "text": "Linear algebra with time series analysis: A connection between linear algebra and time series analysis, which can be used to solve systems of linear equations using time series analysis techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8617530598066777
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Spectral Methods: A connection between linear algebra and spectral methods, which can be used to solve systems of linear equations using spectral methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Spectral Methods: A connection between linear algebra and spectral methods, which can be used to solve systems of linear equations using spectral methods.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear algebra with spectral methods: A connection between linear algebra and spectral methods, which can be used to solve systems of linear equations using spectral methods.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9542382642733503
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Numerical Methods: A connection between linear algebra and numerical methods, which can be used to solve systems of linear equations using numerical methods.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Numerical Methods: A connection between linear algebra and numerical methods, which can be used to solve systems of linear equations using numerical methods.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Perturbation Theory: A connection between linear algebra and perturbation theory, which can be used to solve systems of linear equations using perturbation theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Perturbation Theory: A connection between linear algebra and perturbation theory, which can be used to solve systems of linear equations using perturbation theory.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear algebra with perturbation theory: A connection between linear algebra and perturbation theory, which can be used to solve systems of linear equations using perturbation theory.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9512853546434824
      },
      {
        "text": "Linear Algebra with Perturbation Theory: A connection between linear algebra and perturbation theory, which can be used to solve systems of linear equations using perturbation theory, such as the method of successive approximations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9501279338094735
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Random Matrix Theory: A connection between linear algebra and random matrix theory, which can be used to solve systems of linear equations using random matrix theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Random Matrix Theory: A connection between linear algebra and random matrix theory, which can be used to solve systems of linear equations using random matrix theory.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Random Matrix Theory: A branch of linear algebra that deals with the properties of random matrices, which can be used to solve systems of linear equations using random matrix techniques, such as random matrix approximation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8721635647517418
      }
    ]
  },
  {
    "representative_text": "Random Matrix Theory: A branch of linear algebra that deals with the properties of random matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Random Matrix Theory: A branch of linear algebra that deals with the properties of random matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Differential Equations: A connection between linear algebra and differential equations, which can be used to solve systems of linear equations with differential equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Differential Equations: A connection between linear algebra and differential equations, which can be used to solve systems of linear equations with differential equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear algebra with differential equations: A connection between linear algebra and differential equations, which can be used to solve systems of linear equations with differential equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9584963787981979
      },
      {
        "text": "Linear Algebra with Differential Equations: A connection between linear algebra and differential equations, which can be used to solve systems of linear equations with differential equations, such as systems of ordinary differential equations (ODEs) and partial differential equations (PDEs).",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9669483572082402
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Quantum Computing: A connection between linear algebra and quantum computing, which can be used to solve systems of linear equations using quantum algorithms.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Quantum Computing: A connection between linear algebra and quantum computing, which can be used to solve systems of linear equations using quantum algorithms.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra with Non-Standard Matrices: A branch of linear algebra that deals with non-standard matrices, such as non-square matrices and matrices with non-standard properties, which can be used to solve systems of linear equations with non-standard matrices.",
    "is_in_domain": true,
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra with Non-Standard Matrices: A branch of linear algebra that deals with non-standard matrices, such as non-square matrices and matrices with non-standard properties, which can be used to solve systems of linear equations with non-standard matrices.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The magnitude of a vector is its length or size.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The magnitude of a vector is its length or size.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The direction of a vector is its orientation in space.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The direction of a vector is its orientation in space.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The unit vector is a vector with magnitude 1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The unit vector is a vector with magnitude 1.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Two vectors are orthonormal if they are orthogonal and have a magnitude of 1.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Two vectors are orthonormal if they are orthogonal and have a magnitude of 1.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The range of a linear transformation is the set of vectors that are mapped from the zero vector.",
    "is_in_domain": false,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The range of a linear transformation is the set of vectors that are mapped from the zero vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The diagonal matrix contains the eigenvalues, and the eigenvectors are used to construct the columns of the matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The diagonal matrix contains the eigenvalues, and the eigenvectors are used to construct the columns of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The augmented matrix is a matrix that contains the coefficients of the variables and the constants on the right-hand side of the equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The augmented matrix is a matrix that contains the coefficients of the variables and the constants on the right-hand side of the equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A Markov chain is a mathematical system that undergoes transitions from one state to another.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A Markov chain is a mathematical system that undergoes transitions from one state to another.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The transition matrix is a matrix that describes the probabilities of transitioning from one state to another.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The transition matrix is a matrix that describes the probabilities of transitioning from one state to another.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonality and Orthonormality: In addition to the basic definitions, there are more advanced concepts such as orthogonal projections, orthogonal decomposition, and orthonormal bases.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonality and Orthonormality: In addition to the basic definitions, there are more advanced concepts such as orthogonal projections, orthogonal decomposition, and orthonormal bases.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination: In addition to the basic method, there are more advanced techniques such as partial pivoting, row echelon form, and reduced row echelon form.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination: In addition to the basic method, there are more advanced techniques such as partial pivoting, row echelon form, and reduced row echelon form.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "NumPy: NumPy is a library for numerical computing in Python that provides support for linear algebra operations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "NumPy: NumPy is a library for numerical computing in Python that provides support for linear algebra operations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "NumPy with SciPy: NumPy with SciPy is a Python library for numerical computing that provides support for linear algebra operations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9250815610874779
      }
    ]
  },
  {
    "representative_text": "SciPy: SciPy is a library for scientific computing in Python that provides support for linear algebra operations, including eigenvalue decomposition and singular value decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "SciPy: SciPy is a library for scientific computing in Python that provides support for linear algebra operations, including eigenvalue decomposition and singular value decomposition.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "MATLAB: MATLAB is a high-level programming language that provides extensive support for linear algebra operations, including matrix operations and eigenvalue decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "MATLAB: MATLAB is a high-level programming language that provides extensive support for linear algebra operations, including matrix operations and eigenvalue decomposition.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "GNU Octave: GNU Octave is a free and open-source alternative to MATLAB that provides support for linear algebra operations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "GNU Octave: GNU Octave is a free and open-source alternative to MATLAB that provides support for linear algebra operations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "GNU Octave with SciPy: GNU Octave with SciPy is a free and open-source alternative to MATLAB that provides support for linear algebra operations and scientific computing.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9491959480670663
      },
      {
        "text": "GNU Octave with SciPy: GNU Octave is a free and open-source alternative to MATLAB that provides support for linear algebra operations and scientific computing. SciPy is a library for scientific computing in Python that provides support for linear algebra operations, including eigenvalue decomposition and singular value decomposition.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8892765998965153
      }
    ]
  },
  {
    "representative_text": "Textbooks: There are many textbooks on linear algebra that cover the basics and advanced topics, including \"Linear Algebra and Its Applications\" by Gilbert Strang and \"Linear Algebra\" by David C. Lay.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Textbooks: There are many textbooks on linear algebra that cover the basics and advanced topics, including \"Linear Algebra and Its Applications\" by Gilbert Strang and \"Linear Algebra\" by David C. Lay.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Online Courses: There are many online courses on linear algebra that cover the basics and advanced topics, including Coursera's \"Linear Algebra\" course by University of Michigan and edX's \"Linear Algebra\" course by MIT.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Online Courses: There are many online courses on linear algebra that cover the basics and advanced topics, including Coursera's \"Linear Algebra\" course by University of Michigan and edX's \"Linear Algebra\" course by MIT.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Videos: There are many videos on linear algebra that cover the basics and advanced topics, including 3Blue1Brown's \"Linear Algebra\" series on YouTube.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Videos: There are many videos on linear algebra that cover the basics and advanced topics, including 3Blue1Brown's \"Linear Algebra\" series on YouTube.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Practice Problems: There are many practice problems on linear algebra that can be used to reinforce understanding and to prepare for exams, including the \"Linear Algebra\" textbook by David C. Lay and the \"MIT OpenCourseWare\" linear algebra course.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Practice Problems: There are many practice problems on linear algebra that can be used to reinforce understanding and to prepare for exams, including the \"Linear Algebra\" textbook by David C. Lay and the \"MIT OpenCourseWare\" linear algebra course.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bilinear Form Properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bilinear Form Properties:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quadratic Form Properties:",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quadratic Form Properties:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Definition: Sylvester's law of inertia is a theorem that states that the rank and nullity of a matrix are preserved under elementary row and column operations.",
    "is_in_domain": false,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Definition: Sylvester's law of inertia is a theorem that states that the rank and nullity of a matrix are preserved under elementary row and column operations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Complete Orthonormality: A set of vectors is said to be complete orthonormal if every vector in the space can be expressed as a linear combination of the orthonormal vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Complete Orthonormality: A set of vectors is said to be complete orthonormal if every vector in the space can be expressed as a linear combination of the orthonormal vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product Identity: The inner product identity states that the inner product of a vector with itself is equal to the square of its magnitude.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner Product Identity: The inner product identity states that the inner product of a vector with itself is equal to the square of its magnitude.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hermitian Bilinear Forms: A Hermitian bilinear form is a bilinear form that is equal to its own conjugate transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Hermitian Bilinear Forms: A Hermitian bilinear form is a bilinear form that is equal to its own conjugate transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "Hermitian Quadratic Forms: A Hermitian quadratic form is a quadratic form that is equal to the inner product of the input vector with its conjugate transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8372398241477019
      }
    ]
  },
  {
    "representative_text": "Positive Definite Bilinear Forms: A positive definite bilinear form is a bilinear form that is equal to the inner product of the input vectors.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Positive Definite Bilinear Forms: A positive definite bilinear form is a bilinear form that is equal to the inner product of the input vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "Positive Definite Quadratic Forms: A positive definite quadratic form is a quadratic form that is equal to the inner product of the input vector with itself.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8671784035595087
      },
      {
        "text": "Positive Definiteness: Positive definiteness is a property of a matrix that states that the matrix is equal to the inner product of the input vector with itself.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8002090480389075
      }
    ]
  },
  {
    "representative_text": "Symmetric Positive Definite Matrix: A symmetric positive definite matrix is a matrix that is symmetric and positive definite.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Symmetric Positive Definite Matrix: A symmetric positive definite matrix is a matrix that is symmetric and positive definite.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hermitian Symmetry: Hermitian symmetry is a property of a matrix that states that the matrix is equal to its own conjugate transpose.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hermitian Symmetry: Hermitian symmetry is a property of a matrix that states that the matrix is equal to its own conjugate transpose.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inertia: Inertia is a property of a matrix that describes the rank and nullity of the matrix.",
    "is_in_domain": false,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inertia: Inertia is a property of a matrix that describes the rank and nullity of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quadratic Programming: Quadratic programming is a method for optimizing a quadratic function subject to quadratic constraints.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Quadratic Programming: Quadratic programming is a method for optimizing a quadratic function subject to quadratic constraints.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Convex Optimization: Convex optimization is a method for optimizing a convex function subject to convex constraints.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Convex Optimization: Convex optimization is a method for optimizing a convex function subject to convex constraints.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-Convex Optimization: Non-convex optimization is a method for optimizing a non-convex function subject to non-convex constraints.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Convex Optimization: Non-convex optimization is a method for optimizing a non-convex function subject to non-convex constraints.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Hypothesis Testing: Hypothesis testing is a method for testing a hypothesis about a population based on a sample of data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Hypothesis Testing: Hypothesis testing is a method for testing a hypothesis about a population based on a sample of data.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Confidence Intervals: Confidence intervals are a method for estimating a population parameter based on a sample of data.",
    "is_in_domain": false,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Confidence Intervals: Confidence intervals are a method for estimating a population parameter based on a sample of data.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Regression Analysis: Regression analysis is a method for modeling the relationship between a dependent variable and one or more independent variables.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Regression Analysis: Regression analysis is a method for modeling the relationship between a dependent variable and one or more independent variables.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Time Series Analysis: Time series analysis is a method for analyzing data that is collected over time.",
    "is_in_domain": false,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Time Series Analysis: Time series analysis is a method for analyzing data that is collected over time.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "SVD with Singular Value Thresholding: SVD with singular value thresholding is a method for thresholding the singular values of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "SVD with Singular Value Thresholding: SVD with singular value thresholding is a method for thresholding the singular values of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "MATLAB with Symbolic Math Toolbox: MATLAB with Symbolic Math Toolbox is a programming language that provides support for linear algebra operations and symbolic mathematics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "MATLAB with Symbolic Math Toolbox: MATLAB with Symbolic Math Toolbox is a programming language that provides support for linear algebra operations and symbolic mathematics.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "R with Matrix Package: R with Matrix Package is a programming language that provides support for linear algebra operations and statistical computing.",
    "is_in_domain": false,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "R with Matrix Package: R with Matrix Package is a programming language that provides support for linear algebra operations and statistical computing.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "R with Matrix Package: R is a programming language that provides support for linear algebra operations and statistical computing. Matrix Package is a package for R that provides support for linear algebra operations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9563571860237768
      }
    ]
  },
  {
    "representative_text": "Sylvester's Law of Inertia with Numerical Stability: Sylvester's law of inertia is a theorem that states that the rank and nullity of a matrix are preserved under elementary row and column operations. Numerical stability refers to the sensitivity of numerical methods to small changes in the input data. Sylvester's law of inertia with numerical stability is used in numerical linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Sylvester's Law of Inertia with Numerical Stability: Sylvester's law of inertia is a theorem that states that the rank and nullity of a matrix are preserved under elementary row and column operations. Numerical stability refers to the sensitivity of numerical methods to small changes in the input data. Sylvester's law of inertia with numerical stability is used in numerical linear algebra to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination with Partial Pivoting and LU Decomposition: Gaussian elimination with partial pivoting is a method for solving systems of linear equations. LU decomposition is a factorization of a matrix into the product of a lower triangular matrix and an upper triangular matrix. Gaussian elimination with partial pivoting and LU decomposition is used in numerical linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination with Partial Pivoting and LU Decomposition: Gaussian elimination with partial pivoting is a method for solving systems of linear equations. LU decomposition is a factorization of a matrix into the product of a lower triangular matrix and an upper triangular matrix. Gaussian elimination with partial pivoting and LU decomposition is used in numerical linear algebra to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Cholesky Decomposition with Partial Pivoting and SVD: Cholesky decomposition is a factorization of a symmetric positive definite matrix into the product of a lower triangular matrix and its transpose. Partial pivoting is a method for selecting the pivot element in Gaussian elimination. SVD (Singular Value Decomposition) is a factorization of a matrix into the product of three matrices: U, Σ, and V. Cholesky decomposition with partial pivoting and SVD is used in numerical linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cholesky Decomposition with Partial Pivoting and SVD: Cholesky decomposition is a factorization of a symmetric positive definite matrix into the product of a lower triangular matrix and its transpose. Partial pivoting is a method for selecting the pivot element in Gaussian elimination. SVD (Singular Value Decomposition) is a factorization of a matrix into the product of three matrices: U, Σ, and V. Cholesky decomposition with partial pivoting and SVD is used in numerical linear algebra to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Interpolation with Polynomial Interpolation and LU Decomposition: Eigenvalue interpolation is a method for interpolating eigenvalues of a matrix. Polynomial interpolation is a method for approximating a function using a polynomial. LU decomposition is a factorization of a matrix into the product of a lower triangular matrix and an upper triangular matrix. Eigenvalue interpolation with polynomial interpolation and LU decomposition is used in numerical linear algebra to solve systems of linear equations.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Interpolation with Polynomial Interpolation and LU Decomposition: Eigenvalue interpolation is a method for interpolating eigenvalues of a matrix. Polynomial interpolation is a method for approximating a function using a polynomial. LU decomposition is a factorization of a matrix into the product of a lower triangular matrix and an upper triangular matrix. Eigenvalue interpolation with polynomial interpolation and LU decomposition is used in numerical linear algebra to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "NumPy with SciPy: NumPy is a library for numerical computing in Python that provides support for linear algebra operations. SciPy is a library for scientific computing in Python that provides support for linear algebra operations, including eigenvalue decomposition and singular value decomposition.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "NumPy with SciPy: NumPy is a library for numerical computing in Python that provides support for linear algebra operations. SciPy is a library for scientific computing in Python that provides support for linear algebra operations, including eigenvalue decomposition and singular value decomposition.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computer Science with Graph Theory: Computer science is a field of study that deals with the use of algorithms to analyze and interpret complex data. Graph theory is a branch of mathematics that deals with the study of graphs. Linear algebra is used in computer science for tasks such as graph theory, computer graphics, and machine learning.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computer Science with Graph Theory: Computer science is a field of study that deals with the use of algorithms to analyze and interpret complex data. Graph theory is a branch of mathematics that deals with the study of graphs. Linear algebra is used in computer science for tasks such as graph theory, computer graphics, and machine learning.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Biology with Molecular Biology: Biology is a field of study that deals with the study of living organisms. Molecular biology is a branch of biology that deals with the study of molecules. Linear algebra is used in biology for tasks such as molecular biology, bioinformatics, and systems biology.",
    "is_in_domain": false,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Biology with Molecular Biology: Biology is a field of study that deals with the study of living organisms. Molecular biology is a branch of biology that deals with the study of molecules. Linear algebra is used in biology for tasks such as molecular biology, bioinformatics, and systems biology.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Economics with Economic Systems: Economics is a field of study that deals with the study of economic systems. Linear algebra is used in economics for tasks such as economic systems, macroeconomics, and microeconomics.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Economics with Economic Systems: Economics is a field of study that deals with the study of economic systems. Linear algebra is used in economics for tasks such as economic systems, macroeconomics, and microeconomics.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Mathematics with Abstract Algebra: Mathematics is a field of study that deals with the study of mathematical structures. Abstract algebra is a branch of mathematics that deals with the study of algebraic structures. Linear algebra is used in mathematics for tasks such as abstract algebra, group theory, and ring theory.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Mathematics with Abstract Algebra: Mathematics is a field of study that deals with the study of mathematical structures. Abstract algebra is a branch of mathematics that deals with the study of algebraic structures. Linear algebra is used in mathematics for tasks such as abstract algebra, group theory, and ring theory.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Non-Singular Matrices: A non-singular matrix is a matrix that has an inverse. Non-singular matrices have many applications in linear algebra, including solving systems of linear equations and finding the inverse of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Non-Singular Matrices: A non-singular matrix is a matrix that has an inverse. Non-singular matrices have many applications in linear algebra, including solving systems of linear equations and finding the inverse of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Skew-Symmetric Matrices: A skew-symmetric matrix is a matrix that is equal to the negative of its own transpose. Skew-symmetric matrices have many applications in linear algebra, including finding the eigenvalues and eigenvectors of a matrix.",
    "is_in_domain": true,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Skew-Symmetric Matrices: A skew-symmetric matrix is a matrix that is equal to the negative of its own transpose. Skew-symmetric matrices have many applications in linear algebra, including finding the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian Elimination with Row Operations: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination: Gaussian elimination with Partial Piv with Partial Piv with Partial Pivector: Gaussian elimination with Partial Pivization: Gaussian Elimination of a Linear Algebraicponentia Matrix: Gaussian elimination with Partial pivoting: Gaussian elimination with Partiallytic with partial pivoting: Gaussian Elimination of a matrix: Gaussian elimination with Numericalgebra: Gaussian elimination with Partial Pivector: Gaussian Elimination: Gaussian Elimination:",
    "is_in_domain": false,
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Gaussian Elimination with Row Operations: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination: Gaussian elimination with Partial Piv with Partial Piv with Partial Pivector: Gaussian elimination with Partial Pivization: Gaussian Elimination of a Linear Algebraicponentia Matrix: Gaussian elimination with Partial pivoting: Gaussian elimination with Partiallytic with partial pivoting: Gaussian Elimination of a matrix: Gaussian elimination with Numericalgebra: Gaussian elimination with Partial Pivector: Gaussian Elimination: Gaussian Elimination:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "Here are a matrix: Gaussian Elimination with SVD (Singular with Partial P with Partial Pivariance: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination with Partial pivoting: Gaussian Elimination: Gaussian Elimination with Partial Pivarity: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination with Partial Pivelihood with Gaussian Elimination with Partial Pivis: Gaussian Elimination: Gaussian Elimination with Partial Pivector: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination with Partial P with Partial Pivector: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination: Gaussian Elimination with Gaussian Elimination: Gaussian Elimination:",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8944200407249537
      }
    ]
  }
]