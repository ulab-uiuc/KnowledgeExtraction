[
  {
    "representative_text": "Addition of vectors: The sum of two or more vectors is the vector that results from placing the tail of the second vector at the head of the first vector and adding the corresponding components.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Addition of vectors: The sum of two or more vectors is the vector that results from placing the tail of the second vector at the head of the first vector and adding the corresponding components.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scalar multiplication of vectors: The product of a scalar and a vector is the vector that results from multiplying each component of the original vector by the scalar.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar multiplication of vectors: The product of a scalar and a vector is the vector that results from multiplying each component of the original vector by the scalar.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector magnitude (length): The magnitude of a vector is the square root of the sum of the squares of its components.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Vector magnitude (length): The magnitude of a vector is the square root of the sum of the squares of its components.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "The magnitude (length) of a vector can be calculated using the Pythagorean theorem.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8171401668366377
      }
    ]
  },
  {
    "representative_text": "Unit vector: A unit vector is a vector with a magnitude of 1.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Unit vector: A unit vector is a vector with a magnitude of 1.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector space: A set of vectors that is closed under addition and scalar multiplication.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Vector space: A set of vectors that is closed under addition and scalar multiplication.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "A vector space is a set of vectors that is closed under addition and scalar multiplication.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8981073863471357
      }
    ]
  },
  {
    "representative_text": "Basis: A set of vectors that spans a vector space and is linearly independent.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Basis: A set of vectors that spans a vector space and is linearly independent.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "A basis for a vector space is a set of vectors that spans the space and is linearly independent.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9010526469652655
      },
      {
        "text": "A basis of a vector space is a set of vectors that spans the space and is linearly independent.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9754893672684567
      }
    ]
  },
  {
    "representative_text": "Dimension: The number of vectors in a basis for a vector space.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Dimension: The number of vectors in a basis for a vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "The dimension of a vector space is the number of vectors in a basis for the space.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9039047730080005
      }
    ]
  },
  {
    "representative_text": "Span: The set of all linear combinations of a basis for a vector space.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Span: The set of all linear combinations of a basis for a vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "The span of a set of vectors is the set of all linear combinations of those vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8976703651118164
      },
      {
        "text": "The span of a set of vectors V is the set of all linear combinations of the vectors in V.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9635457918790553
      },
      {
        "text": "The span of a vector x is the set of all scalar multiples of x.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8337720162309212
      },
      {
        "text": "The span of a set of vectors x1, x2, ..., xn is the set of all linear combinations of the vectors x1, x2, ..., xn.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.957591828123626
      }
    ]
  },
  {
    "representative_text": "Linear independence: A set of vectors is linearly independent if none of the vectors can be expressed as a linear combination of the others.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear independence: A set of vectors is linearly independent if none of the vectors can be expressed as a linear combination of the others.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "A set of vectors x1, x2, ..., xn is said to be linearly independent if the equation a1x1 + a2x2 + ... + anxn = 0 has only the trivial solution, where a1, a2, ..., an are scalars.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8387295307265147
      }
    ]
  },
  {
    "representative_text": "Linear dependence: A set of vectors is linearly dependent if at least one vector can be expressed as a linear combination of the others.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear dependence: A set of vectors is linearly dependent if at least one vector can be expressed as a linear combination of the others.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Spanning set: A set of vectors that spans a vector space is a basis for the space.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Spanning set: A set of vectors that spans a vector space is a basis for the space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear transformation: A function from a vector space to another vector space that preserves the operations of vector addition and scalar multiplication.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear transformation: A function from a vector space to another vector space that preserves the operations of vector addition and scalar multiplication.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "A linear transformation is a function from a vector space to another vector space that preserves the operations of vector addition and scalar multiplication.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9055570405924258
      }
    ]
  },
  {
    "representative_text": "Matrix representation: A matrix that represents a linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Matrix representation: A matrix that represents a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "A linear transformation can be represented by a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8333047180342803
      },
      {
        "text": "The matrix of a linear transformation can be used to find the image of a vector under the transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.848613954298552
      }
    ]
  },
  {
    "representative_text": "Inverse transformation: The inverse of a linear transformation is a linear transformation that \"reverses\" the original transformation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Inverse transformation: The inverse of a linear transformation is a linear transformation that \"reverses\" the original transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Inverse matrix: The matrix that represents the inverse of a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8610307011618721
      }
    ]
  },
  {
    "representative_text": "Eigenvalue: A scalar that represents how much a linear transformation changes a vector.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Eigenvalue: A scalar that represents how much a linear transformation changes a vector.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "The scalar by which the eigenvector is scaled is the eigenvalue of the linear transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.805468926542011
      },
      {
        "text": "An eigenvalue of a matrix A is a scalar value λ such that there exists a non-zero vector x such that Ax = λx.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8169619070467452
      }
    ]
  },
  {
    "representative_text": "Eigenvector: A non-zero vector that is scaled by a linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Eigenvector: A non-zero vector that is scaled by a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "An eigenvector of a linear transformation is a non-zero vector that, when the transformation is applied to it, results in a scaled version of itself.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8679845045013104
      },
      {
        "text": "An eigenvector of a matrix A is a non-zero vector x such that Ax = λx, where λ is an eigenvalue of A.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8663332049435072
      }
    ]
  },
  {
    "representative_text": "Eigenspace: The set of all eigenvectors of a linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenspace: The set of all eigenvectors of a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal vectors: Vectors that are perpendicular to each other.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Orthogonal vectors: Vectors that are perpendicular to each other.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Two vectors are orthogonal if their dot product is zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8218945123360326
      },
      {
        "text": "Two vectors x and y are orthogonal if their dot product is zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.94503335836199
      }
    ]
  },
  {
    "representative_text": "Orthogonal projection: The projection of a vector onto another vector, resulting in a vector that is orthogonal to the original vector.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal projection: The projection of a vector onto another vector, resulting in a vector that is orthogonal to the original vector.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gram-Schmidt process: A method for finding an orthogonal basis for a vector space.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Gram-Schmidt process: A method for finding an orthogonal basis for a vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Gram-Schmidt process is a method for orthonormalizing a set of vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9228404008764423
      },
      {
        "text": "The Gram-Schmidt process can be used to find an orthonormal basis for a vector space.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9399729893667633
      }
    ]
  },
  {
    "representative_text": "Determinant: A scalar that represents the volume scaling factor of a linear transformation.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Determinant: A scalar that represents the volume scaling factor of a linear transformation.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "The determinant of a matrix is a scalar value that can be used to determine the invertibility of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8067281933040037
      },
      {
        "text": "The determinant of a matrix A is a scalar value that can be used to determine the invertibility of A.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9353809395714018
      }
    ]
  },
  {
    "representative_text": "Cofactor expansion: A method for calculating the determinant of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Cofactor expansion: A method for calculating the determinant of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear equation: An equation in which the highest power of the variable is 1.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear equation: An equation in which the highest power of the variable is 1.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear system: A set of linear equations.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear system: A set of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "A system of linear equations is a set of linear equations in which the variables are the unknowns.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8623491177430843
      }
    ]
  },
  {
    "representative_text": "Augmented matrix: A matrix that represents a linear system.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Augmented matrix: A matrix that represents a linear system.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Gaussian elimination: A method for solving a linear system.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Gaussian elimination: A method for solving a linear system.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Gaussian elimination is a method for transforming a matrix into row echelon form.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8270666198634874
      },
      {
        "text": "Gaussian elimination involves a series of row operations to eliminate entries below the main diagonal.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9117438969678071
      },
      {
        "text": "Row reduction is a method for transforming a matrix into row echelon form.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8699568655230508
      },
      {
        "text": "Row reduction involves a series of row operations to eliminate entries below the main diagonal.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8716408013406016
      },
      {
        "text": "The row echelon form can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8334348646562979
      },
      {
        "text": "The reduced row echelon form can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8628621589672162
      },
      {
        "text": "The column echelon form can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8377706544954102
      },
      {
        "text": "The column reduced echelon form can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8540094387313194
      }
    ]
  },
  {
    "representative_text": "Matrix multiplication: The product of two matrices, resulting in a new matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix multiplication: The product of two matrices, resulting in a new matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix addition: The sum of two matrices, resulting in a new matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix addition: The sum of two matrices, resulting in a new matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Matrix inverse: The inverse of a matrix, resulting in a new matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix inverse: The inverse of a matrix, resulting in a new matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computer graphics: Linear algebra is used to perform transformations and projections in 2D and 3D graphics.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computer graphics: Linear algebra is used to perform transformations and projections in 2D and 3D graphics.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Machine learning: Linear algebra is used in neural networks and other machine learning algorithms.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Machine learning: Linear algebra is used in neural networks and other machine learning algorithms.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Physics and engineering: Linear algebra is used to describe the laws of physics and solve problems in mechanics, electromagnetism, and other fields.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Physics and engineering: Linear algebra is used to describe the laws of physics and solve problems in mechanics, electromagnetism, and other fields.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Data analysis: Linear algebra is used in data analysis and statistics to perform regression, principal component analysis, and other techniques.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Data analysis: Linear algebra is used in data analysis and statistics to perform regression, principal component analysis, and other techniques.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Inner Product Space: A vector space equipped with an inner product, which is a way to measure the similarity between two vectors.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Inner Product Space: A vector space equipped with an inner product, which is a way to measure the similarity between two vectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "An inner product space is a vector space equipped with an inner product, which is a function that takes two vectors as input and produces a scalar value.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8910508092819631
      },
      {
        "text": "The Hilbert space is equipped with an inner product, which is a function that takes two vectors as input and produces a scalar value.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8392812986811586
      }
    ]
  },
  {
    "representative_text": "Orthonormal Basis: A basis for a vector space where all vectors are orthogonal to each other and have a magnitude of 1.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthonormal Basis: A basis for a vector space where all vectors are orthogonal to each other and have a magnitude of 1.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "An orthonormal basis for a vector space is a basis consisting of orthonormal vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8884561125429618
      }
    ]
  },
  {
    "representative_text": "Orthogonal Diagonalization: A method for diagonalizing a matrix using an orthogonal matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Diagonalization: A method for diagonalizing a matrix using an orthogonal matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD): A factorization of a matrix into three matrices that can be used to reduce the dimensionality of a vector space.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD): A factorization of a matrix into three matrices that can be used to reduce the dimensionality of a vector space.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "The SVD is a factorization of a matrix A into three matrices: U, Σ, and V^T.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8014768796419524
      },
      {
        "text": "The eigenvalue decomposition of a matrix is a factorization of the matrix into the product of three matrices: D, U, and V^T.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8047506744895737
      }
    ]
  },
  {
    "representative_text": "Polar Decomposition: A factorization of a matrix into a product of a positive semi-definite matrix and an orthogonal matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Polar Decomposition: A factorization of a matrix into a product of a positive semi-definite matrix and an orthogonal matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Symmetric Positive Definite Matrices: Matrices that are symmetric and have all positive eigenvalues.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Symmetric Positive Definite Matrices: Matrices that are symmetric and have all positive eigenvalues.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Decomposition: A factorization of a matrix into a product of a diagonal matrix containing eigenvalues and an orthogonal matrix containing eigenvectors.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Decomposition: A factorization of a matrix into a product of a diagonal matrix containing eigenvalues and an orthogonal matrix containing eigenvectors.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Bilinear Forms: A way to measure the similarity between two vectors using a quadratic form.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Bilinear Forms: A way to measure the similarity between two vectors using a quadratic form.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Quadratic Forms: A way to measure the similarity between two vectors using a quadratic expression.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9369998552008233
      }
    ]
  },
  {
    "representative_text": "Clifford Algebra: A mathematical structure that extends the geometric algebra and is used to describe geometric transformations and relationships.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Clifford Algebra: A mathematical structure that extends the geometric algebra and is used to describe geometric transformations and relationships.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Geometric Algebra: A mathematical structure that combines vectors, scalars, and multivectors into a single algebraic structure.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9072226695348172
      }
    ]
  },
  {
    "representative_text": "Multilinear Forms: A way to measure the similarity between multiple vectors using a multilinear expression.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Multilinear Forms: A way to measure the similarity between multiple vectors using a multilinear expression.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Analysis: A way to analyze and manipulate tensors, which are multilinear forms in multiple vector spaces.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tensor Analysis: A way to analyze and manipulate tensors, which are multilinear forms in multiple vector spaces.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lie Algebras: Mathematical structures that are used to describe the infinitesimal symmetries of a system.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lie Algebras: Mathematical structures that are used to describe the infinitesimal symmetries of a system.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Representation Theory: The study of linear representations of groups, which is closely related to linear algebra.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Representation Theory: The study of linear representations of groups, which is closely related to linear algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Group Theory: The study of linear transformations that preserve the operations of vector addition and scalar multiplication.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Group Theory: The study of linear transformations that preserve the operations of vector addition and scalar multiplication.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Galois Theory: A branch of mathematics that studies the symmetries of polynomials and other algebraic structures.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Galois Theory: A branch of mathematics that studies the symmetries of polynomials and other algebraic structures.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Linear Algebra: A field of study that focuses on the efficient computation of linear algebra operations, such as matrix multiplication and eigenvalue decomposition.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "Computational Linear Algebra: A field of study that focuses on the efficient computation of linear algebra operations, such as matrix multiplication and eigenvalue decomposition.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Numerical Linear Algebra: A field of study that focuses on the numerical approximation of linear algebra operations, such as matrix inversion and eigenvalue decomposition.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8931049657851038
      },
      {
        "text": "Computational Linear Algebra in High-Performance Computing: A field of study that focuses on the efficient computation of linear algebra operations on large-scale systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.873012244098139
      },
      {
        "text": "Computational Linear Algebra in Machine Learning: A field of study that focuses on the efficient computation of linear algebra operations on large-scale systems, with applications in machine learning.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8958767258518772
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Codes: A way to encode data using linear algebra operations, which is used in error-correcting codes and cryptography.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Codes: A way to encode data using linear algebra operations, which is used in error-correcting codes and cryptography.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebraic Coding Theory: A branch of mathematics that studies the properties of linear codes and their applications in coding theory.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8221917066089078
      },
      {
        "text": "Linear Algebraic Coding Theory: A thorough discussion on linear algebraic coding theory, its properties, and its applications in coding theory.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8410464711206114
      }
    ]
  },
  {
    "representative_text": "Bilinear Forms and Quadratic Forms in Higher Dimensions: While the points cover bilinear forms and quadratic forms in two dimensions, it's essential to explore these concepts in higher dimensions.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Bilinear Forms and Quadratic Forms in Higher Dimensions: While the points cover bilinear forms and quadratic forms in two dimensions, it's essential to explore these concepts in higher dimensions.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) of Matrices with Non-Positive Eigenvalues: SVD is a fundamental concept, but its application to matrices with non-positive eigenvalues is not explicitly mentioned.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) of Matrices with Non-Positive Eigenvalues: SVD is a fundamental concept, but its application to matrices with non-positive eigenvalues is not explicitly mentioned.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Clifford Algebra and its Applications: Clifford algebra is a fundamental structure in geometric algebra, but its applications and implications in linear algebra are not fully explored.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Clifford Algebra and its Applications: Clifford algebra is a fundamental structure in geometric algebra, but its applications and implications in linear algebra are not fully explored.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Geometric Algebra and its Applications: Geometric algebra is a fundamental structure, but its applications in linear algebra and other areas of mathematics are not fully explored.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9008983045960275
      }
    ]
  },
  {
    "representative_text": "Tensor Analysis and Its Relation to Linear Algebra: Tensor analysis is a crucial area of study, but its connection to linear algebra is not thoroughly discussed.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tensor Analysis and Its Relation to Linear Algebra: Tensor analysis is a crucial area of study, but its connection to linear algebra is not thoroughly discussed.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lie Algebras and Representation Theory: While representation theory is mentioned, the connection to Lie algebras is not fully explored, and the implications of this relationship on linear algebra are not discussed.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Lie Algebras and Representation Theory: While representation theory is mentioned, the connection to Lie algebras is not fully explored, and the implications of this relationship on linear algebra are not discussed.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Advanced Linear Algebra Topics, Such as Representation Theory of Matrices and Linear Algebraic Groups: The points cover some basic linear algebra concepts, but more advanced topics, such as representation theory of matrices and linear algebraic groups, are not mentioned.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8022410685318171
      }
    ]
  },
  {
    "representative_text": "Computational Linear Algebra and Numerical Methods: The points cover some computational aspects of linear algebra, but more advanced numerical methods, such as iterative methods for solving linear systems, are not mentioned.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Computational Linear Algebra and Numerical Methods: The points cover some computational aspects of linear algebra, but more advanced numerical methods, such as iterative methods for solving linear systems, are not mentioned.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Codes and Error-Correcting Codes: The points cover linear algebraic codes, but the connection to error-correcting codes and their applications in cryptography is not thoroughly discussed.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Codes and Error-Correcting Codes: The points cover linear algebraic codes, but the connection to error-correcting codes and their applications in cryptography is not thoroughly discussed.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal Polynomials and Their Relation to Linear Algebra: Orthogonal polynomials are used in various applications, but their connection to linear algebra is not thoroughly discussed.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal Polynomials and Their Relation to Linear Algebra: Orthogonal polynomials are used in various applications, but their connection to linear algebra is not thoroughly discussed.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra in Other Areas of Mathematics, Such as Algebraic Geometry and Differential Equations: Linear algebra is a fundamental tool in many areas of mathematics, but its applications in algebraic geometry and differential equations are not fully explored.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra in Other Areas of Mathematics, Such as Algebraic Geometry and Differential Equations: Linear algebra is a fundamental tool in many areas of mathematics, but its applications in algebraic geometry and differential equations are not fully explored.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Its Relation to Other Fields, Such as Physics and Computer Science: Linear algebra has numerous applications in physics, computer science, and other fields, but its connections to these areas are not thoroughly discussed.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Its Relation to Other Fields, Such as Physics and Computer Science: Linear algebra has numerous applications in physics, computer science, and other fields, but its connections to these areas are not thoroughly discussed.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebra and Its Connection to Other Areas of Computer Science, Such as Machine Learning and Data Science: Linear algebra is a fundamental tool in machine learning and data science, but its connections to these areas are not thoroughly discussed.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8891094142866469
      },
      {
        "text": "Linear Algebra and Its Applications in Signal Processing and Image Analysis: Linear algebra has numerous applications in signal processing and image analysis, but its connections to these areas are not fully explored.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8901072984823266
      },
      {
        "text": "Linear Algebra and Its Relation to Other Areas of Mathematics, Such as Topology and Geometry: Linear algebra has connections to topology and geometry, but these connections are not thoroughly discussed.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9138985343918309
      },
      {
        "text": "Linear Algebra and Its Connection to Other Fields, Such as Physics and Computer Science: A more in-depth discussion on linear algebra's applications in physics, computer science, and other fields.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8320871465659258
      },
      {
        "text": "Linear Algebra and Its Connection to Other Fields, Such as Physics and Computer Science: A more in-depth discussion on linear algebra's applications in physics, computer science, and other fields, including the study of linear algebra and its connections to quantum mechanics, quantum computing, and machine learning.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8749340123464171
      }
    ]
  },
  {
    "representative_text": "Schur's Decomposition: A method for decomposing a matrix into a unitary matrix, a diagonal matrix, and an upper triangular matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Schur's Decomposition: A method for decomposing a matrix into a unitary matrix, a diagonal matrix, and an upper triangular matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Lanczos Algorithm: A method for decomposing a matrix into a symmetric tridiagonal matrix and an orthogonal matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Lanczos Algorithm: A method for decomposing a matrix into a symmetric tridiagonal matrix and an orthogonal matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "QR Decomposition: A method for decomposing a matrix into an orthogonal matrix and an upper triangular matrix.",
    "pipelines_covered": [
      "ReflectionPipeline",
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "QR Decomposition: A method for decomposing a matrix into an orthogonal matrix and an upper triangular matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "The QR decomposition of a matrix is a factorization of the matrix into the product of two matrices: Q and R.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8743611822337112
      },
      {
        "text": "The QR decomposition can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8144621983738356
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Perturbation Theory: A method for analyzing the effects of small changes in the eigenvalues and eigenvectors of a matrix.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Perturbation Theory: A method for analyzing the effects of small changes in the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Geometric Methods in Linear Algebra: A set of methods that use geometric intuition to solve problems in linear algebra, such as the use of orthogonal projections to solve systems of linear equations.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Geometric Methods in Linear Algebra: A set of methods that use geometric intuition to solve problems in linear algebra, such as the use of orthogonal projections to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Methods in Signal Processing: A set of methods that use linear algebra techniques to analyze and manipulate signals, such as the use of eigendecomposition to filter signals.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Methods in Signal Processing: A set of methods that use linear algebra techniques to analyze and manipulate signals, such as the use of eigendecomposition to filter signals.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebraic Methods in Machine Learning: A set of methods that use linear algebra techniques to analyze and manipulate data, such as the use of eigendecomposition to dimensionality reduction.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8513069630787066
      },
      {
        "text": "Linear Algebraic Methods in Signal Processing and Image Analysis: A set of methods that use linear algebra techniques to analyze and manipulate signals, such as the use of eigendecomposition to filter signals.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9489918261614243
      }
    ]
  },
  {
    "representative_text": "Representation Theory of Matrices: A branch of mathematics that studies the representation of matrices as linear transformations on vector spaces.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Representation Theory of Matrices: A branch of mathematics that studies the representation of matrices as linear transformations on vector spaces.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Groups: A branch of mathematics that studies the properties of groups that are defined by linear algebraic equations.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Groups: A branch of mathematics that studies the properties of groups that are defined by linear algebraic equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Analysis and Its Applications in Physics: A set of methods that use tensors to analyze and describe physical systems, such as the use of tensors to describe the curvature of spacetime.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tensor Analysis and Its Applications in Physics: A set of methods that use tensors to analyze and describe physical systems, such as the use of tensors to describe the curvature of spacetime.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Clifford Algebra and its Applications in Physics: A set of methods that use Clifford algebra to analyze and describe physical systems, such as the use of Clifford algebra to describe the geometry of spacetime.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "Clifford Algebra and its Applications in Physics: A set of methods that use Clifford algebra to analyze and describe physical systems, such as the use of Clifford algebra to describe the geometry of spacetime.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Clifford Algebra and its Applications: A thorough discussion on Clifford algebra, its properties, and its applications in linear algebra and other areas of mathematics.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8004473310365533
      },
      {
        "text": "Geometric Algebra and its Applications: A thorough discussion on geometric algebra, its properties, and its applications in linear algebra and other areas of mathematics.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8953429685262593
      },
      {
        "text": "Clifford Algebra and its Applications in Physics: A more in-depth discussion on Clifford algebra, its properties, and its applications in physics, including the description of spacetime geometry and the study of gravitational waves.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8167481607665258
      },
      {
        "text": "Geometric Algebra and its Applications: A more in-depth discussion on geometric algebra, its properties, and its applications in linear algebra and other areas of mathematics, including the study of geometric transformations and the description of spacetime geometry.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9059168012230012
      }
    ]
  },
  {
    "representative_text": "Numerical Linear Algebra Methods: A set of methods that use numerical techniques to solve linear algebra problems, such as the use of iterative methods to solve systems of linear equations.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Numerical Linear Algebra Methods: A set of methods that use numerical techniques to solve linear algebra problems, such as the use of iterative methods to solve systems of linear equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Numerical Linear Algebra Methods: A more detailed explanation of numerical linear algebra methods, such as iterative methods for solving linear systems.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.8351747594464421
      }
    ]
  },
  {
    "representative_text": "Inner Product Spaces and Orthonormal Bases: A more in-depth discussion on inner product spaces, orthonormal bases, and their properties, such as completeness and orthonormality.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Inner Product Spaces and Orthonormal Bases: A more in-depth discussion on inner product spaces, orthonormal bases, and their properties, such as completeness and orthonormality.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Singular Value Decomposition (SVD) of Matrices with Non-Positive Eigenvalues: A more detailed explanation of SVD and its application to matrices with non-positive eigenvalues.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Singular Value Decomposition (SVD) of Matrices with Non-Positive Eigenvalues: A more detailed explanation of SVD and its application to matrices with non-positive eigenvalues.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Singular Value Decomposition (SVD) of Matrices with Non-Positive Eigenvalues: A more detailed explanation of SVD and its application to matrices with non-positive eigenvalues, including the study of singular value decomposition and its applications in data analysis and machine learning.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.955187557772946
      }
    ]
  },
  {
    "representative_text": "Tensor Analysis and Its Relation to Linear Algebra: A more detailed explanation of tensor analysis, its properties, and its connection to linear algebra.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Tensor Analysis and Its Relation to Linear Algebra: A more detailed explanation of tensor analysis, its properties, and its connection to linear algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Tensor Analysis and Its Relation to Linear Algebra: A more detailed explanation of tensor analysis, its properties, and its connection to linear algebra, including the study of tensor fields and their applications in physics and engineering.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9570086778753751
      }
    ]
  },
  {
    "representative_text": "Lie Algebras and Representation Theory: A more in-depth discussion on Lie algebras, their properties, and their connection to representation theory and linear algebra.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Lie Algebras and Representation Theory: A more in-depth discussion on Lie algebras, their properties, and their connection to representation theory and linear algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Lie Algebras and Representation Theory: A more in-depth discussion on Lie algebras, their properties, and their connection to representation theory and linear algebra, including the study of Lie groups and their applications in physics and engineering.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9591982386302328
      }
    ]
  },
  {
    "representative_text": "Orthogonal Polynomials and Their Relation to Linear Algebra: A more in-depth discussion on orthogonal polynomials, their properties, and their connection to linear algebra.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Orthogonal Polynomials and Their Relation to Linear Algebra: A more in-depth discussion on orthogonal polynomials, their properties, and their connection to linear algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal Polynomials and Their Relation to Linear Algebra: A more in-depth discussion on orthogonal polynomials, their properties, and their connection to linear algebra, including the study of orthogonal polynomials and their applications in quantum mechanics and signal processing.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9427047726898993
      }
    ]
  },
  {
    "representative_text": "Linear Algebra in Other Areas of Mathematics, Such as Algebraic Geometry and Differential Equations: A more detailed explanation of linear algebra's role in algebraic geometry and differential equations.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra in Other Areas of Mathematics, Such as Algebraic Geometry and Differential Equations: A more detailed explanation of linear algebra's role in algebraic geometry and differential equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Advanced Linear Algebra Topics, Such as Representation Theory of Matrices and Linear Algebraic Groups: A thorough discussion on advanced linear algebra topics, such as representation theory of matrices and linear algebraic groups.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Advanced Linear Algebra Topics, Such as Representation Theory of Matrices and Linear Algebraic Groups: A thorough discussion on advanced linear algebra topics, such as representation theory of matrices and linear algebraic groups.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Computational Linear Algebra in High-Performance Computing: A more in-depth discussion on computational linear algebra in high-performance computing.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Computational Linear Algebra in High-Performance Computing: A more in-depth discussion on computational linear algebra in high-performance computing.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Computational Linear Algebra in High-Performance Computing: A more in-depth discussion on computational linear algebra in high-performance computing, including the study of parallel algorithms and their applications in scientific computing and data analysis.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.895373766657299
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Methods in Signal Processing and Image Analysis: A thorough discussion on linear algebraic methods in signal processing and image analysis.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Methods in Signal Processing and Image Analysis: A thorough discussion on linear algebraic methods in signal processing and image analysis.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear Algebraic Methods in Signal Processing and Image Analysis: A more in-depth discussion on linear algebraic methods in signal processing and image analysis, including the study of eigendecomposition and its applications in image compression and signal filtering.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.816177197924431
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Its Relation to Topology and Geometry: A more in-depth discussion on linear algebra's connections to topology and geometry.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Its Relation to Topology and Geometry: A more in-depth discussion on linear algebra's connections to topology and geometry.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Quadratic Forms and Bilinear Forms in Higher Dimensions: A more in-depth discussion on quadratic forms and bilinear forms in higher dimensions, and their applications in linear algebra and other areas of mathematics.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Quadratic Forms and Bilinear Forms in Higher Dimensions: A more in-depth discussion on quadratic forms and bilinear forms in higher dimensions, and their applications in linear algebra and other areas of mathematics.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      },
      {
        "text": "Bilinear Forms and Quadratic Forms in Higher Dimensions: A more in-depth discussion on bilinear forms and quadratic forms in higher dimensions, and their applications in linear algebra and other areas of mathematics.",
        "pipeline": "ReflectionPipeline",
        "similarity": 0.9874061174224992
      }
    ]
  },
  {
    "representative_text": "Eigenvalue Decomposition of Matrices with Non-Symmetric Entries: A more detailed explanation of eigenvalue decomposition and its application to matrices with non-symmetric entries.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue Decomposition of Matrices with Non-Symmetric Entries: A more detailed explanation of eigenvalue decomposition and its application to matrices with non-symmetric entries.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Its Applications in Machine Learning: A thorough discussion on linear algebra's applications in machine learning, including dimensionality reduction, clustering, and classification.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Its Applications in Machine Learning: A thorough discussion on linear algebra's applications in machine learning, including dimensionality reduction, clustering, and classification.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Networks and Linear Algebra: A discussion on tensor networks, their properties, and their connection to linear algebra.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tensor Networks and Linear Algebra: A discussion on tensor networks, their properties, and their connection to linear algebra.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebra and Its Relation to Quantum Mechanics: A discussion on linear algebra's applications in quantum mechanics, including the representation of quantum systems and the study of quantum entanglement.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebra and Its Relation to Quantum Mechanics: A discussion on linear algebra's applications in quantum mechanics, including the representation of quantum systems and the study of quantum entanglement.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear Algebraic Methods in Differential Equations: A set of methods that use linear algebra techniques to solve differential equations, such as the use of eigendecomposition to solve systems of differential equations.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear Algebraic Methods in Differential Equations: A set of methods that use linear algebra techniques to solve differential equations, such as the use of eigendecomposition to solve systems of differential equations.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Tensor Analysis and Its Applications in Machine Learning: A discussion on tensor analysis, its properties, and its applications in machine learning, including the study of tensor networks and their applications in deep learning.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Tensor Analysis and Its Applications in Machine Learning: A discussion on tensor analysis, its properties, and its applications in machine learning, including the study of tensor networks and their applications in deep learning.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Geometric Algebra and its Applications in Signal Processing: A more in-depth discussion on geometric algebra, its properties, and its applications in signal processing, including the study of geometric transformations and the description of signal spaces.",
    "pipelines_covered": [
      "ReflectionPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Geometric Algebra and its Applications in Signal Processing: A more in-depth discussion on geometric algebra, its properties, and its applications in signal processing, including the study of geometric transformations and the description of signal spaces.",
        "pipeline": "ReflectionPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A vector is a mathematical object that has both magnitude and direction.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A vector is a mathematical object that has both magnitude and direction.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vectors can be represented graphically using arrows.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vectors can be represented graphically using arrows.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector addition is commutative, associative, and distributive.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector addition is commutative, associative, and distributive.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Vector subtraction is commutative and associative.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Vector subtraction is commutative and associative.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Scalar multiplication is distributive.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Scalar multiplication is distributive.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The dot product of two vectors is a scalar value that represents the amount of \"similarity\" between the vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The dot product of two vectors is a scalar value that represents the amount of \"similarity\" between the vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The cross product of two vectors is a vector that is perpendicular to both original vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The cross product of two vectors is a vector that is perpendicular to both original vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A vector space must satisfy certain properties, including:",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A vector space must satisfy certain properties, including:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The null space of a matrix is the set of vectors that, when multiplied by the matrix, result in the zero vector.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The null space of a matrix is the set of vectors that, when multiplied by the matrix, result in the zero vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The kernel of a linear transformation is the set of vectors that are mapped to the zero vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8482766263600324
      }
    ]
  },
  {
    "representative_text": "The column space of a matrix is the set of all linear combinations of the columns of the matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The column space of a matrix is the set of all linear combinations of the columns of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The eigenvectors of a linear transformation can be used to diagonalize the transformation.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The eigenvectors of a linear transformation can be used to diagonalize the transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The diagonalization of a linear transformation involves finding a basis of eigenvectors and using them to construct a diagonal matrix that represents the transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8761497952734737
      },
      {
        "text": "A matrix A can be diagonalized if it has a full set of linearly independent eigenvectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8093891443484665
      },
      {
        "text": "The diagonalization of a matrix A is given by A = PDP^(-1), where P is a matrix whose columns are the eigenvectors of A and D is a diagonal matrix whose entries are the eigenvalues of A.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.829025222718406
      },
      {
        "text": "The eigenvectors of a matrix can be used to diagonalize the matrix and to find the eigenvalues of a linear transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8939031709890938
      }
    ]
  },
  {
    "representative_text": "A diagonal matrix is a square matrix with non-zero entries only on the main diagonal.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A diagonal matrix is a square matrix with non-zero entries only on the main diagonal.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The eigenvalues of a diagonal matrix are the entries on the main diagonal.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The eigenvalues of a diagonal matrix are the entries on the main diagonal.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Two vectors are orthonormal if they are both orthogonal and have a magnitude of 1.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Two vectors are orthonormal if they are both orthogonal and have a magnitude of 1.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "Two vectors x and y are orthonormal if they are both orthogonal and have a magnitude of 1.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9855681198713754
      }
    ]
  },
  {
    "representative_text": "The determinant of a matrix can be calculated using the formula:",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The determinant of a matrix can be calculated using the formula:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The determinant of a triangular matrix is the product of the entries on the main diagonal.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The determinant of a triangular matrix is the product of the entries on the main diagonal.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A matrix is invertible if it has a non-zero determinant.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "A matrix is invertible if it has a non-zero determinant.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The determinant of a matrix can be used to determine the invertibility of the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8981450294416059
      },
      {
        "text": "The determinant can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8315119402107988
      },
      {
        "text": "A matrix is invertible if and only if it has a non-zero determinant.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9103745318916119
      },
      {
        "text": "The determinant can be used to determine the invertibility of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9637048876042615
      }
    ]
  },
  {
    "representative_text": "The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The rank of a matrix is also equal to the number of pivot columns in row echelon form.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The rank of a matrix is also equal to the number of pivot columns in row echelon form.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The row echelon form of a matrix is a matrix in which all the entries below the main diagonal are zero.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The row echelon form of a matrix is a matrix in which all the entries below the main diagonal are zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The column echelon form of a matrix is a matrix in which all the entries above the main diagonal are zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9116698796282082
      },
      {
        "text": "The reduced row echelon form of a matrix is a matrix in which all the entries above the main diagonal are zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9307387169308842
      },
      {
        "text": "The reduced row echelon form of a matrix is a matrix in which all the entries below the main diagonal are zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.959747270056842
      },
      {
        "text": "The column reduced echelon form of a matrix is a matrix in which all the entries above the main diagonal are zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.937495533188628
      }
    ]
  },
  {
    "representative_text": "The column echelon form of a matrix is a matrix in which all the pivot columns are to the right of the main diagonal.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The column echelon form of a matrix is a matrix in which all the pivot columns are to the right of the main diagonal.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The reduced row echelon form of a matrix is a matrix in which all the pivot columns are to the right of the main diagonal.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The reduced row echelon form of a matrix is a matrix in which all the pivot columns are to the right of the main diagonal.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A system of linear equations can be represented by a matrix equation of the form:",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A system of linear equations can be represented by a matrix equation of the form:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The solution to a system of linear equations can be found using Gaussian elimination or other methods.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The solution to a system of linear equations can be found using Gaussian elimination or other methods.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The solution to a system of linear equations can also be found using matrix inversion.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8221034037341399
      },
      {
        "text": "The system can be solved using Gaussian elimination or other methods.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8180654263892104
      },
      {
        "text": "The inverse of a matrix can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8245871412581122
      }
    ]
  },
  {
    "representative_text": "Linear algebra has many applications in physics, engineering, computer science, and other fields.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Linear algebra has many applications in physics, engineering, computer science, and other fields.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Linear algebra is used to solve systems of linear equations and to find the eigenvalues and eigenvectors of matrices.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Linear algebra is used to solve systems of linear equations and to find the eigenvalues and eigenvectors of matrices.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "Linear algebra is used to transform vectors and matrices in a variety of ways, including rotation, scaling, and projection.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8315681796902072
      },
      {
        "text": "Linear algebra is used to find the inverse of a matrix and to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9045978797012394
      }
    ]
  },
  {
    "representative_text": "The inner product satisfies certain properties, including:",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The inner product satisfies certain properties, including:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The inner product space is said to have an inner product if it satisfies the properties of an inner product.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8568299466395194
      }
    ]
  },
  {
    "representative_text": "The inner product can be used to define the norm of a vector, which is the square root of the inner product of the vector with itself.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The inner product can be used to define the norm of a vector, which is the square root of the inner product of the vector with itself.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "An orthogonal projection is a linear transformation that projects a vector onto a subspace.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "An orthogonal projection is a linear transformation that projects a vector onto a subspace.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The orthogonal projection of a vector x onto a subspace V is given by:",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8129865402714395
      },
      {
        "text": "The orthogonal projection can be used to decompose a vector into its orthogonal components.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8540303369495946
      }
    ]
  },
  {
    "representative_text": "The SVD is given by:",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The SVD is given by:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The SVD can be used to find the eigenvalues and eigenvectors of a matrix, and to solve systems of linear equations.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The SVD can be used to find the eigenvalues and eigenvectors of a matrix, and to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The SVD can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8837820602391407
      }
    ]
  },
  {
    "representative_text": "The least squares problem is a minimization problem in which we seek to find the vector x that minimizes the norm of the difference between Ax and b.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The least squares problem is a minimization problem in which we seek to find the vector x that minimizes the norm of the difference between Ax and b.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The least squares problem can be solved using the normal equation, which is given by:",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The least squares problem can be solved using the normal equation, which is given by:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The least squares problem can be used to find the best fit line or plane to a set of data points.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The least squares problem can be used to find the best fit line or plane to a set of data points.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Fourier analysis is a mathematical tool used to decompose a function into its frequency components.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 9,
    "detailed_sources": [
      {
        "text": "Fourier analysis is a mathematical tool used to decompose a function into its frequency components.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Fourier transform is a linear transformation that maps a function to its frequency domain representation.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8669455282949083
      },
      {
        "text": "The Fourier transform can be used to solve problems in signal processing, image analysis, and other fields.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8349592721881193
      },
      {
        "text": "The Fourier series is a mathematical tool used to decompose a function into its frequency components.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8992151733600171
      },
      {
        "text": "The Fourier series can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8568115912061445
      },
      {
        "text": "The Fourier series can be used to process images by representing the image data in terms of a set of Fourier series.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8003961043750439
      },
      {
        "text": "The Fourier series can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9116185065121076
      },
      {
        "text": "The Fourier transform is a mathematical tool used to decompose a function into its frequency components.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8980480386716857
      },
      {
        "text": "The Fourier transform can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9320519675781525
      }
    ]
  },
  {
    "representative_text": "Two vectors x and y are linearly independent if and only if the equation x + λy = 0 has no solution for any value of λ.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Two vectors x and y are linearly independent if and only if the equation x + λy = 0 has no solution for any value of λ.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A bilinear form is a function that takes two vectors as input and produces a scalar value.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A bilinear form is a function that takes two vectors as input and produces a scalar value.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The bilinear form satisfies certain properties, including:",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The bilinear form satisfies certain properties, including:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The bilinear form can be used to define the inner product of two vectors.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The bilinear form can be used to define the inner product of two vectors.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A Hilbert space is a complete inner product space.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "A Hilbert space is a complete inner product space.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Hilbert space is said to be complete if every Cauchy sequence in the space converges to a vector in the space.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8284773595162687
      }
    ]
  },
  {
    "representative_text": "The orthogonal complement of a subspace V is the set of all vectors that are orthogonal to every vector in V.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The orthogonal complement of a subspace V is the set of all vectors that are orthogonal to every vector in V.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The orthogonal complement can be used to find the null space of a linear transformation.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The orthogonal complement can be used to find the null space of a linear transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The minimal polynomial of a linear transformation T is the monic polynomial of smallest degree that annihilates T.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The minimal polynomial of a linear transformation T is the monic polynomial of smallest degree that annihilates T.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The minimal polynomial of a matrix A is the monic polynomial of smallest degree that annihilates A.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.895672823482261
      }
    ]
  },
  {
    "representative_text": "The minimal polynomial can be used to determine the eigenvalues of T.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The minimal polynomial can be used to determine the eigenvalues of T.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The minimal polynomial can be used to determine the eigenvalues of A.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9274323899796038
      }
    ]
  },
  {
    "representative_text": "The Jordan canonical form of a matrix is a block diagonal matrix, where each block is a Jordan block.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 5,
    "detailed_sources": [
      {
        "text": "The Jordan canonical form of a matrix is a block diagonal matrix, where each block is a Jordan block.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Jordan canonical form can be used to diagonalize a matrix and to find the eigenvalues and eigenvectors of a linear transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.805691017149226
      },
      {
        "text": "The Jordan normal form of a matrix is a block diagonal matrix, where each block is a Jordan block.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9183101671519658
      },
      {
        "text": "The Jordan normal form can be used to diagonalize a matrix and to find the eigenvalues and eigenvectors of a linear transformation.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9059291173561936
      },
      {
        "text": "The Jordan normal form can be used to diagonalize a matrix and to find the eigenvalues of a",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8806685405137551
      }
    ]
  },
  {
    "representative_text": "Matrix exponentiation is a mathematical operation that involves raising a matrix to a power.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Matrix exponentiation is a mathematical operation that involves raising a matrix to a power.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The matrix exponential can be used to solve differential equations and to find the eigenvalues and eigenvectors of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The matrix exponential can be used to solve differential equations and to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The SVD can be used to find the inverse of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The SVD can be used to find the inverse of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The eigenvalue decomposition can be used to find the eigenvalues and eigenvectors of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The eigenvalue decomposition can be used to find the eigenvalues and eigenvectors of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The rank can be used to determine the invertibility of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The rank can be used to determine the invertibility of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Householder transformation is a method for orthogonalizing a vector.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Householder transformation is a method for orthogonalizing a vector.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Householder transformation can be used to find an orthonormal basis for a vector space.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8946747358683936
      }
    ]
  },
  {
    "representative_text": "The Jacobi iteration is a method for solving a system of linear equations.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Jacobi iteration is a method for solving a system of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Jacobi iteration can be used to find the solution to a system of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9405085785509065
      }
    ]
  },
  {
    "representative_text": "The LU decomposition of a matrix is a factorization of the matrix into the product of two matrices: L and U.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The LU decomposition of a matrix is a factorization of the matrix into the product of two matrices: L and U.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The LU decomposition can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8314233126648657
      }
    ]
  },
  {
    "representative_text": "The Cholesky decomposition of a matrix is a factorization of the matrix into the product of two matrices: L and U.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Cholesky decomposition of a matrix is a factorization of the matrix into the product of two matrices: L and U.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Cholesky decomposition can be used to solve systems of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8009035045330077
      }
    ]
  },
  {
    "representative_text": "Eigenvalue pivoting is a method for selecting the largest eigenvalue of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue pivoting is a method for selecting the largest eigenvalue of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Eigenvalue pivoting can be used to find the eigenvalues of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Eigenvalue pivoting can be used to find the eigenvalues of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal polynomials are a set of polynomials that are orthogonal to each other.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal polynomials are a set of polynomials that are orthogonal to each other.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal polynomials can be used to solve problems in signal processing and image analysis.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "Orthogonal polynomials can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "Orthogonal polynomials can be used to process images by representing the image data in terms of a set of orthogonal polynomials.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8718658824653482
      },
      {
        "text": "Orthogonal polynomials can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9715491986974957
      }
    ]
  },
  {
    "representative_text": "The DFT is a mathematical tool used to decompose a discrete-time signal into its frequency components.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The DFT is a mathematical tool used to decompose a discrete-time signal into its frequency components.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The DFT can be used to solve problems in signal processing and image analysis.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "The DFT can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The FFT can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8968453252952671
      },
      {
        "text": "The DFT can be used to process images by representing the image data in terms of a set of DFTs.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8390117196159067
      },
      {
        "text": "The DFT can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9616175806378654
      },
      {
        "text": "The FFT can be used to process images by representing the image data in terms of a set of FFTs.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8796916183073995
      },
      {
        "text": "The FFT can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.952246041563506
      }
    ]
  },
  {
    "representative_text": "The FFT is an algorithm for computing the DFT of a discrete-time signal.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The FFT is an algorithm for computing the DFT of a discrete-time signal.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Z-transform is a mathematical tool used to analyze discrete-time systems.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The Z-transform is a mathematical tool used to analyze discrete-time systems.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Z-transform can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8544336282903842
      },
      {
        "text": "The Z-transform can be used to process images by representing the image data in terms of a set of Z-transforms.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8469495248234841
      },
      {
        "text": "The Z-transform can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9587262662997184
      }
    ]
  },
  {
    "representative_text": "The transfer function is a mathematical tool used to analyze continuous-time systems.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The transfer function is a mathematical tool used to analyze continuous-time systems.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The transfer function can be used to solve problems in signal processing and image analysis.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The transfer function can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The transfer function can be used to process images by representing the image data in terms of a set of transfer functions.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8422639296695027
      },
      {
        "text": "The transfer function can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9645635125338601
      }
    ]
  },
  {
    "representative_text": "The state-space model is a mathematical tool used to analyze continuous-time systems.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The state-space model is a mathematical tool used to analyze continuous-time systems.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The state-space model can be used to solve problems in signal processing and image analysis.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 6,
    "detailed_sources": [
      {
        "text": "The state-space model can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Kalman filter can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8086389737598597
      },
      {
        "text": "The state-space model can be used to process images by representing the image data in terms of a set of state-space models.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8183638628207625
      },
      {
        "text": "The state-space model can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9488001550308663
      },
      {
        "text": "The Kalman filter can be used to process images by representing the image data in terms of a set of Kalman filters.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8432315257543977
      },
      {
        "text": "The Kalman filter can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9122899010979485
      }
    ]
  },
  {
    "representative_text": "The LQR is a mathematical tool used to analyze and design control systems.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The LQR is a mathematical tool used to analyze and design control systems.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The LQR can be used to solve problems in signal processing and image analysis.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The LQR can be used to solve problems in signal processing and image analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The LQR can be used to process images by representing the image data in terms of a set of LQRs.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8465177926524982
      },
      {
        "text": "The LQR can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9613935490098818
      }
    ]
  },
  {
    "representative_text": "The Kalman filter is a mathematical tool used to analyze and estimate the state of a system.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Kalman filter is a mathematical tool used to analyze and estimate the state of a system.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The SVD can be used to compress images by reducing the dimensionality of the image data.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The SVD can be used to compress images by reducing the dimensionality of the image data.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The SVD can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8772603967197908
      },
      {
        "text": "The eigenvalue decomposition can be used to analyze and process images.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8120674215046555
      },
      {
        "text": "The eigenvalue decomposition can be used to solve problems in image processing and analysis.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.905365539302718
      }
    ]
  },
  {
    "representative_text": "The Cayley-Hamilton theorem states that every square matrix satisfies its own characteristic equation.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Cayley-Hamilton theorem states that every square matrix satisfies its own characteristic equation.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The characteristic equation of a matrix A is given by det(A - λI) = 0, where I is the identity matrix and λ is the eigenvalue.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The characteristic equation of a matrix A is given by det(A - λI) = 0, where I is the identity matrix and λ is the eigenvalue.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The eigenvalue multiplicities of a matrix A are the number of times each eigenvalue appears as a root of the characteristic equation of A.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The eigenvalue multiplicities of a matrix A are the number of times each eigenvalue appears as a root of the characteristic equation of A.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The eigenvalue multiplicities can be used to determine the rank of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The eigenvalue multiplicities can be used to determine the rank of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A matrix A is said to be normal if AA^ = A^A, where A^* is the conjugate transpose of A.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "A matrix A is said to be normal if AA^ = A^A, where A^* is the conjugate transpose of A.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "A matrix A is said to be normal if AA^T = A^T A.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.934334456391006
      }
    ]
  },
  {
    "representative_text": "Normal matrices have real eigenvalues and are diagonalizable.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Normal matrices have real eigenvalues and are diagonalizable.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A matrix A is said to be orthogonal if AA^T = I, where I is the identity matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A matrix A is said to be orthogonal if AA^T = I, where I is the identity matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Orthogonal matrices have real eigenvalues and are diagonalizable.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Orthogonal matrices have real eigenvalues and are diagonalizable.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A matrix A is said to be symmetric if A = A^T.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A matrix A is said to be symmetric if A = A^T.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Symmetric matrices have real eigenvalues and are diagonalizable.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "Symmetric matrices have real eigenvalues and are diagonalizable.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "Hermitian matrices have real eigenvalues and are diagonalizable.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8978846958374497
      }
    ]
  },
  {
    "representative_text": "A matrix A is said to be skew-symmetric if A = -A^T.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A matrix A is said to be skew-symmetric if A = -A^T.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Skew-symmetric matrices have purely imaginary eigenvalues and are diagonalizable.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Skew-symmetric matrices have purely imaginary eigenvalues and are diagonalizable.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The trigonometric functions sine and cosine can be used to represent the entries of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The trigonometric functions sine and cosine can be used to represent the entries of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The trigonometric functions can be used to diagonalize a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The trigonometric functions can be used to diagonalize a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Chebyshev polynomials are a set of polynomials that can be used to represent the entries of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Chebyshev polynomials are a set of polynomials that can be used to represent the entries of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The Chebyshev polynomials can be used to diagonalize a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The Chebyshev polynomials can be used to diagonalize a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The commutator of two matrices A and B is defined as [A, B] = AB - BA.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 4,
    "detailed_sources": [
      {
        "text": "The commutator of two matrices A and B is defined as [A, B] = AB - BA.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Lie bracket of two matrices A and B is defined as [A, B] = AB - BA.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9089725978030251
      },
      {
        "text": "The commutator of a matrix A is the set of all matrices B such that AB - BA is zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8088054315356848
      },
      {
        "text": "The Lie bracket of a matrix A is the set of all matrices B such that [A, B] = AB - BA is zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8751794064497229
      }
    ]
  },
  {
    "representative_text": "The commutator can be used to determine the commutativity of two matrices.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The commutator can be used to determine the commutativity of two matrices.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The commutator can be used to determine the commutativity of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9775444628225765
      }
    ]
  },
  {
    "representative_text": "The anticommutator of two matrices A and B is defined as {A, B} = AB + BA.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The anticommutator of two matrices A and B is defined as {A, B} = AB + BA.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The anticommutator can be used to determine the anticommutativity of two matrices.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The anticommutator can be used to determine the anticommutativity of two matrices.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The anticommutator can be used to determine the anticommutativity of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9778878457383411
      }
    ]
  },
  {
    "representative_text": "The Lie bracket can be used to determine the Lie bracket of two matrices.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 2,
    "detailed_sources": [
      {
        "text": "The Lie bracket can be used to determine the Lie bracket of two matrices.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The Lie bracket can be used to determine the Lie bracket of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.9707495893035463
      }
    ]
  },
  {
    "representative_text": "A matrix A is said to be Hermitian if A = A^H, where A^H is the conjugate transpose of A.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A matrix A is said to be Hermitian if A = A^H, where A^H is the conjugate transpose of A.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "A matrix A is said to be unitary if AA^H = I, where I is the identity matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "A matrix A is said to be unitary if AA^H = I, where I is the identity matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Unitary matrices have real eigenvalues and are diagonalizable.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Unitary matrices have real eigenvalues and are diagonalizable.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The inverse of a matrix A is a matrix A^(-1) such that AA^(-1) = I, where I is the identity matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The inverse of a matrix A is a matrix A^(-1) such that AA^(-1) = I, where I is the identity matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The eigenvalues of a matrix can be used to determine the stability of the system.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The eigenvalues of a matrix can be used to determine the stability of the system.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The span of a set of vectors can be used to determine the dimension of a vector space.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 3,
    "detailed_sources": [
      {
        "text": "The span of a set of vectors can be used to determine the dimension of a vector space.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      },
      {
        "text": "The basis of a vector space can be used to determine the dimension of the space.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8908979789687617
      },
      {
        "text": "The dimension of a vector space can be used to determine the number of independent variables in a system of linear equations.",
        "pipeline": "SequentialPipeline",
        "similarity": 0.8265047833286481
      }
    ]
  },
  {
    "representative_text": "The normalizer of a matrix A is the set of all matrices B such that B^(-1)AB is normal.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The normalizer of a matrix A is the set of all matrices B such that B^(-1)AB is normal.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The normalizer can be used to determine the normality of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The normalizer can be used to determine the normality of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The centralizer of a matrix A is the set of all matrices B such that BA = AB.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The centralizer of a matrix A is the set of all matrices B such that BA = AB.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The centralizer can be used to determine the centralization of a matrix.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The centralizer can be used to determine the centralization of a matrix.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "The anticommutator of a matrix A is the set of all matrices B such that AB + BA is zero.",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "The anticommutator of a matrix A is the set of all matrices B such that AB + BA is zero.",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "assistant:",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "assistant:",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  },
  {
    "representative_text": "Here are",
    "pipelines_covered": [
      "SequentialPipeline"
    ],
    "occurrence_count": 1,
    "detailed_sources": [
      {
        "text": "Here are",
        "pipeline": "SequentialPipeline",
        "similarity": 1.0
      }
    ]
  }
]